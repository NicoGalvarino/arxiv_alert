<link href='https://fonts.googleapis.com/css?family=Montserrat' rel='stylesheet'><style>     body {font-family: 'Montserrat'; background: #F3F3F3; width: 740px; margin: 0 auto; line-height: 150%; margin-top: 50px; font-size: 15px}         h1 {font-size: 70px}         a {color: #45ABC2}         em {font-size: 120%} </style><h1><center>ArXiv Alert</center></h1><font color='#DDAD5C'><em>Update: from 04 Jun 2025 to 06 Jun 2025</em></font><a href="http://arxiv.org/pdf/2506.05161v1" target="_blank"><h2>The Hourglass Simulation: A Catalog for the Roman High-Latitude
  Time-Domain Core Community Survey</h2></a><strong><u>Authors:</u></strong>  B. M. Rose, M. Vincenzi, R. Hounsell, H. Qu, L. Aldoroty, D. Scolnic, R. Kessler, P. Macias, D. Brout, M. Acevedo, R. C. Chen, S. Gomez, E. Peterson, D. Rubin, M. Sako</br><strong><u>Categories:</u></strong> astro-ph.IM, astro-ph.CO, astro-ph.HE</br><strong><u>Comments:</u></strong> submitted to ApJ</br><p><strong><u>Abstract:</u></strong> We present a simulation of the time-domain catalog for the Nancy Grace Roman
Space Telescope's High-Latitude Time-Domain Core Community Survey. This
simulation, called the Hourglass simulation, uses the most up-to-date spectral
energy distribution models and rate measurements for ten extra-galactic
time-domain sources. We simulate these models through the design reference
Roman Space Telescope survey: four filters per tier, a five day cadence, over
two years, a wide tier of 19 deg$^2$ and a deep tier of 4.2 deg$^2$, with
$\sim$20% of those areas also covered with prism observations. We find that a
science-independent Roman time-domain catalog, assuming a S/N at max of >5,
would have approximately 21,000 Type Ia supernovae, 40,000 core-collapse
supernovae, around 70 superluminous supernovae, $\sim$35 tidal disruption
events, 3 kilonovae, and possibly pair-instability supernovae. In total,
Hourglass has over 64,000 transient objects, 11 million photometric
observations, and 500,000 spectra. Additionally, Hourglass is a useful data set
to train machine learning classification algorithms. We show that SCONE is able
to photometrically classify Type Ia supernovae with high precision ($\sim$95%)
to a z > 2. Finally, we present the first realistic simulations of non-Type Ia
supernovae spectral-time series data from Roman's prism.</p></br><a href="http://arxiv.org/pdf/2506.03964v1" target="_blank"><h2>Causality-Aware Contrastive Learning for Robust Multivariate Time-Series
  Anomaly Detection</h2></a><strong><u>Authors:</u></strong>  HyunGi Kim, Jisoo Mok, Dongjun Lee, Jaihyun Lew, Sungjae Kim, Sungroh Yoon</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> Accepted to ICML 2025</br><p><strong><u>Abstract:</u></strong> Utilizing the complex inter-variable causal relationships within multivariate
time-series provides a promising avenue toward more robust and reliable
multivariate time-series anomaly detection (MTSAD) but remains an underexplored
area of research. This paper proposes Causality-Aware contrastive learning for
RObust multivariate Time-Series (CAROTS), a novel MTSAD pipeline that
incorporates the notion of causality into contrastive learning. CAROTS employs
two data augmentors to obtain causality-preserving and -disturbing samples that
serve as a wide range of normal variations and synthetic anomalies,
respectively. With causality-preserving and -disturbing samples as positives
and negatives, CAROTS performs contrastive learning to train an encoder whose
latent space separates normal and abnormal samples based on causality.
Moreover, CAROTS introduces a similarity-filtered one-class contrastive loss
that encourages the contrastive learning process to gradually incorporate more
semantically diverse samples with common causal relationships. Extensive
experiments on five real-world and two synthetic datasets validate that the
integration of causal relationships endows CAROTS with improved MTSAD
capabilities. The code is available at https://github.com/kimanki/CAROTS.</p></br><a href="http://arxiv.org/pdf/2506.04339v1" target="_blank"><h2>Savage-Dickey density ratio estimation with normalizing flows for
  Bayesian model comparison</h2></a><strong><u>Authors:</u></strong>  Kiyam Lin, Alicja Polanska, Davide Piras, Alessio Spurio Mancini, Jason D. McEwen</br><strong><u>Categories:</u></strong> astro-ph.CO, astro-ph.IM, stat.ML</br><strong><u>Comments:</u></strong> 9 pages, 1 figure. Submitted to the Open Journal of Astrophysics. Codes available atthis https URL</br><p><strong><u>Abstract:</u></strong> A core motivation of science is to evaluate which scientific model best
explains observed data. Bayesian model comparison provides a principled
statistical approach to comparing scientific models and has found widespread
application within cosmology and astrophysics. Calculating the Bayesian
evidence is computationally challenging, especially as we continue to explore
increasingly more complex models. The Savage-Dickey density ratio (SDDR)
provides a method to calculate the Bayes factor (evidence ratio) between two
nested models using only posterior samples from the super model. The SDDR
requires the calculation of a normalised marginal distribution over the extra
parameters of the super model, which has typically been performed using
classical density estimators, such as histograms. Classical density estimators,
however, can struggle to scale to high-dimensional settings. We introduce a
neural SDDR approach using normalizing flows that can scale to settings where
the super model contains a large number of extra parameters. We demonstrate the
effectiveness of this neural SDDR methodology applied to both toy and realistic
cosmological examples. For a field-level inference setting, we show that Bayes
factors computed for a Bayesian hierarchical model (BHM) and simulation-based
inference (SBI) approach are consistent, providing further validation that SBI
extracts as much cosmological information from the field as the BHM approach.
The SDDR estimator with normalizing flows is implemented in the open-source
harmonic Python package.</p></br><a href="http://arxiv.org/pdf/2506.04757v1" target="_blank"><h2>Modelling the selection of galaxy groups with end to end simulations</h2></a><strong><u>Authors:</u></strong>  R. Seppi, D. Eckert, A. Finoguenov, S . Shreeram, E. Tempel, G. Gozaliasl, M. Lorenz, J. Wilms, G. A. Mamon, F. Gastaldello, L. Lovisari, E. O'Sullivan, K. Kolokythas, M. A. Bourne, M. Sun, A. Pillepich</br><strong><u>Categories:</u></strong> astro-ph.CO, astro-ph.GA, astro-ph.HE</br><strong><u>Comments:</u></strong> Accepted for publication on A&A</br><p><strong><u>Abstract:</u></strong> Feedback from supernovae and AGN shapes galaxy formation and evolution, yet
its impact remains unclear. Galaxy groups offer a crucial probe, as their
binding energy is comparable to that available from their central AGN. The
XMM-Newton Group AGN Project (X-GAP) is a sample of 49 groups selected in X-ray
(ROSAT) and optical (SDSS) bands, providing a benchmark for hydrodynamical
simulations. In sight of such a comparison, understanding selection effects is
essential. We aim to model the selection function of X-GAP by forward modelling
the detection process in the X-ray and optical bands. Using the Uchuu
simulation, we build a halo light cone, predict X-ray group properties with a
neural network trained on hydro simulations, and assign galaxies matching
observed properties. We compare the selected sample to the parent population.
Our method provides a sample that matches the observed distribution of X-ray
luminosity and velocity dispersion. The 50% completeness is reached at a
velocity dispersion of 450 km/s in the X-GAP redshift range. The selection is
driven by X-ray flux, with secondary dependence on velocity dispersion and
redshift. We estimate a 93% purity level in the X-GAP parent sample. We
calibrate the velocity dispersion-halo mass relation. We find a normalisation
and slope in agreement with the literature, and an intrinsic scatter of about
0.06 dex. The measured velocity dispersion is accurate within 10% only for rich
systems with more than about 20 members, while the velocity dispersion for
groups with less than 10 members is biased at more than 20%. The X-ray
follow-up refines the optical selection, enhancing purity but reducing
completeness. In an SDSS-like setup, velocity dispersion measurement errors
dominate over intrinsic scatter. Our selection model will enable the
comparisons of thermodynamic properties and gas fractions between X-GAP groups
and hydro simulations.</p></br><a href="http://arxiv.org/pdf/2506.04329v1" target="_blank"><h2>Estimating Bolometric Luminosities of Type 1 Quasars with
  Self-Organizing Maps</h2></a><strong><u>Authors:</u></strong>  Jie Chen, Linhua Jiang, Shengxiu Sun, Zijian Zhang, Mouyuan Sun</br><strong><u>Categories:</u></strong> astro-ph.CO, astro-ph.GA, astro-ph.HE</br><strong><u>Comments:</u></strong> 18 pages, 13 figures. Resubmitted to ApJ based on reviewer report. Code QSOLbol is available at this https URLthis https URL</br><p><strong><u>Abstract:</u></strong> We present a new method to calculate bolometric luminosities for unobscured,
type 1 quasars with multi-band photometric data. Bolometric luminosity is a
fundamental property to understand quasars and it is commonly estimated from
monochromatic luminosities using bolometric corrections that often neglect
quasar SED diversity. We take advantage of the fact that most quasars now have
multi-band observations from UV to mid-IR, and construct SEDs for a
well-defined sample of SDSS quasars at $0.5\leq z\leq 2$. Based on this
fiducial sample, we explore quasar SEDs, their diversity, and their relations
with bolometric luminosities. We then use unsupervised neural network
self-organizing maps (SOM) to describe the SED diversity and compute the
bolometric luminosities with a fully-trained SOM model. This method reduces
systematical uncertainties compared to the traditional method. In addition, we
update the multi-linear regression relations between bolometric luminosity and
monochromatic luminosities at restframe 1450\r{A}, 3000\r{A}, and 5100\r{A}.
Our method is applicable to large quasar samples with a wide range of
luminosity and redshift. We have applied it to the SDSS DR16 quasars. We have
also made our code publicly available.</p></br><a href="http://arxiv.org/pdf/2506.04340v1" target="_blank"><h2>A large, chemically enriched, neutral gas reservoir in a galaxy at z =
  6.782</h2></a><strong><u>Authors:</u></strong>  A. Saccardi, S. D. Vergani, L. Izzo, V. D'Elia, K. E. Heintz, A. De Cia, D. B. Malesani, J. T. Palmerio, P. Petitjean, S. Savaglio, N. R. Tanvir, R. Salvaterra, R. Brivio, S. Campana, L. Christensen, S. Covino, J. P. U. Fynbo, D. H. Hartmann, C. Konstantopoulou, A. J. Levan, A. Martin-Carrillo, A. Melandri, L. Piro, G. Pugliese, P. Schady, B. Schneider</br><strong><u>Categories:</u></strong> astro-ph.GA, astro-ph.CO, astro-ph.HE</br><strong><u>Comments:</u></strong> Submitted to A&A - 12 pages, 6 figures, 3 tables - Appendix: 4 figures, 2 tables</br><p><strong><u>Abstract:</u></strong> The chemical characterization of galaxies in the first billion years after
the Big Bang is one of the central goals of current astrophysics.
Optical/near-infrared spectroscopy of long gamma-ray bursts (GRBs) have been
heralded as an effective diagnostic to probe the interstellar medium of their
host galaxies and their metal and dust content, up to the highest redshift. An
opportunity to fulfill this expectation was provided by the recent blast
triggered by the Neil Gehrels Swift Observatory of GRB 240218A at redshift
z=6.782. We study a high-redshift galaxy selected in a complementary way with
respect to flux-limited surveys, not depending on galaxy luminosity and stellar
mass. We present the VLT/X-shooter spectrum of its afterglow enabling the
detection of neutral-hydrogen, low-ionization, high-ionization and
fine-structure absorption lines. We determine the metallicity, kinematics and
chemical abundance pattern, providing the first detailed characterization of
the neutral gas of a galaxy at z>6.5. From the analysis of fine-structure lines
we estimate the distance of the closest gas clouds as
$d_{II}=620^{+230}_{-140}$ pc. We determine a high neutral hydrogen column
density, $\log(N(HI)/cm^{-2})=22.5\pm0.3$, which is the highest one at z>6
determined so far for a GRB host galaxy, as well as a surprisingly high metal
column density, $\log(N(ZnII)/cm^{-2})>14.3$. The observed metallicity of the
host galaxy system is [Zn/H]>-0.8. We find evidence of a high amount of dust
depletion and of aluminum overabundance, although a number of transitions are
saturated. The high hydrogen column density, metal abundances and dust
depletion in the neutral gas align with those of the ionized gas of very
high-redshift galaxies unveiled by ALMA and JWST, testifying that a rapid build
up of metals and dust, and massive neutral hydrogen reservoirs seem to be
common features of galaxies in the early Universe.</p></br><a href="http://arxiv.org/pdf/2506.04100v1" target="_blank"><h2>Deep Neural Networks Hunting Ultra-Light Dark Matter</h2></a><strong><u>Authors:</u></strong>  Pavel Kůs, Diana López Nacir, Federico R. Urban</br><strong><u>Categories:</u></strong> astro-ph.HE, astro-ph.CO</br><strong><u>Comments:</u></strong> 34 pages, 28 figures</br><p><strong><u>Abstract:</u></strong> Ultra-light dark matter (ULDM) is a compelling candidate for cosmological
dark matter. If ULDM interacts with ordinary matter, it can induce measurable,
characteristic signals in pulsar-timing data because it causes the orbits of
pulsars in binary systems to osculate. In this work, we investigate the
potential of machine learning (ML) techniques to detect such ULDM signals. To
this end, we construct three types of neural networks: an autoencoder, a binary
classifier, and a multiclass classifier. We apply these methods to four
theoretically well-motivated ULDM models: a linearly coupled scalar field, a
quadratically coupled scalar field, a vector field and a tensor field. We show
that the sensitivity achieved using ML methods is comparable to that of a
semi-analytical Bayesian approach, which to date has only been applied to the
linear scalar case. The ML approach is readily applicable to all four ULDM
models and, in the case of the multiclass classifier, can distinguish between
them. Our results, derived from simulated data, lay the foundation for future
applications to real pulsar-timing observations.</p></br><a href="http://arxiv.org/pdf/2506.04434v1" target="_blank"><h2>Grokking and Generalization Collapse: Insights from \texttt{HTSR} theory</h2></a><strong><u>Authors:</u></strong>  Hari K. Prakash, Charles H. Martin</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML</br><strong><u>Comments:</u></strong> 15 pages,7 figs</br><p><strong><u>Abstract:</u></strong> We study the well-known grokking phenomena in neural networks (NNs) using a
3-layer MLP trained on 1 k-sample subset of MNIST, with and without weight
decay, and discover a novel third phase -- \emph{anti-grokking} -- that occurs
very late in training and resembles but is distinct from the familiar
\emph{pre-grokking} phases: test accuracy collapses while training accuracy
stays perfect. This late-stage collapse is distinct, from the known
pre-grokking and grokking phases, and is not detected by other proposed
grokking progress measures. Leveraging Heavy-Tailed Self-Regularization HTSR
through the open-source WeightWatcher tool, we show that the HTSR layer quality
metric $\alpha$ alone delineates all three phases, whereas the best competing
metrics detect only the first two. The \emph{anti-grokking} is revealed by
training for $10^7$ and is invariably heralded by $\alpha < 2$ and the
appearance of \emph{Correlation Traps} -- outlier singular values in the
randomized layer weight matrices that make the layer weight matrix atypical and
signal overfitting of the training set. Such traps are verified by visual
inspection of the layer-wise empirical spectral densities, and by using
Kolmogorov--Smirnov tests on randomized spectra. Comparative metrics, including
activation sparsity, absolute weight entropy, circuit complexity, and $l^2$
weight norms track pre-grokking and grokking but fail to distinguish grokking
from anti-grokking. This discovery provides a way to measure overfitting and
generalization collapse without direct access to the test data. These results
strengthen the claim that the \emph{HTSR} $\alpha$ provides universal
layer-convergence target at $\alpha \approx 2$ and underscore the value of
using the HTSR alpha $(\alpha)$ metric as a measure of generalization.</p></br><a href="http://arxiv.org/pdf/2506.04859v1" target="_blank"><h2>Sparse Autoencoders, Again?</h2></a><strong><u>Authors:</u></strong>  Yin Lu, Tong He, Xuening Zhu, David Wipf</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> Accepted to the International Conference on Machine Learning (ICML) 2025</br><p><strong><u>Abstract:</u></strong> Is there really much more to say about sparse autoencoders (SAEs)?
Autoencoders in general, and SAEs in particular, represent deep architectures
that are capable of modeling low-dimensional latent structure in data. Such
structure could reflect, among other things, correlation patterns in large
language model activations, or complex natural image manifolds. And yet despite
the wide-ranging applicability, there have been relatively few changes to SAEs
beyond the original recipe from decades ago, namely, standard deep
encoder/decoder layers trained with a classical/deterministic sparse
regularizer applied within the latent space. One possible exception is the
variational autoencoder (VAE), which adopts a stochastic encoder module capable
of producing sparse representations when applied to manifold data. In this work
we formalize underappreciated weaknesses with both canonical SAEs, as well as
analogous VAEs applied to similar tasks, and propose a hybrid alternative model
that circumvents these prior limitations. In terms of theoretical support, we
prove that global minima of our proposed model recover certain forms of
structured data spread across a union of manifolds. Meanwhile, empirical
evaluations on synthetic and real-world datasets substantiate the efficacy of
our approach in accurately estimating underlying manifold dimensions and
producing sparser latent representations without compromising reconstruction
error. In general, we are able to exceed the performance of equivalent-capacity
SAEs and VAEs, as well as recent diffusion models where applicable, within
domains such as images and language model activation patterns.</p></br><a href="http://arxiv.org/pdf/2506.04190v1" target="_blank"><h2>How to Use Graph Data in the Wild to Help Graph Anomaly Detection?</h2></a><strong><u>Authors:</u></strong>  Yuxuan Cao, Jiarong Xu, Chen Zhao, Jiaan Wang, Carl Yang, Chunping Wang, Yang Yang</br><strong><u>Categories:</u></strong> cs.LG</br><strong><u>Comments:</u></strong> Accepted by SIGKDD2025</br><p><strong><u>Abstract:</u></strong> In recent years, graph anomaly detection has found extensive applications in
various domains such as social, financial, and communication networks. However,
anomalies in graph-structured data present unique challenges, including label
scarcity, ill-defined anomalies, and varying anomaly types, making supervised
or semi-supervised methods unreliable. Researchers often adopt unsupervised
approaches to address these challenges, assuming that anomalies deviate
significantly from the normal data distribution. Yet, when the available data
is insufficient, capturing the normal distribution accurately and
comprehensively becomes difficult. To overcome this limitation, we propose to
utilize external graph data (i.e., graph data in the wild) to help anomaly
detection tasks. This naturally raises the question: How can we use external
data to help graph anomaly detection tasks? To answer this question, we propose
a framework called Wild-GAD. It is built upon a unified database, UniWildGraph,
which comprises a large and diverse collection of graph data with broad domain
coverage, ample data volume, and a unified feature space. Further, we develop
selection criteria based on representativity and diversity to identify the most
suitable external data for anomaly detection task. Extensive experiments on six
real-world datasets demonstrate the effectiveness of Wild-GAD. Compared to the
baseline methods, our framework has an average 18% AUCROC and 32% AUCPR
improvement over the best-competing methods.</p></br><a href="http://arxiv.org/pdf/2506.04014v1" target="_blank"><h2>Time domain astrophysics with transient sources. Delay estimate via
  Cross Correlation Function techniques</h2></a><strong><u>Authors:</u></strong>  W. Leone, L. Burderi, T. di Salvo, A. Anitra, A. Sanna, A. Riggio, R. Iaria, F. Fiore, F. Longo, M. Ďurišková, A. Tsvetkova, C. Maraventano, C. Miceli</br><strong><u>Categories:</u></strong> astro-ph.IM, astro-ph.HE</br><strong><u>Comments:</u></strong> Accepted under minor reviews on 29 May 2025</br><p><strong><u>Abstract:</u></strong> The timing analysis of transient events allows for investigating numerous
still open areas of modern astrophysics. The article explores all the
mathematical and physical tools required to estimate delays and associated
errors between two Times of Arrival (ToA) lists, by exploiting Cross
Correlation Function (CCF) techniques. The CCF permits the establishment of the
delay between two observed signals and is defined on two continuous functions.
A detector does not directly measure the intensity of the electromagnetic
signal (interacting with its material) but rather detects each photon ToA
through a probabilistic process. Since the CCF is defined on continuous
functions, the crucial step is to obtain a continuous rate curve from a list of
ToA. This step is treated in the article and the constructed rate functions are
light curves that are continuous functions. This allows, in principle, the
estimation of delays with any desired resolution. Due to the statistical nature
of the measurement process, two independent detections of the same signal yield
different photon times. Consequently, light curves derived from these lists
differ due to Poisson fluctuations, leading the CCF between them to fluctuate
around the true theoretical delay. This article describes a Monte Carlo
technique that enables reliable delay estimation by providing a robust measure
of the uncertainties induced by Poissonian fluctuations. GRB data are
considered as they offer optimal test cases for the proposed techniques. The
developed techniques provides a significant computational advantage and are
useful analyzing of data characterized by low-count statistics (i.e., low
photon count rates in c/s), as they allow overcoming the limitations associated
with traditional fixed bin-size methods.</p></br><a href="http://arxiv.org/pdf/2506.03672v1" target="_blank"><h2>Latent Guided Sampling for Combinatorial Optimization</h2></a><strong><u>Authors:</u></strong>  Sobihan Surendran, Adeline Fermanian, Sylvain Le Corff</br><strong><u>Categories:</u></strong> stat.ML, cs.LG, math.OC</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Combinatorial Optimization problems are widespread in domains such as
logistics, manufacturing, and drug discovery, yet their NP-hard nature makes
them computationally challenging. Recent Neural Combinatorial Optimization
methods leverage deep learning to learn solution strategies, trained via
Supervised or Reinforcement Learning (RL). While promising, these approaches
often rely on task-specific augmentations, perform poorly on
out-of-distribution instances, and lack robust inference mechanisms. Moreover,
existing latent space models either require labeled data or rely on pre-trained
policies. In this work, we propose LGS-Net, a novel latent space model that
conditions on problem instances, and introduce an efficient inference method,
Latent Guided Sampling (LGS), based on Markov Chain Monte Carlo and Stochastic
Approximation. We show that the iterations of our method form a
time-inhomogeneous Markov Chain and provide rigorous theoretical convergence
guarantees. Empirical results on benchmark routing tasks show that our method
achieves state-of-the-art performance among RL-based approaches.</p></br><a href="http://arxiv.org/pdf/2506.03595v1" target="_blank"><h2>Purifying Shampoo: Investigating Shampoo's Heuristics by Decomposing its
  Preconditioner</h2></a><strong><u>Authors:</u></strong>  Runa Eschenhagen, Aaron Defazio, Tsung-Hsien Lee, Richard E. Turner, Hao-Jun Michael Shi</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> The recent success of Shampoo in the AlgoPerf contest has sparked renewed
interest in Kronecker-factorization-based optimization algorithms for training
neural networks. Despite its success, Shampoo relies heavily on several
heuristics such as learning rate grafting and stale preconditioning to achieve
performance at-scale. These heuristics increase algorithmic complexity,
necessitate further hyperparameter tuning, and lack theoretical justification.
This paper investigates these heuristics from the angle of Frobenius norm
approximation to full-matrix Adam and decouples the preconditioner's
eigenvalues and eigenbasis updates. We show that grafting from Adam mitigates
the staleness and mis-scaling of the preconditioner's eigenvalues and how
correcting the eigenvalues directly can eliminate the need for learning rate
grafting. To manage the error induced by infrequent eigenbasis computations, we
propose an adaptive criterion for determining the eigenbasis computation
frequency motivated by terminating a warm-started QR algorithm. This criterion
decouples the update frequency of different preconditioner matrices and enables
us to investigate the impact of approximation error on convergence. These
practical techniques offer a principled angle towards removing Shampoo's
heuristics and developing improved Kronecker-factorization-based training
algorithms.</p></br><a href="http://arxiv.org/pdf/2506.03784v1" target="_blank"><h2>When Does Closeness in Distribution Imply Representational Similarity?
  An Identifiability Perspective</h2></a><strong><u>Authors:</u></strong>  Beatrix M. G. Nielsen, Emanuele Marconato, Andrea Dittadi, Luigi Gresele</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> When and why representations learned by different deep neural networks are
similar is an active research topic. We choose to address these questions from
the perspective of identifiability theory, which suggests that a measure of
representational similarity should be invariant to transformations that leave
the model distribution unchanged. Focusing on a model family which includes
several popular pre-training approaches, e.g., autoregressive language models,
we explore when models which generate distributions that are close have similar
representations. We prove that a small Kullback-Leibler divergence between the
model distributions does not guarantee that the corresponding representations
are similar. This has the important corollary that models arbitrarily close to
maximizing the likelihood can still learn dissimilar representations, a
phenomenon mirrored in our empirical observations on models trained on
CIFAR-10. We then define a distributional distance for which closeness implies
representational similarity, and in synthetic experiments, we find that wider
networks learn distributions which are closer with respect to our distance and
have more similar representations. Our results establish a link between
closeness in distribution and representational similarity.</p></br><a href="http://arxiv.org/pdf/2506.04700v1" target="_blank"><h2>Explicit Density Approximation for Neural Implicit Samplers Using a
  Bernstein-Based Convex Divergence</h2></a><strong><u>Authors:</u></strong>  José Manuel de Frutos, Manuel A. Vázquez, Pablo M. Olmos, Joaquín Míguez</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, math.PR, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Rank-based statistical metrics, such as the invariant statistical loss (ISL),
have recently emerged as robust and practically effective tools for training
implicit generative models. In this work, we introduce dual-ISL, a novel
likelihood-free objective for training implicit generative models that
interchanges the roles of the target and model distributions in the ISL
framework, yielding a convex optimization problem in the space of model
densities. We prove that the resulting rank-based discrepancy $d_K$ is i)
continuous under weak convergence and with respect to the $L^1$ norm, and ii)
convex in its first argument-properties not shared by classical divergences
such as KL or Wasserstein distances. Building on this, we develop a theoretical
framework that interprets $d_K$ as an $L^2$-projection of the density ratio $q
= p/\tilde p$ onto a Bernstein polynomial basis, from which we derive exact
bounds on the truncation error, precise convergence rates, and a closed-form
expression for the truncated density approximation. We further extend our
analysis to the multivariate setting via random one-dimensional projections,
defining a sliced dual-ISL divergence that retains both convexity and
continuity. We empirically show that these theoretical advantages translate
into practical ones. Specifically, across several benchmarks dual-ISL converges
more rapidly, delivers markedly smoother and more stable training, and more
effectively prevents mode collapse than classical ISL and other leading
implicit generative methods-while also providing an explicit density
approximation.</p></br><a href="http://arxiv.org/pdf/2506.05295v1" target="_blank"><h2>Sample Complexity and Representation Ability of Test-time Scaling
  Paradigms</h2></a><strong><u>Authors:</u></strong>  Baihe Huang, Shanda Li, Tianhao Wu, Yiming Yang, Ameet Talwalkar, Kannan Ramchandran, Michael I. Jordan, Jiantao Jiao</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Test-time scaling paradigms have significantly advanced the capabilities of
large language models (LLMs) on complex tasks. Despite their empirical success,
theoretical understanding of the sample efficiency of various test-time
strategies -- such as self-consistency, best-of-$n$, and self-correction --
remains limited. In this work, we first establish a separation result between
two repeated sampling strategies: self-consistency requires
$\Theta(1/\Delta^2)$ samples to produce the correct answer, while best-of-$n$
only needs $\Theta(1/\Delta)$, where $\Delta < 1$ denotes the probability gap
between the correct and second most likely answers. Next, we present an
expressiveness result for the self-correction approach with verifier feedback:
it enables Transformers to simulate online learning over a pool of experts at
test time. Therefore, a single Transformer architecture can provably solve
multiple tasks without prior knowledge of the specific task associated with a
user query, extending the representation theory of Transformers from
single-task to multi-task settings. Finally, we empirically validate our
theoretical results, demonstrating the practical effectiveness of
self-correction methods.</p></br><a href="http://arxiv.org/pdf/2506.04695v1" target="_blank"><h2>On the Mechanism of Reasoning Pattern Selection in Reinforcement
  Learning for Language Models</h2></a><strong><u>Authors:</u></strong>  Xingwu Chen, Tianle Li, Difan Zou</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML</br><strong><u>Comments:</u></strong> 30 pages, 6 figures, 1 table</br><p><strong><u>Abstract:</u></strong> Reinforcement learning (RL) has demonstrated remarkable success in enhancing
model capabilities, including instruction-following, preference learning, and
reasoning. Yet despite its empirical successes, the mechanisms by which RL
improves reasoning abilities remain poorly understood. We present a systematic
study of Reinforcement Learning with Verifiable Rewards (RLVR), showing that
its primary benefit comes from optimizing the selection of existing reasoning
patterns. Through extensive experiments, we demonstrate that RLVR-trained
models preferentially adopt high-success-rate reasoning patterns while mostly
maintaining stable performance on individual patterns. We further develop
theoretical analyses on the convergence and training dynamics of RLVR based on
a simplified question-reason-answer model. We study the gradient flow and show
that RLVR can indeed find the solution that selects the reason pattern with the
highest success rate. Besides, our theoretical results
  reveal two distinct regimes regarding the convergence of RLVR training: (1)
rapid convergence for models with relatively strong initial reasoning
capabilities versus (2) slower optimization dynamics for weaker models.
Furthermore, we show that the slower optimization for weaker models can be
mitigated by applying the supervised fine-tuning (SFT) before RLVR, when using
a feasibly high-quality SFT dataset. We validate the theoretical findings
through extensive experiments. This work advances our theoretical understanding
of RL's role in LLM fine-tuning and offers insights for further enhancing
reasoning capabilities.</p></br><a href="http://arxiv.org/pdf/2506.05014v1" target="_blank"><h2>Towards Reasonable Concept Bottleneck Models</h2></a><strong><u>Authors:</u></strong>  Nektarios Kalampalikis, Kavya Gupta, Georgi Vitanov, Isabel Valera</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML</br><strong><u>Comments:</u></strong> 26 pages, 17 figures</br><p><strong><u>Abstract:</u></strong> In this paper, we propose $\textbf{C}$oncept $\textbf{REA}$soning
$\textbf{M}$odels (CREAM), a novel family of Concept Bottleneck Models (CBMs)
that: (i) explicitly encodes concept-concept (${\texttt{C-C}}$) and
concept-task (${\texttt{C$\rightarrow$Y}}$) relationships to enforce a desired
model reasoning; and (ii) use a regularized side-channel to achieve competitive
task performance, while keeping high concept importance. Specifically, CREAM
architecturally embeds (bi)directed concept-concept, and concept to task
relationships specified by a human expert, while severing undesired information
flows (e.g., to handle mutually exclusive concepts). Moreover, CREAM integrates
a black-box side-channel that is regularized to encourage task predictions to
be grounded in the relevant concepts, thereby utilizing the side-channel only
when necessary to enhance performance. Our experiments show that: (i) CREAM
mainly relies on concepts while achieving task performance on par with
black-box models; and (ii) the embedded ${\texttt{C-C}}$ and
${\texttt{C$\rightarrow$Y}}$ relationships ease model interventions and
mitigate concept leakage.</p></br><a href="http://arxiv.org/pdf/2506.04487v1" target="_blank"><h2>Orthogonal Gradient Descent Improves Neural Calibration</h2></a><strong><u>Authors:</u></strong>  C. Evans Hedges</br><strong><u>Categories:</u></strong> cs.LG, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> We provide evidence that orthogonalizing gradients during training improves
model calibration without sacrificing accuracy. On CIFAR-10 with 10% labeled
data, $\perp$Grad matches SGD in accuracy but yields consistently improved
calibration metrics such as lower test loss, reduced softmax overconfidence,
and higher predictive entropy. These benefits persist under input corruption
(CIFAR-10C) and extended training, where $\perp$Grad models degrade more
gracefully than SGD-trained counterparts. $\perp$Grad is optimizer-agnostic,
incurs minimal overhead, and works well with post-hoc calibration techniques
like temperature scaling.
  Theoretically, we prove convergence of a simplified version of $\perp$Grad
under mild assumptions and characterize its stationary points in positive
homogeneous networks: $\perp$Grad converges to solutions where further loss
reduction requires confidence scaling rather than decision boundary
improvement.</p></br><a href="http://arxiv.org/pdf/2506.04571v1" target="_blank"><h2>OpenAg: Democratizing Agricultural Intelligence</h2></a><strong><u>Authors:</u></strong>  Srikanth Thudumu, Jason Fisher</br><strong><u>Categories:</u></strong> cs.AI</br><strong><u>Comments:</u></strong> 10 pages, 1 figure</br><p><strong><u>Abstract:</u></strong> Agriculture is undergoing a major transformation driven by artificial
intelligence (AI), machine learning, and knowledge representation technologies.
However, current agricultural intelligence systems often lack contextual
understanding, explainability, and adaptability, especially for smallholder
farmers with limited resources. General-purpose large language models (LLMs),
while powerful, typically lack the domain-specific knowledge and contextual
reasoning needed for practical decision support in farming. They tend to
produce recommendations that are too generic or unrealistic for real-world
applications. To address these challenges, we present OpenAg, a comprehensive
framework designed to advance agricultural artificial general intelligence
(AGI). OpenAg combines domain-specific foundation models, neural knowledge
graphs, multi-agent reasoning, causal explainability, and adaptive transfer
learning to deliver context-aware, explainable, and actionable insights. The
system includes: (i) a unified agricultural knowledge base that integrates
scientific literature, sensor data, and farmer-generated knowledge; (ii) a
neural agricultural knowledge graph for structured reasoning and inference;
(iii) an adaptive multi-agent reasoning system where AI agents specialize and
collaborate across agricultural domains; and (iv) a causal transparency
mechanism that ensures AI recommendations are interpretable, scientifically
grounded, and aligned with real-world constraints. OpenAg aims to bridge the
gap between scientific knowledge and the tacit expertise of experienced farmers
to support scalable and locally relevant agricultural decision-making.</p></br></body>