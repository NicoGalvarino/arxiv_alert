<link href='https://fonts.googleapis.com/css?family=Montserrat' rel='stylesheet'>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$','$']],
            processEscapes: true
        },
        "HTML-CSS": {
            availableFonts: ["TeX"]
        }
    });
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <style>     body {font-family: 'Montserrat'; background: #F3F3F3; width: 740px; margin: 0 auto; line-height: 150%; margin-top: 50px; font-size: 15px}         h1 {font-size: 70px}         a {color: #45ABC2}         em {font-size: 120%} </style><h1><center>ArXiv Alert</center></h1><font color='#DDAD5C'><em>Update: from 25 Jul 2025 to 29 Jul 2025</em></font><a href="http://arxiv.org/pdf/2507.20373v1" target="_blank"><h2>WBHT: A Generative Attention Architecture for Detecting Black Hole
  Anomalies in Backbone Networks</h2></a><strong><u>Authors:</u></strong>  Kiymet Kaya, Elif Ak, Sule Gunduz Oguducu</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), anomaly detection (abstract), latent space (abstract), transformer (abstract), attention (title, abstract)</br><p><strong><u>Abstract:</u></strong> We propose the Wasserstein Black Hole Transformer (WBHT) framework for
detecting black hole (BH) anomalies in communication networks. These anomalies
cause packet loss without failure notifications, disrupting connectivity and
leading to financial losses. WBHT combines generative modeling, sequential
learning, and attention mechanisms to improve BH anomaly detection. It
integrates a Wasserstein generative adversarial network with attention
mechanisms for stable training and accurate anomaly identification. The model
uses long-short-term memory layers to capture long-term dependencies and
convolutional layers for local temporal patterns. A latent space encoding
mechanism helps distinguish abnormal network behavior. Tested on real-world
network data, WBHT outperforms existing models, achieving significant
improvements in F1 score (ranging from 1.65% to 58.76%). Its efficiency and
ability to detect previously undetected anomalies make it a valuable tool for
proactive network monitoring and security, especially in mission-critical
networks.</p></br><a href="http://arxiv.org/pdf/2507.20840v1" target="_blank"><h2>Towards Explainable Deep Clustering for Time Series Data</h2></a><strong><u>Authors:</u></strong>  Udo Schlegel, Gabriel Marques Tavares, Thomas Seidl</br><strong><u>Categories:</u></strong> cs.LG</br><strong><u>Comments:</u></strong> 14 pages, accepted at TempXAI Workshop at ECML-PKDD 2025</br><strong><u>Matching Keywords:</u></strong> explainable (title, abstract), attention (abstract)</br><p><strong><u>Abstract:</u></strong> Deep clustering uncovers hidden patterns and groups in complex time series
data, yet its opaque decision-making limits use in safety-critical settings.
This survey offers a structured overview of explainable deep clustering for
time series, collecting current methods and their real-world applications. We
thoroughly discuss and compare peer-reviewed and preprint papers through
application domains across healthcare, finance, IoT, and climate science. Our
analysis reveals that most work relies on autoencoder and attention
architectures, with limited support for streaming, irregularly sampled, or
privacy-preserved series, and interpretability is still primarily treated as an
add-on. To push the field forward, we outline six research opportunities: (1)
combining complex networks with built-in interpretability; (2) setting up
clear, faithfulness-focused evaluation metrics for unsupervised explanations;
(3) building explainers that adapt to live data streams; (4) crafting
explanations tailored to specific domains; (5) adding human-in-the-loop methods
that refine clusters and explanations together; and (6) improving our
understanding of how time series clustering models work internally. By making
interpretability a primary design goal rather than an afterthought, we propose
the groundwork for the next generation of trustworthy deep clustering time
series analytics.</p></br><a href="http://arxiv.org/pdf/2507.19682v1" target="_blank"><h2>DeepJIVE: Learning Joint and Individual Variation Explained from
  Multimodal Data Using Deep Learning</h2></a><strong><u>Authors:</u></strong>  Matthew Drexler, Benjamin Risk, James J Lah, Suprateek Kundu, Deqiang Qiu</br><strong><u>Categories:</u></strong> cs.CV, cs.AI</br><strong><u>Comments:</u></strong> 26 pages, 10 figures</br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)</br><p><strong><u>Abstract:</u></strong> Conventional multimodal data integration methods provide a comprehensive
assessment of the shared or unique structure within each individual data type
but suffer from several limitations such as the inability to handle
high-dimensional data and identify nonlinear structures. In this paper, we
introduce DeepJIVE, a deep-learning approach to performing Joint and Individual
Variance Explained (JIVE). We perform mathematical derivation and experimental
validations using both synthetic and real-world 1D, 2D, and 3D datasets.
Different strategies of achieving the identity and orthogonality constraints
for DeepJIVE were explored, resulting in three viable loss functions. We found
that DeepJIVE can successfully uncover joint and individual variations of
multimodal datasets. Our application of DeepJIVE to the Alzheimer's Disease
Neuroimaging Initiative (ADNI) also identified biologically plausible
covariation patterns between the amyloid positron emission tomography (PET) and
magnetic resonance (MR) images. In conclusion, the proposed DeepJIVE can be a
useful tool for multimodal data analysis.</p></br><a href="http://arxiv.org/pdf/2507.20349v1" target="_blank"><h2>From Observations to Causations: A GNN-based Probabilistic Prediction
  Framework for Causal Discovery</h2></a><strong><u>Authors:</u></strong>  Rezaur Rashid, Gabriel Terejanu</br><strong><u>Categories:</u></strong> cs.LG, stat.ME</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> neural network (abstract), causation (title)</br><p><strong><u>Abstract:</u></strong> Causal discovery from observational data is challenging, especially with
large datasets and complex relationships. Traditional methods often struggle
with scalability and capturing global structural information. To overcome these
limitations, we introduce a novel graph neural network (GNN)-based
probabilistic framework that learns a probability distribution over the entire
space of causal graphs, unlike methods that output a single deterministic
graph. Our framework leverages a GNN that encodes both node and edge attributes
into a unified graph representation, enabling the model to learn complex causal
structures directly from data. The GNN model is trained on a diverse set of
synthetic datasets augmented with statistical and information-theoretic
measures, such as mutual information and conditional entropy, capturing both
local and global data properties. We frame causal discovery as a supervised
learning problem, directly predicting the entire graph structure. Our approach
demonstrates superior performance, outperforming both traditional and recent
non-GNN-based methods, as well as a GNN-based approach, in terms of accuracy
and scalability on synthetic and real-world datasets without further training.
This probabilistic framework significantly improves causal structure learning,
with broad implications for decision-making and scientific discovery across
various fields.</p></br><a href="http://arxiv.org/pdf/2507.19368v1" target="_blank"><h2>Counterfactual Explanations in Medical Imaging: Exploring SPN-Guided
  Latent Space Manipulation</h2></a><strong><u>Authors:</u></strong>  Julia Siekiera, Stefan Kramer</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> 10 pages, 3 figures</br><strong><u>Matching Keywords:</u></strong> variational autoencoder (abstract), VAE (abstract), latent space (title, abstract)</br><p><strong><u>Abstract:</u></strong> Artificial intelligence is increasingly leveraged across various domains to
automate decision-making processes that significantly impact human lives. In
medical image analysis, deep learning models have demonstrated remarkable
performance. However, their inherent complexity makes them black box systems,
raising concerns about reliability and interpretability. Counterfactual
explanations provide comprehensible insights into decision processes by
presenting hypothetical "what-if" scenarios that alter model classifications.
By examining input alterations, counterfactual explanations provide patterns
that influence the decision-making process. Despite their potential, generating
plausible counterfactuals that adhere to similarity constraints providing
human-interpretable explanations remains a challenge. In this paper, we
investigate this challenge by a model-specific optimization approach. While
deep generative models such as variational autoencoders (VAEs) exhibit
significant generative power, probabilistic models like sum-product networks
(SPNs) efficiently represent complex joint probability distributions. By
modeling the likelihood of a semi-supervised VAE's latent space with an SPN, we
leverage its dual role as both a latent space descriptor and a classifier for a
given discrimination task. This formulation enables the optimization of latent
space counterfactuals that are both close to the original data distribution and
aligned with the target class distribution. We conduct experimental evaluation
on the cheXpert dataset. To evaluate the effectiveness of the integration of
SPNs, our SPN-guided latent space manipulation is compared against a neural
network baseline. Additionally, the trade-off between latent variable
regularization and counterfactual quality is analyzed.</p></br><a href="http://arxiv.org/pdf/2507.19968v1" target="_blank"><h2>Dimer-Enhanced Optimization: A First-Order Approach to Escaping Saddle
  Points in Neural Network Training</h2></a><strong><u>Authors:</u></strong>  Yue Hu, Zanxia Cao, Yingchao Liu</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML</br><strong><u>Comments:</u></strong> 8 pages, 2 figures</br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract), transformer (abstract)</br><p><strong><u>Abstract:</u></strong> First-order optimization methods, such as SGD and Adam, are widely used for
training large-scale deep neural networks due to their computational efficiency
and robust performance. However, relying solely on gradient information, these
methods often struggle to navigate complex loss landscapes with flat regions,
plateaus, and saddle points. Second-order methods, which use curvature
information from the Hessian matrix, can address these challenges but are
computationally infeasible for large models. The Dimer method, a first-order
technique that constructs two closely spaced points to probe the local geometry
of a potential energy surface, efficiently estimates curvature using only
gradient information. Inspired by its use in molecular dynamics simulations for
locating saddle points, we propose Dimer-Enhanced Optimization (DEO), a novel
framework to escape saddle points in neural network training. DEO adapts the
Dimer method to explore a broader region of the loss landscape, approximating
the Hessian's smallest eigenvector without computing the full matrix. By
periodically projecting the gradient onto the subspace orthogonal to the
minimum curvature direction, DEO guides the optimizer away from saddle points
and flat regions, enhancing training efficiency with non-stepwise updates.
Preliminary experiments on a Transformer toy model show DEO achieves
competitive performance compared to standard first-order methods, improving
navigation of complex loss landscapes. Our work repurposes physics-inspired,
first-order curvature estimation to enhance neural network training in
high-dimensional spaces.</p></br><a href="http://arxiv.org/pdf/2507.20714v1" target="_blank"><h2>Prostate Cancer Classification Using Multimodal Feature Fusion and
  Explainable AI</h2></a><strong><u>Authors:</u></strong>  Asma Sadia Khan, Fariba Tasnia Khan, Tanjim Mahmud, Salman Karim Khan, Rishita Chakma, Nahed Sharmen, Mohammad Shahadat Hossain, Karl Andersson</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, q-bio.QM, stat.AP</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> explainable (title, abstract), multimodal (title, abstract)</br><p><strong><u>Abstract:</u></strong> Prostate cancer, the second most prevalent male malignancy, requires advanced
diagnostic tools. We propose an explainable AI system combining BERT (for
textual clinical notes) and Random Forest (for numerical lab data) through a
novel multimodal fusion strategy, achieving superior classification performance
on PLCO-NIH dataset (98% accuracy, 99% AUC). While multimodal fusion is
established, our work demonstrates that a simple yet interpretable BERT+RF
pipeline delivers clinically significant improvements - particularly for
intermediate cancer stages (Class 2/3 recall: 0.900 combined vs 0.824
numerical/0.725 textual). SHAP analysis provides transparent feature importance
rankings, while ablation studies prove textual features' complementary value.
This accessible approach offers hospitals a balance of high performance
(F1=89%), computational efficiency, and clinical interpretability - addressing
critical needs in prostate cancer diagnostics.</p></br><a href="http://arxiv.org/pdf/2507.20089v1" target="_blank"><h2>Meta Fusion: A Unified Framework For Multimodality Fusion with Mutual
  Learning</h2></a><strong><u>Authors:</u></strong>  Ziyi Liang, Annie Qu, Babak Shahbaba</br><strong><u>Categories:</u></strong> cs.LG, stat.ME, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract), multimodality (title)</br><p><strong><u>Abstract:</u></strong> Developing effective multimodal data fusion strategies has become
increasingly essential for improving the predictive power of statistical
machine learning methods across a wide range of applications, from autonomous
driving to medical diagnosis. Traditional fusion methods, including early,
intermediate, and late fusion, integrate data at different stages, each
offering distinct advantages and limitations. In this paper, we introduce Meta
Fusion, a flexible and principled framework that unifies these existing
strategies as special cases. Motivated by deep mutual learning and ensemble
learning, Meta Fusion constructs a cohort of models based on various
combinations of latent representations across modalities, and further boosts
predictive performance through soft information sharing within the cohort. Our
approach is model-agnostic in learning the latent representations, allowing it
to flexibly adapt to the unique characteristics of each modality.
Theoretically, our soft information sharing mechanism reduces the
generalization error. Empirically, Meta Fusion consistently outperforms
conventional fusion strategies in extensive simulation studies. We further
validate our approach on real-world applications, including Alzheimer's disease
detection and neural decoding.</p></br><a href="http://arxiv.org/pdf/2507.20108v1" target="_blank"><h2>Graded Transformers: A Symbolic-Geometric Approach to Structured
  Learning</h2></a><strong><u>Authors:</u></strong>  Tony Shaska Sr</br><strong><u>Categories:</u></strong> cs.LG, cs.IT, math.IT, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), neural network (abstract), transformer (title, abstract), attention (abstract)</br><p><strong><u>Abstract:</u></strong> We introduce the Graded Transformer framework, a novel class of sequence
models that embeds algebraic inductive biases through grading transformations
on vector spaces. Extending the theory of Graded Neural Networks (GNNs), we
propose two architectures: the Linearly Graded Transformer (LGT) and the
Exponentially Graded Transformer (EGT). These models apply parameterized
scaling operators-governed by fixed or learnable grading tuples and, for EGT,
exponential factors to infuse hierarchical structure into attention and
representation layers, enhancing efficiency for structured data.
  We derive rigorous theoretical guarantees, including universal approximation
theorems for continuous and Sobolev functions, reduced sample complexity via
effective VC dimension bounds, Lipschitz continuity of graded operations, and
robustness to adversarial perturbations. A graded loss function ensures
gradient stability and alignment with domain priors during optimization. By
treating grades as differentiable parameters, the framework enables adaptive
feature prioritization, overcoming limitations of fixed grades in prior work.
  The Graded Transformer holds transformative potential for hierarchical
learning and neurosymbolic reasoning, with applications spanning algebraic
geometry (e.g., moduli spaces and zeta functions), physics (e.g., multiscale
simulations), natural language processing (e.g., syntactic parsing), biological
sequence analysis (e.g., variant prediction), and emerging areas like graph
neural networks and financial modeling. This work advances structured deep
learning by fusing geometric and algebraic principles with attention
mechanisms, offering a mathematically grounded alternative to data-driven
models and paving the way for interpretable, efficient systems in complex
domains.</p></br><a href="http://arxiv.org/pdf/2507.20670v1" target="_blank"><h2>A Multimodal Architecture for Endpoint Position Prediction in Team-based
  Multiplayer Games</h2></a><strong><u>Authors:</u></strong>  Jonas Peche, Aliaksei Tsishurou, Alexander Zap, Guenter Wallner</br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.LG</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> anomaly detection (abstract), multimodal (title, abstract), attention (abstract)</br><p><strong><u>Abstract:</u></strong> Understanding and predicting player movement in multiplayer games is crucial
for achieving use cases such as player-mimicking bot navigation, preemptive bot
control, strategy recommendation, and real-time player behavior analytics.
However, the complex environments allow for a high degree of navigational
freedom, and the interactions and team-play between players require models that
make effective use of the available heterogeneous input data. This paper
presents a multimodal architecture for predicting future player locations on a
dynamic time horizon, using a U-Net-based approach for calculating endpoint
location probability heatmaps, conditioned using a multimodal feature encoder.
The application of a multi-head attention mechanism for different groups of
features allows for communication between agents. In doing so, the architecture
makes efficient use of the multimodal game state including image inputs,
numerical and categorical features, as well as dynamic game data. Consequently,
the presented technique lays the foundation for various downstream tasks that
rely on future player positions such as the creation of player-predictive bot
behavior or player anomaly detection.</p></br><a href="http://arxiv.org/pdf/2507.20019v1" target="_blank"><h2>Anomaly Detection in Human Language via Meta-Learning: A Few-Shot
  Approach</h2></a><strong><u>Authors:</u></strong>  Saurav Singla, Aarav Singla, Advik Gupta, Parnika Gupta</br><strong><u>Categories:</u></strong> cs.CL, cs.AI</br><strong><u>Comments:</u></strong> 15 pages. PyTorch code for few-shot anomaly detection using meta-learning is available upon request or can be shared via GitHub</br><strong><u>Matching Keywords:</u></strong> anomaly detection (title, abstract)</br><p><strong><u>Abstract:</u></strong> We propose a meta learning framework for detecting anomalies in human
language across diverse domains with limited labeled data. Anomalies in
language ranging from spam and fake news to hate speech pose a major challenge
due to their sparsity and variability. We treat anomaly detection as a few shot
binary classification problem and leverage meta-learning to train models that
generalize across tasks. Using datasets from domains such as SMS spam, COVID-19
fake news, and hate speech, we evaluate model generalization on unseen tasks
with minimal labeled anomalies. Our method combines episodic training with
prototypical networks and domain resampling to adapt quickly to new anomaly
detection tasks. Empirical results show that our method outperforms strong
baselines in F1 and AUC scores. We also release the code and benchmarks to
facilitate further research in few-shot text anomaly detection.</p></br><a href="http://arxiv.org/pdf/2507.20369v1" target="_blank"><h2>Clustering by Attention: Leveraging Prior Fitted Transformers for Data
  Partitioning</h2></a><strong><u>Authors:</u></strong>  Ahmed Shokry, Ayman Khalafallah</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract), attention (title, abstract)</br><p><strong><u>Abstract:</u></strong> Clustering is a core task in machine learning with wide-ranging applications
in data mining and pattern recognition. However, its unsupervised nature makes
it inherently challenging. Many existing clustering algorithms suffer from
critical limitations: they often require careful parameter tuning, exhibit high
computational complexity, lack interpretability, or yield suboptimal accuracy,
especially when applied to large-scale datasets. In this paper, we introduce a
novel clustering approach based on meta-learning. Our approach eliminates the
need for parameter optimization while achieving accuracy that outperforms
state-of-the-art clustering techniques. The proposed technique leverages a few
pre-clustered samples to guide the clustering process for the entire dataset in
a single forward pass. Specifically, we employ a pre-trained Prior-Data Fitted
Transformer Network (PFN) to perform clustering. The algorithm computes
attention between the pre-clustered samples and the unclustered samples,
allowing it to infer cluster assignments for the entire dataset based on the
learned relation. We theoretically and empirically demonstrate that, given just
a few pre-clustered examples, the model can generalize to accurately cluster
the rest of the dataset. Experiments on challenging benchmark datasets show
that our approach can successfully cluster well-separated data without any
pre-clustered samples, and significantly improves performance when a few
clustered samples are provided. We show that our approach is superior to the
state-of-the-art techniques. These results highlight the effectiveness and
scalability of our approach, positioning it as a promising alternative to
existing clustering techniques.</p></br><a href="http://arxiv.org/pdf/2507.19898v1" target="_blank"><h2>TS-Insight: Visualizing Thompson Sampling for Verification and XAI</h2></a><strong><u>Authors:</u></strong>  Parsa Vares, Éloi Durant, Jun Pang, Nicolas Médoc, Mohammad Ghoniem</br><strong><u>Categories:</u></strong> cs.HC, cs.AI, cs.LG, stat.ML, I.2.6; H.5.2</br><strong><u>Comments:</u></strong> Accepted as a poster at IEEE VIS 2025 ("TS-Insight: Visual Fingerprinting of Multi-Armed Bandits"). Open-source tool available atthis https URL</br><strong><u>Matching Keywords:</u></strong> explainability (abstract)</br><p><strong><u>Abstract:</u></strong> Thompson Sampling (TS) and its variants are powerful Multi-Armed Bandit
algorithms used to balance exploration and exploitation strategies in active
learning. Yet, their probabilistic nature often turns them into a ``black
box'', hindering debugging and trust. We introduce TS-Insight, a visual
analytics tool explicitly designed to shed light on the internal decision
mechanisms of Thompson Sampling-based algorithms, for model developers. It
comprises multiple plots, tracing for each arm the evolving posteriors,
evidence counts, and sampling outcomes, enabling the verification, diagnosis,
and explainability of exploration/exploitation dynamics. This tool aims at
fostering trust and facilitating effective debugging and deployment in complex
binary decision-making scenarios especially in sensitive domains requiring
interpretable decision-making.</p></br><a href="http://arxiv.org/pdf/2507.20221v1" target="_blank"><h2>Multi-Attention Stacked Ensemble for Lung Cancer Detection in CT Scans</h2></a><strong><u>Authors:</u></strong>  Uzzal Saha, Surya Prakash</br><strong><u>Categories:</u></strong> cs.CV, cs.AI</br><strong><u>Comments:</u></strong> 26 pages, 14 figures</br><strong><u>Matching Keywords:</u></strong> neural network (abstract), attention (title, abstract)</br><p><strong><u>Abstract:</u></strong> In this work, we address the challenge of binary lung nodule classification
(benign vs malignant) using CT images by proposing a multi-level attention
stacked ensemble of deep neural networks. Three pretrained backbones -
EfficientNet V2 S, MobileViT XXS, and DenseNet201 - are each adapted with a
custom classification head tailored to 96 x 96 pixel inputs. A two-stage
attention mechanism learns both model-wise and class-wise importance scores
from concatenated logits, and a lightweight meta-learner refines the final
prediction. To mitigate class imbalance and improve generalization, we employ
dynamic focal loss with empirically calculated class weights, MixUp
augmentation during training, and test-time augmentation at inference.
Experiments on the LIDC-IDRI dataset demonstrate exceptional performance,
achieving 98.09 accuracy and 0.9961 AUC, representing a 35 percent reduction in
error rate compared to state-of-the-art methods. The model exhibits balanced
performance across sensitivity (98.73) and specificity (98.96), with
particularly strong results on challenging cases where radiologist disagreement
was high. Statistical significance testing confirms the robustness of these
improvements across multiple experimental runs. Our approach can serve as a
robust, automated aid for radiologists in lung cancer screening.</p></br><a href="http://arxiv.org/pdf/2507.18983v1" target="_blank"><h2>KASPER: Kolmogorov Arnold Networks for Stock Prediction and Explainable
  Regimes</h2></a><strong><u>Authors:</u></strong>  Vidhi Oad, Param Pathak, Nouhaila Innan, Shalini D, Muhammad Shafique</br><strong><u>Categories:</u></strong> cs.LG</br><strong><u>Comments:</u></strong> 11 pages, 7 figures, 3 tables</br><strong><u>Matching Keywords:</u></strong> explainable (title, abstract)</br><p><strong><u>Abstract:</u></strong> Forecasting in financial markets remains a significant challenge due to their
nonlinear and regime-dependent dynamics. Traditional deep learning models, such
as long short-term memory networks and multilayer perceptrons, often struggle
to generalize across shifting market conditions, highlighting the need for a
more adaptive and interpretable approach. To address this, we introduce
Kolmogorov-Arnold networks for stock prediction and explainable regimes
(KASPER), a novel framework that integrates regime detection, sparse
spline-based function modeling, and symbolic rule extraction. The framework
identifies hidden market conditions using a Gumbel-Softmax-based mechanism,
enabling regime-specific forecasting. For each regime, it employs
Kolmogorov-Arnold networks with sparse spline activations to capture intricate
price behaviors while maintaining robustness. Interpretability is achieved
through symbolic learning based on Monte Carlo Shapley values, which extracts
human-readable rules tailored to each regime. Applied to real-world financial
time series from Yahoo Finance, the model achieves an $R^2$ score of 0.89, a
Sharpe Ratio of 12.02, and a mean squared error as low as 0.0001, outperforming
existing methods. This research establishes a new direction for regime-aware,
transparent, and robust forecasting in financial markets.</p></br><a href="http://arxiv.org/pdf/2507.19991v1" target="_blank"><h2>Efficient Vocal-Conditioned Music Generation via Soft Alignment
  Attention and Latent Diffusion</h2></a><strong><u>Authors:</u></strong>  Hei Shing Cheung, Boya Zhang</br><strong><u>Categories:</u></strong> cs.SD, cs.LG</br><strong><u>Comments:</u></strong> 6 page, 3 figures</br><strong><u>Matching Keywords:</u></strong> variational autoencoder (abstract), latent space (abstract), attention (title, abstract)</br><p><strong><u>Abstract:</u></strong> We present a lightweight latent diffusion model for vocal-conditioned musical
accompaniment generation that addresses critical limitations in existing music
AI systems. Our approach introduces a novel soft alignment attention mechanism
that adaptively combines local and global temporal dependencies based on
diffusion timesteps, enabling efficient capture of multi-scale musical
structure. Operating in the compressed latent space of a pre-trained
variational autoencoder, the model achieves a 220 times parameter reduction
compared to state-of-the-art systems while delivering 52 times faster
inference. Experimental evaluation demonstrates competitive performance with
only 15M parameters, outperforming OpenAI Jukebox in production quality and
content unity while maintaining reasonable musical coherence. The
ultra-lightweight architecture enables real-time deployment on consumer
hardware, making AI-assisted music creation accessible for interactive
applications and resource-constrained environments.</p></br><a href="http://arxiv.org/pdf/2507.19680v1" target="_blank"><h2>Feature learning is decoupled from generalization in high capacity
  neural networks</h2></a><strong><u>Authors:</u></strong>  Niclas Alexander Göring, Charles London, Abdurrahman Hadi Erturk, Chris Mingard, Yoonsoo Nam, Ard A. Louis</br><strong><u>Categories:</u></strong> cs.LG, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)</br><p><strong><u>Abstract:</u></strong> Neural networks outperform kernel methods, sometimes by orders of magnitude,
e.g. on staircase functions. This advantage stems from the ability of neural
networks to learn features, adapting their hidden representations to better
capture the data. We introduce a concept we call feature quality to measure
this performance improvement. We examine existing theories of feature learning
and demonstrate empirically that they primarily assess the strength of feature
learning, rather than the quality of the learned features themselves.
Consequently, current theories of feature learning do not provide a sufficient
foundation for developing theories of neural network generalization.</p></br><a href="http://arxiv.org/pdf/2507.19686v1" target="_blank"><h2>KD-GAT: Combining Knowledge Distillation and Graph Attention Transformer
  for a Controller Area Network Intrusion Detection System</h2></a><strong><u>Authors:</u></strong>  Robert Frenken, Sidra Ghayour Bhatti, Hanqin Zhang, Qadeer Ahmed</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> transformer (title), attention (title, abstract)</br><p><strong><u>Abstract:</u></strong> The Controller Area Network (CAN) protocol is widely adopted for in-vehicle
communication but lacks inherent security mechanisms, making it vulnerable to
cyberattacks. This paper introduces KD-GAT, an intrusion detection framework
that combines Graph Attention Networks (GATs) with knowledge distillation (KD)
to enhance detection accuracy while reducing computational complexity. In our
approach, CAN traffic is represented as graphs using a sliding window to
capture temporal and relational patterns. A multi-layer GAT with jumping
knowledge aggregation acting as the teacher model, while a compact student
GAT--only 6.32% the size of the teacher--is trained via a two-phase process
involving supervised pretraining and knowledge distillation with both soft and
hard label supervision. Experiments on three benchmark datasets--Car-Hacking,
Car-Survival, and can-train-and-test demonstrate that both teacher and student
models achieve strong results, with the student model attaining 99.97% and
99.31% accuracy on Car-Hacking and Car-Survival, respectively. However,
significant class imbalance in can-train-and-test has led to reduced
performance for both models on this dataset. Addressing this imbalance remains
an important direction for future work.</p></br><a href="http://arxiv.org/pdf/2507.20272v1" target="_blank"><h2>Approximating Full Conformal Prediction for Neural Network Regression
  with Gauss-Newton Influence</h2></a><strong><u>Authors:</u></strong>  Dharmesh Tailor, Alvaro H. C. Correia, Eric Nalisnick, Christos Louizos</br><strong><u>Categories:</u></strong> cs.LG, stat.ML</br><strong><u>Comments:</u></strong> Accepted at the 13th International Conference on Learning Representations (ICLR 2025)</br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)</br><p><strong><u>Abstract:</u></strong> Uncertainty quantification is an important prerequisite for the deployment of
deep learning models in safety-critical areas. Yet, this hinges on the
uncertainty estimates being useful to the extent the prediction intervals are
well-calibrated and sharp. In the absence of inherent uncertainty estimates
(e.g. pretrained models predicting only point estimates), popular approaches
that operate post-hoc include Laplace's method and split conformal prediction
(split-CP). However, Laplace's method can be miscalibrated when the model is
misspecified and split-CP requires sample splitting, and thus comes at the
expense of statistical efficiency. In this work, we construct prediction
intervals for neural network regressors post-hoc without held-out data. This is
achieved by approximating the full conformal prediction method (full-CP).
Whilst full-CP nominally requires retraining the model for every test point and
candidate label, we propose to train just once and locally perturb model
parameters using Gauss-Newton influence to approximate the effect of
retraining. Coupled with linearization of the network, we express the absolute
residual nonconformity score as a piecewise linear function of the candidate
label allowing for an efficient procedure that avoids the exhaustive search
over the output space. On standard regression benchmarks and bounding box
localization, we show the resulting prediction intervals are locally-adaptive
and often tighter than those of split-CP.</p></br><a href="http://arxiv.org/pdf/2507.20872v1" target="_blank"><h2>Not Only Grey Matter: OmniBrain for Robust Multimodal Classification of
  Alzheimer's Disease</h2></a><strong><u>Authors:</u></strong>  Ahmed Sharshar, Yasser Ashraf, Tameem Bakr, Salma Hassan, Hosam Elgendy, Mohammad Yaqub, Mohsen Guizani</br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.LG</br><strong><u>Comments:</u></strong> Published in Third Workshop on Computer Vision for Automated Medical Diagnosis CVAMD 2025 in ICCV 2025</br><strong><u>Matching Keywords:</u></strong> explainability (abstract), multimodal (title, abstract), attention (abstract)</br><p><strong><u>Abstract:</u></strong> Alzheimer's disease affects over 55 million people worldwide and is projected
to more than double by 2050, necessitating rapid, accurate, and scalable
diagnostics. However, existing approaches are limited because they cannot
achieve clinically acceptable accuracy, generalization across datasets,
robustness to missing modalities, and explainability all at the same time. This
inability to satisfy all these requirements simultaneously undermines their
reliability in clinical settings. We propose OmniBrain, a multimodal framework
that integrates brain MRI, radiomics, gene expression, and clinical data using
a unified model with cross-attention and modality dropout. OmniBrain achieves
$92.2 \pm 2.4\%$accuracy on the ANMerge dataset and generalizes to the MRI-only
ADNI dataset with $70.4 \pm 2.7\%$ accuracy, outperforming unimodal and prior
multimodal approaches. Explainability analyses highlight neuropathologically
relevant brain regions and genes, enhancing clinical trust. OmniBrain offers a
robust, interpretable, and practical solution for real-world Alzheimer's
diagnosis.</p></br><a href="http://arxiv.org/pdf/2507.20164v1" target="_blank"><h2>ASNN: Learning to Suggest Neural Architectures from Performance
  Distributions</h2></a><strong><u>Authors:</u></strong>  Jinwook Hong</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, 68T05, 68T07, 62M45</br><strong><u>Comments:</u></strong> 10 pages</br><strong><u>Matching Keywords:</u></strong> neural network (abstract)</br><p><strong><u>Abstract:</u></strong> The architecture of a neural network (NN) plays a critical role in
determining its performance. However, there is no general closed-form function
that maps between network structure and accuracy, making the process of
architecture design largely heuristic or search-based. In this study, we
propose the Architecture Suggesting Neural Network (ASNN), a model designed to
learn the relationship between NN architecture and its test accuracy, and to
suggest improved architectures accordingly. To train ASNN, we constructed
datasets using TensorFlow-based models with varying numbers of layers and
nodes. Experimental results were collected for both 2-layer and 3-layer
architectures across a grid of configurations, each evaluated with 10 repeated
trials to account for stochasticity. Accuracy values were treated as inputs,
and architectural parameters as outputs. The trained ASNN was then used
iteratively to predict architectures that yield higher performance. In both
2-layer and 3-layer cases, ASNN successfully suggested architectures that
outperformed the best results found in the original training data. Repeated
prediction and retraining cycles led to the discovery of architectures with
improved mean test accuracies, demonstrating the model's capacity to generalize
the performance-structure relationship. These results suggest that ASNN
provides an efficient alternative to random search for architecture
optimization, and offers a promising approach toward automating neural network
design. "Parts of the manuscript, including text editing and expression
refinement, were supported by OpenAI's ChatGPT. All content was reviewed and
verified by the authors."</p></br><a href="http://arxiv.org/pdf/2507.19844v1" target="_blank"><h2>VAE-GAN Based Price Manipulation in Coordinated Local Energy Markets</h2></a><strong><u>Authors:</u></strong>  Biswarup Mukherjee, Li Zhou, S. Gokul Krishnan, Milad Kabirifar, Subhash Lakshminarayana, Charalambos Konstantinou</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.MA, cs.SY, eess.SY</br><strong><u>Comments:</u></strong> 2025 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)</br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), VAE (title, abstract)</br><p><strong><u>Abstract:</u></strong> This paper introduces a model for coordinating prosumers with heterogeneous
distributed energy resources (DERs), participating in the local energy market
(LEM) that interacts with the market-clearing entity. The proposed LEM scheme
utilizes a data-driven, model-free reinforcement learning approach based on the
multi-agent deep deterministic policy gradient (MADDPG) framework, enabling
prosumers to make real-time decisions on whether to buy, sell, or refrain from
any action while facilitating efficient coordination for optimal energy trading
in a dynamic market. In addition, we investigate a price manipulation strategy
using a variational auto encoder-generative adversarial network (VAE-GAN)
model, which allows utilities to adjust price signals in a way that induces
financial losses for the prosumers. Our results show that under adversarial
pricing, heterogeneous prosumer groups, particularly those lacking generation
capabilities, incur financial losses. The same outcome holds across LEMs of
different sizes. As the market size increases, trading stabilizes and fairness
improves through emergent cooperation among agents.</p></br><a href="http://arxiv.org/pdf/2507.20968v1" target="_blank"><h2>From Entanglement to Alignment: Representation Space Decomposition for
  Unsupervised Time Series Domain Adaptation</h2></a><strong><u>Authors:</u></strong>  Rongyao Cai, Ming Jin, Qingsong Wen, Kexin Zhang</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> explainability (abstract), domain adaptation (title, abstract)</br><p><strong><u>Abstract:</u></strong> Domain shift poses a fundamental challenge in time series analysis, where
models trained on source domain often fail dramatically when applied in target
domain with different yet similar distributions. While current unsupervised
domain adaptation (UDA) methods attempt to align cross-domain feature
distributions, they typically treat features as indivisible entities, ignoring
their intrinsic compositions that governs domain adaptation. We introduce
DARSD, a novel UDA framework with theoretical explainability that explicitly
realizes UDA tasks from the perspective of representation space decomposition.
Our core insight is that effective domain adaptation requires not just
alignment, but principled disentanglement of transferable knowledge from mixed
representations. DARSD consists three synergistic components: (I) An
adversarial learnable common invariant basis that projects original features
into a domain-invariant subspace while preserving semantic content; (II) A
prototypical pseudo-labeling mechanism that dynamically separates target
features based on confidence, hindering error accumulation; (III) A hybrid
contrastive optimization strategy that simultaneously enforces feature
clustering and consistency while mitigating emerging distribution gaps.
Comprehensive experiments conducted on four benchmark datasets (WISDM, HAR,
HHAR, and MFD) demonstrate DARSD's superiority against 12 UDA algorithms,
achieving optimal performance in 35 out of 53 cross-domain scenarios.</p></br><a href="http://arxiv.org/pdf/2507.20666v1" target="_blank"><h2>MIMII-Agent: Leveraging LLMs with Function Calling for Relative
  Evaluation of Anomalous Sound Detection</h2></a><strong><u>Authors:</u></strong>  Harsh Purohit, Tomoya Nishida, Kota Dohi, Takashi Endo, Yohei Kawaguchi</br><strong><u>Categories:</u></strong> eess.AS, cs.AI, cs.LG, cs.SD</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> data augmentation (abstract)</br><p><strong><u>Abstract:</u></strong> This paper proposes a method for generating machine-type-specific anomalies
to evaluate the relative performance of unsupervised anomalous sound detection
(UASD) systems across different machine types, even in the absence of real
anomaly sound data. Conventional keyword-based data augmentation methods often
produce unrealistic sounds due to their reliance on manually defined labels,
limiting scalability as machine types and anomaly patterns diversify. Advanced
audio generative models, such as MIMII-Gen, show promise but typically depend
on anomalous training data, making them less effective when diverse anomalous
examples are unavailable. To address these limitations, we propose a novel
synthesis approach leveraging large language models (LLMs) to interpret textual
descriptions of faults and automatically select audio transformation functions,
converting normal machine sounds into diverse and plausible anomalous sounds.
We validate this approach by evaluating a UASD system trained only on normal
sounds from five machine types, using both real and synthetic anomaly data.
Experimental results reveal consistent trends in relative detection difficulty
across machine types between synthetic and real anomalies. This finding
supports our hypothesis and highlights the effectiveness of the proposed
LLM-based synthesis approach for relative evaluation of UASD systems.</p></br><a href="http://arxiv.org/pdf/2507.21004v1" target="_blank"><h2>Compositional Function Networks: A High-Performance Alternative to Deep
  Neural Networks with Built-in Interpretability</h2></a><strong><u>Authors:</u></strong>  Fang Li</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> explainable (abstract), neural network (title, abstract)</br><p><strong><u>Abstract:</u></strong> Deep Neural Networks (DNNs) deliver impressive performance but their
black-box nature limits deployment in high-stakes domains requiring
transparency. We introduce Compositional Function Networks (CFNs), a novel
framework that builds inherently interpretable models by composing elementary
mathematical functions with clear semantics. Unlike existing interpretable
approaches that are limited to simple additive structures, CFNs support diverse
compositional patterns -- sequential, parallel, and conditional -- enabling
complex feature interactions while maintaining transparency. A key innovation
is that CFNs are fully differentiable, allowing efficient training through
standard gradient descent. We demonstrate CFNs' versatility across multiple
domains, from symbolic regression to image classification with deep
hierarchical networks. Our empirical evaluation shows CFNs achieve competitive
performance against black-box models (96.24% accuracy on CIFAR-10) while
outperforming state-of-the-art interpretable models like Explainable Boosting
Machines. By combining the hierarchical expressiveness and efficient training
of deep learning with the intrinsic interpretability of well-defined
mathematical functions, CFNs offer a powerful framework for applications where
both performance and accountability are paramount.</p></br><a href="http://arxiv.org/pdf/2507.21040v1" target="_blank"><h2>Transformers as Unrolled Inference in Probabilistic Laplacian Eigenmaps:
  An Interpretation and Potential Improvements</h2></a><strong><u>Authors:</u></strong>  Aditya Ravuri, Neil D. Lawrence</br><strong><u>Categories:</u></strong> cs.LG, stat.ML</br><strong><u>Comments:</u></strong> Initial version</br><strong><u>Matching Keywords:</u></strong> dimensionality reduction (abstract), transformer (title, abstract), attention (abstract)</br><p><strong><u>Abstract:</u></strong> We propose a probabilistic interpretation of transformers as unrolled
inference steps assuming a probabilistic Laplacian Eigenmaps model from the
ProbDR framework. Our derivation shows that at initialisation, transformers
perform "linear" dimensionality reduction. We also show that within the
transformer block, a graph Laplacian term arises from our arguments, rather
than an attention matrix (which we interpret as an adjacency matrix). We
demonstrate that simply subtracting the identity from the attention matrix (and
thereby taking a graph diffusion step) improves validation performance on a
language model and a simple vision transformer.</p></br><a href="http://arxiv.org/pdf/2507.19774v1" target="_blank"><h2>Bag of Coins: A Statistical Probe into Neural Confidence Structures</h2></a><strong><u>Authors:</u></strong>  Agnideep Aich, Ashit Baran Aich, Md Monzur Murshed, Sameera Hewage, Bruce Wade</br><strong><u>Categories:</u></strong> stat.ML, cs.LG, 62M45, 62H30, 62P30</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract), transformer (abstract)</br><p><strong><u>Abstract:</u></strong> Modern neural networks, despite their high accuracy, often produce poorly
calibrated confidence scores, limiting their reliability in high-stakes
applications. Existing calibration methods typically post-process model outputs
without interrogating the internal consistency of the predictions themselves.
In this work, we introduce a novel, non-parametric statistical probe, the
Bag-of-Coins (BoC) test, that examines the internal consistency of a
classifier's logits. The BoC test reframes confidence estimation as a
frequentist hypothesis test: does the model's top-ranked class win 1-v-1
contests against random competitors at a rate consistent with its own stated
softmax probability? When applied to modern deep learning architectures, this
simple probe reveals a fundamental dichotomy. On Vision Transformers (ViTs),
the BoC output serves as a state-of-the-art confidence score, achieving
near-perfect calibration with an ECE of 0.0212, an 88% improvement over a
temperature-scaled baseline. Conversely, on Convolutional Neural Networks
(CNNs) like ResNet, the probe reveals a deep inconsistency between the model's
predictions and its internal logit structure, a property missed by traditional
metrics. We posit that BoC is not merely a calibration method, but a new
diagnostic tool for understanding and exposing the differing ways that popular
architectures represent uncertainty.</p></br><a href="http://arxiv.org/pdf/2507.19233v1" target="_blank"><h2>Component-Based Machine Learning for Indoor Flow and Temperature Fields
  Prediction Latent Feature Aggregation and Flow Interaction</h2></a><strong><u>Authors:</u></strong>  Shaofan Wang, Nils Thuerey, Philipp Geyer</br><strong><u>Categories:</u></strong> cs.LG, physics.flu-dyn</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract)</br><p><strong><u>Abstract:</u></strong> Accurate and efficient prediction of indoor airflow and temperature
distributions is essential for building energy optimization and occupant
comfort control. However, traditional CFD simulations are computationally
intensive, limiting their integration into real-time or design-iterative
workflows. This study proposes a component-based machine learning (CBML)
surrogate modeling approach to replace conventional CFD simulation for fast
prediction of indoor velocity and temperature fields. The model consists of
three neural networks: a convolutional autoencoder with residual connections
(CAER) to extract and compress flow features, a multilayer perceptron (MLP) to
map inlet velocities to latent representations, and a convolutional neural
network (CNN) as an aggregator to combine single-inlet features into dual-inlet
scenarios. A two-dimensional room with varying left and right air inlet
velocities is used as a benchmark case, with CFD simulations providing training
and testing data. Results show that the CBML model accurately and fast predicts
two-component aggregated velocity and temperature fields across both training
and testing datasets.</p></br><a href="http://arxiv.org/pdf/2507.20058v1" target="_blank"><h2>Predicting Parkinson's Disease Progression Using Statistical and Neural
  Mixed Effects Models: A Comparative Study on Longitudinal Biomarkers</h2></a><strong><u>Authors:</u></strong>  Ran Tong, Lanruo Wang, Tong Wang, Wei Yan</br><strong><u>Categories:</u></strong> stat.ML, cs.LG, stat.AP</br><strong><u>Comments:</u></strong> 20pages,3 figures,currently under review</br><strong><u>Matching Keywords:</u></strong> neural network (abstract)</br><p><strong><u>Abstract:</u></strong> Predicting Parkinson's Disease (PD) progression is crucial, and voice
biomarkers offer a non-invasive method for tracking symptom severity (UPDRS
scores) through telemonitoring. Analyzing this longitudinal data is challenging
due to within-subject correlations and complex, nonlinear patient-specific
progression patterns. This study benchmarks LMMs against two advanced hybrid
approaches: the Generalized Neural Network Mixed Model (GNMM) (Mandel 2021),
which embeds a neural network within a GLMM structure, and the Neural Mixed
Effects (NME) model (Wortwein 2023), allowing nonlinear subject-specific
parameters throughout the network. Using the Oxford Parkinson's telemonitoring
voice dataset, we evaluate these models' performance in predicting Total UPDRS
to offer practical guidance for PD research and clinical applications.</p></br><a href="http://arxiv.org/pdf/2507.20326v1" target="_blank"><h2>MIPS: a Multimodal Infinite Polymer Sequence Pre-training Framework for
  Polymer Property Prediction</h2></a><strong><u>Authors:</u></strong>  Jiaxi Wang, Yaosen Min, Xun Zhu, Miao Li, Ji Wu</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> 14 pages, 8 figures, accepted by ACM Multimedia 2025 (oral)</br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract), attention (abstract)</br><p><strong><u>Abstract:</u></strong> Polymers, composed of repeating structural units called monomers, are
fundamental materials in daily life and industry. Accurate property prediction
for polymers is essential for their design, development, and application.
However, existing modeling approaches, which typically represent polymers by
the constituent monomers, struggle to capture the whole properties of polymer,
since the properties change during the polymerization process. In this study,
we propose a Multimodal Infinite Polymer Sequence (MIPS) pre-training
framework, which represents polymers as infinite sequences of monomers and
integrates both topological and spatial information for comprehensive modeling.
From the topological perspective, we generalize message passing mechanism (MPM)
and graph attention mechanism (GAM) to infinite polymer sequences. For MPM, we
demonstrate that applying MPM to infinite polymer sequences is equivalent to
applying MPM on the induced star-linking graph of monomers. For GAM, we propose
to further replace global graph attention with localized graph attention (LGA).
Moreover, we show the robustness of the "star linking" strategy through Repeat
and Shift Invariance Test (RSIT). Despite its robustness, "star linking"
strategy exhibits limitations when monomer side chains contain ring structures,
a common characteristic of polymers, as it fails the Weisfeiler-Lehman~(WL)
test. To overcome this issue, we propose backbone embedding to enhance the
capability of MPM and LGA on infinite polymer sequences. From the spatial
perspective, we extract 3D descriptors of repeating monomers to capture spatial
information. Finally, we design a cross-modal fusion mechanism to unify the
topological and spatial information. Experimental validation across eight
diverse polymer property prediction tasks reveals that MIPS achieves
state-of-the-art performance.</p></br><a href="http://arxiv.org/pdf/2507.20189v1" target="_blank"><h2>NeuroCLIP: A Multimodal Contrastive Learning Method for rTMS-treated
  Methamphetamine Addiction Analysis</h2></a><strong><u>Authors:</u></strong>  Chengkai Wang, Di Wu, Yunsheng Liao, Wenyao Zheng, Ziyi Zeng, Xurong Gao, Hemmings Wu, Zhoule Zhu, Jie Yang, Lihua Zhong, Weiwei Cheng, Yun-Hsuan Chen, Mohamad Sawan</br><strong><u>Categories:</u></strong> eess.SP, cs.AI, cs.LG, q-bio.NC</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), multimodal (title, abstract)</br><p><strong><u>Abstract:</u></strong> Methamphetamine dependence poses a significant global health challenge, yet
its assessment and the evaluation of treatments like repetitive transcranial
magnetic stimulation (rTMS) frequently depend on subjective self-reports, which
may introduce uncertainties. While objective neuroimaging modalities such as
electroencephalography (EEG) and functional near-infrared spectroscopy (fNIRS)
offer alternatives, their individual limitations and the reliance on
conventional, often hand-crafted, feature extraction can compromise the
reliability of derived biomarkers. To overcome these limitations, we propose
NeuroCLIP, a novel deep learning framework integrating simultaneously recorded
EEG and fNIRS data through a progressive learning strategy. This approach
offers a robust and trustworthy biomarker for methamphetamine addiction.
Validation experiments show that NeuroCLIP significantly improves
discriminative capabilities among the methamphetamine-dependent individuals and
healthy controls compared to models using either EEG or only fNIRS alone.
Furthermore, the proposed framework facilitates objective, brain-based
evaluation of rTMS treatment efficacy, demonstrating measurable shifts in
neural patterns towards healthy control profiles after treatment. Critically,
we establish the trustworthiness of the multimodal data-driven biomarker by
showing its strong correlation with psychometrically validated craving scores.
These findings suggest that biomarker derived from EEG-fNIRS data via NeuroCLIP
offers enhanced robustness and reliability over single-modality approaches,
providing a valuable tool for addiction neuroscience research and potentially
improving clinical assessments.</p></br><a href="http://arxiv.org/pdf/2507.20499v1" target="_blank"><h2>DmC: Nearest Neighbor Guidance Diffusion Model for Offline Cross-domain
  Reinforcement Learning</h2></a><strong><u>Authors:</u></strong>  Linh Le Pham Van, Minh Hoang Nguyen, Duc Kieu, Hung Le, Hung The Tran, Sunil Gupta</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> accepted at ECAI 2025</br><strong><u>Matching Keywords:</u></strong> neural network (abstract)</br><p><strong><u>Abstract:</u></strong> Cross-domain offline reinforcement learning (RL) seeks to enhance sample
efficiency in offline RL by utilizing additional offline source datasets. A key
challenge is to identify and utilize source samples that are most relevant to
the target domain. Existing approaches address this challenge by measuring
domain gaps through domain classifiers, target transition dynamics modeling, or
mutual information estimation using contrastive loss. However, these methods
often require large target datasets, which is impractical in many real-world
scenarios. In this work, we address cross-domain offline RL under a limited
target data setting, identifying two primary challenges: (1) Dataset imbalance,
which is caused by large source and small target datasets and leads to
overfitting in neural network-based domain gap estimators, resulting in
uninformative measurements; and (2) Partial domain overlap, where only a subset
of the source data is closely aligned with the target domain. To overcome these
issues, we propose DmC, a novel framework for cross-domain offline RL with
limited target samples. Specifically, DmC utilizes $k$-nearest neighbor
($k$-NN) based estimation to measure domain proximity without neural network
training, effectively mitigating overfitting. Then, by utilizing this domain
proximity, we introduce a nearest-neighbor-guided diffusion model to generate
additional source samples that are better aligned with the target domain, thus
enhancing policy learning with more effective source samples. Through
theoretical analysis and extensive experiments in diverse MuJoCo environments,
we demonstrate that DmC significantly outperforms state-of-the-art cross-domain
offline RL methods, achieving substantial performance gains.</p></br><a href="http://arxiv.org/pdf/2507.20853v1" target="_blank"><h2>Geometry of Neural Reinforcement Learning in Continuous State and Action
  Spaces</h2></a><strong><u>Authors:</u></strong>  Saket Tiwari, Omer Gottesman, George Konidaris</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> arXiv admin note: text overlap witharXiv:2301.00009</br><strong><u>Matching Keywords:</u></strong> neural network (abstract)</br><p><strong><u>Abstract:</u></strong> Advances in reinforcement learning (RL) have led to its successful
application in complex tasks with continuous state and action spaces. Despite
these advances in practice, most theoretical work pertains to finite state and
action spaces. We propose building a theoretical understanding of continuous
state and action spaces by employing a geometric lens to understand the locally
attained set of states. The set of all parametrised policies learnt through a
semi-gradient based approach induces a set of attainable states in RL. We show
that the training dynamics of a two-layer neural policy induce a low
dimensional manifold of attainable states embedded in the high-dimensional
nominal state space trained using an actor-critic algorithm. We prove that,
under certain conditions, the dimensionality of this manifold is of the order
of the dimensionality of the action space. This is the first result of its
kind, linking the geometry of the state space to the dimensionality of the
action space. We empirically corroborate this upper bound for four MuJoCo
environments and also demonstrate the results in a toy environment with varying
dimensionality. We also show the applicability of this theoretical result by
introducing a local manifold learning layer to the policy and value function
networks to improve the performance in control environments with very high
degrees of freedom by changing one layer of the neural network to learn sparse
representations.</p></br><a href="http://arxiv.org/pdf/2507.20576v1" target="_blank"><h2>Fusing CFD and measurement data using transfer learning</h2></a><strong><u>Authors:</u></strong>  Alexander Barklage, Philipp Bekemeyer</br><strong><u>Categories:</u></strong> cs.LG</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), neural network (abstract), transfer learning (title, abstract)</br><p><strong><u>Abstract:</u></strong> Aerodynamic analysis during aircraft design usually involves methods of
varying accuracy and spatial resolution, which all have their advantages and
disadvantages. It is therefore desirable to create data-driven models which
effectively combine these advantages. Such data fusion methods for distributed
quantities mainly rely on proper orthogonal decomposition as of now, which is a
linear method. In this paper, we introduce a non-linear method based on neural
networks combining simulation and measurement data via transfer learning. The
network training accounts for the heterogeneity of the data, as simulation data
usually features a high spatial resolution, while measurement data is sparse
but more accurate. In a first step, the neural network is trained on simulation
data to learn spatial features of the distributed quantities. The second step
involves transfer learning on the measurement data to correct for systematic
errors between simulation and measurement by only re-training a small subset of
the entire neural network model. This approach is applied to a multilayer
perceptron architecture and shows significant improvements over the established
method based on proper orthogonal decomposition by producing more physical
solutions near nonlinearities. In addition, the neural network provides
solutions at arbitrary flow conditions, thus making the model useful for flight
mechanical design, structural sizing, and certification. As the proposed
training strategy is very general, it can also be applied to more complex
neural network architectures in the future.</p></br><a href="http://arxiv.org/pdf/2507.20804v1" target="_blank"><h2>MMGraphRAG: Bridging Vision and Language with Interpretable Multimodal
  Knowledge Graphs</h2></a><strong><u>Authors:</u></strong>  Xueyao Wan, Hang Yu</br><strong><u>Categories:</u></strong> cs.AI</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)</br><p><strong><u>Abstract:</u></strong> Retrieval-Augmented Generation (RAG) enhances language model generation by
retrieving relevant information from external knowledge bases. However,
conventional RAG methods face the issue of missing multimodal information.
Multimodal RAG methods address this by fusing images and text through mapping
them into a shared embedding space, but they fail to capture the structure of
knowledge and logical chains between modalities. Moreover, they also require
large-scale training for specific tasks, resulting in limited generalizing
ability. To address these limitations, we propose MMGraphRAG, which refines
visual content through scene graphs and constructs a multimodal knowledge graph
(MMKG) in conjunction with text-based KG. It employs spectral clustering to
achieve cross-modal entity linking and retrieves context along reasoning paths
to guide the generative process. Experimental results show that MMGraphRAG
achieves state-of-the-art performance on the DocBench and MMLongBench datasets,
demonstrating strong domain adaptability and clear reasoning paths.</p></br><a href="http://arxiv.org/pdf/2507.20613v1" target="_blank"><h2>Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache
  Compression</h2></a><strong><u>Authors:</u></strong>  Te Zhang, Yuheng Li, Junxiang Wang, Lujun Li</br><strong><u>Categories:</u></strong> cs.AI, cs.LG</br><strong><u>Comments:</u></strong> 6 pages</br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)</br><p><strong><u>Abstract:</u></strong> Large multimodal models (LMMs) have advanced significantly by integrating
visual encoders with extensive language models, enabling robust reasoning
capabilities. However, compressing LMMs for deployment on edge devices remains
a critical challenge. In this work, we propose an adaptive search algorithm
that optimizes sparsity and KV cache compression to enhance LMM efficiency.
Utilizing the Tree-structured Parzen Estimator, our method dynamically adjusts
pruning ratios and KV cache quantization bandwidth across different LMM layers,
using model performance as the optimization objective. This approach uniquely
combines pruning with key-value cache quantization and incorporates a fast
pruning technique that eliminates the need for additional fine-tuning or weight
adjustments, achieving efficient compression without compromising accuracy.
Comprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and
13B, demonstrate our method superiority over state-of-the-art techniques such
as SparseGPT and Wanda across various compression levels. Notably, our
framework automatic allocation of KV cache compression resources sets a new
standard in LMM optimization, delivering memory efficiency without sacrificing
much performance.</p></br><a href="http://arxiv.org/pdf/2507.20191v1" target="_blank"><h2>Partial Domain Adaptation via Importance Sampling-based Shift Correction</h2></a><strong><u>Authors:</u></strong>  Cheng-Jun Guo, Chuan-Xian Ren, You-Wei Luo, Xiao-Lin Xu, Hong Yan</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> domain adaptation (title, abstract)</br><p><strong><u>Abstract:</u></strong> Partial domain adaptation (PDA) is a challenging task in real-world machine
learning scenarios. It aims to transfer knowledge from a labeled source domain
to a related unlabeled target domain, where the support set of the source label
distribution subsumes the target one. Previous PDA works managed to correct the
label distribution shift by weighting samples in the source domain. However,
the simple reweighing technique cannot explore the latent structure and
sufficiently use the labeled data, and then models are prone to over-fitting on
the source domain. In this work, we propose a novel importance sampling-based
shift correction (IS$^2$C) method, where new labeled data are sampled from a
built sampling domain, whose label distribution is supposed to be the same as
the target domain, to characterize the latent structure and enhance the
generalization ability of the model. We provide theoretical guarantees for
IS$^2$C by proving that the generalization error can be sufficiently dominated
by IS$^2$C. In particular, by implementing sampling with the mixture
distribution, the extent of shift between source and sampling domains can be
connected to generalization error, which provides an interpretable way to build
IS$^2$C. To improve knowledge transfer, an optimal transport-based independence
criterion is proposed for conditional distribution alignment, where the
computation of the criterion can be adjusted to reduce the complexity from
$\mathcal{O}(n^3)$ to $\mathcal{O}(n^2)$ in realistic PDA scenarios. Extensive
experiments on PDA benchmarks validate the theoretical results and demonstrate
the effectiveness of our IS$^2$C over existing methods.</p></br><a href="http://arxiv.org/pdf/2507.20068v1" target="_blank"><h2>PERRY: Policy Evaluation with Confidence Intervals using Auxiliary Data</h2></a><strong><u>Authors:</u></strong>  Aishwarya Mandyam, Jason Meng, Ge Gao, Jiankai Sun, Mac Schwager, Barbara E. Engelhardt, Emma Brunskill</br><strong><u>Categories:</u></strong> cs.LG, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> data augmentation (abstract)</br><p><strong><u>Abstract:</u></strong> Off-policy evaluation (OPE) methods aim to estimate the value of a new
reinforcement learning (RL) policy prior to deployment. Recent advances have
shown that leveraging auxiliary datasets, such as those synthesized by
generative models, can improve the accuracy of these value estimates.
Unfortunately, such auxiliary datasets may also be biased, and existing methods
for using data augmentation for OPE in RL lack principled uncertainty
quantification. In high stakes settings like healthcare, reliable uncertainty
estimates are important for comparing policy value estimates. In this work, we
propose two approaches to construct valid confidence intervals for OPE when
using data augmentation. The first provides a confidence interval over the
policy performance conditioned on a particular initial state $V^{\pi}(s_0)$--
such intervals are particularly important for human-centered applications. To
do so we introduce a new conformal prediction method for high dimensional state
MDPs. Second, we consider the more common task of estimating the average policy
performance over many initial states; to do so we draw on ideas from doubly
robust estimation and prediction powered inference. Across simulators spanning
robotics, healthcare and inventory management, and a real healthcare dataset
from MIMIC-IV, we find that our methods can use augmented data and still
consistently produce intervals that cover the ground truth values, unlike
previously proposed methods.</p></br><a href="http://arxiv.org/pdf/2507.19726v1" target="_blank"><h2>HypKG: Hypergraph-based Knowledge Graph Contextualization for Precision
  Healthcare</h2></a><strong><u>Authors:</u></strong>  Yuzhang Xie, Xu Han, Ran Xu, Xiao Hu, Jiaying Lu, Carl Yang</br><strong><u>Categories:</u></strong> cs.AI, cs.LG</br><strong><u>Comments:</u></strong> Extended version of paper accepted at the 24th International Semantic Web Conference (ISWC 2025), Main Tracks, Research Track, Oral</br><strong><u>Matching Keywords:</u></strong> transformer (abstract)</br><p><strong><u>Abstract:</u></strong> Knowledge graphs (KGs) are important products of the semantic web, which are
widely used in various application domains. Healthcare is one of such domains
where KGs are intensively used, due to the high requirement for knowledge
accuracy and interconnected nature of healthcare data. However, KGs storing
general factual information often lack the ability to account for important
contexts of the knowledge such as the status of specific patients, which are
crucial in precision healthcare. Meanwhile, electronic health records (EHRs)
provide rich personal data, including various diagnoses and medications, which
provide natural contexts for general KGs. In this paper, we propose HypKG, a
framework that integrates patient information from EHRs into KGs to generate
contextualized knowledge representations for accurate healthcare predictions.
Using advanced entity-linking techniques, we connect relevant knowledge from
general KGs with patient information from EHRs, and then utilize a hypergraph
model to "contextualize" the knowledge with the patient information. Finally,
we employ hypergraph transformers guided by downstream prediction tasks to
jointly learn proper contextualized representations for both KGs and patients,
fully leveraging existing knowledge in KGs and patient contexts in EHRs. In
experiments using a large biomedical KG and two real-world EHR datasets, HypKG
demonstrates significant improvements in healthcare prediction tasks across
multiple evaluation metrics. Additionally, by integrating external contexts,
HypKG can learn to adjust the representations of entities and relations in KG,
potentially improving the quality and real-world utility of knowledge.</p></br></body>