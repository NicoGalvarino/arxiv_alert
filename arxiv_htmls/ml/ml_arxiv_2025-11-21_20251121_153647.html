<!DOCTYPE html><html><head><meta charset='utf-8'><link href='https://fonts.googleapis.com/css?family=Montserrat' rel='stylesheet'>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$','$']],
            processEscapes: true
        },
        "HTML-CSS": {
            availableFonts: ["TeX"]
        }
    });
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <style>
    body {font-family: 'Montserrat', sans-serif; background: #F3F3F3; width: 740px; margin: 0 auto; line-height: 150%; margin-top: 50px; font-size: 15px}
    h1 {font-size: 70px}
    a {color: #45ABC2}
    em {font-size: 120%}
    </style>
    </head><body><h1><center>ArXiv Alert</center></h1><font color='#DDAD5C'><em>Update: from 11 Nov 2025 to 20 Nov 2025</em></font><br><br><a href="https://arxiv.org/pdf/2511.16671v1" target="_blank"><h2>Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Ziyu Guo, Renrui Zhang, Hongyu Li, Manyuan Zhang, Xinyan Chen, Sifan Wang, Yan Feng, Peng Pei, Pheng-Ann Heng<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.CL<br><strong><u>Comments:</u></strong> Project Page:this https URLCode:this https URL<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process. As visual content is progressively generating, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones. This dynamic interplay produces more context-aware and semantically rich visual outputs. To unveil the potential of this framework, we investigate three candidate strategies, zero-shot prompting, supervised fine-tuning (SFT) on our curated TwiG-50K dataset, and reinforcement learning (RL) via a customized TwiG-GRPO strategy, each offering unique insights into the dynamics of interleaved reasoning. We hope this work inspires further research into interleaving textual reasoning for enhanced visual generation. Code will be released at: https://github.com/ZiyuGuo99/Thinking-while-Generating.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16658v1" target="_blank"><h2>Prospects for Neutrino Observation and Mass Measurement from Binary Neutron Star Mergers <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Vedran Brdar, Dibya S. Chattopadhyay, Samiur R. Mir, Tousif Raza, Marc S. Romanowski<br><strong><u>Categories:</u></strong> hep-ph, astro-ph.HE, hep-ex<br><strong><u>Comments:</u></strong> 13 pages, 8 figures<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> VAE (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Over the next decade, $\mathcal{O}(100)$ diffuse supernova neutrino background (DSNB) events are expected in Hyper-Kamiokande. Another neutrino source that has received far less attention is binary neutron star mergers. Including the data from recent simulations, we find that detection in current and near-future neutrino experiments is not feasible, and a megaton-scale detector with $\mathcal{O}(10)$ MeV threshold, such as the proposed Deep-TITAND, MEMPHYS, or MICA, will be required. This is due to the updated binary neutron star merger rate and the time-of-flight delay caused by the nonzero neutrino mass. Regarding the former, recent results from LIGO, Virgo, and KAGRA has significantly lowered the upper limit on the neutron star merger rate. As for the latter, neutrino events from neutron star mergers are expected to be recorded shortly after the gravitational wave signal. Limiting the analysis to such short time windows can significantly reduce background rates. While this approach has been qualitatively discussed in the literature, the effect of the time delay caused by neutrino mass, which can substantially extend the observation windows, has been disregarded. We present a refined analysis employing energy-dependent time windows and luminosity distance cuts for the mergers and provide realistic estimates of the detector runtime required to record neutrinos from binary neutron star mergers with small background contamination. The relative timing between the neutrino and gravitational wave signals can also be employed to probe the scale of neutrino mass. We find that the sensitivity to the lightest neutrino mass exceeds both the most stringent terrestrial bounds from KATRIN and the projections based on galactic supernovae. This level of sensitivity may become particularly relevant in the future if terrestrial and supernova constraints are not significantly improved.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16652v1" target="_blank"><h2>Evolution Strategies at the Hyperscale <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Bidipta Sarkar, Mattie Fellows, Juan Agustin Duque, Alistair Letcher, Antonio León Villares, Anya Sims, Dylan Cope, Jarek Liesen, Lukas Seier, Theo Wolf, Uljad Berdica, Alexander David Goldie, Aaron Courville, Karin Sevegnani, Shimon Whiteson, Jakob Nicolaus Foerster<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 48 pages, 12 figures, Website atthis https URL<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> We introduce Evolution Guided General Optimization via Low-rank Learning (EGGROLL), an evolution strategies (ES) algorithm designed to scale backprop-free optimization to large population sizes for modern large neural network architectures with billions of parameters. ES is a set of powerful blackbox optimisation methods that can handle non-differentiable or noisy objectives with excellent scaling potential through parallelisation. Na{ï}ve ES becomes prohibitively expensive at scale due to the computational and memory costs associated with generating matrix perturbations $E\in\mathbb{R}^{m\times n}$ and the batched matrix multiplications needed to compute per-member forward passes. EGGROLL overcomes these bottlenecks by generating random matrices $A\in \mathbb{R}^{m\times r},\ B\in \mathbb{R}^{n\times r}$ with $r\ll \min(m,n)$ to form a low-rank matrix perturbation $A B^\top$ that are used in place of the full-rank perturbation $E$. As the overall update is an average across a population of $N$ workers, this still results in a high-rank update but with significant memory and computation savings, reducing the auxiliary storage from $mn$ to $r(m+n)$ per layer and the cost of a forward pass from $\mathcal{O}(mn)$ to $\mathcal{O}(r(m+n))$ when compared to full-rank ES. A theoretical analysis reveals our low-rank update converges to the full-rank update at a fast $\mathcal{O}\left(\frac{1}{r}\right)$ rate. Our experiments show that (1) EGGROLL does not compromise the performance of ES in tabula-rasa RL settings, despite being faster, (2) it is competitive with GRPO as a technique for improving LLM reasoning, and (3) EGGROLL enables stable pre-training of nonlinear recurrent language models that operate purely in integer datatypes.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16653v1" target="_blank"><h2>Teacher-Guided One-Shot Pruning via Context-Aware Knowledge Distillation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Md. Samiul Alim, Sharjil Khan, Amrijit Biswas, Fuad Rahman, Shafin Rahman, Nabeel Mohammed<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> Accepted at 2025 IEEE International Conference on Big Data (IEEE BigData 2025)<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Unstructured pruning remains a powerful strategy for compressing deep neural networks, yet it often demands iterative train-prune-retrain cycles, resulting in significant computational overhead. To address this challenge, we introduce a novel teacher-guided pruning framework that tightly integrates Knowledge Distillation (KD) with importance score estimation. Unlike prior approaches that apply KD as a post-pruning recovery step, our method leverages gradient signals informed by the teacher during importance score calculation to identify and retain parameters most critical for both task performance and knowledge transfer. Our method facilitates a one-shot global pruning strategy that efficiently eliminates redundant weights while preserving essential representations. After pruning, we employ sparsity-aware retraining with and without KD to recover accuracy without reactivating pruned connections. Comprehensive experiments across multiple image classification benchmarks, including CIFAR-10, CIFAR-100, and TinyImageNet, demonstrate that our method consistently achieves high sparsity levels with minimal performance degradation. Notably, our approach outperforms state-of-the-art baselines such as EPG and EPSD at high sparsity levels, while offering a more computationally efficient alternative to iterative pruning schemes like COLT. The proposed framework offers a computation-efficient, performance-preserving solution well suited for deployment in resource-constrained environments.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16631v1" target="_blank"><h2>A Core-Collapse Supernova Neutrino Parameterization with Enhanced Physical Interpretability <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Haihao Shi, Zhenyang Huang, Junda Zhou, Guoliang Lü, Xuefei Chen<br><strong><u>Categories:</u></strong> astro-ph.HE<br><strong><u>Comments:</u></strong> 48 pages, 31 figures, It has been accepted by APJS<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> We introduce a novel parameterization of supernova neutrino energy spectra with a clear physical motivation. Its central parameter, $τ(t)$, quantifies the characteristic thermal-diffusion area during the explosion. When applied to the historic SN1987A data, this parameterization yields statistically significant fits and provides robust constraints on the unobserved low-energy portion of the spectrum. Beyond this specific application, we demonstrate the model's power on a suite of 3D core-collapse supernova simulations, finding that the temporal evolution of $τ(t)$ distinctly separates successful from failed explosions. Furthermore, we constrain the progenitor mass of SN 1987A to approximately 19 solar masses by applying Smoothed Isotonic Regression, while noting the sensitivity of this estimate to observational uncertainties. Moreover, in these simulations, $τ(t)$ and the gravitational-wave strain amplitude display a strong, synergistic co-evolution, directly linking the engine's energetic evolution to its geometric asymmetry. This implies that the thermodynamic state of the explosion is imprinted not only on the escaping neutrino flux, but also recorded in the shape of the energy spectrum. Our framework therefore offers a valuable tool for decoding the detailed core dynamics and multi-messenger processes of future galactic supernovae.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16625v1" target="_blank"><h2>MedBayes-Lite: Bayesian Uncertainty Quantification for Safe Clinical Decision Support <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Elias Hossain, Md Mehedi Hasan Nipu, Maleeha Sheikh, Rajib Rana, Subash Neupane, Niloofar Yousefi<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> We propose MedBayes-Lite, a lightweight Bayesian enhancement for transformer-based clinical language models designed to produce reliable, uncertainty-aware predictions. Although transformers show strong potential for clinical decision support, they remain prone to overconfidence, especially in ambiguous medical cases where calibrated uncertainty is critical. MedBayes-Lite embeds uncertainty quantification directly into existing transformer pipelines without any retraining or architectural rewiring, adding no new trainable layers and keeping parameter overhead under 3 percent. The framework integrates three components: (i) Bayesian Embedding Calibration using Monte Carlo dropout for epistemic uncertainty, (ii) Uncertainty-Weighted Attention that marginalizes over token reliability, and (iii) Confidence-Guided Decision Shaping inspired by clinical risk minimization. Across biomedical QA and clinical prediction benchmarks (MedQA, PubMedQA, MIMIC-III), MedBayes-Lite consistently improves calibration and trustworthiness, reducing overconfidence by 32 to 48 percent. In simulated clinical settings, it can prevent up to 41 percent of diagnostic errors by flagging uncertain predictions for human review. These results demonstrate its effectiveness in enabling reliable uncertainty propagation and improving interpretability in medical AI systems.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16614v1" target="_blank"><h2>Deep Learning Framework for Enhanced Neutrino Reconstruction of Single-line Events in the ANTARES Telescope <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> A. Albert, S. Alves, M. André, M. Ardid, S. Ardid, J. -J. Aubert, J. Aublin, B. Baret, S. Basa, Y. Becherini, B. Belhorma, F. Benfenati, V. Bertin, S. Biagi, J. Boumaaza, M. Bouta, M. C. Bouwhuis, H. Brânzaş, R. Bruijn, J. Brunner, J. Busto, B. Caiffi, D. Calvo, S. Campion, A. Capone, F. Carenini, J. Carr, V. Carretero, T. Cartraud, S. Celli, L. Cerisy, M. Chabab, R. Cherkaoui El Moursli, T. Chiarusi, M. Circella, J. A. B. Coelho, A. Coleiro, R. Coniglione, P. Coyle, A. Creusot, A. F. Díaz, B. De Martino, C. Distefano, I. Di Palma, C. Donzaud, D. Dornic, D. Drouhin, T. Eberl, A. Eddymaoui, T. van Eeden, D. van Eijk, S. El Hedri, N. El Khayati, A. Enzenhöfer, P. Fermani, G. Ferrara, F. Filippini, L. Fusco, S. Gagliardini, J. García-Méndez, C. Gatius Oliver, P. Gay, N. Geißelbrecht, H. Glotin, R. Gozzini, R. Gracia Ruiz, K. Graf, C. Guidi, L. Haegel, H. van Haren, A. J. Heijboer, Y. Hello, L. Hennig, J. J. Hernández-Rey, J. Hößl, F. Huang, G. Illuminati, B. Jisse-Jung, M. de Jong, P. de Jong, M. Kadler, O. Kalekin, U. Katz, A. Kouchner, I. Kreykenbohm, V. Kulikovskiy, R. Lahmann, M. Lamoureux, A. Lazo, D. Lefèvre, E. Leonora, G. Levi, S. Le Stum, S. Loucatos, J. Manczak, M. Marcelin, A. Margiotta, A. Marinelli, J. A. Martínez-Mora, P. Migliozzi, A. Moussa, R. Muller, S. Navas, E. Nezri, B. Ó Fearraigh, E. Oukacha, A. M. Păun, G. E. Păvălaş, S. Peña-Martínez, M. Perrin-Terrin, P. Piattelli, C. Poiré, V. Popa, T. Pradier, N. Randazzo, D. Real, G. Riccobene, A. Romanov, A. Sánchez Losa, A. Saina, F. Salesa Greus, D. F. E. Samtleben, M. Sanguineti, P. Sapienza, F. Schüssler, J. Seneca, M. Spurio, Th. Stolarczyk, M. Taiuti, Y. Tayalati, B. Vallage, G. Vannoye, V. Van Elewyck, S. Viola, D. Vivolo, J. Wilms, S. Zavatarelli, A. Zegarelli, J. D. Zornoza, J. Zúñiga<br><strong><u>Categories:</u></strong> physics.comp-ph, astro-ph.IM<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract), transfer learning (abstract)<br><p><strong><u>Abstract:</u></strong> We present the $N$-fit algorithm designed to improve the reconstruction of neutrino events detected by a single line of the ANTARES underwater telescope, usually associated with low energy neutrino events ($\sim$ 100 GeV). $N$-Fit is a neural network model that relies on deep learning and combines several advanced techniques in machine learning --deep convolutional layers, mixture density output layers, and transfer learning. This framework divides the reconstruction process into two dedicated branches for each neutrino event topology --tracks and showers-- composed of sub-models for spatial estimation --direction and position-- and energy inference, which later on are combined for event classification. Regarding the direction of single-line events, the $N$-Fit algorithm significantly refines the estimation of the zenithal angle, and delivers reliable azimuthal angle predictions that were previously unattainable with traditional $χ^2$-fit methods. Improving on energy estimation of single-line events is a tall order; $N$-Fit benefits from transfer learning to efficiently integrate key characteristics, such as the estimation of the closest distance from the event to the detector. $N$-Fit also takes advantage from transfer learning in event topology classification by freezing convolutional layers of the pretrained branches. Tests on Monte Carlo simulations and data demonstrate a significant reduction in mean and median absolute errors across all reconstructed parameters. The improvements achieved by $N$-Fit highlight its potential for advancing multimessenger astrophysics and enhancing our ability to probe fundamental physics beyond the Standard Model using single-line events from ANTARES data.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16600v1" target="_blank"><h2>You Only Forward Once: An Efficient Compositional Judging Paradigm <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Tianlong Zhang, Hongwei Xue, Shilin Yan, Di Wu, Chen Xu, Yunyun Yang<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Multimodal large language models (MLLMs) show strong potential as judges. However, existing approaches face a fundamental trade-off: adapting MLLMs to output a single score misaligns with the generative nature of MLLMs and limits fine-grained requirement understanding, whereas autoregressively generating judging analyses is prohibitively slow in high-throughput settings. Observing that judgment reduces to verifying whether inputs satisfy a set of structured requirements, we propose YOFO, a template-conditioned method that judges all requirements in a single forward pass. Built on an autoregressive model, YOFO accepts a structured requirement template and, in one inference step, produces a binary yes/no decision for each requirement by reading the logits of the final token associated with that requirement. This design yields orders-of-magnitude speedups while preserving interpretability. Extensive experiments show that YOFO not only achieves state-of-the-art results on standard recommendation datasets, but also supports dependency-aware analysis-where subsequent judgments are conditioned on previous ones-and further benefits from post-hoc CoT.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16595v1" target="_blank"><h2>TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Boshen Xu, Zihan Xiao, Jiaze Li, Jianzhong Ju, Zhenbo Luo, Jian Luan, Qin Jin<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.CL<br><strong><u>Comments:</u></strong> Project page:this https URL<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract), transformer (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> We introduce TimeViper, a hybrid vision-language model designed to tackle challenges of long video understanding. Processing long videos demands both an efficient model architecture and an effective mechanism for handling extended temporal contexts. To this end, TimeViper adopts a hybrid Mamba-Transformer backbone that combines the efficiency of state-space models with the expressivity of attention mechanisms. Through this hybrid design, we reveal the vision-to-text information aggregation phenomenon, where information progressively flows from vision tokens to text tokens across increasing LLM depth, resulting in severe vision token redundancy. Motivated by this observation, we propose TransV, a token information transfer module that transfers and compresses vision tokens into instruction tokens while maintaining multimodal understanding capabilities. This design enables TimeViper to process hour-long videos exceeding 10,000 frames. Extensive experiments across multiple benchmarks demonstrate that TimeViper competes with state-of-the-art models while extending frame numbers. We further analyze attention behaviors of both Mamba and Transformer layers, offering new insights into hybrid model interpretability. This work represents an initial step towards developing, interpreting, and compressing hybrid Mamba-Transformer architectures.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16588v1" target="_blank"><h2>Formal Abductive Latent Explanations for Prototype-Based Networks <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jules Soria, Zakaria Chihani, Julien Girard-Satabin, Alban Grastien, Romain Xu-Darme, Daniela Cancila<br><strong><u>Categories:</u></strong> cs.AI, cs.LO<br><strong><u>Comments:</u></strong> Accepted at AAAI-26<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> explainable (abstract)<br><p><strong><u>Abstract:</u></strong> Case-based reasoning networks are machine-learning models that make predictions based on similarity between the input and prototypical parts of training samples, called prototypes. Such models are able to explain each decision by pointing to the prototypes that contributed the most to the final outcome. As the explanation is a core part of the prediction, they are often qualified as ``interpretable by design". While promising, we show that such explanations are sometimes misleading, which hampers their usefulness in safety-critical contexts. In particular, several instances may lead to different predictions and yet have the same explanation. Drawing inspiration from the field of formal eXplainable AI (FXAI), we propose Abductive Latent Explanations (ALEs), a formalism to express sufficient conditions on the intermediate (latent) representation of the instance that imply the prediction. Our approach combines the inherent interpretability of case-based reasoning models and the guarantees provided by formal XAI. We propose a solver-free and scalable algorithm for generating ALEs based on three distinct paradigms, compare them, and present the feasibility of our approach on diverse datasets for both standard and fine-grained image classification. The associated code can be found at https://github.com/julsoria/ale</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16573v1" target="_blank"><h2>An Exterior-Embedding Neural Operator Framework for Preserving Conservation Laws <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Huanshuo Dong, Hong Wang, Hao Wu, Zhiwei Zhuang, Xuanze Yang, Ruiqi Shu, Yuan Gao, Xiaomeng Huang<br><strong><u>Categories:</u></strong> cs.OH, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Neural operators have demonstrated considerable effectiveness in accelerating the solution of time-dependent partial differential equations (PDEs) by directly learning governing physical laws from data. However, for PDEs governed by conservation laws(e.g., conservation of mass, energy, or matter), existing neural operators fail to satisfy conservation properties, which leads to degraded model performance and limited generalizability. Moreover, we observe that distinct PDE problems generally require different optimal neural network architectures. This finding underscores the inherent limitations of specialized models in generalizing across diverse problem domains.
  To address these limitations, we propose Exterior-Embedded Conservation Framework (ECF), a universal conserving framework that can be integrated with various data-driven neural operators to enforce conservation laws strictly in predictions. The framework consists of two key components: a conservation quantity encoder that extracts conserved quantities from input data, and a conservation quantity decoder that adjusts the neural operator's predictions using these quantities to ensure strict conservation compliance in the final output. Since our architecture enforces conservation laws, we theoretically prove that it enhances model performance. To validate the performance of our method, we conduct experiments on multiple conservation-law-constrained PDE scenarios, including adiabatic systems, shallow water equations, and the Allen-Cahn problem. These baselines demonstrate that our method effectively improves model accuracy while strictly enforcing conservation laws in the predictions.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16571v1" target="_blank"><h2>Boosting Predictive Performance on Tabular Data through Data Augmentation with Latent-Space Flow-Based Diffusion <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Md. Tawfique Ihsan, Md. Rakibul Hasan Rafi, Ahmed Shoyeb Raihan, Imtiaz Ahmed, Abdullahil Azeem<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 35 Pages<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> VAE (abstract), latent space (abstract), attention (abstract), data augmentation (title, abstract)<br><p><strong><u>Abstract:</u></strong> Severe class imbalance is common in real-world tabular learning, where rare but important minority classes are essential for reliable prediction. Existing generative oversampling methods such as GANs, VAEs, and diffusion models can improve minority-class performance, but they often struggle with tabular heterogeneity, training stability, and privacy concerns. We propose a family of latent-space, tree-driven diffusion methods for minority oversampling that use conditional flow matching with gradient-boosted trees as the vector-field learner. The models operate in compact latent spaces to preserve tabular structure and reduce computation. We introduce three variants: PCAForest, which uses linear PCA embedding; EmbedForest, which uses a learned nonlinear embedding; and AttentionForest, which uses an attention-augmented embedding. Each method couples a GBT-based flow with a decoder back to the original feature space. Across 11 datasets from healthcare, finance, and manufacturing, AttentionForest achieves the best average minority recall while maintaining competitive precision, calibration, and distributional similarity. PCAForest and EmbedForest reach similar utility with much faster generation, offering favorable accuracy-efficiency trade-offs. Privacy evaluated with nearest-neighbor distance ratio and distance-to-closest-record is comparable to or better than the ForestDiffusion baseline. Ablation studies show that smaller embeddings tend to improve minority recall, while aggressive learning rates harm stability. Overall, latent-space, tree-driven diffusion provides an efficient and privacy-aware approach to high-fidelity tabular data augmentation under severe class imbalance.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16566v1" target="_blank"><h2>NutriScreener: Retrieval-Augmented Multi-Pose Graph Attention Network for Malnourishment Screening <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Misaal Khan, Mayank Vatsa, Kuldeep Singh, Richa Singh<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> Accepted in AAAI 2026 Special Track on AI for Social Impact<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Child malnutrition remains a global crisis, yet existing screening methods are laborious and poorly scalable, hindering early intervention. In this work, we present NutriScreener, a retrieval-augmented, multi-pose graph attention network that combines CLIP-based visual embeddings, class-boosted knowledge retrieval, and context awareness to enable robust malnutrition detection and anthropometric prediction from children's images, simultaneously addressing generalizability and class imbalance. In a clinical study, doctors rated it 4.3/5 for accuracy and 4.6/5 for efficiency, confirming its deployment readiness in low-resource settings. Trained and tested on 2,141 children from AnthroVision and additionally evaluated on diverse cross-continent populations, including ARAN and an in-house collected CampusPose dataset, it achieves 0.79 recall, 0.82 AUC, and significantly lower anthropometric RMSEs, demonstrating reliable measurement in unconstrained pediatric settings. Cross-dataset results show up to 25% recall gain and up to 3.5 cm RMSE reduction using demographically matched knowledge bases. NutriScreener offers a scalable and accurate solution for early malnutrition detection in low-resource environments.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16554v1" target="_blank"><h2>Dark Matter-Dark Radiation Interactions and the Hubble Tension <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Manuel A. Buen-Abad, Zackaria Chacko, Ina Flood, Can Kilic, Gustavo Marques-Tavares, Taewook Youn<br><strong><u>Categories:</u></strong> astro-ph.CO, hep-ph<br><strong><u>Comments:</u></strong> 41 pages, 19 figures<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> Models in which a subcomponent of dark matter interacts with dark radiation have been proposed as a solution to the Hubble tension. In this framework, the interacting subcomponent of dark matter is in thermal equilibrium with the dark radiation in the early universe, but decouples from it around the time of matter-radiation equality. We study this general class of models and evaluate the quality of fit to recent cosmological data on the cosmic microwave background (from Planck 2018 and ACT DR6), baryon acoustic oscillations, large-scale structure, supernovae type Ia, and Cepheid variables. We focus on three benchmark scenarios that differ in the rate at which the dark matter decouples from the dark radiation, resulting in different patterns of dark acoustic oscillations. Fitting without ACT DR6 data, we find that all three scenarios significantly reduce the Hubble tension relative to $Λ$CDM, with an exponentially fast decoupling being the most preferred. The tension is reduced to less than $2 \, σ$ in fits that don't include the SH0ES collaboration results as part of the data and to less than $1 \, σ$ when these are included. When ACT DR6 data is included, the fit is significantly worsened. We find that the largest $H_0$ value at the $95 \%$ confidence region is $70.1$ km/s/Mpc without the SH0ES data, leading to only a mild reduction in the tension. This increases to $72.5$ km/s/Mpc, corresponding to a reduction in the tension to less than $3 \, σ$, if the SH0ES results are included in the fit.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16551v1" target="_blank"><h2>Toward Valid Generative Clinical Trial Data with Survival Endpoints <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Perrine Chassat, Van Tuan Nguyen, Lucas Ducrot, Emilie Lanoy, Agathe Guilloux<br><strong><u>Categories:</u></strong> cs.LG, stat.AP, stat.ME, stat.ML<br><strong><u>Comments:</u></strong> P. Chassat and V.T. Nguyen contributed equally to this work<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> variational autoencoder (abstract), VAE (abstract)<br><p><strong><u>Abstract:</u></strong> Clinical trials face mounting challenges: fragmented patient populations, slow enrollment, and unsustainable costs, particularly for late phase trials in oncology and rare diseases. While external control arms built from real-world data have been explored, a promising alternative is the generation of synthetic control arms using generative AI. A central challenge is the generation of time-to-event outcomes, which constitute primary endpoints in oncology and rare disease trials, but are difficult to model under censoring and small sample sizes. Existing generative approaches, largely GAN-based, are data-hungry, unstable, and rely on strong assumptions such as independent censoring. We introduce a variational autoencoder (VAE) that jointly generates mixed-type covariates and survival outcomes within a unified latent variable framework, without assuming independent censoring. Across synthetic and real trial datasets, we evaluate our model in two realistic scenarios: (i) data sharing under privacy constraints, where synthetic controls substitute for original data, and (ii) control-arm augmentation, where synthetic patients mitigate imbalances between treated and control groups. Our method outperforms GAN baselines on fidelity, utility, and privacy metrics, while revealing systematic miscalibration of type I error and power. We propose a post-generation selection procedure that improves calibration, highlighting both progress and open challenges for generative survival modeling.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16550v1" target="_blank"><h2>Broad stochastic configuration residual learning system for norm-convergent universal approximation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Han Su, Zhongyan Li, Wanquan Liu<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Universal approximation serves as the foundation of neural network learning algorithms. However, some networks establish their universal approximation property by demonstrating that the iterative errors converge in probability measure rather than the more rigorous norm convergence, which makes the universal approximation property of randomized learning networks highly sensitive to random parameter selection, Broad residual learning system (BRLS), as a member of randomized learning models, also encounters this issue. We theoretically demonstrate the limitation of its universal approximation property, that is, the iterative errors do not satisfy norm convergence if the selection of random parameters is inappropriate and the convergence rate meets certain conditions. To address this issue, we propose the broad stochastic configuration residual learning system (BSCRLS) algorithm, which features a novel supervisory mechanism adaptively constraining the range settings of random parameters on the basis of BRLS framework, Furthermore, we prove the universal approximation theorem of BSCRLS based on the more stringent norm convergence. Three versions of incremental BSCRLS algorithms are presented to satisfy the application requirements of various network updates. Solar panels dust detection experiments are performed on publicly available dataset and compared with 13 deep and broad learning algorithms. Experimental results reveal the effectiveness and superiority of BSCRLS algorithms.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16543v1" target="_blank"><h2>The Oracle and The Prism: A Decoupled and Efficient Framework for Generative Recommendation Explanation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jiaheng Zhang, Daqiang Zhang<br><strong><u>Categories:</u></strong> cs.IR, cs.AI, cs.CL, cs.LG<br><strong><u>Comments:</u></strong> 11 pages,3 figures<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> explainable (abstract)<br><p><strong><u>Abstract:</u></strong> The integration of Large Language Models (LLMs) into explainable recommendation systems often leads to a performance-efficiency trade-off in end-to-end architectures, where joint optimization of ranking and explanation can result in suboptimal compromises. To resolve this, we propose Prism, a novel decoupled framework that rigorously separates the recommendation process into a dedicated ranking stage and an explanation generation stage.
  Inspired by knowledge distillation, Prism leverages a powerful teacher LLM (e.g., FLAN-T5-XXL) as an Oracle to produce high-fidelity explanatory knowledge. A compact, fine-tuned student model (e.g., BART-Base), the Prism, then specializes in synthesizing this knowledge into personalized explanations. This decomposition ensures that each component is optimized for its specific objective, eliminating inherent conflicts in coupled models.
  Extensive experiments on benchmark datasets demonstrate that our 140M-parameter Prism model significantly outperforms its 11B-parameter teacher in human evaluations of faithfulness and personalization, while achieving a 24 times speedup and a 10 times reduction in memory consumption during inference. These results validate that decoupling, coupled with targeted distillation, provides an efficient and effective pathway to high-quality explainable recommendation.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16527v1" target="_blank"><h2>Contrastive vision-language learning with paraphrasing and negation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Kwun Ho Ngan, Saman Sadeghi Afgeh, Joe Townsend, Artur d'Avila Garcez<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> latent space (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Contrastive vision-language models continue to be the dominant approach for image and text retrieval. Contrastive Language-Image Pre-training (CLIP) trains two neural networks in contrastive manner to align their image and text embeddings in a shared latent space. Recent results evaluating CLIP on negated or paraphrased text have shown mixed performance because negation changes meaning radically with minimal lexical changes, while paraphrasing can create very different textual expressions with the same intended meaning. This poses a significant challenge for improving the evaluation results and alignment of vision-language models. To address this challenge, this paper evaluates the combination of paraphrasing and negation, proposes a new CLIP contrastive loss function accounting for both paraphrasing and negation, and applies LLM-generated training triples consisting of original, paraphrased and negated textual captions to CLIP-like training models. The approach, called SemCLIP, is shown to move paraphrased captions towards the original image embeddings while pushing negated captions further away in embedding space. Empirically, SemCLIP is shown to be capable of preserving CLIP's performance while increasing considerably the distances to negated captions. On the CC-Neg benchmark using an original over negation image-retrieval accuracy metric, SemCLIP improves accuracy from 68.1% to 78.1%. Although results are mixed when compared with CLIP on the Sugarcrepe++ benchmark, SemCLIP's performance is generally better than the models trained with negated captions. This robustness to negation extends to downstream zero-shot classification tasks where SemCLIP pre-trained on Sugarcrepe++ performs better than CLIP on all tested downstream tasks. These results indicate that SemCLIP can achieve significant robustness to semantic transformations.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16506v1" target="_blank"><h2>Two-beam Multiparticle Many-body simulations of Inhomogeneous FFI <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zoha Laraib, Sherwood Richers<br><strong><u>Categories:</u></strong> astro-ph.HE<br><strong><u>Comments:</u></strong> 15 pages, 14 figures<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> Neutrino flavor evolution in dense astrophysical environments is inherently nonlinear and sensitive to many-body (MB) quantum effects beyond the mean-field (MF) approximation. Existing MB studies are constrained by small system sizes, closed boundaries, and highly idealized symmetry assumptions. We present a unified tensor-network framework that enables simulations of inhomogeneous and anisotropic flavor evolution under conditions relevant to core-collapse supernovae and neutron-star mergers. Within this framework, we examine the effects of inhomogeneity, boundary conditions, and convergence with resolution for multiple neutrino distributions, allowing direct comparison of these setups under one consistent formulation. In our simulations, many-body systems equilibrate earlier than their mean-field counterparts while approaching similar final flavor states. Enlarging the interaction region allows open boundaries to reproduce closed-system behavior, but only when the beams begin superimposed and interact continuously. By contrast, initially separated configurations develop entanglement more slowly, interact over longer times, and equilibrate to a flavor content that differs from that obtained from initially superimposed calculations.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16501v1" target="_blank"><h2>ODE-ViT: Plug & Play Attention Layer from the Generalization of the ViT as an Ordinary Differential Equation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Carlos Boned Riera, David Romero Sanchez, Oriol Ramos Terrades<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), transformer (abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> In recent years, increasingly large models have achieved outstanding performance across CV tasks. However, these models demand substantial computational resources and storage, and their growing complexity limits our understanding of how they make decisions. Most of these architectures rely on the attention mechanism within Transformer-based designs. Building upon the connection between residual neural networks and ordinary differential equations (ODEs), we introduce ODE-ViT, a Vision Transformer reformulated as an ODE system that satisfies the conditions for well-posed and stable dynamics. Experiments on CIFAR-10 and CIFAR-100 demonstrate that ODE-ViT achieves stable, interpretable, and competitive performance with up to one order of magnitude fewer parameters, surpassing prior ODE-based Transformer approaches in classification tasks. We further propose a plug-and-play teacher-student framework in which a discrete ViT guides the continuous trajectory of ODE-ViT by treating the intermediate representations of the teacher as solutions of the ODE. This strategy improves performance by more than 10% compared to training a free ODE-ViT from scratch.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16494v1" target="_blank"><h2>Physics-Informed Machine Learning for Efficient Sim-to-Real Data Augmentation in Micro-Object Pose Estimation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zongcai Tan, Lan Wei, Dandan Zhang<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> data augmentation (title, abstract)<br><p><strong><u>Abstract:</u></strong> Precise pose estimation of optical microrobots is essential for enabling high-precision object tracking and autonomous biological studies. However, current methods rely heavily on large, high-quality microscope image datasets, which are difficult and costly to acquire due to the complexity of microrobot fabrication and the labour-intensive labelling. Digital twin systems offer a promising path for sim-to-real data augmentation, yet existing techniques struggle to replicate complex optical microscopy phenomena, such as diffraction artifacts and depth-dependent imaging.This work proposes a novel physics-informed deep generative learning framework that, for the first time, integrates wave optics-based physical rendering and depth alignment into a generative adversarial network (GAN), to synthesise high-fidelity microscope images for microrobot pose estimation efficiently. Our method improves the structural similarity index (SSIM) by 35.6% compared to purely AI-driven methods, while maintaining real-time rendering speeds (0.022 s/frame).The pose estimator (CNN backbone) trained on our synthetic data achieves 93.9%/91.9% (pitch/roll) accuracy, just 5.0%/5.4% (pitch/roll) below that of an estimator trained exclusively on real data. Furthermore, our framework generalises to unseen poses, enabling data augmentation and robust pose estimation for novel microrobot configurations without additional training data.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16482v1" target="_blank"><h2>Correlation-Aware Feature Attribution Based Explainable AI <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Poushali Sengupta, Yan Zhang, Frank Eliassen, Sabita Maharjan<br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML<br><strong><u>Comments:</u></strong> Accepted, 2026 International Conference on Advances in Artificial Intelligence and Machine Learning (AAIML 2026)<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), explainability (abstract), explainable (title, abstract)<br><p><strong><u>Abstract:</u></strong> Explainable AI (XAI) is increasingly essential as modern models become more complex and high-stakes applications demand transparency, trust, and regulatory compliance. Existing global attribution methods often incur high computational costs, lack stability under correlated inputs, and fail to scale efficiently to large or heterogeneous datasets. We address these gaps with \emph{ExCIR} (Explainability through Correlation Impact Ratio), a correlation-aware attribution score equipped with a lightweight transfer protocol that reproduces full-model rankings using only a fraction of the data. ExCIR quantifies sign-aligned co-movement between features and model outputs after \emph{robust centering} (subtracting a robust location estimate, e.g., median or mid-mean, from features and outputs). We further introduce \textsc{BlockCIR}, a \emph{groupwise} extension of ExCIR that scores \emph{sets} of correlated features as a single unit. By aggregating the same signed-co-movement numerators and magnitudes over predefined or data-driven groups, \textsc{BlockCIR} mitigates double-counting in collinear clusters (e.g., synonyms or duplicated sensors) and yields smoother, more stable rankings when strong dependencies are present. Across diverse text, tabular, signal, and image datasets, ExCIR shows trustworthy agreement with established global baselines and the full model, delivers consistent top-$k$ rankings across settings, and reduces runtime via lightweight evaluation on a subset of rows. Overall, ExCIR provides \emph{computationally efficient}, \emph{consistent}, and \emph{scalable} explainability for real-world deployment.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16475v1" target="_blank"><h2>A Comparison Between Decision Transformers and Traditional Offline Reinforcement Learning Algorithms <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Ali Murtaza Caunhye, Asad Jeewa<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 15 pages, 4 figures, published in the Proceedings of the 46th Annual conference of the South African Institute of Computer Scientists and Information Technologists (SIACSIT 2025)<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> The field of Offline Reinforcement Learning (RL) aims to derive effective policies from pre-collected datasets without active environment interaction. While traditional offline RL algorithms like Conservative Q-Learning (CQL) and Implicit Q-Learning (IQL) have shown promise, they often face challenges in balancing exploration and exploitation, especially in environments with varying reward densities. The recently proposed Decision Transformer (DT) approach, which reframes offline RL as a sequence modelling problem, has demonstrated impressive results across various benchmarks. This paper presents a comparative study evaluating the performance of DT against traditional offline RL algorithms in dense and sparse reward settings for the ANT continous control environment. Our research investigates how these algorithms perform when faced with different reward structures, examining their ability to learn effective policies and generalize across varying levels of feedback. Through empirical analysis in the ANT environment, we found that DTs showed less sensitivity to varying reward density compared to other methods and particularly excelled with medium-expert datasets in sparse reward scenarios. In contrast, traditional value-based methods like IQL showed improved performance in dense reward settings with high-quality data, while CQL offered balanced performance across different data qualities. Additionally, DTs exhibited lower variance in performance but required significantly more computational resources compared to traditional approaches. These findings suggest that sequence modelling approaches may be more suitable for scenarios with uncertain reward structures or mixed-quality data, while value-based methods remain competitive in settings with dense rewards and high-quality demonstrations.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16468v1" target="_blank"><h2>Optimizing Quantum Key Distribution Network Performance using Graph Neural Networks <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Akshit Pramod Anchan, Ameiy Acharya, Leki Chom Thungon<br><strong><u>Categories:</u></strong> quant-ph, cs.CR, cs.LG, cs.NI<br><strong><u>Comments:</u></strong> 11 pages, 4 figures, and 2 tables<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> This paper proposes an optimization of Quantum Key Distribution (QKD) Networks using Graph Neural Networks (GNN) framework. Today, the development of quantum computers threatens the security systems of classical cryptography. Moreover, as QKD networks are designed for protecting secret communication, they suffer from multiple operational difficulties: adaptive to dynamic conditions, optimization for multiple parameters and effective resource utilization. In order to overcome these obstacles, we propose a GNN-based framework which can model QKD networks as dynamic graphs and extracts exploitable characteristics from these networks' structure. The graph contains not only topological information but also specific characteristics associated with quantum communication (the number of edges between nodes, etc). Experimental results demonstrate that the GNN-optimized QKD network achieves a substantial increase in total key rate (from 27.1 Kbits/s to 470 Kbits/s), a reduced average QBER (from 6.6% to 6.0%), and maintains path integrity with a slight reduction in average transmission distance (from 7.13 km to 6.42 km). Furthermore, we analyze network performance across varying scales (10 to 250 nodes), showing improved link prediction accuracy and enhanced key generation rate in medium-sized networks. This work introduces a novel operation mode for QKD networks, shifting the paradigm of network optimization through adaptive and scalable quantum communication systems that enhance security and performance.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16467v1" target="_blank"><h2>Anatomy of an Idiom: Tracing Non-Compositionality in Language Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Andrew Gomes<br><strong><u>Categories:</u></strong> cs.CL, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> We investigate the processing of idiomatic expressions in transformer-based language models using a novel set of techniques for circuit discovery and analysis. First discovering circuits via a modified path patching algorithm, we find that idiom processing exhibits distinct computational patterns. We identify and investigate ``Idiom Heads,'' attention heads that frequently activate across different idioms, as well as enhanced attention between idiom tokens due to earlier processing, which we term ``augmented reception.'' We analyze these phenomena and the general features of the discovered circuits as mechanisms by which transformers balance computational efficiency and robustness. Finally, these findings provide insights into how transformers handle non-compositional language and suggest pathways for understanding the processing of more complex grammatical constructions.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16449v1" target="_blank"><h2>VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Ziyan Liu, Yeqiu Chen, Hongyi Cai, Tao Lin, Shuo Yang, Zheng Liu, Bo Zhao<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Vision-Language-Action (VLA) models have shown great promise for embodied AI, yet the heavy computational cost of processing continuous visual streams severely limits their real-time deployment. Token pruning (keeping salient visual tokens and dropping redundant ones) has emerged as an effective approach for accelerating Vision-Language Models (VLMs), offering a solution for efficient VLA. However, these VLM-specific token pruning methods select tokens based solely on semantic salience metrics (e.g., prefill attention), while overlooking the VLA's intrinsic dual-system nature of high-level semantic understanding and low-level action execution. Consequently, these methods bias token retention toward semantic cues, discard critical information for action generation, and significantly degrade VLA performance. To bridge this gap, we propose VLA-Pruner, a versatile plug-and-play VLA-specific token prune method that aligns with the dual-system nature of VLA models and exploits the temporal continuity in robot manipulation. Specifically, VLA-Pruner adopts a dual-level importance criterion for visual token retention: vision-language prefill attention for semantic-level relevance and action decode attention, estimated via temporal smoothing, for action-level importance. Based on this criterion, VLA-Pruner proposes a novel dual-level token selection strategy that adaptively preserves a compact, informative set of visual tokens for both semantic understanding and action execution under given compute budget. Experiments show that VLA-Pruner achieves state-of-the-art performance across multiple VLA architectures and diverse robotic tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16445v1" target="_blank"><h2>PersonaDrift: A Benchmark for Temporal Anomaly Detection in Language-Based Dementia Monitoring <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Joy Lai, Alex Mihailidis<br><strong><u>Categories:</u></strong> cs.AI, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> anomaly detection (title, abstract)<br><p><strong><u>Abstract:</u></strong> People living with dementia (PLwD) often show gradual shifts in how they communicate, becoming less expressive, more repetitive, or drifting off-topic in subtle ways. While caregivers may notice these changes informally, most computational tools are not designed to track such behavioral drift over time. This paper introduces PersonaDrift, a synthetic benchmark designed to evaluate machine learning and statistical methods for detecting progressive changes in daily communication, focusing on user responses to a digital reminder system. PersonaDrift simulates 60-day interaction logs for synthetic users modeled after real PLwD, based on interviews with caregivers. These caregiver-informed personas vary in tone, modality, and communication habits, enabling realistic diversity in behavior. The benchmark focuses on two forms of longitudinal change that caregivers highlighted as particularly salient: flattened sentiment (reduced emotional tone and verbosity) and off-topic replies (semantic drift). These changes are injected progressively at different rates to emulate naturalistic cognitive trajectories, and the framework is designed to be extensible to additional behaviors in future use cases. To explore this novel application space, we evaluate several anomaly detection approaches, unsupervised statistical methods (CUSUM, EWMA, One-Class SVM), sequence models using contextual embeddings (GRU + BERT), and supervised classifiers in both generalized and personalized settings. Preliminary results show that flattened sentiment can often be detected with simple statistical models in users with low baseline variability, while detecting semantic drift requires temporal modeling and personalized baselines. Across both tasks, personalized classifiers consistently outperform generalized ones, highlighting the importance of individual behavioral context.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16432v1" target="_blank"><h2>From generative AI to the brain: five takeaways <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Claudius Gros<br><strong><u>Categories:</u></strong> cs.AI, q-bio.NC<br><strong><u>Comments:</u></strong> Frontiers in Computational Neuroscience, in press<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> The big strides seen in generative AI are not based on somewhat obscure algorithms, but due to clearly defined generative principles. The resulting concrete implementations have proven themselves in large numbers of applications. We suggest that it is imperative to thoroughly investigate which of these generative principles may be operative also in the brain, and hence relevant for cognitive neuroscience. In addition, ML research led to a range of interesting characterizations of neural information processing systems. We discuss five examples, the shortcomings of world modelling, the generation of thought processes, attention, neural scaling laws, and quantization, that illustrate how much neuroscience could potentially learn from ML research.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16430v1" target="_blank"><h2>Graph Neural Networks for Surgical Scene Segmentation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yihan Li, Nikhil Churamani, Maria Robu, Imanol Luengo, Danail Stoyanov<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> 12 pages, 4 figures, 3 tables<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (title, abstract), transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Purpose: Accurate identification of hepatocystic anatomy is critical to preventing surgical complications during laparoscopic cholecystectomy. Deep learning models often struggle with occlusions, long-range dependencies, and capturing the fine-scale geometry of rare structures. This work addresses these challenges by introducing graph-based segmentation approaches that enhance spatial and semantic understanding in surgical scene analyses.
  Methods: We propose two segmentation models integrating Vision Transformer (ViT) feature encoders with Graph Neural Networks (GNNs) to explicitly model spatial relationships between anatomical regions. (1) A static k Nearest Neighbours (k-NN) graph with a Graph Convolutional Network with Initial Residual and Identity Mapping (GCNII) enables stable long-range information propagation. (2) A dynamic Differentiable Graph Generator (DGG) with a Graph Attention Network (GAT) supports adaptive topology learning. Both models are evaluated on the Endoscapes-Seg50 and CholecSeg8k benchmarks.
  Results: The proposed approaches achieve up to 7-8% improvement in Mean Intersection over Union (mIoU) and 6% improvement in Mean Dice (mDice) scores over state-of-the-art baselines. It produces anatomically coherent predictions, particularly on thin, rare and safety-critical structures.
  Conclusion: The proposed graph-based segmentation methods enhance both performance and anatomical consistency in surgical scene segmentation. By combining ViT-based global context with graph-based relational reasoning, the models improve interpretability and reliability, paving the way for safer laparoscopic and robot-assisted surgery through a precise identification of critical anatomical features.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16423v1" target="_blank"><h2>TOFA: Training-Free One-Shot Federated Adaptation for Vision-Language Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Li Zhang, Zhongxuan Han, XiaoHua Feng, Jiaming Zhang, Yuyuan Li, Linbo Jiang, Jianan Lin, Chaochao Chen<br><strong><u>Categories:</u></strong> cs.AI, cs.CL<br><strong><u>Comments:</u></strong> Accepted by AAAI 2026<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Efficient and lightweight adaptation of pre-trained Vision-Language Models (VLMs) to downstream tasks through collaborative interactions between local clients and a central server is a rapidly emerging research topic in federated learning. Existing adaptation algorithms are typically trained iteratively, which incur significant communication costs and increase the susceptibility to potential attacks. Motivated by the one-shot federated training techniques that reduce client-server exchanges to a single round, developing a lightweight one-shot federated VLM adaptation method to alleviate these issues is particularly attractive. However, current one-shot approaches face certain challenges in adapting VLMs within federated settings: (1) insufficient exploitation of the rich multimodal information inherent in VLMs; (2) lack of specialized adaptation strategies to systematically handle the severe data heterogeneity; and (3) requiring additional training resource of clients or server. To bridge these gaps, we propose a novel Training-free One-shot Federated Adaptation framework for VLMs, named TOFA. To fully leverage the generalizable multimodal features in pre-trained VLMs, TOFA employs both visual and textual pipelines to extract task-relevant representations. In the visual pipeline, a hierarchical Bayesian model learns personalized, class-specific prototype distributions. For the textual pipeline, TOFA evaluates and globally aligns the generated local text prompts for robustness. An adaptive weight calibration mechanism is also introduced to combine predictions from both modalities, balancing personalization and robustness to handle data heterogeneity. Our method is training-free, not relying on additional training resources on either the client or server side. Extensive experiments across 9 datasets in various federated settings demonstrate the effectiveness of the proposed TOFA method.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16417v1" target="_blank"><h2>Pharos-ESG: A Framework for Multimodal Parsing, Contextual Narration, and Hierarchical Labeling of ESG Report <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yan Chen, Yu Zou, Jialei Zeng, Haoran You, Xiaorui Zhou, Aixi Zhong<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> Accepted to AAAI 26:main technical track Oral<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Environmental, Social, and Governance (ESG) principles are reshaping the foundations of global financial gover- nance, transforming capital allocation architectures, regu- latory frameworks, and systemic risk coordination mecha- nisms. However, as the core medium for assessing corpo- rate ESG performance, the ESG reports present significant challenges for large-scale understanding, due to chaotic read- ing order from slide-like irregular layouts and implicit hier- archies arising from lengthy, weakly structured content. To address these challenges, we propose Pharos-ESG, a uni- fied framework that transforms ESG reports into structured representations through multimodal parsing, contextual nar- ration, and hierarchical labeling. It integrates a reading-order modeling module based on layout flow, hierarchy-aware seg- mentation guided by table-of-contents anchors, and a multi- modal aggregation pipeline that contextually transforms vi- sual elements into coherent natural language. The framework further enriches its outputs with ESG, GRI, and sentiment labels, yielding annotations aligned with the analytical de- mands of financial research. Extensive experiments on anno- tated benchmarks demonstrate that Pharos-ESG consistently outperforms both dedicated document parsing systems and general-purpose multimodal models. In addition, we release Aurora-ESG, the first large-scale public dataset of ESG re- ports, spanning Mainland China, Hong Kong, and U.S. mar- kets, featuring unified structured representations of multi- modal content, enriched with fine-grained layout and seman- tic annotations to better support ESG integration in financial governance and decision-making.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16374v1" target="_blank"><h2>Unsupervised Graph Neural Network Framework for Balanced Multipatterning in Advanced Electronic Design Automation Layouts <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Abdelrahman Helaly, Nourhan Sakr, Kareem Madkour, Ilhami Torunoglu<br><strong><u>Categories:</u></strong> cs.AR, cs.LG<br><strong><u>Comments:</u></strong> manuscript under review<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (title)<br><p><strong><u>Abstract:</u></strong> Multipatterning is an essential decomposition strategy in electronic design automation (EDA) that overcomes lithographic limitations when printing dense circuit layouts. Although heuristic-based backtracking and SAT solvers can address these challenges, they often struggle to simultaneously handle both complex constraints and secondary objectives. In this study, we present a hybrid workflow that casts multipatterning as a variant of a constrained graph coloring problem with the primary objective of minimizing feature violations and a secondary objective of balancing the number of features on each mask. Our pipeline integrates two main components: (1) A GNN-based agent, trained in an unsupervised manner to generate initial color predictions, which are refined by (2) refinement strategies (a GNN-based heuristic and simulated annealing) that together enhance solution quality and balance. Experimental evaluation in both proprietary data sets and publicly available open source layouts demonstrate complete conflict-free decomposition and consistent color balancing. The proposed framework provides a reproducible, data-efficient and deployable baseline for scalable layout decomposition in EDA workflows.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16353v1" target="_blank"><h2>Learning from Sufficient Rationales: Analysing the Relationship Between Explanation Faithfulness and Token-level Regularisation Strategies <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jonathan Kamp, Lisa Beinborn, Antske Fokkens<br><strong><u>Categories:</u></strong> cs.CL, cs.AI<br><strong><u>Comments:</u></strong> Long paper accepted to the main conference of AACL 2025. Please cite the conference proceedings when available<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Human explanations of natural language, rationales, form a tool to assess whether models learn a label for the right reasons or rely on dataset-specific shortcuts. Sufficiency is a common metric for estimating the informativeness of rationales, but it provides limited insight into the effects of rationale information on model performance. We address this limitation by relating sufficiency to two modelling paradigms: the ability of models to identify which tokens are part of the rationale (through token classification) and the ability of improving model performance by incorporating rationales in the input (through attention regularisation). We find that highly informative rationales are not likely to help classify the instance correctly. Sufficiency conversely captures the classification impact of the non-rationalised context, which interferes with rationale information in the same input. We also find that incorporating rationale information in model inputs can boost cross-domain classification, but results are inconsistent per task and model type. Finally, sufficiency and token classification appear to be unrelated. These results exemplify the complexity of rationales, showing that metrics capable of systematically capturing this type of information merit further investigation.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16346v1" target="_blank"><h2>VersaPants: A Loose-Fitting Textile Capacitive Sensing System for Lower-Body Motion Capture <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Deniz Kasap, Taraneh Aminosharieh Najafi, Jérôme Paul Rémy Thevenot, Jonathan Dan, Stefano Albini, David Atienza<br><strong><u>Categories:</u></strong> eess.SP, cs.LG, eess.SY<br><strong><u>Comments:</u></strong> 14 pages, 8 figures<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> We present VersaPants, the first loose-fitting, textile-based capacitive sensing system for lower-body motion capture, built on the open-hardware VersaSens platform. By integrating conductive textile patches and a compact acquisition unit into a pair of pants, the system reconstructs lower-body pose without compromising comfort. Unlike IMU-based systems that require user-specific fitting or camera-based methods that compromise privacy, our approach operates without fitting adjustments and preserves user privacy. VersaPants is a custom-designed smart garment featuring 6 capacitive channels per leg. We employ a lightweight Transformer-based deep learning model that maps capacitance signals to joint angles, enabling embedded implementation on edge platforms. To test our system, we collected approximately 3.7 hours of motion data from 11 participants performing 16 daily and exercise-based movements. The model achieves a mean per-joint position error (MPJPE) of 11.96 cm and a mean per-joint angle error (MPJAE) of 12.3 degrees across the hip, knee, and ankle joints, indicating the model's ability to generalize to unseen users and movements. A comparative analysis of existing textile-based deep learning architectures reveals that our model achieves competitive reconstruction performance with up to 22 times fewer parameters and 18 times fewer FLOPs, enabling real-time inference at 42 FPS on a commercial smartwatch without quantization. These results position VersaPants as a promising step toward scalable, comfortable, and embedded motion-capture solutions for fitness, healthcare, and wellbeing applications.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16334v1" target="_blank"><h2>OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Kaichen Zhang, Keming Wu, Zuhao Yang, Kairui Hu, Bin Wang, Ziwei Liu, Xingxuan Li, Lidong Bing<br><strong><u>Categories:</u></strong> cs.AI, cs.CL<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Recent advancements in large reasoning models have fueled growing interest in extending such capabilities to multimodal domains. However, despite notable progress in visual reasoning, the lack of transparent and reproducible data curation and training strategies remains a major barrier to scalable research. In this work, we introduce OpenMMReasoner, a fully transparent two-stage recipe for multimodal reasoning spanning supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we construct an 874K-sample cold-start dataset with rigorous step-by-step validation, providing a strong foundation for reasoning capabilities. The subsequent RL stage leverages a 74K-sample dataset across diverse domains to further sharpen and stabilize these abilities, resulting in a more robust and efficient learning process. Extensive evaluations demonstrate that our training recipe not only surpasses strong baselines but also highlights the critical role of data quality and training design in shaping multimodal reasoning performance. Notably, our method achieves a 11.6% improvement over the Qwen2.5-VL-7B-Instruct baseline across nine multimodal reasoning benchmarks, establishing a solid empirical foundation for future large-scale multimodal reasoning research. We open-sourced all our codes, pipeline, and data at https://github.com/EvolvingLMMs-Lab/OpenMMReasoner.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16333v1" target="_blank"><h2>Beyond Generative AI: World Models for Clinical Prediction, Counterfactuals, and Planning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Mohammad Areeb Qazi, Maryam Nadeem, Mohammad Yaqub<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 2 Figures, 1 Table<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> VAE (abstract), multimodal (abstract), transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Healthcare requires AI that is predictive, reliable, and data-efficient. However, recent generative models lack physical foundation and temporal reasoning required for clinical decision support. As scaling language models show diminishing returns for grounded clinical reasoning, world models are gaining traction because they learn multimodal, temporally coherent, and action-conditioned representations that reflect the physical and causal structure of care. This paper reviews World Models for healthcare systems that learn predictive dynamics to enable multistep rollouts, counterfactual evaluation and planning. We survey recent work across three domains: (i) medical imaging and diagnostics (e.g., longitudinal tumor simulation, projection-transition modeling, and Joint Embedding Predictive Architecture i.e., JEPA-style predictive representation learning), (ii) disease progression modeling from electronic health records (generative event forecasting at scale), and (iii) robotic surgery and surgical planning (action-conditioned guidance and control). We also introduce a capability rubric: L1 temporal prediction, L2 action-conditioned prediction, L3 counterfactual rollouts for decision support, and L4 planning/control. Most reviewed systems achieve L1--L2, with fewer instances of L3 and rare L4. We identify cross-cutting gaps that limit clinical reliability; under-specified action spaces and safety constraints, weak interventional validation, incomplete multimodal state construction, and limited trajectory-level uncertainty calibration. This review outlines a research agenda for clinically robust prediction-first world models that integrate generative backbones (transformers, diffusion, VAE) with causal/mechanical foundation for safe decision support in healthcare.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16314v1" target="_blank"><h2>Thermal equilibrium curves of accretion disks driven by magnetorotational instability <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Shigenobu Hirose<br><strong><u>Categories:</u></strong> astro-ph.HE, astro-ph.EP, astro-ph.SR<br><strong><u>Comments:</u></strong> This paper is based on an invited talk presented at the 87th Fujihara Seminar: The 50th Anniversary Workshop of the Disk Instability Model in Compact Binary Stars (DIM50TH2025), held on 22--26 September 2025 in Tomakomai, Japan. It has been accepted for publication in PoS (Proceedings of Science), and is scheduled to appear in February 2026 atthis https URL<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> Analogous to the HR diagram for stars, the thermal equilibrium curve encodes the thermodynamics of accretion disks by expressing the local balance between heating -- primarily via viscous dissipation -- and cooling -- typically through radiative transfer. These curves are commonly plotted as surface density versus effective temperature. When an S-shaped locus appears, local annuli become bistable, and limit-cycle oscillations arise when the external mass-transfer rate falls within an unstable band. This behavior underpins the disk instability model for recurring outbursts in cataclysmic variables. This paper reviews first-principles thermal equilibrium curves for accretion disks driven by magnetorotational instability (MRI), with emphasis on dwarf novae. Unlike the parameterized $α$-viscosity approach, the curves are obtained by solving the governing equations with radiation magnetohydrodynamics simulations, thereby reproducing S-shaped loci without prescribing $α$. The disk instability in dwarf-nova systems and the physical origin of angular-momentum transport (shear stresses) are also briefly reviewed. Notes on the stability of radiation-dominated accretion flows are included in the Appendix.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16256v1" target="_blank"><h2>Accelerating Reionization Constraints: An ANN-Emulator Framework for the SCRIPT Semi-numerical Model <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Saptarshi Sarkar, Tirthankar Roy Choudhury<br><strong><u>Categories:</u></strong> astro-ph.IM, astro-ph.CO<br><strong><u>Comments:</u></strong> 22 pages, 5 figures. To be submitted to JCAP<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Constraining the Epoch of Reionization (EoR) with physically motivated simulations is hampered by the high cost of conventional parameter inference. We present an efficient emulator-based framework that dramatically reduces this bottleneck for the photon-conserving semi-numerical code SCRIPT. Our approach combines (i) a reliable coarse-resolution MCMC to locate the high-likelihood region (exploiting the large-scale convergence of SCRIPT) with (ii) an adaptive, targeted sampling strategy to build a compact high-resolution training set for an artificial neural network based emulator of the model likelihood. With only $\approx 10^3$ high-resolution simulations, the trained emulators achieve excellent predictive accuracy ($R^2 \approx 0.97$--$0.99$) and, when embedded within an MCMC framework, reproduce posterior distributions from full high-resolution runs. Compared to conventional MCMC, our pipeline reduces the number of expensive simulations by a factor of $\sim 100$ and lowers total CPU cost by up to a factor of $\sim 70$, while retaining statistical fidelity. This computational speedup makes inference in much higher-dimensional models tractable (e.g., those needed to incorporate JWST and upcoming 21 cm datasets) and provides a general strategy for building efficient emulators for next generation of EoR constraints.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16248v1" target="_blank"><h2>Revisiting Fairness-aware Interactive Recommendation: Item Lifecycle as a Control Knob <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yun Lu, Xiaoyu Shi, Hong Xie, Chongjun Xia, Zhenhui Gong, Mingsheng Shang<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> 8 pages, 5 figures, conference<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> This paper revisits fairness-aware interactive recommendation (e.g., TikTok, KuaiShou) by introducing a novel control knob, i.e., the lifecycle of items. We make threefold contributions. First, we conduct a comprehensive empirical analysis and uncover that item lifecycles in short-video platforms follow a compressed three-phase pattern, i.e., rapid growth, transient stability, and sharp decay, which significantly deviates from the classical four-stage model (introduction, growth, maturity, decline). Second, we introduce LHRL, a lifecycle-aware hierarchical reinforcement learning framework that dynamically harmonizes fairness and accuracy by leveraging phase-specific exposure dynamics. LHRL consists of two key components: (1) PhaseFormer, a lightweight encoder combining STL decomposition and attention mechanisms for robust phase detection; (2) a two-level HRL agent, where the high-level policy imposes phase-aware fairness constraints, and the low-level policy optimizes immediate user engagement. This decoupled optimization allows for effective reconciliation between long-term equity and short-term utility. Third, experiments on multiple real-world interactive recommendation datasets demonstrate that LHRL significantly improves both fairness and user engagement. Furthermore, the integration of lifecycle-aware rewards into existing RL-based models consistently yields performance gains, highlighting the generalizability and practical value of our approach.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16231v1" target="_blank"><h2>Pass@k Metric for RLVR: A Diagnostic Tool of Exploration, But Not an Objective <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yang Yu<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> The ability of Large Language Models (LLMs) to perform complex, multi-step reasoning is a central focus of modern AI research. To evaluate and enhance this capability, the pass@k metric, which measures the probability of obtaining at least one correct solution in k independent samples, has received significant attention. Its intuitive appeal has led to its adoption not only as an evaluation standard but also as a direct optimization objective in reinforcement learning. In this paper, we analyze the pass@k objective, derive its gradient, and demonstrate that it is fundamentally a per-example positive reweighting of the simpler pass@1 objective. Our analysis reveals that the pass@k objective provides a vanishing learning signal in regimes where exploration is most critical. We further analyze the dynamics of "exploration collapse", showing that as the policy concentrates probability mass, the gap between pass@k and pass@1 diminishes. We conclude that while pass@k is a useful diagnostic tool, it may be an unsuitable direct objective for optimization. Instead, mechanisms explicitly encouraging efficient exploration could offer a more effective path forward for reinforcement learning in reasoning tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16229v1" target="_blank"><h2>Q-MLLM: Vector Quantization for Robust Multimodal Large Language Model Security <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Wei Zhao, Zhe Li, Yige Li, Jun Sun<br><strong><u>Categories:</u></strong> cs.CR, cs.AI<br><strong><u>Comments:</u></strong> Accepted by NDSS 2026<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities in cross-modal understanding, but remain vulnerable to adversarial attacks through visual inputs despite robust textual safety mechanisms. These vulnerabilities arise from two core weaknesses: the continuous nature of visual representations, which allows for gradient-based attacks, and the inadequate transfer of text-based safety mechanisms to visual content. We introduce Q-MLLM, a novel architecture that integrates two-level vector quantization to create a discrete bottleneck against adversarial attacks while preserving multimodal reasoning capabilities. By discretizing visual representations at both pixel-patch and semantic levels, Q-MLLM blocks attack pathways and bridges the cross-modal safety alignment gap. Our two-stage training methodology ensures robust learning while maintaining model utility. Experiments demonstrate that Q-MLLM achieves significantly better defense success rate against both jailbreak attacks and toxic image attacks than existing approaches. Notably, Q-MLLM achieves perfect defense success rate (100\%) against jailbreak attacks except in one arguable case, while maintaining competitive performance on multiple utility benchmarks with minimal inference overhead. This work establishes vector quantization as an effective defense mechanism for secure multimodal AI systems without requiring expensive safety-specific fine-tuning or detection overhead. Code is available at https://github.com/Amadeuszhao/QMLLM.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16226v1" target="_blank"><h2>Deep SOR Minimax Q-learning for Two-player Zero-sum Game <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Saksham Gautam, Lakshmi Mandal, Shalabh Bhatnagar<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> In this work, we consider the problem of a two-player zero-sum game. In the literature, the successive over-relaxation Q-learning algorithm has been developed and implemented, and it is seen to result in a lower contraction factor for the associated Q-Bellman operator resulting in a faster value iteration-based procedure. However, this has been presented only for the tabular case and not for the setting with function approximation that typically caters to real-world high-dimensional state-action spaces. Furthermore, such settings in the case of two-player zero-sum games have not been considered. We thus propose a deep successive over-relaxation minimax Q-learning algorithm that incorporates deep neural networks as function approximators and is suitable for high-dimensional spaces. We prove the finite-time convergence of the proposed algorithm. Through numerical experiments, we show the effectiveness of the proposed method over the existing Q-learning algorithm. Our ablation studies demonstrate the effect of different values of the crucial successive over-relaxation parameter.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16225v1" target="_blank"><h2>Real-Time Inference for Distributed Multimodal Systems under Communication Delay Uncertainty <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Victor Croisfelt, João Henrique Inacio de Souza, Shashi Raj Pandey, Beatriz Soret, Petar Popovski<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 6 pages, 3 figures, submitted to IEEE ICC 2026<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (title)<br><p><strong><u>Abstract:</u></strong> Connected cyber-physical systems perform inference based on real-time inputs from multiple data streams. Uncertain communication delays across data streams challenge the temporal flow of the inference process. State-of-the-art (SotA) non-blocking inference methods rely on a reference-modality paradigm, requiring one modality input to be fully received before processing, while depending on costly offline profiling. We propose a novel, neuro-inspired non-blocking inference paradigm that primarily employs adaptive temporal windows of integration (TWIs) to dynamically adjust to stochastic delay patterns across heterogeneous streams while relaxing the reference-modality requirement. Our communication-delay-aware framework achieves robust real-time inference with finer-grained control over the accuracy-latency tradeoff. Experiments on the audio-visual event localization (AVEL) task demonstrate superior adaptability to network dynamics compared to SotA approaches.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16205v1" target="_blank"><h2>ChemLabs on ChemO: A Multi-Agent System for Multimodal Reasoning on IChO 2025 <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Xu Qiang, Shengyuan Bai, Leqing Chen, Zijing Liu, Yu Li<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> 13 pages, 1 figures<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Olympiad-level benchmarks in mathematics and physics are crucial testbeds for advanced AI reasoning, but chemistry, with its unique multimodal symbolic language, has remained an open challenge. We introduce ChemO, a new benchmark built from the International Chemistry Olympiad (IChO) 2025. ChemO features two key innovations for automated assessment: Assessment-Equivalent Reformulation (AER), which converts problems requiring visual outputs (e.g., drawing molecules) into computationally tractable formats, and Structured Visual Enhancement (SVE), a diagnostic mechanism to disentangle a model's visual perception capabilities from its core chemical reasoning. To tackle this benchmark, we propose ChemLabs, a hierarchical multi-agent framework that mimics human expert collaboration through specialized agents for problem decomposition, perception, reasoning, and auditing. Experiments on state-of-the-art multimodal models demonstrate that combining SVE with our multi-agent system yields dramatic performance gains. Our top configuration achieves a score of 93.6 out of 100, surpassing an estimated human gold medal threshold and establishing a new state-of-the-art in automated chemical problem-solving. ChemO Dataset: https://huggingface.co/datasets/IDEA-AI4SCI/ChemO</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16203v1" target="_blank"><h2>When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yuping Yan, Yuhan Xie, Yinxin Zhang, Lingjuan Lyu, Yaochu Jin<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Vision-Language-Action models (VLAs) have recently demonstrated remarkable progress in embodied environments, enabling robots to perceive, reason, and act through unified multimodal understanding. Despite their impressive capabilities, the adversarial robustness of these systems remains largely unexplored, especially under realistic multimodal and black-box conditions. Existing studies mainly focus on single-modality perturbations and overlook the cross-modal misalignment that fundamentally affects embodied reasoning and decision-making. In this paper, we introduce VLA-Fool, a comprehensive study of multimodal adversarial robustness in embodied VLA models under both white-box and black-box settings. VLA-Fool unifies three levels of multimodal adversarial attacks: (1) textual perturbations through gradient-based and prompt-based manipulations, (2) visual perturbations via patch and noise distortions, and (3) cross-modal misalignment attacks that intentionally disrupt the semantic correspondence between perception and instruction. We further incorporate a VLA-aware semantic space into linguistic prompts, developing the first automatically crafted and semantically guided prompting framework. Experiments on the LIBERO benchmark using a fine-tuned OpenVLA model reveal that even minor multimodal perturbations can cause significant behavioral deviations, demonstrating the fragility of embodied multimodal alignment.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16201v1" target="_blank"><h2>From Performance to Understanding: A Vision for Explainable Automated Algorithm Design <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Niki van Stein, Anna V. Kononova, Thomas Bäck<br><strong><u>Categories:</u></strong> cs.AI, cs.NE<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> explainable (title, abstract)<br><p><strong><u>Abstract:</u></strong> Automated algorithm design is entering a new phase: Large Language Models can now generate full optimisation (meta)heuristics, explore vast design spaces and adapt through iterative feedback. Yet this rapid progress is largely performance-driven and opaque. Current LLM-based approaches rarely reveal why a generated algorithm works, which components matter or how design choices relate to underlying problem structures. This paper argues that the next breakthrough will come not from more automation, but from coupling automation with understanding from systematic benchmarking. We outline a vision for explainable automated algorithm design, built on three pillars: (i) LLM-driven discovery of algorithmic variants, (ii) explainable benchmarking that attributes performance to components and hyperparameters and (iii) problem-class descriptors that connect algorithm behaviour to landscape structure. Together, these elements form a closed knowledge loop in which discovery, explanation and generalisation reinforce each other. We argue that this integration will shift the field from blind search to interpretable, class-specific algorithm design, accelerating progress while producing reusable scientific insight into when and why optimisation strategies succeed.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16191v1" target="_blank"><h2>CausalMamba: Interpretable State Space Modeling for Temporal Rumor Causality <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Xiaotong Zhan, Xi Cheng<br><strong><u>Categories:</u></strong> cs.LG, cs.SI<br><strong><u>Comments:</u></strong> Preprint. 9 pages, 3 figures, 2 tables. Code and implementation details available at:this https URL<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), explainable (abstract), causality (title)<br><p><strong><u>Abstract:</u></strong> Rumor detection on social media remains a challenging task due to the complex propagation dynamics and the limited interpretability of existing models. While recent neural architectures capture content and structural features, they often fail to reveal the underlying causal mechanisms of misinformation spread. We propose CausalMamba, a novel framework that integrates Mamba-based sequence modeling, graph convolutional networks (GCNs), and differentiable causal discovery via NOTEARS. CausalMamba learns joint representations of temporal tweet sequences and reply structures, while uncovering latent causal graphs to identify influential nodes within each propagation chain. Experiments on the Twitter15 dataset show that our model achieves competitive classification performance compared to strong baselines, and uniquely enables counterfactual intervention analysis. Qualitative results demonstrate that removing top-ranked causal nodes significantly alters graph connectivity, offering interpretable insights into rumor dynamics. Our framework provides a unified approach for rumor classification and influence analysis, paving the way for more explainable and actionable misinformation detection systems.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16183v1" target="_blank"><h2>FOOTPASS: A Multi-Modal Multi-Agent Tactical Context Dataset for Play-by-Play Action Spotting in Soccer Broadcast Videos <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jeremie Ochin, Raphael Chekroun, Bogdan Stanciulescu, Sotiris Manitsaris<br><strong><u>Categories:</u></strong> cs.AI, cs.CV<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), multi-modal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Soccer video understanding has motivated the creation of datasets for tasks such as temporal action localization, spatiotemporal action detection (STAD), or multiobject tracking (MOT). The annotation of structured sequences of events (who does what, when, and where) used for soccer analytics requires a holistic approach that integrates both STAD and MOT. However, current action recognition methods remain insufficient for constructing reliable play-by-play data and are typically used to assist rather than fully automate annotation. Parallel research has advanced tactical modeling, trajectory forecasting, and performance analysis, all grounded in game-state and play-by-play data. This motivates leveraging tactical knowledge as a prior to support computer-vision-based predictions, enabling more automated and reliable extraction of play-by-play data. We introduce Footovision Play-by-Play Action Spotting in Soccer Dataset (FOOTPASS), the first benchmark for play-by-play action spotting over entire soccer matches in a multi-modal, multi-agent tactical context. It enables the development of methods for player-centric action spotting that exploit both outputs from computer-vision tasks (e.g., tracking, identification) and prior knowledge of soccer, including its tactical regularities over long time horizons, to generate reliable play-by-play data streams. These streams form an essential input for data-driven sports analytics.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16175v1" target="_blank"><h2>Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yi Yang, Xueqi Li, Yiyang Chen, Jin Song, Yihan Wang, Zipeng Xiao, Jiadi Su, You Qiaoben, Pengfei Liu, Zhijie Deng<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Recent advances in Vision-Language-Action (VLA) models demonstrate that visual signals can effectively complement sparse action supervisions. However, letting VLA directly predict high-dimensional visual states can distribute model capacity and incur prohibitive training cost, while compressing visual states into more compact supervisory signals inevitably incurs information bottlenecks. Moreover, existing methods often suffer from poor comprehension and reasoning capabilities due to the neglect of language supervision. This paper introduces Mantis, a novel framework featuring a Disentangled Visual Foresight (DVF) to tackle these issues. Specifically, Mantis decouples visual foresight prediction from the backbone with the combination of meta queries and a diffusion Transformer (DiT) head. With the current visual state provided to the DiT via a residual connection, a simple next-state prediction objective enables the meta queries to automatically capture the latent actions that delineate the visual trajectory, and hence boost the learning of explicit actions. The disentanglement reduces the burden of the VLA backbone, enabling it to maintain comprehension and reasoning capabilities through language supervision. Empirically, pretrained on human manipulation videos, robot demonstrations, and image-text pairs, Mantis achieves a 96.7% success rate on LIBERO benchmark after fine-tuning, surpassing powerful baselines while exhibiting high convergence speed. Real-world evaluations show that Mantis outperforms $π_{0.5}$, a leading open-source VLA model, particularly in instruction-following capability, generalization to unseen instructions, and reasoning ability. Code and weights are released to support the open-source community.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16149v1" target="_blank"><h2>Approximation rates of quantum neural networks for periodic functions via Jackson's inequality <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Ariel Neufeld, Philipp Schmocker, Viet Khoa Tran<br><strong><u>Categories:</u></strong> quant-ph, cs.LG, math.NA, stat.ML<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> Quantum neural networks (QNNs) are an analog of classical neural networks in the world of quantum computing, which are represented by a unitary matrix with trainable parameters. Inspired by the universal approximation property of classical neural networks, ensuring that every continuous function can be arbitrarily well approximated uniformly on a compact set of a Euclidean space, some recent works have established analogous results for QNNs, ranging from single-qubit to multi-qubit QNNs, and even hybrid classical-quantum models. In this paper, we study the approximation capabilities of QNNs for periodic functions with respect to the supremum norm. We use the Jackson inequality to approximate a given function by implementing its approximating trigonometric polynomial via a suitable QNN. In particular, we see that by restricting to the class of periodic functions, one can achieve a quadratic reduction of the number of parameters, producing better approximation results than in the literature. Moreover, the smoother the function, the fewer parameters are needed to construct a QNN to approximate the function.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16148v1" target="_blank"><h2>Enhancing Nuclear Reactor Core Simulation through Data-Based Surrogate Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Perceval Beja-Battais, Alain Grossetête, Nicolas Vayatis<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> In recent years, there has been an increasing need for Nuclear Power Plants (NPPs) to improve flexibility in order to match the rapid growth of renewable energies. The Operator Assistance Predictive System (OAPS) developed by Framatome addresses this problem through Model Predictive Control (MPC). In this work, we aim to improve MPC methods through data-driven simulation schemes. Thus, from a set of nonlinear stiff ordinary differential equations (ODEs), this paper introduces two surrogate models acting as alternative simulation schemes to enhance nuclear reactor core simulation. We show that both data-driven and physics-informed models can rapidly integrate complex dynamics, with a very low computational time (up to 1000x time reduction).</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16145v1" target="_blank"><h2>Labels Matter More Than Models: Quantifying the Benefit of Supervised Time Series Anomaly Detection <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zhijie Zhong, Zhiwen Yu, Kaixiang Yang, C. L. Philip Chen<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 16 pages, 14 figures, 7 tables. Under review<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> anomaly detection (title, abstract)<br><p><strong><u>Abstract:</u></strong> Time series anomaly detection (TSAD) is a critical data mining task often constrained by label scarcity. Consequently, current research predominantly focuses on Unsupervised Time-series Anomaly Detection (UTAD), relying on complex architectures to model normal data distributions. However, this approach often overlooks the significant performance gains available from limited anomaly labels achievable in practical scenarios. This paper challenges the premise that architectural complexity is the optimal path for TSAD. We conduct the first methodical comparison between supervised and unsupervised paradigms and introduce STAND, a streamlined supervised baseline. Extensive experiments on five public datasets demonstrate that: (1) Labels matter more than models: under a limited labeling budget, simple supervised models significantly outperform complex state-of-the-art unsupervised methods; (2) Supervision yields higher returns: the performance gain from minimal supervision far exceeds that from architectural innovations; and (3) Practicality: STAND exhibits superior prediction consistency and anomaly localization compared to unsupervised counterparts. These findings advocate for a data-centric shift in TSAD research, emphasizing label utilization over purely algorithmic complexity. The code is publicly available at https://github.com/EmorZz1G/STAND.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16139v1" target="_blank"><h2>Multidimensional Rubric-oriented Reward Model Learning via Geometric Projection Reference Constraints <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yongnan Jin, Xurui Li, Feng Cao, Liucun Gao, Juanjuan Yao<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> The integration of large language models (LLMs) into medical practice holds transformative potential, yet their real-world clinical utility remains limited by critical alignment challenges: (1) a disconnect between static evaluation benchmarks and dynamic clinical cognitive needs, (2) difficulties in adapting to evolving, multi-source medical standards, and (3) the inability of conventional reward models to capture nuanced, multi-dimensional medical quality criteria. To address these gaps, we propose MR-RML (Multidimensional Rubric-oriented Reward Model Learning) via GPRC (Geometric Projection Reference Constraints), a novel alignment framework that integrates medical standards into a structured "Dimensions-Scenarios-Disciplines" matrix to guide data generation and model optimization. MR-RML introduces three core innovations: (1) a "Dimensions-Scenarios-Disciplines" medical standard system that embeds domain standards into the full training pipeline; (2) an independent multi-dimensional reward model that decomposes evaluation criteria, shifting from real-time rubric-based scoring to internalized reward modeling for improved consistency and cost-efficiency; (3) geometric projection reference constraints that transform medical cognitive logic into mathematical regularization, aligning scoring gradients with clinical reasoning and enabling synthetic data-driven training. Through extensive evaluations on the authoritative medical benchmark Healthbench, our method yields substantial performance gains over the base LLM Qwen-32B (45% on the full subset and 85% on Hard subset, respectively). It achieves a SOTA among open-source LLMs with scores of 62.7 (full subset) and 44.7 (hard subset), while also outperforming the majority of closed-source models.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16130v1" target="_blank"><h2>Creation of Viscous Dark Energy by the Hubble Flow: Comparison with SNe Ia Master Sample Binned Data <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Iolanda Navonea, Maria Giovanna Dainotti, Elisa Fazzari, Giovanni Montani, Naoto Maki<br><strong><u>Categories:</u></strong> astro-ph.CO<br><strong><u>Comments:</u></strong> 14 pages, 10 figures<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> We study a cosmological model featuring evolutionary dark energy, according to the idea that the creation of its constituents arises from the gravitational field of the expanding universe, whose non-equilibrium physics is described by a non-zero bulk viscosity coefficient. This physical scenario calls for two additional parameters with respect to the ΛCDM model, one of which is the equation of state parameter of the created dark energy. The model is constrained by the requirement that its deceleration parameter coincides with the one predicted by the ΛCDM model. Then, we construct the effective running Hubble constant, a theoretical function that corresponds to the ratio of the Hubble parameter in our model to the ΛCDM expansion rate. The model's theoretical predictions for the effective running Hubble constant are compared with the binned data of the Supernovae Ia Master Sample. The comparison is performed by a MCMC procedure for each bin, with three parameters left free to vary, while the particle creation rate is taken from a grid of values, each of which is fixed in the given MCMC run. The most important result emerging from this analysis is that the created dark energy constituent corresponds to an equation of state parameter with phantom character. Only if particle creation is removed do the dark energy constituents acquire a quintessence character. No matter the intrinsic nature of the constituents, their effective z-dependent equation of state parameter is, both with and without considering particle creation, entirely of phantom nature across the considered redshift range.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16105v1" target="_blank"><h2>Pathlet Variational Auto-Encoder for Robust Trajectory Generation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yuanbo Tang, Yan Tang, Zixuan Zhang, Zihui Zhao, Yang Li<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> variational autoencoder (abstract), VAE (abstract)<br><p><strong><u>Abstract:</u></strong> Trajectory generation has recently drawn growing interest in privacy-preserving urban mobility studies and location-based service applications. Although many studies have used deep learning or generative AI methods to model trajectories and have achieved promising results, the robustness and interpretability of such models are largely unexplored. This limits the application of trajectory generation algorithms on noisy real-world data and their trustworthiness in downstream tasks. To address this issue, we exploit the regular structure in urban trajectories and propose a deep generative model based on the pathlet representation, which encode trajectories with binary vectors associated with a learned dictionary of trajectory segments. Specifically, we introduce a probabilistic graphical model to describe the trajectory generation process, which includes a Variational Autoencoder (VAE) component and a linear decoder component. During training, the model can simultaneously learn the latent embedding of pathlet representations and the pathlet dictionary that captures mobility patterns in the trajectory dataset. The conditional version of our model can also be used to generate customized trajectories based on temporal and spatial constraints.
  Our model can effectively learn data distribution even using noisy data, achieving relative improvements of $35.4\%$ and $26.3\%$ over strong baselines on two real-world trajectory datasets. Moreover, the generated trajectories can be conveniently utilized for multiple downstream tasks, including trajectory prediction and data denoising. Lastly, the framework design offers a significant efficiency advantage, saving $64.8\%$ of the time and $56.5\%$ of GPU memory compared to previous approaches.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16101v1" target="_blank"><h2>HybSpecNet: A Critical Analysis of Architectural Instability in Hybrid-Domain Spectral GNNs <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Huseyin Goksu<br><strong><u>Categories:</u></strong> cs.LG, eess.SP<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Spectral Graph Neural Networks offer a principled approach to graph filtering but face a fundamental "Stability-vs-Adaptivity" trade-off. This trade-off is dictated by the choice of spectral domain. Filters in the finite [-1, 1] domain (e.g., ChebyNet) are numerically stable at high polynomial degrees (K) but are static and low-pass, causing them to fail on heterophilic graphs. Conversely, filters in the semi-infinite [0, infty) domain (e.g., KrawtchoukNet) are highly adaptive and achieve SOTA results on heterophily by learning non-low-pass responses. However, as we demonstrate, these adaptive filters can also suffer from numerical instability, leading to catastrophic performance collapse at high K. In this paper, we propose to resolve this trade-off by designing a hybrid-domain GNN, HybSpecNet, which combines a stable `ChebyNet` branch with an adaptive `KrawtchoukNet` branch. We first demonstrate that a "naive" hybrid architecture, which fuses the branches via concatenation, successfully unifies performance at low K, achieving strong results on both homophilic and heterophilic benchmarks. However, we then prove that this naive architecture fails the stability test. Our K-ablation experiments show that this architecture catastrophically collapses at K=25, exactly mirroring the collapse of its unstable `KrawtchoukNet` branch. We identify this critical finding as "Instability Poisoning," where `NaN`/`Inf` gradients from the adaptive branch destroy the training of the model. Finally, we propose and validate an advanced architecture that uses "Late Fusion" to completely isolate the gradient pathways. We demonstrate that this successfully solves the instability problem, remaining perfectly stable up to K=30 while retaining its SOTA performance across all graph types. This work identifies a critical architectural pitfall in hybrid GNN design and provides the robust architectural solution.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16087v1" target="_blank"><h2>AssayMatch: Learning to Select Data for Molecular Activity Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Vincent Fan, Regina Barzilay<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> The performance of machine learning models in drug discovery is highly dependent on the quality and consistency of the underlying training data. Due to limitations in dataset sizes, many models are trained by aggregating bioactivity data from diverse sources, including public databases such as ChEMBL. However, this approach often introduces significant noise due to variability in experimental protocols. We introduce AssayMatch, a framework for data selection that builds smaller, more homogenous training sets attuned to the test set of interest. AssayMatch leverages data attribution methods to quantify the contribution of each training assay to model performance. These attribution scores are used to finetune language embeddings of text-based assay descriptions to capture not just semantic similarity, but also the compatibility between assays. Unlike existing data attribution methods, our approach enables data selection for a test set with unknown labels, mirroring real-world drug discovery campaigns where the activities of candidate molecules are not known in advance. At test time, embeddings finetuned with AssayMatch are used to rank all available training data. We demonstrate that models trained on data selected by AssayMatch are able to surpass the performance of the model trained on the complete dataset, highlighting its ability to effectively filter out harmful or noisy experiments. We perform experiments on two common machine learning architectures and see increased prediction capability over a strong language-only baseline for 9/12 model-target pairs. AssayMatch provides a data-driven mechanism to curate higher-quality datasets, reducing noise from incompatible experiments and improving the predictive power and data efficiency of models for drug discovery. AssayMatch is available at https://github.com/Ozymandias314/AssayMatch.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16062v1" target="_blank"><h2>Gauge-Equivariant Graph Networks via Self-Interference Cancellation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yoonhyuk Choi, Chong-Kwon Kim<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Graph Neural Networks (GNNs) excel on homophilous graphs but often fail under heterophily due to self-reinforcing and phase-inconsistent signals. We propose a Gauge-Equivariant Graph Network with Self-Interference Cancellation (GESC), which replaces additive aggregation with a projection-based interference mechanism. Unlike prior magnetic or gauge-equivariant GNNs that typically focus on phase handling in spectral filtering while largely relying on scalar weighting, GESC introduces a $\mathrm{U}(1)$ phase connection followed by a rank-1 projection that attenuates self-parallel components before attention. A sign- and phase-aware gate further regulates neighbor influence, attenuating components aligned with current node states and acting as a local notch on low-frequency modes. Across diverse graph benchmarks, our method consistently outperforms recent state-of-the-art models while offering a unified, interference-aware view of message passing. Our code is available at \href{here}{https://anonymous.4open.science/r/GESC-1B22}.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16060v1" target="_blank"><h2>Neuromorphic Astronomy: An End-to-End SNN Pipeline for RFI Detection Hardware <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Nicholas J. Pritchard, Andreas Wicenec, Richard Dodson, Mohammed Bennamoun, Dylan R. Muir<br><strong><u>Categories:</u></strong> cs.NE, astro-ph.IM<br><strong><u>Comments:</u></strong> 21 pages, 4 figures, 12 tables<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Imminent radio telescope observatories provide massive data rates making deep learning based processing appealing while simultaneously demanding real-time performance at low-energy; prohibiting the use of many artificial neural network based approaches. We begin tackling the scientifically existential challenge of Radio Frequency Interference (RFI) detection by deploying deep Spiking Neural Networks (SNNs) on resource-constrained neuromorphic hardware. Our approach partitions large, pre-trained networks onto SynSense Xylo hardware using maximal splitting, a novel greedy algorithm. We validate this pipeline with on-chip power measurements, achieving instrument-scaled inference at 100mW. While our full-scale SNN achieves state-of-the-art accuracy among SNN baselines, our experiments reveal a more important insight that a smaller un-partitioned model significantly outperforms larger, split models. This finding highlights that hardware co-design is paramount for optimal performance. Our work thus provides a practical deployment blueprint, a key insight into the challenges of model scaling, and reinforces radio astronomy as a demanding yet ideal domain for advancing applied neuromorphic computing.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16048v1" target="_blank"><h2>Semantic Glitch: Agency and Artistry in an Autonomous Pixel Cloud <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Qing Zhang, Jing Huang, Mingyang Xu, Jun Rekimoto<br><strong><u>Categories:</u></strong> cs.RO, cs.AI, cs.HC<br><strong><u>Comments:</u></strong> NeurIPS 2025 Creative AI Track, The Thirty-Ninth Annual Conference on Neural Information Processing Systems<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> While mainstream robotics pursues metric precision and flawless performance, this paper explores the creative potential of a deliberately "lo-fi" approach. We present the "Semantic Glitch," a soft flying robotic art installation whose physical form, a 3D pixel style cloud, is a "physical glitch" derived from digital archaeology. We detail a novel autonomous pipeline that rejects conventional sensors like LiDAR and SLAM, relying solely on the qualitative, semantic understanding of a Multimodal Large Language Model to navigate. By authoring a bio-inspired personality for the robot through a natural language prompt, we create a "narrative mind" that complements the "weak," historically, loaded body. Our analysis begins with a 13-minute autonomous flight log, and a follow-up study statistically validates the framework's robustness for authoring quantifiably distinct personas. The combined analysis reveals emergent behaviors, from landmark-based navigation to a compelling "plan to execution" gap, and a character whose unpredictable, plausible behavior stems from a lack of precise proprioception. This demonstrates a lo-fi framework for creating imperfect companions whose success is measured in character over efficiency.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16027v1" target="_blank"><h2>HGCN2SP: Hierarchical Graph Convolutional Network for Two-Stage Stochastic Programming <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yang Wu, Yifan Zhang, Zhenxing Liang, Jian Cheng<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 17 pages, 4 figures<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> convolutional (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Two-stage Stochastic Programming (2SP) is a standard framework for modeling decision-making problems under uncertainty. While numerous methods exist, solving such problems with many scenarios remains challenging. Selecting representative scenarios is a practical method for accelerating solutions. However, current approaches typically rely on clustering or Monte Carlo sampling, failing to integrate scenario information deeply and overlooking the significant impact of the scenario order on solving time. To address these issues, we develop HGCN2SP, a novel model with a hierarchical graph designed for 2SP problems, encoding each scenario and modeling their relationships hierarchically. The model is trained in a reinforcement learning paradigm to utilize the feedback of the solver. The policy network is equipped with a hierarchical graph convolutional network for feature encoding and an attention-based decoder for scenario selection in proper order. Evaluation of two classic 2SP problems demonstrates that HGCN2SP provides high-quality decisions in a short computational time. Furthermore, HGCN2SP exhibits remarkable generalization capabilities in handling large-scale instances, even with a substantial number of variables or scenarios that were unseen during the training phase.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16026v1" target="_blank"><h2>Towards a Safer and Sustainable Manufacturing Process: Material classification in Laser Cutting Using Deep Learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Mohamed Abdallah Salem, Hamdy Ahmed Ashur, Ahmed Elshinnawy<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.LG, cs.RO<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Laser cutting is a widely adopted technology in material processing across various industries, but it generates a significant amount of dust, smoke, and aerosols during operation, posing a risk to both the environment and workers' health. Speckle sensing has emerged as a promising method to monitor the cutting process and identify material types in real-time. This paper proposes a material classification technique using a speckle pattern of the material's surface based on deep learning to monitor and control the laser cutting process. The proposed method involves training a convolutional neural network (CNN) on a dataset of laser speckle patterns to recognize distinct material types for safe and efficient cutting. Previous methods for material classification using speckle sensing may face issues when the color of the laser used to produce the speckle pattern is changed. Experiments conducted in this study demonstrate that the proposed method achieves high accuracy in material classification, even when the laser color is changed. The model achieved an accuracy of 98.30 % on the training set and 96.88% on the validation set. Furthermore, the model was evaluated on a set of 3000 new images for 30 different materials, achieving an F1-score of 0.9643. The proposed method provides a robust and accurate solution for material-aware laser cutting using speckle sensing.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16020v1" target="_blank"><h2>Physically Realistic Sequence-Level Adversarial Clothing for Robust Human-Detection Evasion <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Dingkun Zhou, Patrick P. K. Chan, Hengxu Wu, Shikang Zheng, Ruiqi Huang, Yuanjie Zhao<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Deep neural networks used for human detection are highly vulnerable to adversarial manipulation, creating safety and privacy risks in real surveillance environments. Wearable attacks offer a realistic threat model, yet existing approaches usually optimize textures frame by frame and therefore fail to maintain concealment across long video sequences with motion, pose changes, and garment deformation. In this work, a sequence-level optimization framework is introduced to generate natural, printable adversarial textures for shirts, trousers, and hats that remain effective throughout entire walking videos in both digital and physical settings. Product images are first mapped to UV space and converted into a compact palette and control-point parameterization, with ICC locking to keep all colors printable. A physically based human-garment pipeline is then employed to simulate motion, multi-angle camera viewpoints, cloth dynamics, and illumination variation. An expectation-over-transformation objective with temporal weighting is used to optimize the control points so that detection confidence is minimized across whole sequences. Extensive experiments demonstrate strong and stable concealment, high robustness to viewpoint changes, and superior cross-model transferability. Physical garments produced with sublimation printing achieve reliable suppression under indoor and outdoor recordings, confirming real-world feasibility.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16014v1" target="_blank"><h2>MUSEKG: A Knowledge Graph Over Museum Collections <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jinhao Li, Jianzhong Qi, Soyeon Caren Han, Eun-Jung Holden<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Digital transformation in the cultural heritage sector has produced vast yet fragmented collections of artefact data. Existing frameworks for museum information systems struggle to integrate heterogeneous metadata, unstructured documents, and multimodal artefacts into a coherent and queryable form. We present MuseKG, an end-to-end knowledge-graph framework that unifies structured and unstructured museum data through symbolic-neural integration. MuseKG constructs a typed property graph linking objects, people, organisations, and visual or textual labels, and supports natural language queries. Evaluations on real museum collections demonstrate robust performance across queries over attributes, relations, and related entities, surpassing large-language-model zero-shot, few-shot and SPARQL prompt baselines. The results highlight the importance of symbolic grounding for interpretable and scalable cultural heritage reasoning, and pave the way for web-scale integration of digital heritage knowledge.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16013v1" target="_blank"><h2>Physics-Guided Inductive Spatiotemporal Kriging for PM2.5 with Satellite Gradient Constraints <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Shuo Wang, Mengfan Teng, Yun Cheng, Lothar Thiele, Olga Saukh, Shuangshuang He, Yuanting Zhang, Jiang Zhang, Gangfeng Zhang, Xingyuan Yuan, Jingfang Fan<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> High-resolution mapping of fine particulate matter (PM2.5) is a cornerstone of sustainable urbanism but remains critically hindered by the spatial sparsity of ground monitoring networks. While traditional data-driven methods attempt to bridge this gap using satellite Aerosol Optical Depth (AOD), they often suffer from severe, non-random data missingness (e.g., due to cloud cover or nighttime) and inversion biases. To overcome these limitations, this study proposes the Spatiotemporal Physics-Guided Inference Network (SPIN), a novel framework designed for inductive spatiotemporal kriging. Unlike conventional approaches, SPIN synergistically integrates domain knowledge into deep learning by explicitly modeling physical advection and diffusion processes via parallel graph kernels. Crucially, we introduce a paradigm-shifting training strategy: rather than using error-prone AOD as a direct input, we repurpose it as a spatial gradient constraint within the loss function. This allows the model to learn structural pollution patterns from satellite data while remaining robust to data voids. Validated in the highly polluted Beijing-Tianjin-Hebei and Surrounding Areas (BTHSA), SPIN achieves a new state-of-the-art with a Mean Absolute Error (MAE) of 9.52 ug/m^3, effectively generating continuous, physically plausible pollution fields even in unmonitored areas. This work provides a robust, low-cost, and all-weather solution for fine-grained environmental management.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16006v1" target="_blank"><h2>Synergizing Deconfounding and Temporal Generalization For Time-series Counterfactual Outcome Estimation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yiling Liu, Juncheng Dong, Chen Fu, Wei Shi, Ziyang Jiang, Zhigang Hua, David Carlson<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> latent space (abstract)<br><p><strong><u>Abstract:</u></strong> Estimating counterfactual outcomes from time-series observations is crucial for effective decision-making, e.g. when to administer a life-saving treatment, yet remains significantly challenging because (i) the counterfactual trajectory is never observed and (ii) confounders evolve with time and distort estimation at every step. To address these challenges, we propose a novel framework that synergistically integrates two complementary approaches: Sub-treatment Group Alignment (SGA) and Random Temporal Masking (RTM). Instead of the coarse practice of aligning marginal distributions of the treatments in latent space, SGA uses iterative treatment-agnostic clustering to identify fine-grained sub-treatment groups. Aligning these fine-grained groups achieves improved distributional matching, thus leading to more effective deconfounding. We theoretically demonstrate that SGA optimizes a tighter upper bound on counterfactual risk and empirically verify its deconfounding efficacy. RTM promotes temporal generalization by randomly replacing input covariates with Gaussian noises during training. This encourages the model to rely less on potentially noisy or spuriously correlated covariates at the current step and more on stable historical patterns, thereby improving its ability to generalize across time and better preserve underlying causal relationships. Our experiments demonstrate that while applying SGA and RTM individually improves counterfactual outcome estimation, their synergistic combination consistently achieves state-of-the-art performance. This success comes from their distinct yet complementary roles: RTM enhances temporal generalization and robustness across time steps, while SGA improves deconfounding at each specific time point.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15997v1" target="_blank"><h2>Sensorium Arc: AI Agent System for Oceanic Data Exploration and Interactive Eco-Art <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Noah Bissell, Ethan Paley, Joshua Harrison, Juliano Calil, Myungin Lee<br><strong><u>Categories:</u></strong> cs.AI, cs.MM<br><strong><u>Comments:</u></strong> (to appear) NeurIPS 2025 Creative AI Track<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Sensorium Arc (AI reflects on climate) is a real-time multimodal interactive AI agent system that personifies the ocean as a poetic speaker and guides users through immersive explorations of complex marine data. Built on a modular multi-agent system and retrieval-augmented large language model (LLM) framework, Sensorium enables natural spoken conversations with AI agents that embodies the ocean's perspective, generating responses that blend scientific insight with ecological poetics. Through keyword detection and semantic parsing, the system dynamically triggers data visualizations and audiovisual playback based on time, location, and thematic cues drawn from the dialogue. Developed in collaboration with the Center for the Study of the Force Majeure and inspired by the eco-aesthetic philosophy of Newton Harrison, Sensorium Arc reimagines ocean data not as an abstract dataset but as a living narrative. The project demonstrates the potential of conversational AI agents to mediate affective, intuitive access to high-dimensional environmental data and proposes a new paradigm for human-machine-ecosystem.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15986v1" target="_blank"><h2>Fairness in Multi-modal Medical Diagnosis with Demonstration Selection <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Dawei Li, Zijian Gu, Peng Wang, Chuhan Song, Zhen Tan, Mohan Zhang, Tianlong Chen, Yu Tian, Song Wang<br><strong><u>Categories:</u></strong> cs.CV, cs.CY, cs.LG<br><strong><u>Comments:</u></strong> 10 pages (including 2 pages of references), 4 figures. This work explores fairness in multi-modal medical image reasoning using in-context learning<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract), multi-modal (title)<br><p><strong><u>Abstract:</u></strong> Multimodal large language models (MLLMs) have shown strong potential for medical image reasoning, yet fairness across demographic groups remains a major concern. Existing debiasing methods often rely on large labeled datasets or fine-tuning, which are impractical for foundation-scale models. We explore In-Context Learning (ICL) as a lightweight, tuning-free alternative for improving fairness. Through systematic analysis, we find that conventional demonstration selection (DS) strategies fail to ensure fairness due to demographic imbalance in selected exemplars. To address this, we propose Fairness-Aware Demonstration Selection (FADS), which builds demographically balanced and semantically relevant demonstrations via clustering-based sampling. Experiments on multiple medical imaging benchmarks show that FADS consistently reduces gender-, race-, and ethnicity-related disparities while maintaining strong accuracy, offering an efficient and scalable path toward fair medical image reasoning. These results highlight the potential of fairness-aware in-context learning as a scalable and data-efficient solution for equitable medical image reasoning.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15975v1" target="_blank"><h2>SN 2019vxm: A Shocking Coincidence between Fermi and TESS <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zachary G. Lane, Ryan Ridden-Harper, Sofia Rest, Armin Rest, Conor L. Ransome, Qinan Wang, Clarinda Montilla, Micaela Steed, Igor Andreoni, Patrick Armstrong, Peter J. Brown, Jeffrey Cooke, David A. Coulter, Ori Fox, James Freeburn, Marco Galoppo, Avishay Gal-Yam, Jared A. Goldberg, Christopher Harvey-Hawes, Rebekah Hounsell, Brayden Leicester, Itai Linial, Thomas Moore, Pierre Mourier, Anya E. Nugent, David O'Neill, Hugh Roxburgh, Koji Shukawa, Stephen J. Smartt, Nathan Smith, Ken W. Smith, Sebastian Vergara Carrasco, V. Ashley Villar, Tal Wasserman, Zenati Yossef, Erez Zimmerman<br><strong><u>Categories:</u></strong> astro-ph.HE<br><strong><u>Comments:</u></strong> 25 pages, 9 figures. Submitted to The Astrophysical Journal<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> Shock breakout and, in some cases, jet-driven high-energy emission are increasingly recognized as key signatures of the earliest phases of core-collapse supernovae, especially in Type IIn systems due to their dense, interaction-dominated circumstellar environments. We present a comprehensive photometric analysis of SN 2019vxm, a long-duration, luminous Type IIn supernova, $M_V^{}=-21.41\pm0.05\;{\rm mag}$, observed from X-ray to near-infrared. SN 2019vxm is the first superluminous supernovae Type IIn to be caught with well-sampled TESS photometric data on the rise and has a convincing coincident X-ray source at the time of first light. The high-cadence TESS light curve captures the early-time rise, which is well described by a broken power law with an index of $n=1.41\pm0.04$, significantly shallower than the canonical $n=2$ behavior. From this, we constrain the time of first light to within 7.2 hours. We identify a spatial and temporal coincidence between SN 2019vxm and the X-ray transient GRB191117A, corresponding to a $3.3σ$ association confidence. Both the short-duration X-ray event and the lightcurve modeling are consistent with shock breakout into a dense, asymmetric circumstellar medium, indicative of a massive, compact progenitor such as a luminous blue variable transitioning to Wolf-Rayet phase embedded in a clumpy, asymmetric environment.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15974v1" target="_blank"><h2>KRAL: Knowledge and Reasoning Augmented Learning for LLM-assisted Clinical Antimicrobial Therapy <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zhe Li, Yehan Qiu, Yujie Chen, Xiang Zhou<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> data augmentation (abstract)<br><p><strong><u>Abstract:</u></strong> Clinical antimicrobial therapy requires the dynamic integration of pathogen profiles, host factors, pharmacological properties of antimicrobials, and the severity of infection.This complexity imposes fundamental limitations on the applicability of Large Language Models (LLMs) in high-stakes clinical decision-making including knowledge gaps, data privacy concerns, high deployment costs, and limited reasoning capabilities. To address these challenges, we propose KRAL (Knowledge and Reasoning Augmented Learning), a low-cost, scalable, privacy-preserving paradigm that leverages teacher-model reasoning to automatically distill knowledge and reasoning trajectories via answer-to-question reverse generation, employs heuristic learning for semi-supervised data augmentation (reducing manual annotation requirements by approximately 80%), and utilizes agentic reinforcement learning to jointly enhance medical knowledge and reasoning while optimizing computational and memory efficiency. A hierarchical evaluation employing diverse teacher-model proxies reduces assessment costs, while modular interface design facilitates seamless system updates. Experimental results demonstrate that KRAL significantly outperforms traditional Retrieval-Augmented Generation (RAG) and Supervised Fine-Tuning (SFT) methods. It improves knowledge question-answering capability (Accuracy@1 on the external open-source benchmark MEDQA increased by 1.8% vs. SFT and 3.6% vs. RAG) and reasoning capability (Pass@1 on the external benchmark PUMCH Antimicrobial increased by 27% vs. SFT and 27.2% vs. RAG), achieved at ~20% of SFT's long-term training costs. This establishes KRAL as an effective solution for enhancing local LLMs' clinical diagnostic capabilities, enabling low-cost, high-safety deployment in complex medical decision support.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15965v1" target="_blank"><h2>Self-supervised and Multi-fidelity Learning for Extended Predictive Soil Spectroscopy <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Luning Sun, José L. Safanelli, Jonathan Sanderman, Katerina Georgiou, Colby Brungard, Kanchan Grover, Bryan G. Hopkins, Shusen Liu, Timo Bremer<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 49 pages, 9 figures, submitted to Geoderma<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> variational autoencoder (abstract), latent space (abstract)<br><p><strong><u>Abstract:</u></strong> We propose a self-supervised machine learning (SSML) framework for multi-fidelity learning and extended predictive soil spectroscopy based on latent space embeddings. A self-supervised representation was pretrained with the large MIR spectral library and the Variational Autoencoder algorithm to obtain a compressed latent space for generating spectral embeddings. At this stage, only unlabeled spectral data were used, allowing us to leverage the full spectral database and the availability of scan repeats for augmented training. We also leveraged and froze the trained MIR decoder for a spectrum conversion task by plugging it into a NIR encoder to learn the mapping between NIR and MIR spectra in an attempt to leverage the predictive capabilities contained in the large MIR library with a low cost portable NIR scanner. This was achieved by using a smaller subset of the KSSL library with paired NIR and MIR spectra. Downstream machine learning models were then trained to map between original spectra, predicted spectra, and latent space embeddings for nine soil properties. The performance of was evaluated independently of the KSSL training data using a gold-standard test set, along with regression goodness-of-fit metrics. Compared to baseline models, the proposed SSML and its embeddings yielded similar or better accuracy in all soil properties prediction tasks. Predictions derived from the spectrum conversion (NIR to MIR) task did not match the performance of the original MIR spectra but were similar or superior to predictive performance of NIR-only models, suggesting the unified spectral latent space can effectively leverage the larger and more diverse MIR dataset for prediction of soil properties not well represented in current NIR libraries.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15960v1" target="_blank"><h2>Machine Learning vs. Randomness: Challenges in Predicting Binary Options Movements <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Gabriel M. Arantes, Richard F. Pinto, Bruno L. Dalmazo, Eduardo N. Borges, Giancarlo Lucca, Viviane L. D. de Mattos, Fabian C. Cardoso, Rafael A. Berri<br><strong><u>Categories:</u></strong> q-fin.CP, cs.LG<br><strong><u>Comments:</u></strong> Accepted for publication at the 26th International Conference on Intelligent Data Engineering and Automated Learning (IDEAL 2025)<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Binary options trading is often marketed as a field where predictive models can generate consistent profits. However, the inherent randomness and stochastic nature of binary options make price movements highly unpredictable, posing significant challenges for any forecasting approach. This study demonstrates that machine learning algorithms struggle to outperform a simple baseline in predicting binary options movements. Using a dataset of EUR/USD currency pairs from 2021 to 2023, we tested multiple models, including Random Forest, Logistic Regression, Gradient Boosting, and k-Nearest Neighbors (kNN), both before and after hyperparameter optimization. Furthermore, several neural network architectures, including Multi-Layer Perceptrons (MLP) and a Long Short-Term Memory (LSTM) network, were evaluated under different training conditions. Despite these exhaustive efforts, none of the models surpassed the ZeroR baseline accuracy, highlighting the inherent randomness of binary options. These findings reinforce the notion that binary options lack predictable patterns, making them unsuitable for machine learning-based forecasting.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15927v1" target="_blank"><h2>Breaking the Bottleneck with DiffuApriel: High-Throughput Diffusion LMs with Mamba Backbone <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Vaibhav Singh, Oleksiy Ostapenko, Pierre-André Noël, Torsten Scholak<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 9 pages, 4 figures<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> time sequence (abstract), transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Diffusion-based language models have recently emerged as a promising alternative to autoregressive generation, yet their reliance on Transformer backbones limits inference efficiency due to quadratic attention and KV-cache overhead. In this work, we introduce DiffuApriel, a masked diffusion language model built on a bidirectional Mamba backbone that combines the diffusion objective with linear-time sequence modeling. DiffuApriel matches the performance of Transformer-based diffusion models while achieving up to 4.4x higher inference throughput for long sequences with a 1.3B model. We further propose DiffuApriel-H, a hybrid variant that interleaves attention and mamba layers, offering up to 2.6x throughput improvement with balanced global and local context modeling. Our results demonstrate that bidirectional state-space architectures serve as strong denoisers in masked diffusion LMs, providing a practical and scalable foundation for faster, memory-efficient text generation.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15902v1" target="_blank"><h2>EEG Emotion Recognition Through Deep Learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Roman Dolgopolyi, Antonis Chatzipanagiotou<br><strong><u>Categories:</u></strong> eess.SP, cs.LG<br><strong><u>Comments:</u></strong> This version corresponds to the original manuscript submitted to the 22nd EMCIS conference prior to peer review. The peer-reviewed and accepted version will appear in the Springer conference proceedings<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> An advanced emotion classification model was developed using a CNN-Transformer architecture for emotion recognition from EEG brain wave signals, effectively distinguishing among three emotional states, positive, neutral and negative. The model achieved a testing accuracy of 91%, outperforming traditional models such as SVM, DNN, and Logistic Regression. Training was conducted on a custom dataset created by merging data from SEED, SEED-FRA, and SEED-GER repositories, comprising 1,455 samples with EEG recordings labeled according to emotional states. The combined dataset represents one of the largest and most culturally diverse collections available. Additionally, the model allows for the reduction of the requirements of the EEG apparatus, by leveraging only 5 electrodes of the 62. This reduction demonstrates the feasibility of deploying a more affordable consumer-grade EEG headset, thereby enabling accessible, at-home use, while also requiring less computational power. This advancement sets the groundwork for future exploration into mood changes induced by media content consumption, an area that remains underresearched. Integration into medical, wellness, and home-health platforms could enable continuous, passive emotional monitoring, particularly beneficial in clinical or caregiving settings where traditional behavioral cues, such as facial expressions or vocal tone, are diminished, restricted, or difficult to interpret, thus potentially transforming mental health diagnostics and interventions...</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15870v1" target="_blank"><h2>AquaSentinel: Next-Generation AI System Integrating Sensor Networks for Urban Underground Water Pipeline Anomaly Detection via Collaborative MoE-LLM Agent Architecture <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Qiming Guo, Bishal Khatri, Wenbo Sun, Jinwen Tang, Hua Zhang, Wenlu Wang<br><strong><u>Categories:</u></strong> cs.CE, cs.AI<br><strong><u>Comments:</u></strong> 7 pages, 1 figure, 2 tables, Accepted to the 40th AAAI Conference on Artificial Intelligence (AAAI 2026), IAAI Deployed Applications Track<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> anomaly detection (title, abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Underground pipeline leaks and infiltrations pose significant threats to water security and environmental safety. Traditional manual inspection methods provide limited coverage and delayed response, often missing critical anomalies. This paper proposes AquaSentinel, a novel physics-informed AI system for real-time anomaly detection in urban underground water pipeline networks. We introduce four key innovations: (1) strategic sparse sensor deployment at high-centrality nodes combined with physics-based state augmentation to achieve network-wide observability from minimal infrastructure; (2) the RTCA (Real-Time Cumulative Anomaly) detection algorithm, which employs dual-threshold monitoring with adaptive statistics to distinguish transient fluctuations from genuine anomalies; (3) a Mixture of Experts (MoE) ensemble of spatiotemporal graph neural networks that provides robust predictions by dynamically weighting model contributions; (4) causal flow-based leak localization that traces anomalies upstream to identify source nodes and affected pipe segments. Our system strategically deploys sensors at critical network junctions and leverages physics-based modeling to propagate measurements to unmonitored nodes, creating virtual sensors that enhance data availability across the entire network. Experimental evaluation using 110 leak scenarios demonstrates that AquaSentinel achieves 100% detection accuracy. This work advances pipeline monitoring by demonstrating that physics-informed sparse sensing can match the performance of dense deployments at a fraction of the cost, providing a practical solution for aging urban infrastructure.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15853v1" target="_blank"><h2>The Ensemble Kalman Inversion Race <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Rebecca Gjini, Matthias Morzfeld, Oliver R. A. Dunbar, Tapio Schneider<br><strong><u>Categories:</u></strong> physics.data-an, stat.ML<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Ensemble Kalman methods were initially developed to solve nonlinear data assimilation problems in oceanography, but are now popular in applications far beyond their original use cases. Of particular interest is climate model calibration. As hybrid physics and machine-learning models evolve, the number of parameters and complexity of parameterizations in climate models will continue to grow. Thus, robust calibration of these parameters plays an increasingly important role. We focus on learning climate model parameters from minimizing the misfit between modeled and observed climate statistics in an idealized setting. Ensemble Kalman methods are a natural choice for this problem because they are derivative-free, scalable to high dimensions, and robust to noise caused by statistical observations. Given the many variants of ensemble methods proposed, an important question is: Which ensemble Kalman method should be used for climate model calibration? To answer this question, we perform systematic numerical experiments to explore the relative computational efficiencies of several ensemble Kalman methods. The numerical experiments involve statistical observations of Lorenz-type models of increasing complexity, frequently used to represent simplified atmospheric systems, and some feature neural network parameterizations. For each test problem, several ensemble Kalman methods and a derivative-based method "race" to reach a specified accuracy, and we measure the computational cost required to achieve the desired accuracy. We investigate how prior information and the parameter or data dimensions play a role in choosing the ensemble method variant. The derivative-based method consistently fails to complete the race because it does not adaptively handle the noisy loss landscape.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15848v1" target="_blank"><h2>Step-Audio-R1 Technical Report <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Fei Tian, Xiangyu Tony Zhang, Yuxin Zhang, Haoyang Zhang, Yuxin Li, Daijiao Liu, Yayue Deng, Donghang Wu, Jun Chen, Liang Zhao, Chengyuan Yao, Hexin Liu, Eng Siong Chng, Xuerui Yang, Xiangyu Zhang, Daxin Jiang, Gang Yu<br><strong><u>Categories:</u></strong> cs.AI, cs.CL, cs.SD<br><strong><u>Comments:</u></strong> 15 pages, 5 figures. Technical Report<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Recent advances in reasoning models have demonstrated remarkable success in text and vision domains through extended chain-of-thought deliberation. However, a perplexing phenomenon persists in audio language models: they consistently perform better with minimal or no reasoning, raising a fundamental question - can audio intelligence truly benefit from deliberate thinking? We introduce Step-Audio-R1, the first audio reasoning model that successfully unlocks reasoning capabilities in the audio domain. Through our proposed Modality-Grounded Reasoning Distillation (MGRD) framework, Step-Audio-R1 learns to generate audio-relevant reasoning chains that genuinely ground themselves in acoustic features rather than hallucinating disconnected deliberations. Our model exhibits strong audio reasoning capabilities, surpassing Gemini 2.5 Pro and achieving performance comparable to the state-of-the-art Gemini 3 Pro across comprehensive audio understanding and reasoning benchmarks spanning speech, environmental sounds, and music. These results demonstrate that reasoning is a transferable capability across modalities when appropriately anchored, transforming extended deliberation from a liability into a powerful asset for audio intelligence. By establishing the first successful audio reasoning model, Step-Audio-R1 opens new pathways toward building truly multimodal reasoning systems that think deeply across all sensory modalities.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15847v1" target="_blank"><h2>Transparent Early ICU Mortality Prediction with Clinical Transformer and Per-Case Modality Attribution <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Alexander Bakumenko, Janine Hoelscher, Hudson Smith<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract), transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> Early identification of intensive care patients at risk of in-hospital mortality enables timely intervention and efficient resource allocation. Despite high predictive performance, existing machine learning approaches lack transparency and robustness, limiting clinical adoption. We present a lightweight, transparent multimodal ensemble that fuses physiological time-series measurements with unstructured clinical notes from the first 48 hours of an ICU stay. A logistic regression model combines predictions from two modality-specific models: a bidirectional LSTM for vitals and a finetuned ClinicalModernBERT transformer for notes. This traceable architecture allows for multilevel interpretability: feature attributions within each modality and direct per-case modality attributions quantifying how vitals and notes influence each decision. On the MIMIC-III benchmark, our late-fusion ensemble improves discrimination over the best single model (AUPRC 0.565 vs. 0.526; AUROC 0.891 vs. 0.876) while maintaining well-calibrated predictions. The system remains robust through a calibrated fallback when a modality is missing. These results demonstrate competitive performance with reliable, auditable risk estimates and transparent, predictable operation, which together are crucial for clinical use.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15846v1" target="_blank"><h2>The Loss of Control Playbook: Degrees, Dynamics, and Preparedness <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Charlotte Stix, Annika Hallensleben, Alejandro Ortega, Matteo Pistillo<br><strong><u>Categories:</u></strong> cs.CY, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> attention (abstract), literature review (abstract)<br><p><strong><u>Abstract:</u></strong> This research report addresses the absence of an actionable definition for Loss of Control (LoC) in AI systems by developing a novel taxonomy and preparedness framework. Despite increasing policy and research attention, existing LoC definitions vary significantly in scope and timeline, hindering effective LoC assessment and mitigation. To address this issue, we draw from an extensive literature review and propose a graded LoC taxonomy, based on the metrics of severity and persistence, that distinguishes between Deviation, Bounded LoC, and Strict LoC. We model pathways toward a societal state of vulnerability in which sufficiently advanced AI systems have acquired or could acquire the means to cause Bounded or Strict LoC once a catalyst, either misalignment or pure malfunction, materializes. We argue that this state becomes increasingly likely over time, absent strategic intervention, and propose a strategy to avoid reaching a state of vulnerability. Rather than focusing solely on intervening on AI capabilities and propensities potentially relevant for LoC or on preventing potential catalysts, we introduce a complementary framework that emphasizes three extrinsic factors: Deployment context, Affordances, and Permissions (the DAP framework). Compared to work on intrinsic factors and catalysts, this framework has the unfair advantage of being actionable today. Finally, we put forward a plan to maintain preparedness and prevent the occurrence of LoC outcomes should a state of societal vulnerability be reached, focusing on governance measures (threat modeling, deployment policies, emergency response) and technical controls (pre-deployment testing, control measures, monitoring) that could maintain a condition of perennial suspension.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15838v1" target="_blank"><h2>Attention-Based Feature Online Conformal Prediction for Time Series <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Meiyi Zhu, Caili Guo, Chunyan Feng, Osvaldo Simeone<br><strong><u>Categories:</u></strong> cs.LG, cs.IT, eess.SP<br><strong><u>Comments:</u></strong> 25 pages, 24 figures<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Online conformal prediction (OCP) wraps around any pre-trained predictor to produce prediction sets with coverage guarantees that hold irrespective of temporal dependencies or distribution shifts. However, standard OCP faces two key limitations: it operates in the output space using simple nonconformity (NC) scores, and it treats all historical observations uniformly when estimating quantiles. This paper introduces attention-based feature OCP (AFOCP), which addresses both limitations through two key innovations. First, AFOCP operates in the feature space of pre-trained neural networks, leveraging learned representations to construct more compact prediction sets by concentrating on task-relevant information while suppressing nuisance variation. Second, AFOCP incorporates an attention mechanism that adaptively weights historical observations based on their relevance to the current test point, effectively handling non-stationarity and distribution shifts. We provide theoretical guarantees showing that AFOCP maintains long-term coverage while provably achieving smaller prediction intervals than standard OCP under mild regularity conditions. Extensive experiments on synthetic and real-world time series datasets demonstrate that AFOCP consistently reduces the size of prediction intervals by as much as $88\%$ as compared to OCP, while maintaining target coverage levels, validating the benefits of both feature-space calibration and attention-based adaptive weighting.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15828v1" target="_blank"><h2>Observational constraints on the product of dark energy chemical potential and number density in out-of-equilibrium models <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> J. M. Costa Netto, Javier E. Gonzalez, H. H. B. Silva<br><strong><u>Categories:</u></strong> astro-ph.CO<br><strong><u>Comments:</u></strong> 11 pages, 6 figures, 3 tables<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> In this work, we impose observational limits on the product of dark energy chemical potential, $μ$, and number density, $n$, at the present time in out-of-equilibrium models, considering that particles can be created or destroyed in the fluid at a rate $Γ=3αH(a)$, where $α$ is a constant and $H(a)\equiv\dot{a}/a$ is the Hubble parameter. We combine the bounds derived from the positivity of entropy and the second law of thermodynamics with observational constraints on the Chevallier-Polarski-Linder (CPL) and Barboza-Alcaniz (BA) parameterizations of the equation of state (EoS) of the component. We use Type Ia supernovae (SN Ia) data from Pantheon+; baryon acoustic oscillation (BAO) data from DESI DR2; and cosmic microwave background (CMB) measurements from Planck. For $α>0$ (particle creation), the thermodynamic restrictions yield only upper limits for the $μ_{0}n_{0}$ product, while in the case of $α<0$ (particle destruction) they establish both upper and lower limits, allowing for a range of values to be obtained. In both scenarios, however, we find that the chemical potential of dark energy must be negative, $μ<0$, which indicates a preference for the phantom regime. In particular, when $α<0$, it is noted that the thermodynamic bounds are simultaneously compatible only for very small absolute values of $α$, with $α=-0.0002$ being the limiting case and resulting in $μ_{0}n_{0}(α=-0.0002)=-2.2_{-0.7}^{+1.0}\,\,GeV/m^{3}$.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15822v1" target="_blank"><h2>Atlas Gaussian processes on restricted domains and point clouds <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Mu Niu, Yue Zhang, Ke Ye, Pokman Cheung, Yizhu Wang, Xiaochen Yang<br><strong><u>Categories:</u></strong> stat.ML, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> latent space (abstract)<br><p><strong><u>Abstract:</u></strong> In real-world applications, data often reside in restricted domains with unknown boundaries, or as high-dimensional point clouds lying on a lower-dimensional, nontrivial, unknown manifold. Traditional Gaussian Processes (GPs) struggle to capture the underlying geometry in such settings. Some existing methods assume a flat space embedded in a point cloud, which can be represented by a single latent chart (latent space), while others exhibit weak performance when the point cloud is sparse or irregularly sampled. The goal of this work is to address these challenges. The main contributions are twofold: (1) We establish the Atlas Brownian Motion (BM) framework for estimating the heat kernel on point clouds with unknown geometries and nontrivial topological structures; (2) Instead of directly using the heat kernel estimates, we construct a Riemannian corrected kernel by combining the global heat kernel with local RBF kernel and leading to the formulation of Riemannian-corrected Atlas Gaussian Processes (RC-AGPs). The resulting RC-AGPs are applied to regression tasks across synthetic and real-world datasets. These examples demonstrate that our method outperforms existing approaches in both heat kernel estimation and regression accuracy. It improves statistical inference by effectively bridging the gap between complex, high-dimensional observations and manifold-based inferences.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15809v1" target="_blank"><h2>Unveiling Chemical Enrichment in the Abell 2029 Core with XRISM, XMM-Newton, and Chandra <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Arnab Sarkar, Eric D. Miller, Brian McNamara, Ming Sun, Richard Mushotzky, Stefano Ettori, Lorenzo Lovisari, Irina Zhuravleva, Naomi Ota<br><strong><u>Categories:</u></strong> astro-ph.GA, astro-ph.CO, astro-ph.HE<br><strong><u>Comments:</u></strong> 5 figures, two tables; Accepted for publication in The Astrophysical Journal Letters<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> We present new measurements of the chemical abundance pattern in the core of the nearby galaxy cluster Abell~2029, based on XRISM observations with Resolve (37 ks) and Xtend (500 ks), combined with archival data from XMM-Newton (EPIC, RGS) and Chandra. Fe abundances derived from Resolve, Xtend, and EPIC are broadly consistent, while RGS gives systematically lower values. Because the XRISM gate valve remained closed during these observations, Resolve spectral fitting is restricted to the 2--10 keV band, providing reliable constraints only for elements with strong lines in this band (S, Ar, Ca, Fe, Ni). Abundances of the $α$-elements are therefore derived using complementary observations from Xtend, EPIC, RGS, and Chandra. We construct an average X/Fe pattern in the cluster core by using Resolve exclusively for S/Fe, Ar/Fe, Ca/Fe, and Ni/Fe, and RGS + Xtend for O/Fe. The Ne/Fe ratio is averaged from Xtend, EPIC, RGS, and Chandra measurements; Mg/Fe from EPIC and Chandra measurements; and Si/Fe from Xtend, EPIC, and Chandra. Comparison with the supernovae yield models indicates that the observed abundance pattern in A2029 core is best reproduced by a combination of core-collapsed yields from low-metallicity progenitors ($Z_{\rm init}=0.001$) and a sub-Chandrasekhar-mass, double-degenerate Type Ia model. Additionally, we find an excess in Ca abundance in the core of A2029 that cannot be reproduced by the standard supernovae yield models.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15807v1" target="_blank"><h2>TopoReformer: Mitigating Adversarial Attacks Using Topological Purification in OCR Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Bhagyesh Kumar, A S Aravinthakashan, Akshat Satyanarayan, Ishaan Gakhar, Ujjwal Verma<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> Accepted at AAAI 2026 AI for CyberSecurity (AICS) Workshop<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> latent space (abstract)<br><p><strong><u>Abstract:</u></strong> Adversarially perturbed images of text can cause sophisticated OCR systems to produce misleading or incorrect transcriptions from seemingly invisible changes to humans. Some of these perturbations even survive physical capture, posing security risks to high-stakes applications such as document processing, license plate recognition, and automated compliance systems. Existing defenses, such as adversarial training, input preprocessing, or post-recognition correction, are often model-specific, computationally expensive, and affect performance on unperturbed inputs while remaining vulnerable to unseen or adaptive attacks. To address these challenges, TopoReformer is introduced, a model-agnostic reformation pipeline that mitigates adversarial perturbations while preserving the structural integrity of text images. Topology studies properties of shapes and spaces that remain unchanged under continuous deformations, focusing on global structures such as connectivity, holes, and loops rather than exact distance. Leveraging these topological features, TopoReformer employs a topological autoencoder to enforce manifold-level consistency in latent space and improve robustness without explicit gradient regularization. The proposed method is benchmarked on EMNIST, MNIST, against standard adversarial attacks (FGSM, PGD, Carlini-Wagner), adaptive attacks (EOT, BDPA), and an OCR-specific watermark attack (FAWA).</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15796v1" target="_blank"><h2>Teukolsky by Design: A Hybrid Spectral-PINN solver for Kerr Quasinormal Modes <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Alexandre M. Pombo, Lorenzo Pizzuti<br><strong><u>Categories:</u></strong> gr-qc, astro-ph.HE<br><strong><u>Comments:</u></strong> 23 pages, 5 figures, 3 tables<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> We introduce SpectralPINN, a hybrid pseudo-spectral/physics-informed neural network (PINN) solver for Kerr quasinormal modes that targets the Teukolsky equation in both the separated (radial/angular) and joint two-dimensional formulations. The solver replaces standard neural activation functions with Chebyshev polynomials of the first kind and supports both soft -- via loss penalties -- and hard -- enforced by analytic masks -- implementations of Leaver's normalization. Benchmarking against Leaver's continued-fraction method shows cumulative (real+imaginary part) relative frequency errors of $\sim 0.001\%$ for the separated formulation with hard normalization, $\sim 0.1\%$ for both the soft separated and soft joint formulations, and $\sim 0.01\%$ for the hard joint case. Exploiting our ability to solve the joint equation, we add a small quadrupolar perturbation to the Teukolsky operator, effectively rendering the problem non-separable. The resulting perturbed quasinormal modes are compared against the expected precision of the Einstein Telescope, allowing us to constrain the magnitude of the perturbation. These proof-of-concept results demonstrate that hybrid spectral-PINN solvers can provide a flexible pathway to quasinormal spectra in settings where separability, asymptotics, or field content become more intricate and high accuracy is required.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15704v1" target="_blank"><h2>In-N-On: Scaling Egocentric Manipulation with in-the-wild and on-task Data <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Xiongyi Cai, Ri-Zhao Qiu, Geng Chen, Lai Wei, Isabella Liu, Tianshu Huang, Xuxin Cheng, Xiaolong Wang<br><strong><u>Categories:</u></strong> cs.RO, cs.AI, cs.CV<br><strong><u>Comments:</u></strong> Project webpage:this https URL<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> domain adaptation (abstract)<br><p><strong><u>Abstract:</u></strong> Egocentric videos are a valuable and scalable data source to learn manipulation policies. However, due to significant data heterogeneity, most existing approaches utilize human data for simple pre-training, which does not unlock its full potential. This paper first provides a scalable recipe for collecting and using egocentric data by categorizing human data into two categories: in-the-wild and on-task alongside with systematic analysis on how to use the data. We first curate a dataset, PHSD, which contains over 1,000 hours of diverse in-the-wild egocentric data and over 20 hours of on-task data directly aligned to the target manipulation tasks. This enables learning a large egocentric language-conditioned flow matching policy, Human0. With domain adaptation techniques, Human0 minimizes the gap between humans and humanoids. Empirically, we show Human0 achieves several novel properties from scaling human data, including language following of instructions from only human data, few-shot learning, and improved robustness using on-task data. Project website: https://xiongyicai.github.io/In-N-On/</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15699v1" target="_blank"><h2>Joint Semantic-Channel Coding and Modulation for Token Communications <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jingkai Ying, Zhijin Qin, Yulong Feng, Liejun Wang, Xiaoming Tao<br><strong><u>Categories:</u></strong> eess.SP, cs.AI<br><strong><u>Comments:</u></strong> 14 pages, 14 figures, 2 tables<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> In recent years, the Transformer architecture has achieved outstanding performance across a wide range of tasks and modalities. Token is the unified input and output representation in Transformer-based models, which has become a fundamental information unit. In this work, we consider the problem of token communication, studying how to transmit tokens efficiently and reliably. Point cloud, a prevailing three-dimensional format which exhibits a more complex spatial structure compared to image or video, is chosen to be the information source. We utilize the set abstraction method to obtain point tokens. Subsequently, to get a more informative and transmission-friendly representation based on tokens, we propose a joint semantic-channel and modulation (JSCCM) scheme for the token encoder, mapping point tokens to standard digital constellation points (modulated tokens). Specifically, the JSCCM consists of two parallel Point Transformer-based encoders and a differential modulator which combines the Gumel-softmax and soft quantization methods. Besides, the rate allocator and channel adapter are developed, facilitating adaptive generation of high-quality modulated tokens conditioned on both semantic information and channel conditions. Extensive simulations demonstrate that the proposed method outperforms both joint semantic-channel coding and traditional separate coding, achieving over 1dB gain in reconstruction and more than 6x compression ratio in modulated symbols.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15687v1" target="_blank"><h2>Impact of cosmic expansion on gravitational wave spectra from strongly supercooled first-order phase transitions <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Marek Lewicki, Ville Vaskonen<br><strong><u>Categories:</u></strong> astro-ph.CO, hep-ph<br><strong><u>Comments:</u></strong> 6 pages, 4 figures<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> causality (abstract)<br><p><strong><u>Abstract:</u></strong> We compute the gravitational wave spectra from strongly supercooled first-order phase transitions, explicitly incorporating the evolution of the background metric across the transition from thermal inflation to radiation domination. We find that the spectral shape remains largely unchanged apart from a causality-induced super-horizon tail. However, in contrast to standard expectations, for slow transitions we show that the peak amplitude and frequency exhibit a weaker dependence on the transition rate $β$ than the usual scaling of $\propto β^{-2}$ and $\proptoβ$, respectively.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15684v1" target="_blank"><h2>Walrus: A Cross-Domain Foundation Model for Continuum Dynamics <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Michael McCabe, Payel Mukhopadhyay, Tanya Marwah, Bruno Regaldo-Saint Blancard, Francois Rozet, Cristiana Diaconu, Lucas Meyer, Kaze W. K. Wong, Hadi Sotoudeh, Alberto Bietti, Irina Espejo, Rio Fear, Siavash Golkar, Tom Hehir, Keiya Hirashima, Geraud Krawezik, Francois Lanusse, Rudy Morel, Ruben Ohana, Liam Parker, Mariel Pettee, Jeff Shen, Kyunghyun Cho, Miles Cranmer, Shirley Ho<br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.CE<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Foundation models have transformed machine learning for language and vision, but achieving comparable impact in physical simulation remains a challenge. Data heterogeneity and unstable long-term dynamics inhibit learning from sufficiently diverse dynamics, while varying resolutions and dimensionalities challenge efficient training on modern hardware. Through empirical and theoretical analysis, we incorporate new approaches to mitigate these obstacles, including a harmonic-analysis-based stabilization method, load-balanced distributed 2D and 3D training strategies, and compute-adaptive tokenization. Using these tools, we develop Walrus, a transformer-based foundation model developed primarily for fluid-like continuum dynamics. Walrus is pretrained on nineteen diverse scenarios spanning astrophysics, geoscience, rheology, plasma physics, acoustics, and classical fluids. Experiments show that Walrus outperforms prior foundation models on both short and long term prediction horizons on downstream tasks and across the breadth of pretraining data, while ablation studies confirm the value of our contributions to forecast stability, training throughput, and transfer performance over conventional approaches. Code and weights are released for community use.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15675v1" target="_blank"><h2>MF-GCN: A Multi-Frequency Graph Convolutional Network for Tri-Modal Depression Detection Using Eye-Tracking, Facial, and Acoustic Features <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Sejuti Rahman, Swakshar Deb, MD. Sameer Iqbal Chowdhury, MD. Jubair Ahmed Sourov, Mohammad Shamsuddin<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> convolutional (title, abstract), multimodal (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Eye tracking data quantifies the attentional bias towards negative stimuli that is frequently observed in depressed groups. Audio and video data capture the affective flattening and psychomotor retardation characteristic of depression. Statistical validation confirmed their significant discriminative power in distinguishing depressed from non depressed groups. We address a critical limitation of existing graph-based models that focus on low-frequency information and propose a Multi-Frequency Graph Convolutional Network (MF-GCN). This framework consists of a novel Multi-Frequency Filter Bank Module (MFFBM), which can leverage both low and high frequency signals. Extensive evaluation against traditional machine learning algorithms and deep learning frameworks demonstrates that MF-GCN consistently outperforms baselines. In binary (depressed and non depressed) classification, the model achieved a sensitivity of 0.96 and F2 score of 0.94. For the 3 class (no depression, mild to moderate depression and severe depression) classification task, the proposed method achieved a sensitivity of 0.79 and specificity of 0.87 and siginificantly suprassed other models. To validate generalizability, the model was also evaluated on the Chinese Multimodal Depression Corpus (CMDC) dataset and achieved a sensitivity of 0.95 and F2 score of 0.96. These results confirm that our trimodal, multi frequency framework effectively captures cross modal interaction for accurate depression detection.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15661v2" target="_blank"><h2>VisPlay: Self-Evolving Vision-Language Models from Images <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yicheng He, Chengsong Huang, Zongxia Li, Jiaxin Huang, Yonghui Yang<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.CL, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Reinforcement learning (RL) provides a principled framework for improving Vision-Language Models (VLMs) on complex reasoning tasks. However, existing RL approaches often rely on human-annotated labels or task-specific heuristics to define verifiable rewards, both of which are costly and difficult to scale. We introduce VisPlay, a self-evolving RL framework that enables VLMs to autonomously improve their reasoning abilities using large amounts of unlabeled image data. Starting from a single base VLM, VisPlay assigns the model into two interacting roles: an Image-Conditioned Questioner that formulates challenging yet answerable visual questions, and a Multimodal Reasoner that generates silver responses. These roles are jointly trained with Group Relative Policy Optimization (GRPO), which incorporates diversity and difficulty rewards to balance the complexity of generated questions with the quality of the silver answers. VisPlay scales efficiently across two model families. When trained on Qwen2.5-VL and MiMo-VL, VisPlay achieves consistent improvements in visual reasoning, compositional generalization, and hallucination reduction across eight benchmarks, including MM-Vet and MMMU, demonstrating a scalable path toward self-evolving multimodal intelligence. The project page is available at https://bruno686.github.io/VisPlay/</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15652v1" target="_blank"><h2>Continual Reinforcement Learning for Cyber-Physical Systems: Lessons Learned and Open Challenges <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Kim N. Nolle, Ivana Dusparic, Rhodri Cusack, Vinny Cahill<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 5 pages, 5 figures, Accepted to RLDM 2025<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Continual learning (CL) is a branch of machine learning that aims to enable agents to adapt and generalise previously learned abilities so that these can be reapplied to new tasks or environments. This is particularly useful in multi-task settings or in non-stationary environments, where the dynamics can change over time. This is particularly relevant in cyber-physical systems such as autonomous driving. However, despite recent advances in CL, successfully applying it to reinforcement learning (RL) is still an open problem.
  This paper highlights open challenges in continual RL (CRL) based on experiments in an autonomous driving environment. In this environment, the agent must learn to successfully park in four different scenarios corresponding to parking spaces oriented at varying angles. The agent is successively trained in these four scenarios one after another, representing a CL environment, using Proximal Policy Optimisation (PPO). These experiments exposed a number of open challenges in CRL: finding suitable abstractions of the environment, oversensitivity to hyperparameters, catastrophic forgetting, and efficient use of neural network capacity.
  Based on these identified challenges, we present open research questions that are important to be addressed for creating robust CRL systems. In addition, the identified challenges call into question the suitability of neural networks for CL. We also identify the need for interdisciplinary research, in particular between computer science and neuroscience.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15633v1" target="_blank"><h2>Hierarchical Semantic Tree Anchoring for CLIP-Based Class-Incremental Learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Tao Hu, Lan Li, Zhen-Hao Xie, Da-Wei Zhou<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multi-modal (abstract)<br><p><strong><u>Abstract:</u></strong> Class-Incremental Learning (CIL) enables models to learn new classes continually while preserving past knowledge. Recently, vision-language models like CLIP offer transferable features via multi-modal pre-training, making them well-suited for CIL. However, real-world visual and linguistic concepts are inherently hierarchical: a textual concept like "dog" subsumes fine-grained categories such as "Labrador" and "Golden Retriever," and each category entails its images. But existing CLIP-based CIL methods fail to explicitly capture this inherent hierarchy, leading to fine-grained class features drift during incremental updates and ultimately to catastrophic forgetting. To address this challenge, we propose HASTEN (Hierarchical Semantic Tree Anchoring) that anchors hierarchical information into CIL to reduce catastrophic forgetting. First, we employ an external knowledge graph as supervision to embed visual and textual features in hyperbolic space, effectively preserving hierarchical structure as data evolves. Second, to mitigate catastrophic forgetting, we project gradients onto the null space of the shared hyperbolic mapper, preventing interference with prior tasks. These two steps work synergistically to enable the model to resist forgetting by maintaining hierarchical relationships. Extensive experiments show that HASTEN consistently outperforms existing methods while providing a unified structured representation.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15632v1" target="_blank"><h2>CODE-II: A large-scale dataset for artificial intelligence in ECG analysis <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Petrus E. O. G. B. Abreu, Gabriela M. M. Paixão, Jiawei Li, Paulo R. Gomes, Peter W. Macfarlane, Ana C. S. Oliveira, Vinicius T. Carvalho, Thomas B. Schön, Antonio Luiz P. Ribeiro, Antônio H. Ribeiro<br><strong><u>Categories:</u></strong> eess.SP, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Data-driven methods for electrocardiogram (ECG) interpretation are rapidly progressing. Large datasets have enabled advances in artificial intelligence (AI) based ECG analysis, yet limitations in annotation quality, size, and scope remain major challenges. Here we present CODE-II, a large-scale real-world dataset of 2,735,269 12-lead ECGs from 2,093,807 adult patients collected by the Telehealth Network of Minas Gerais (TNMG), Brazil. Each exam was annotated using standardized diagnostic criteria and reviewed by cardiologists. A defining feature of CODE-II is a set of 66 clinically meaningful diagnostic classes, developed with cardiologist input and routinely used in telehealth practice. We additionally provide an open available subset: CODE-II-open, a public subset of 15,000 patients, and the CODE-II-test, a non-overlapping set of 8,475 exams reviewed by multiple cardiologists for blinded evaluation. A neural network pre-trained on CODE-II achieved superior transfer performance on external benchmarks (PTB-XL and CPSC 2018) and outperformed alternatives trained on larger datasets.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15623v1" target="_blank"><h2>Sufficient Explanations in Databases and their Connections to Necessary Explanations and Repairs <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Leopoldo Bertossi, Nina Pardal<br><strong><u>Categories:</u></strong> cs.DB, cs.AI, cs.LO<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> causality (abstract)<br><p><strong><u>Abstract:</u></strong> The notion of cause, as formalized by Halpern and Pearl, has been recently applied to relational databases, to characterize and compute causal explanations for query answers. In this work we consider the alternative notion of sufficient explanation. We investigate its connections with database repairs as used for dealing with inconsistent databases, and with causality-based necessary explanations. We also obtain some computational results.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15619v1" target="_blank"><h2>CODE: A global approach to ODE dynamics learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Nils Wildt, Daniel M. Tartakovsky, Sergey Oladyshkin, Wolfgang Nowak<br><strong><u>Categories:</u></strong> cs.LG, stat.ML<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Ordinary differential equations (ODEs) are a conventional way to describe the observed dynamics of physical systems. Scientists typically hypothesize about dynamical behavior, propose a mathematical model, and compare its predictions to data. However, modern computing and algorithmic advances now enable purely data-driven learning of governing dynamics directly from observations. In data-driven settings, one learns the ODE's right-hand side (RHS). Dense measurements are often assumed, yet high temporal resolution is typically both cumbersome and expensive. Consequently, one usually has only sparsely sampled data. In this work we introduce ChaosODE (CODE), a Polynomial Chaos ODE Expansion in which we use an arbitrary Polynomial Chaos Expansion (aPCE) for the ODE's right-hand side, resulting in a global orthonormal polynomial representation of dynamics. We evaluate the performance of CODE in several experiments on the Lotka-Volterra system, across varying noise levels, initial conditions, and predictions far into the future, even on previously unseen initial conditions. CODE exhibits remarkable extrapolation capabilities even when evaluated under novel initial conditions and shows advantages compared to well-examined methods using neural networks (NeuralODE) or kernel approximators (KernelODE) as the RHS representer. We observe that the high flexibility of NeuralODE and KernelODE degrades extrapolation capabilities under scarce data and measurement noise. Finally, we provide practical guidelines for robust optimization of dynamics-learning problems and illustrate them in the accompanying code.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15600v1" target="_blank"><h2>US-X Complete: A Multi-Modal Approach to Anatomical 3D Shape Recovery <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Miruna-Alexandra Gafencu, Yordanka Velikova, Nassir Navab, Mohammad Farid Azampour<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> Accepted at the Workshop on Shape in Medical Imaging at MICCAI 2025<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multi-modal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Ultrasound offers a radiation-free, cost-effective solution for real-time visualization of spinal landmarks, paraspinal soft tissues and neurovascular structures, making it valuable for intraoperative guidance during spinal procedures. However, ultrasound suffers from inherent limitations in visualizing complete vertebral anatomy, in particular vertebral bodies, due to acoustic shadowing effects caused by bone. In this work, we present a novel multi-modal deep learning method for completing occluded anatomical structures in 3D ultrasound by leveraging complementary information from a single X-ray image. To enable training, we generate paired training data consisting of: (1) 2D lateral vertebral views that simulate X-ray scans, and (2) 3D partial vertebrae representations that mimic the limited visibility and occlusions encountered during ultrasound spine imaging. Our method integrates morphological information from both imaging modalities and demonstrates significant improvements in vertebral reconstruction (p < 0.001) compared to state of art in 3D ultrasound vertebral completion. We perform phantom studies as an initial step to future clinical translation, and achieve a more accurate, complete volumetric lumbar spine visualization overlayed on the ultrasound scan without the need for registration with preoperative modalities such as computed tomography. This demonstrates that integrating a single X-ray projection mitigates ultrasound's key limitation while preserving its strengths as the primary imaging modality. Code and data can be found at https://github.com/miruna20/US-X-Complete</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15552v2" target="_blank"><h2>Multimodal Evaluation of Russian-language Architectures <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Artem Chervyakov, Ulyana Isaeva, Anton Emelyanov, Artem Safin, Maria Tikhonova, Alexander Kharitonov, Yulia Lyakh, Petr Surovtsev, Denis Shevelev, Vildan Saburov, Vasily Konovalov, Elisei Rykov, Ivan Sviridov, Amina Miftakhova, Ilseyar Alimova, Alexander Panchenko, Alexander Kapitanov, Alena Fenogenova<br><strong><u>Categories:</u></strong> cs.CL, cs.AI, cs.CV<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15543v1" target="_blank"><h2>A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Georgios Venianakis, Constantinos Theodoropoulos, Michail Kavousanakis<br><strong><u>Categories:</u></strong> stat.ML, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Parameter estimation remains a challenging task across many areas of engineering. Because data acquisition can often be costly, limited, or prone to inaccuracies (noise, uncertainty) it is crucial to identify sensor configurations that provide the maximum amount of information about the unknown parameters, in particular for the case of distributed-parameter systems, where spatial variations are important. Physics-Informed Neural Networks (PINNs) have recently emerged as a powerful machine-learning (ML) tool for parameter estimation, particularly in cases with sparse or noisy measurements, overcoming some of the limitations of traditional optimization-based and Bayesian approaches. Despite the widespread use of PINNs for solving inverse problems, relatively little attention has been given to how their performance depends on sensor placement. This study addresses this gap by introducing a comprehensive PINN-based framework that simultaneously tackles optimal sensor placement and parameter estimation. Our approach involves training a PINN model in which the parameters of interest are included as additional inputs. This enables the efficient computation of sensitivity functions through automatic differentiation, which are then used to determine optimal sensor locations exploiting the D-optimality criterion. The framework is validated on two illustrative distributed-parameter reaction-diffusion-advection problems of increasing complexity. The results demonstrate that our PINNs-based methodology consistently achieves higher accuracy compared to parameter values estimated from intuitively or randomly selected sensor positions.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15541v1" target="_blank"><h2>The VVVX quest for satellites around the Circinus galaxy <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> L. D. Baravalle, A. L. O'Mill, M. V. Alonso, C. Obasi, D. Minniti, M. Gómez, C. Villalon, J. Nilo-Castellón, C. Valotto, M. Soto, I. V. Daza Perilla, M. A. Sgró, J. G. Fernández-Trincado<br><strong><u>Categories:</u></strong> astro-ph.GA<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> The Circinus galaxy is the nearest type-2 Seyfert galaxy, which is at a distance of 4.2 Mpc. Its environment is challenging to explore because it is located at low Galactic latitudes, behind the Galactic disc. The long-term goal is to characterise the Circinus galaxy halo and determine the presence of dwarf satellites using near-infrared data. We selected 1,542 galaxies from the VVV NIRGC within a 2-degree radius around Circinus, representing 2/3 of the virial radius. Structural parameters such as half-light radii and colours were used, and correlations were examined. A neural network was trained with 486 galaxies with known spectroscopic redshifts to estimate photometric redshifts for all galaxies. Potential satellites were defined based on half-light radii compatible with the typical sizes of dwarf satellites, and combined with photometric redshifts. The galaxy properties are reliably characterised down to $K_{s}$ $\sim$ 15.5 mag, which represents about 90% completeness of detections. At the distance of Circinus, this limiting magnitude corresponds to $K_{s}$ absolute magnitude of $-12.6$ mag, which allows us to find dwarf galaxies. There are 20 galaxies with half-light radii larger than 2.45 arcsec, only 8 have photometric redshifts below 0.04. None of these galaxies is close to Circinus, which has a redshift of 0.0015. The ANNz model exhibited a high degree of accuracy in the range $0.001 < z_{phot} < 0.023$. The presence of dwarf satellites could not be confirmed with the available data in the studied region. The apparent lack of satellites may be genuine, possibly related to AGN feedback effects. This work demonstrates the effectiveness of combining near-infrared data and machine learning techniques to estimate photometric redshifts at low Galactic latitudes, providing useful information for future spectroscopic follow-up campaigns.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15530v1" target="_blank"><h2>Convergence and Sketching-Based Efficient Computation of Neural Tangent Kernel Weights in Physics-Based Loss <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Max Hirsch, Federico Pichi<br><strong><u>Categories:</u></strong> math.NA, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> In multi-objective optimization, multiple loss terms are weighted and added together to form a single objective. These weights are chosen to properly balance the competing losses according to some meta-goal. For example, in physics-informed neural networks (PINNs), these weights are often adaptively chosen to improve the network's generalization error. A popular choice of adaptive weights is based on the neural tangent kernel (NTK) of the PINN, which describes the evolution of the network in predictor space during training. The convergence of such an adaptive weighting algorithm is not clear a priori. Moreover, these NTK-based weights would be updated frequently during training, further increasing the computational burden of the learning process. In this paper, we prove that under appropriate conditions, gradient descent enhanced with adaptive NTK-based weights is convergent in a suitable sense. We then address the problem of computational efficiency by developing a randomized algorithm inspired by a predictor-corrector approach and matrix sketching, which produces unbiased estimates of the NTK up to an arbitrarily small discretization error. Finally, we provide numerical experiments to support our theoretical findings and to show the efficacy of our randomized algorithm. Code Availability: https://github.com/maxhirsch/Efficient-NTK</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15522v1" target="_blank"><h2>PCARNN-DCBF: Minimal-Intervention Geofence Enforcement for Ground Vehicles <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yinan Yu, Samuel Scheidegger<br><strong><u>Categories:</u></strong> cs.LG, cs.RO<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Runtime geofencing for ground vehicles is rapidly emerging as a critical technology for enforcing Operational Design Domains (ODDs). However, existing solutions struggle to reconcile high-fidelity learning with the structural requirements of verifiable control. We address this by introducing PCARNN-DCBF, a novel pipeline integrating a Physics-encoded Control-Affine Residual Neural Network with a preview-based Discrete Control Barrier Function. Unlike generic learned models, PCARNN explicitly preserves the control-affine structure of vehicle dynamics, ensuring the linearity required for reliable optimization. This enables the DCBF to enforce polygonal keep-in constraints via a real-time Quadratic Program (QP) that handles high relative degree and mitigates actuator saturation. Experiments in CARLA across electric and combustion platforms demonstrate that this structure-preserving approach significantly outperforms analytical and unstructured neural baselines.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15520v1" target="_blank"><h2>Theoretical Closed-loop Stability Bounds for Dynamical System Coupled with Diffusion Policies <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Gabriel Lauzier, Alexandre Girard, François Ferland<br><strong><u>Categories:</u></strong> cs.RO, cs.AI<br><strong><u>Comments:</u></strong> 5 pages, 3 figures<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Diffusion Policy has shown great performance in robotic manipulation tasks under stochastic perturbations, due to its ability to model multimodal action distributions. Nonetheless, its reliance on a computationally expensive reverse-time diffusion (denoising) process, for action inference, makes it challenging to use for real-time applications where quick decision-making is mandatory. This work studies the possibility of conducting the denoising process only partially before executing an action, allowing the plant to evolve according to its dynamics in parallel to the reverse-time diffusion dynamics ongoing on the computer. In a classical diffusion policy setting, the plant dynamics are usually slow and the two dynamical processes are uncoupled. Here, we investigate theoretical bounds on the stability of closed-loop systems using diffusion policies when the plant dynamics and the denoising dynamics are coupled. The contribution of this work gives a framework for faster imitation learning and a metric that yields if a controller will be stable based on the variance of the demonstrations.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15763v1" target="_blank"><h2>Identifying the Supply Chain of AI for Trustworthiness and Risk Management in Critical Applications <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Raymond K. Sheh, Karen Geappen<br><strong><u>Categories:</u></strong> cs.AI, cs.CR, cs.SE<br><strong><u>Comments:</u></strong> Presented at the 2025 AAAI Fall Symposium - AI Trustworthiness and Risk Assessment for Challenged Contexts (ATRACC)<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Risks associated with the use of AI, ranging from algorithmic bias to model hallucinations, have received much attention and extensive research across the AI community, from researchers to end-users. However, a gap exists in the systematic assessment of supply chain risks associated with the complex web of data sources, pre-trained models, agents, services, and other systems that contribute to the output of modern AI systems. This gap is particularly problematic when AI systems are used in critical applications, such as the food supply, healthcare, utilities, law, insurance, and transport.
  We survey the current state of AI risk assessment and management, with a focus on the supply chain of AI and risks relating to the behavior and outputs of the AI system. We then present a proposed taxonomy specifically for categorizing AI supply chain entities. This taxonomy helps stakeholders, especially those without extensive AI expertise, to "consider the right questions" and systematically inventory dependencies across their organization's AI systems. Our contribution bridges a gap between the current state of AI governance and the urgent need for actionable risk assessment and management of AI use in critical applications.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15476v1" target="_blank"><h2>RS-CA-HSICT: A Residual and Spatial Channel Augmented CNN Transformer Framework for Monkeypox Detection <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Rashid Iqbal, Saddam Hussain Khan<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> 33 Pages, 12 Figure, 4 Tables<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> This work proposes a hybrid deep learning approach, namely Residual and Spatial Learning based Channel Augmented Integrated CNN-Transformer architecture, that leverages the strengths of CNN and Transformer towards enhanced MPox detection. The proposed RS-CA-HSICT framework is composed of an HSICT block, a residual CNN module, a spatial CNN block, and a CA, which enhances the diverse feature space, detailed lesion information, and long-range dependencies. The new HSICT module first integrates an abstract representation of the stem CNN and customized ICT blocks for efficient multihead attention and structured CNN layers with homogeneous (H) and structural (S) operations. The customized ICT blocks learn global contextual interactions and local texture extraction. Additionally, H and S layers learn spatial homogeneity and fine structural details by reducing noise and modeling complex morphological variations. Moreover, inverse residual learning enhances vanishing gradient, and stage-wise resolution reduction ensures scale invariance. Furthermore, the RS-CA-HSICT framework augments the learned HSICT channels with the TL-driven Residual and Spatial CNN maps for enhanced multiscale feature space capturing global and localized structural cues, subtle texture, and contrast variations. These channels, preceding augmentation, are refined through the Channel-Fusion-and-Attention block, which preserves discriminative channels while suppressing redundant ones, thereby enabling efficient computation. Finally, the spatial attention mechanism refines pixel selection to detect subtle patterns and intra-class contrast variations in Mpox. Experimental results on both the Kaggle benchmark and a diverse MPox dataset reported classification accuracy as high as 98.30% and an F1-score of 98.13%, which outperforms the existing CNNs and ViTs.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15475v1" target="_blank"><h2>LCS: A Learnlet-Based Sparse Framework for Blind Source Separation <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> V. Bonjean, A. Gkogkou, J. L. Starck, P. Tsakalides<br><strong><u>Categories:</u></strong> astro-ph.IM, astro-ph.CO<br><strong><u>Comments:</u></strong> 11 pages, 8 figures<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Blind source separation (BSS) plays a pivotal role in modern astrophysics by enabling the extraction of scientifically meaningful signals from multi-frequency observations. Traditional BSS methods, such as those relying on fixed wavelet dictionaries, enforce sparsity during component separation, but may fall short when faced with the inherent complexity of real astrophysical signals. In this work, we introduce the Learnlet Component Separator (LCS), a novel BSS framework that bridges classical sparsity-based techniques with modern deep learning. LCS utilizes the Learnlet transform: a structured convolutional neural network designed to serve as a learned, wavelet-like multiscale representation. This hybrid design preserves the interpretability and sparsity, promoting properties of wavelets while gaining the adaptability and expressiveness of learned models. The LCS algorithm integrates this learned sparse representation into an iterative source separation process, enabling effective decomposition of multi-channel observations. While conceptually inspired by sparse BSS methods, LCS introduces a learned representation layer that significantly departs from classical fixed-basis assumptions. We evaluate LCS on both synthetic and real datasets, demonstrating superior separation performance compared to state-of-the-art methods (average gain of about 5 dB on toy model examples). Our results highlight the potential of hybrid approaches that combine signal processing priors with deep learning to address the challenges of next-generation cosmological experiments.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15470v1" target="_blank"><h2>Advancing Identification method of Gamma-Ray Bursts with Data and Feature Enhancement <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Peng Zhang, Bing Li, Ren-Zhou Gui, Shao-Lin Xiong, Yu Wang, Shi-Jie Zheng, Guang-Cheng Xiao, Xiao-Bo Li, Yue Huang, Chen-Wei Wang, Jia-Cong Liu, Yan-Qiu Zhang, Wang-Chen Xue, Chao Zheng, Yue Wang<br><strong><u>Categories:</u></strong> astro-ph.HE<br><strong><u>Comments:</u></strong> Under review. Dataset and model related discussions are welcome!<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), dimensionality reduction (abstract), neural network (abstract), time-domain (abstract), data augmentation (abstract)<br><p><strong><u>Abstract:</u></strong> Gamma-ray bursts (GRBs) are challenging to identify due to their transient nature, complex temporal profiles, and limited observational datasets. We address this with a one-dimensional convolutional neural network integrated with an Adaptive Frequency Feature Enhancement module and physics-informed data augmentation. Our framework generates 100,000 synthetic GRB samples, expanding training data diversity and volume while preserving physical fidelity-especially for low-significance events. The model achieves 97.46% classification accuracy, outperforming all tested variants with conventional enhancement modules, highlighting enhanced domain-specific feature capture. Feature visualization shows model focuses on deep-seated morphological features and confirms the capability of extracting physically meaningful burst characteristics. Dimensionality reduction and clustering reveal GRBs with similar morphologies or progenitor origins cluster in the feature space, linking learned features to physical properties. This perhaps offers a novel diagnostic tool for identifying kilonova- and supernova-associated GRB candidates, establishing criteria to enhance multi-messenger early-warning systems. The framework aids current time-domain surveys, generalizes to other rare transients, and advances automated detection in large-volume observational data.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15464v1" target="_blank"><h2>SIGMMA: Hierarchical Graph-Based Multi-Scale Multi-modal Contrastive Alignment of Histopathology Image and Spatial Transcriptome <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Dabin Jeong, Amirhossein Vahidi, Ciro Ramírez-Suástegui, Marie Moullet, Kevin Ly, Mohammad Vali Sanian, Sebastian Birk, Yinshui Chang, Adam Boxall, Daniyal Jafree, Lloyd Steele, Vijaya Baskar MS, Muzlifah Haniffa, Mohammad Lotfollahi<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multi-modal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Recent advances in computational pathology have leveraged vision-language models to learn joint representations of Hematoxylin and Eosin (HE) images with spatial transcriptomic (ST) profiles. However, existing approaches typically align HE tiles with their corresponding ST profiles at a single scale, overlooking fine-grained cellular structures and their spatial organization. To address this, we propose Sigmma, a multi-modal contrastive alignment framework for learning hierarchical representations of HE images and spatial transcriptome profiles across multiple scales. Sigmma introduces multi-scale contrastive alignment, ensuring that representations learned at different scales remain coherent across modalities. Furthermore, by representing cell interactions as a graph and integrating inter- and intra-subgraph relationships, our approach effectively captures cell-cell interactions, ranging from fine to coarse, within the tissue microenvironment. We demonstrate that Sigmm learns representations that better capture cross-modal correspondences, leading to an improvement of avg. 9.78\% in the gene-expression prediction task and avg. 26.93\% in the cross-modal retrieval task across datasets. We further show that it learns meaningful multi-tissue organization in downstream analyses.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15456v1" target="_blank"><h2>Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Qian'ang Mao, Yuxuan Zhang, Jiaman Chen, Wenjun Zhou, Jiaqi Yan<br><strong><u>Categories:</u></strong> cs.AI, q-fin.GN<br><strong><u>Comments:</u></strong> Written in 2025 Q1<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multi-modal (abstract)<br><p><strong><u>Abstract:</u></strong> As Decentralized Finance (DeFi) develops, understanding user intent behind DeFi transactions is crucial yet challenging due to complex smart contract interactions, multifaceted on-/off-chain factors, and opaque hex logs. Existing methods lack deep semantic insight. To address this, we propose the Transaction Intent Mining (TIM) framework. TIM leverages a DeFi intent taxonomy built on grounded theory and a multi-agent Large Language Model (LLM) system to robustly infer user intents. A Meta-Level Planner dynamically coordinates domain experts to decompose multiple perspective-specific intent analyses into solvable subtasks. Question Solvers handle the tasks with multi-modal on/off-chain data. While a Cognitive Evaluator mitigates LLM hallucinations and ensures verifiability. Experiments show that TIM significantly outperforms machine learning models, single LLMs, and single Agent baselines. We also analyze core challenges in intent inference. This work helps provide a more reliable understanding of user motivations in DeFi, offering context-aware explanations for complex blockchain activity.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15762v1" target="_blank"><h2>A time for monsters: Organizational knowing after LLMs <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Samer Faraj, Joel Perez Torrents, Saku Mantere, Anand Bhardwaj<br><strong><u>Categories:</u></strong> cs.CY, cs.AI<br><strong><u>Comments:</u></strong> Forthcoming at Strategic Organization<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Large Language Models (LLMs) are reshaping organizational knowing by unsettling the epistemological foundations of representational and practice-based perspectives. We conceptualize LLMs as Haraway-ian monsters, that is, hybrid, boundary-crossing entities that destabilize established categories while opening new possibilities for inquiry. Focusing on analogizing as a fundamental driver of knowledge, we examine how LLMs generate connections through large-scale statistical inference. Analyzing their operation across the dimensions of surface/deep analogies and near/far domains, we highlight both their capacity to expand organizational knowing and the epistemic risks they introduce. Building on this, we identify three challenges of living with such epistemic monsters: the transformation of inquiry, the growing need for dialogical vetting, and the redistribution of agency. By foregrounding the entangled dynamics of knowing-with-LLMs, the paper extends organizational theory beyond human-centered epistemologies and invites renewed attention to how knowledge is created, validated, and acted upon in the age of intelligent technologies.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15445v1" target="_blank"><h2>Neural network-driven domain decomposition for efficient solutions to the Helmholtz equation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Victorita Dolean, Daria Hrebenshchykova, Stéphane Lanteri, Victor Michel-Dansac<br><strong><u>Categories:</u></strong> math.NA, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> Accurately simulating wave propagation is crucial in fields such as acoustics, electromagnetism, and seismic analysis. Traditional numerical methods, like finite difference and finite element approaches, are widely used to solve governing partial differential equations (PDEs) such as the Helmholtz equation. However, these methods face significant computational challenges when applied to high-frequency wave problems in complex two-dimensional domains. This work investigates Finite Basis Physics-Informed Neural Networks (FBPINNs) and their multilevel extensions as a promising alternative. These methods leverage domain decomposition, partitioning the computational domain into overlapping sub-domains, each governed by a local neural network. We assess their accuracy and computational efficiency in solving the Helmholtz equation for the homogeneous case, demonstrating their potential to mitigate the limitations of traditional approaches.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15435v1" target="_blank"><h2>HV-Attack: Hierarchical Visual Attack for Multimodal Retrieval Augmented Generation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Linyin Luo, Yujuan Ding, Yunshan Ma, Wenqi Fan, Hanjiang Lai<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.IR<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Advanced multimodal Retrieval-Augmented Generation (MRAG) techniques have been widely applied to enhance the capabilities of Large Multimodal Models (LMMs), but they also bring along novel safety issues. Existing adversarial research has revealed the vulnerability of MRAG systems to knowledge poisoning attacks, which fool the retriever into recalling injected poisoned contents. However, our work considers a different setting: visual attack of MRAG by solely adding imperceptible perturbations at the image inputs of users, without manipulating any other components. This is challenging due to the robustness of fine-tuned retrievers and large-scale generators, and the effect of visual perturbation may be further weakened by propagation through the RAG chain. We propose a novel Hierarchical Visual Attack that misaligns and disrupts the two inputs (the multimodal query and the augmented knowledge) of MRAG's generator to confuse its generation. We further design a hierarchical two-stage strategy to obtain misaligned augmented knowledge. We disrupt the image input of the retriever to make it recall irrelevant knowledge from the original database, by optimizing the perturbation which first breaks the cross-modal alignment and then disrupts the multimodal semantic alignment. We conduct extensive experiments on two widely-used MRAG datasets: OK-VQA and InfoSeek. We use CLIP-based retrievers and two LMMs BLIP-2 and LLaVA as generators. Results demonstrate the effectiveness of our visual attack on MRAG through the significant decrease in both retrieval and generation performance.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15432v1" target="_blank"><h2>Towards Understanding Layer Contributions in Tabular In-Context Learning Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Amir Rezaei Balef, Mykhailo Koshil, Katharina Eggensperger<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> Accepted at the EurIPS 2025 Workshop on AI for Tabular Data<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> latent space (abstract)<br><p><strong><u>Abstract:</u></strong> Despite the architectural similarities between tabular in-context learning (ICL) models and large language models (LLMs), little is known about how individual layers contribute to tabular prediction. In this paper, we investigate how the latent spaces evolve across layers in tabular ICL models, identify potential redundant layers, and compare these dynamics with those observed in LLMs. We analyze TabPFN and TabICL through the "layers as painters" perspective, finding that only subsets of layers share a common representational language, suggesting structural redundancy and offering opportunities for model compression and improved interpretability.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15414v1" target="_blank"><h2>RRT*former: Environment-Aware Sampling-Based Motion Planning using Transformer <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Mingyang Feng, Shaoyuan Li, Xiang Yin<br><strong><u>Categories:</u></strong> cs.RO, cs.AI<br><strong><u>Comments:</u></strong> Accepted to IROS 2025<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> We investigate the sampling-based optimal path planning problem for robotics in complex and dynamic environments. Most existing sampling-based algorithms neglect environmental information or the information from previous samples. Yet, these pieces of information are highly informative, as leveraging them can provide better heuristics when sampling the next state. In this paper, we propose a novel sampling-based planning algorithm, called \emph{RRT*former}, which integrates the standard RRT* algorithm with a Transformer network in a novel way. Specifically, the Transformer is used to extract features from the environment and leverage information from previous samples to better guide the sampling process. Our extensive experiments demonstrate that, compared to existing sampling-based approaches such as RRT*, Neural RRT*, and their variants, our algorithm achieves considerable improvements in both the optimality of the path and sampling efficiency. The code for our implementation is available on https://github.com/fengmingyang666/RRTformer.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15407v1" target="_blank"><h2>IPR-1: Interactive Physical Reasoner <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Mingyu Zhang, Lifeng Zhuo, Tianxi Tan, Guocan Xie, Xian Nie, Yan Li, Renjie Zhao, Zizhu He, Ziyu Wang, Jiting Cai, Yong-Lu Li<br><strong><u>Categories:</u></strong> cs.AI, cs.CV<br><strong><u>Comments:</u></strong> 11 pages, 5 figures<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> causality (abstract)<br><p><strong><u>Abstract:</u></strong> Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. We study this in a Game-to-Unseen (G2U) setting, curating 1,000+ heterogeneous games with diverse physical and causal mechanisms, and evaluate at three human-like levels: Survival, Curiosity, Utility, from primitive intuition to goal-driven reasoning. Our analysis reveals complementary failures: VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on three levels, matches GPT-5 overall, and surpasses it on Curiosity. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15401v1" target="_blank"><h2>Explosions in the Empty: A Survey of Transients in Local Void Galaxies <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Suo-Ning Wang, Bin-Bin Zhang, Rubén García Benito<br><strong><u>Categories:</u></strong> astro-ph.HE, astro-ph.GA<br><strong><u>Comments:</u></strong> 52 pages, 4 figures, 6 tables<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> We present a systematic analysis of transient astrophysical events -- including supernovae (SNe), gamma-ray bursts (GRBs), and fast radio bursts (FRBs) -- in void and non-void galaxies within the local universe ($0.005 < z < 0.05$). Cosmic voids, defined by low galaxy densities and characterized by minimal environmental interactions, offer a natural laboratory for isolating the impact of large-scale underdensities on stellar evolution and transient production. Using multi-wavelength data from the Sloan Digital Sky Survey, the Sternberg Astronomical Institute Supernova Catalogue, and high-energy space observatories, we compare transient occurrence rates and host galaxy properties across environments. We find that core-collapse supernovae (CCSNe) are significantly more common in void galaxies, indicating that massive star formation remains active in underdense regions. In contrast, Type Ia supernovae are less frequent in voids, consistent with a scarcity of older stellar populations. Notably, we identify a short-duration GRB hosted by a void galaxy, demonstrating that compact object mergers can occur in isolated environments. Additionally, we find no FRBs associated with void galaxies. Taken together, these results show that cosmic voids exert a measurable influence on the star formation history of galaxies and hence on the production of transients.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15393v1" target="_blank"><h2>EVA-Net: Interpretable Brain Age Prediction via Continuous Aging Prototypes from EEG <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Kunyu Zhang, Mingxuan Wang, Xiangjie Shi, Haoxing Xu, Chao Zhang<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> anomaly detection (abstract), transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> The brain age is a key indicator of brain health. While electroencephalography (EEG) is a practical tool for this task, existing models struggle with the common challenge of imperfect medical data, such as learning a ``normal'' baseline from weakly supervised, healthy-only cohorts. This is a critical anomaly detection task for identifying disease, but standard models are often black boxes lacking an interpretable structure. We propose EVA-Net, a novel framework that recasts brain age as an interpretable anomaly detection problem. EVA-Net uses an efficient, sparsified-attention Transformer to model long EEG sequences. To handle noise and variability in imperfect data, it employs a Variational Information Bottleneck to learn a robust, compressed representation. For interpretability, this representation is aligned to a continuous prototype network that explicitly learns the normative healthy aging manifold. Trained on 1297 healthy subjects, EVA-Net achieves state-of-the-art accuracy. We validated its anomaly detection capabilities on an unseen cohort of 27 MCI and AD patients. This pathological group showed significantly higher brain-age gaps and a novel Prototype Alignment Error, confirming their deviation from the healthy manifold. EVA-Net provides an interpretable framework for healthcare intelligence using imperfect medical data.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15375v1" target="_blank"><h2>Parameter Importance-Driven Continual Learning for Foundation Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Lingxiang Wang, Hainan Zhang, Zhiming Zheng<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Domain-specific post-training often causes catastrophic forgetting, making foundation models lose their general reasoning ability and limiting their adaptability to dynamic real-world environments. Preserving general capabilities while acquiring downstream domain knowledge is a central challenge for large language and multimodal models. Traditional continual learning methods, such as regularization, replay and architectural isolation, suffer from poor downstream performance, reliance on inaccessible historical data, or additional parameter overhead. While recent parameter-efficient tuning (PET) methods can alleviate forgetting, their effectiveness strongly depends on the choice of parameters and update strategies. In this paper, we introduce PIECE, a Parameter Importance Estimation-based Continual Enhancement method that preserves general ability while efficiently learning domain knowledge without accessing prior training data or increasing model parameters. PIECE selectively updates only 0.1% of core parameters most relevant to new tasks, guided by two importance estimators: PIECE-F based on Fisher Information, and PIECE-S based on a second-order normalization that combines gradient and curvature information. Experiments across three language models and two multimodal models show that PIECE maintains general capabilities and achieves state-of-the-art continual learning performance across diverse downstream tasks. Our results highlight a practical path to scalable, domain-adaptive foundation models without catastrophic forgetting.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15370v1" target="_blank"><h2>The Empowerment of Science of Science by Large Language Models: New Tools and Methods <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Guoqiang Liang, Jingqian Gong, Mengxuan Li, Gege Lin, Shuo Zhang<br><strong><u>Categories:</u></strong> cs.CL, cs.AI<br><strong><u>Comments:</u></strong> The manuscript is currently ongoing the underreview process of the journal of information science<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Large language models (LLMs) have exhibited exceptional capabilities in natural language understanding and generation, image recognition, and multimodal tasks, charting a course towards AGI and emerging as a central issue in the global technological race. This manuscript conducts a comprehensive review of the core technologies that support LLMs from a user standpoint, including prompt engineering, knowledge-enhanced retrieval augmented generation, fine tuning, pretraining, and tool learning. Additionally, it traces the historical development of Science of Science (SciSci) and presents a forward looking perspective on the potential applications of LLMs within the scientometric domain. Furthermore, it discusses the prospect of an AI agent based model for scientific evaluation, and presents new research fronts detection and knowledge graph building methods with LLMs.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15369v1" target="_blank"><h2>IPTQ-ViT: Post-Training Quantization of Non-linear Functions for Integer-only Vision Transformers <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Gihwan Kim, Jemin Lee, Hyungshin Kim<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> accepted in WACV 2026 (10 pages)<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> Previous Quantization-Aware Training (QAT) methods for vision transformers rely on expensive retraining to recover accuracy loss in non-linear layer quantization, limiting their use in resource-constrained environments. In contrast, existing Post-Training Quantization (PTQ) methods either partially quantize non-linear functions or adjust activation distributions to maintain accuracy but fail to achieve fully integer-only inference. In this paper, we introduce IPTQ-ViT, a novel PTQ framework for fully integer-only vision transformers without retraining. We present approximation functions: a polynomial-based GELU optimized for vision data and a bit-shifting-based Softmax designed to improve approximation accuracy in PTQ. In addition, we propose a unified metric integrating quantization sensitivity, perturbation, and computational cost to select the optimal approximation function per activation layer. IPTQ-ViT outperforms previous PTQ methods, achieving up to 6.44\%p (avg. 1.78\%p) top-1 accuracy improvement for image classification, 1.0 mAP for object detection. IPTQ-ViT outperforms partial floating-point PTQ methods under W8A8 and W4A8, and achieves accuracy and latency comparable to integer-only QAT methods. We plan to release our code https://github.com/gihwan-kim/IPTQ-ViT.git.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15351v1" target="_blank"><h2>Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yifu Guo, Zishan Xu, Zhiyuan Yao, Yuquan Lu, Jiaye Lin, Sen Hu, Zhenheng Tang, Yingchao Li, Huacan Wang, Ronghao Chen<br><strong><u>Categories:</u></strong> cs.AI, cs.CV<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Existing multimodal reasoning models and frameworks suffer from fundamental architectural limitations: most lack the human-like ability to autonomously explore diverse reasoning pathways-whether in direct inference, tool-driven visual exploration, programmatic visual manipulation, or intrinsic visual imagination. Consequently, they struggle to adapt to dynamically changing capability requirements in real-world tasks. Meanwhile, humans exhibit a complementary set of thinking abilities when addressing such tasks, whereas existing methods typically cover only a subset of these dimensions. Inspired by this, we propose Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration, a new paradigm for multimodal agentic reasoning. We define six core capabilities essential for multimodal reasoning and organize a comprehensive evaluation benchmark, Octopus-Bench, accordingly. Octopus is capable of autonomously exploring during reasoning and dynamically selecting the most appropriate capability based on the current state. Experimental results show that Octopus achieves the best performance on the vast majority of tasks in Octopus-Bench, highlighting the crucial role of capability coordination in agentic multimodal reasoning.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15349v2" target="_blank"><h2>Type Iax supernovae as a source of iron-rich silicate dust <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Aman Kumar, Arkaprabha Sarangi<br><strong><u>Categories:</u></strong> astro-ph.SR, astro-ph.GA, astro-ph.HE<br><strong><u>Comments:</u></strong> Submitted to The Astrophysical Journal<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> VAE (title, abstract)<br><p><strong><u>Abstract:</u></strong> We model the formation of dust in the ejecta of Type Iax supernovae (SNe), which is a low-luminosity subclass of Type Ia SNe. A non-equilibrium chemical kinetic approach is adopted to trace the synthesis of molecules, molecular clusters, and dust grains in the ejecta of thermonuclear SNe. We find that Type Iax SNe provide conditions conducive to the formation of several O-rich dust species in the ejecta. Particularly, iron-rich silicates of chemical type FeSiO3, Fe2SiO4, and MgFeSiO4 are found to form in abundance, suggesting that the ejecta of low-luminosity thermonuclear SNe can be a site where a large fraction of iron is locked up in dust, unlike other stellar sources. The final mass of dust formed in the ejecta ranges between 10^{-5} and 10^{-4} Msun, where most of the dust forms between 1000 and 2000 days post-explosion. Apart from Fe-rich silicates, Mg-silicates, and silicon carbide are also formed in the ejecta of Type Iax SNe. When compared to the dust budget of typical Type Ia SNe, we find that the expected dust-to-ejecta mass ratio is one or two orders of magnitude larger in Type Iax SNe. We conclude that the ejecta of typical Type Ia SNe form a negligible amount of dust, in agreement with observation, while the low-luminosity subclass Type Iax SNe are potential producers of iron-rich silicates.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15342v1" target="_blank"><h2>Reflexive Evidence-Based Multimodal Learning for Clean Energy Transitions: Causal Insights on Cooking Fuel Access, Urbanization, and Carbon Emissions <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Shan Shan<br><strong><u>Categories:</u></strong> cs.HC, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (title), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Achieving Sustainable Development Goal 7 (Affordable and Clean Energy) requires not only technological innovation but also a deeper understanding of the socioeconomic factors influencing energy access and carbon emissions. While these factors are gaining attention, critical questions remain, particularly regarding how to quantify their impacts on energy systems, model their cross-domain interactions, and capture feedback dynamics in the broader context of energy transitions. To address these gaps, this study introduces ClimateAgents, an AI-based framework that combines large language models with domain-specialized agents to support hypothesis generation and scenario exploration. Leveraging 20 years of socioeconomic and emissions data from 265 economies, countries and regions, and 98 indicators drawn from the World Bank database, the framework applies a machine learning based causal inference approach to identify key determinants of carbon emissions in an evidence-based, data driven manner. The analysis highlights three primary drivers: access to clean cooking fuels in rural areas, access to clean cooking fuels in urban areas, and the percentage of population living in urban areas. These findings underscore the critical role of clean cooking technologies and urbanization patterns in shaping emission outcomes. In line with growing calls for evidence-based AI policy, ClimateAgents offers a modular and reflexive learning system that supports the generation of credible and actionable insights for policy. By integrating heterogeneous data modalities, including structured indicators, policy documents, and semantic reasoning, the framework contributes to adaptive policymaking infrastructures that can evolve with complex socio-technical challenges. This approach aims to support a shift from siloed modeling to reflexive, modular systems designed for dynamic, context-aware climate action.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15339v1" target="_blank"><h2>STREAM-VAE: Dual-Path Routing for Slow and Fast Dynamics in Vehicle Telemetry Anomaly Detection <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Kadir-Kaan Özer, René Ebeling, Markus Enzweiler<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 8 Pages, 4 Figures, 4 Tables<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> variational autoencoder (abstract), VAE (title, abstract), anomaly detection (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Automotive telemetry data exhibits slow drifts and fast spikes, often within the same sequence, making reliable anomaly detection challenging. Standard reconstruction-based methods, including sequence variational autoencoders (VAEs), use a single latent process and therefore mix heterogeneous time scales, which can smooth out spikes or inflate variances and weaken anomaly separation.
  In this paper, we present STREAM-VAE, a variational autoencoder for anomaly detection in automotive telemetry time-series data. Our model uses a dual-path encoder to separate slow drift and fast spike signal dynamics, and a decoder that represents transient deviations separately from the normal operating pattern. STREAM-VAE is designed for deployment, producing stable anomaly scores across operating modes for both in-vehicle monitors and backend fleet analytics.
  Experiments on an automotive telemetry dataset and the public SMD benchmark show that explicitly separating drift and spike dynamics improves robustness compared to strong forecasting, attention, graph, and VAE baselines.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15328v1" target="_blank"><h2>LaguerreNet: Advancing a Unified Solution for Heterophily and Over-smoothing with Adaptive Continuous Polynomials <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Huseyin Goksu<br><strong><u>Categories:</u></strong> cs.LG, eess.SP<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Spectral Graph Neural Networks (GNNs) suffer from two critical limitations: poor performance on "heterophilic" graphs and performance collapse at high polynomial degrees (K), known as over-smoothing. Both issues stem from the static, low-pass nature of standard filters (e.g., ChebyNet). While adaptive polynomial filters, such as the discrete MeixnerNet, have emerged as a potential unified solution, their extension to the continuous domain and stability with unbounded coefficients remain open questions. In this work, we propose `LaguerreNet`, a novel GNN filter based on continuous Laguerre polynomials. `LaguerreNet` learns the filter's spectral shape by making its core alpha parameter trainable, thereby advancing the adaptive polynomial approach. We solve the severe O(k^2) numerical instability of these unbounded polynomials using a `LayerNorm`-based stabilization technique. We demonstrate experimentally that this approach is highly effective: 1) `LaguerreNet` achieves state-of-the-art results on challenging heterophilic benchmarks. 2) It is exceptionally robust to over-smoothing, with performance peaking at K=10, an order of magnitude beyond where ChebyNet collapses.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15327v1" target="_blank"><h2>KrawtchoukNet: A Unified GNN Solution for Heterophily and Over-smoothing with Adaptive Bounded Polynomials <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Huseyin Goksu<br><strong><u>Categories:</u></strong> cs.LG, eess.SP<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Spectral Graph Neural Networks (GNNs) based on polynomial filters, such as ChebyNet, suffer from two critical limitations: 1) performance collapse on "heterophilic" graphs and 2) performance collapse at high polynomial degrees (K), known as over-smoothing. Both issues stem from the static, low-pass nature of standard filters. In this work, we propose `KrawtchoukNet`, a GNN filter based on the discrete Krawtchouk polynomials. We demonstrate that `KrawtchoukNet` provides a unified solution to both problems through two key design choices. First, by fixing the polynomial's domain N to a small constant (e.g., N=20), we create the first GNN filter whose recurrence coefficients are \textit{inherently bounded}, making it exceptionally robust to over-smoothing (achieving SOTA results at K=10). Second, by making the filter's shape parameter p learnable, the filter adapts its spectral response to the graph data. We show this adaptive nature allows `KrawtchoukNet` to achieve SOTA performance on challenging heterophilic benchmarks (Texas, Cornell), decisively outperforming standard GNNs like GAT and APPNP.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15324v1" target="_blank"><h2>On the Internal Semantics of Time-Series Foundation Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Atharva Pandey, Abhilash Neog, Gautam Jajoo<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> time-domain (abstract)<br><p><strong><u>Abstract:</u></strong> Time-series Foundation Models (TSFMs) have recently emerged as a universal paradigm for learning across diverse temporal domains. However, despite their empirical success, the internal mechanisms by which these models represent fundamental time-series concepts remain poorly understood. In this work, we undertake a systematic investigation of concept interpretability in TSFMs. Specifically, we examine: (i) which layers encode which concepts, (ii) whether concept parameters are linearly recoverable, (iii) how representations evolve in terms of concept disentanglement and abstraction across model depth, and (iv) how models process compositions of concepts. We systematically probe these questions using layer-wise analyses, linear recoverability tests, and representation similarity measures, providing a structured account of TSFM semantics. The resulting insights show that early layers mainly capture local, time-domain patterns (e.g., AR(1), level shifts, trends), while deeper layers encode dispersion and change-time signals, with spectral and warping factors remaining the hardest to recover linearly. In compositional settings, however, probe performance degrades, revealing interference between concepts. This highlights that while atomic concepts are reliably localized, composition remains a challenge, underscoring a key limitation in current TSFMs' ability to represent interacting temporal phenomena.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15313v1" target="_blank"><h2>A Method for Gamma-Ray Energy Spectrum Inversion and Correction <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zhi-Qiang Ding, Xin-Qiao Li, Da-Li Zhang, Zheng-Hua An, Zhen-Xia Zhang, Roberto Battiston, Roberto Iuppa, Zhuo Li, Yan-Qiu Zhang, Yan Huang, Chao Zheng, Yan-Bing Xu, Xiao-Yun Zhao, Lu Wang, Ping Wang, Hong Lu<br><strong><u>Categories:</u></strong> astro-ph.HE, astro-ph.IM<br><strong><u>Comments:</u></strong> The Astrophysical Journal has accepted<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Accurate spectral analysis of high-energy astrophysical sources often relies on comparing observed data to incident spectral models convolved with the instrument response. However, for Gamma-Ray Bursts and other high-energy transient events observed at high count rates, significant distortions (e.g., pile-up, dead time, and large signal trailing) are introduced, complicating this analysis. We present a method framework to address the model dependence problem, especially to solve the problem of energy spectrum distortion caused by instrument signal pile-up due to high counting rate and high-rate effects, applicable to X-ray, gamma-ray, and particle detectors. Our approach combines physics-based Monte Carlo (MC) simulations with a model-independent spectral inversion technique. The MC simulations quantify instrumental effects and enable correction of the distorted spectrum. Subsequently, the inversion step reconstructs the incident spectrum using an inverse response matrix approach, conceptually equivalent to deconvolving the detector response. The inversion employs a Convolutional Neural Network, selected for its numerical stability and effective handling of complex detector responses. Validation using simulations across diverse input spectra demonstrates high fidelity. Specifically, for 27 different parameter sets of the brightest gamma-ray bursts, goodness-of-fit tests confirm the reconstructed spectra are in excellent statistical agreement with the input spectra, and residuals are typically within $\pm 2σ$. This method enables precise analysis of intense transients and other high-flux events, overcoming limitations imposed by instrumental effects in traditional analyses.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15759v1" target="_blank"><h2>Securing AI Agents Against Prompt Injection Attacks <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Badrinath Ramakrishnan, Akshaya Balaji<br><strong><u>Categories:</u></strong> cs.CR, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> anomaly detection (abstract)<br><p><strong><u>Abstract:</u></strong> Retrieval-augmented generation (RAG) systems have become widely used for enhancing large language model capabilities, but they introduce significant security vulnerabilities through prompt injection attacks. We present a comprehensive benchmark for evaluating prompt injection risks in RAG-enabled AI agents and propose a multi-layered defense framework. Our benchmark includes 847 adversarial test cases across five attack categories: direct injection, context manipulation, instruction override, data exfiltration, and cross-context contamination. We evaluate three defense mechanisms: content filtering with embedding-based anomaly detection, hierarchical system prompt guardrails, and multi-stage response verification, across seven state-of-the-art language models. Our combined framework reduces successful attack rates from 73.2% to 8.7% while maintaining 94.3% of baseline task performance. We release our benchmark dataset and defense implementation to support future research in AI agent security.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15271v1" target="_blank"><h2>Graph Query Networks for Object Detection with Automotive Radar <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Loveneet Saini, Hasan Tercan, Tobias Meisen<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> Accepted in WACV 2026 Main Conference<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Object detection with 3D radar is essential for 360-degree automotive perception, but radar's long wavelengths produce sparse and irregular reflections that challenge traditional grid and sequence-based convolutional and transformer detectors. This paper introduces Graph Query Networks (GQN), an attention-based framework that models objects sensed by radar as graphs, to extract individualized relational and contextual features. GQN employs a novel concept of graph queries to dynamically attend over the bird's-eye view (BEV) space, constructing object-specific graphs processed by two novel modules: EdgeFocus for relational reasoning and DeepContext Pooling for contextual aggregation. On the NuScenes dataset, GQN improves relative mAP by up to +53%, including a +8.2% gain over the strongest prior radar method, while reducing peak graph construction overhead by 80% with moderate FLOPs cost.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15262v1" target="_blank"><h2>Reinforcement Learning in Queue-Reactive Models: Application to Optimal Execution <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Tomas Espana, Yadh Hafsi, Fabrizio Lillo, Edoardo Vittori<br><strong><u>Categories:</u></strong> q-fin.TR, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> We investigate the use of Reinforcement Learning for the optimal execution of meta-orders, where the objective is to execute incrementally large orders while minimizing implementation shortfall and market impact over an extended period of time. Departing from traditional parametric approaches to price dynamics and impact modeling, we adopt a model-free, data-driven framework. Since policy optimization requires counterfactual feedback that historical data cannot provide, we employ the Queue-Reactive Model to generate realistic and tractable limit order book simulations that encompass transient price impact, and nonlinear and dynamic order flow responses. Methodologically, we train a Double Deep Q-Network agent on a state space comprising time, inventory, price, and depth variables, and evaluate its performance against established benchmarks. Numerical simulation results show that the agent learns a policy that is both strategic and tactical, adapting effectively to order book conditions and outperforming standard approaches across multiple training configurations. These findings provide strong evidence that model-free Reinforcement Learning can yield adaptive and robust solutions to the optimal execution problem.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15253v1" target="_blank"><h2>PresentCoach: Dual-Agent Presentation Coaching through Exemplars and Interactive Feedback <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Sirui Chen, Jinsong Zhou, Xinli Xu, Xiaoyu Yang, Litao Guo, Ying-Cong Chen<br><strong><u>Categories:</u></strong> cs.HC, cs.AI<br><strong><u>Comments:</u></strong> 13pages,6figures<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Effective presentation skills are essential in education, professional communication, and public speaking, yet learners often lack access to high-quality exemplars or personalized coaching. Existing AI tools typically provide isolated functionalities such as speech scoring or script generation without integrating reference modeling and interactive feedback into a cohesive learning experience. We introduce a dual-agent system that supports presentation practice through two complementary roles: the Ideal Presentation Agent and the Coach Agent. The Ideal Presentation Agent converts user-provided slides into model presentation videos by combining slide processing, visual-language analysis, narration script generation, personalized voice synthesis, and synchronized video assembly. The Coach Agent then evaluates user-recorded presentations against these exemplars, conducting multimodal speech analysis and delivering structured feedback in an Observation-Impact-Suggestion (OIS) format. To enhance the authenticity of the learning experience, the Coach Agent incorporates an Audience Agent, which simulates the perspective of a human listener and provides humanized feedback reflecting audience reactions and engagement. Together, these agents form a closed loop of observation, practice, and feedback. Implemented on a robust backend with multi-model integration, voice cloning, and error handling mechanisms, the system demonstrates how AI-driven agents can provide engaging, human-centered, and scalable support for presentation skill development in both educational and professional contexts.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15251v1" target="_blank"><h2>PLATONT: Learning a Platonic Representation for Unified Network Tomography <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Chengze Du, Heng Xu, Zhiwei Yu, Bo Liu, Jialong Li<br><strong><u>Categories:</u></strong> cs.LG, cs.NI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> latent space (abstract), multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Network tomography aims to infer hidden network states, such as link performance, traffic load, and topology, from external observations. Most existing methods solve these problems separately and depend on limited task-specific signals, which limits generalization and interpretability. We present PLATONT, a unified framework that models different network indicators (e.g., delay, loss, bandwidth) as projections of a shared latent network state. Guided by the Platonic Representation Hypothesis, PLATONT learns this latent state through multimodal alignment and contrastive learning. By training multiple tomography tasks within a shared latent space, it builds compact and structured representations that improve cross-task generalization. Experiments on synthetic and real-world datasets show that PLATONT consistently outperforms existing methods in link estimation, topology inference, and traffic prediction, achieving higher accuracy and stronger robustness under varying network conditions.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15246v1" target="_blank"><h2>D2D Power Allocation via Quantum Graph Neural Network <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Tung Giang Le, Xuan Tung Nguyen, Won-Joo Hwang<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> Increasing wireless network complexity demands scalable resource management. Classical GNNs excel at graph learning but incur high computational costs in large-scale settings. We present a fully quantum Graph Neural Network (QGNN) that implements message passing via Parameterized Quantum Circuits (PQCs). Our Quantum Graph Convolutional Layers (QGCLs) encode features into quantum states, process graphs with NISQ-compatible unitaries, and retrieve embeddings through measurement. Applied to D2D power control for SINR maximization, our QGNN matches classical performance with fewer parameters and inherent parallelism. This end-to-end PQC-based GNN marks a step toward quantum-accelerated wireless optimization.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15222v1" target="_blank"><h2>Why Physics Still Matters: Improving Machine Learning Prediction of Material Properties with Phonon-Informed Datasets <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Pol Benítez, Cibrán López, Edgardo Saucedo, Teruyasu Mizoguchi, Claudio Cazorla<br><strong><u>Categories:</u></strong> cond-mat.mtrl-sci, cs.LG<br><strong><u>Comments:</u></strong> 12 pages; 5 figures<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> explainability (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Machine learning (ML) methods have become powerful tools for predicting material properties with near first-principles accuracy and vastly reduced computational cost. However, the performance of ML models critically depends on the quality, size, and diversity of the training dataset. In materials science, this dependence is particularly important for learning from low-symmetry atomistic configurations that capture thermal excitations, structural defects, and chemical disorder, features that are ubiquitous in real materials but underrepresented in most datasets. The absence of systematic strategies for generating representative training data may therefore limit the predictive power of ML models in technologically critical fields such as energy conversion and photonics. In this work, we assess the effectiveness of graph neural network (GNN) models trained on two fundamentally different types of datasets: one composed of randomly generated atomic configurations and another constructed using physically informed sampling based on lattice vibrations. As a case study, we address the challenging task of predicting electronic and mechanical properties of a prototypical family of optoelectronic materials under realistic finite-temperature conditions. We find that the phonons-informed model consistently outperforms the randomly trained counterpart, despite relying on fewer data points. Explainability analyses further reveal that high-performing models assign greater weight to chemically meaningful bonds that control property variations, underscoring the importance of physically guided data generation. Overall, this work demonstrates that larger datasets do not necessarily yield better GNN predictive models and introduces a simple and general strategy for efficiently constructing high-quality training data in materials informatics.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15211v2" target="_blank"><h2>OEMA: Ontology-Enhanced Multi-Agent Collaboration Framework for Zero-Shot Clinical Named Entity Recognition <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Xinli Tao, Xin Dong, Xuezhong Zhou<br><strong><u>Categories:</u></strong> cs.CL, cs.AI<br><strong><u>Comments:</u></strong> 12 pages, 4 figures, 4 tables<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> VAE (abstract), domain adaptation (abstract)<br><p><strong><u>Abstract:</u></strong> With the rapid expansion of unstructured clinical texts in electronic health records (EHRs), clinical named entity recognition (NER) has become a crucial technique for extracting medical information. However, traditional supervised models such as CRF and BioClinicalBERT suffer from high annotation costs. Although zero-shot NER based on large language models (LLMs) reduces the dependency on labeled data, challenges remain in aligning example selection with task granularity and effectively integrating prompt design with self-improvement frameworks. To address these limitations, we propose OEMA, a novel zero-shot clinical NER framework based on multi-agent collaboration. OEMA consists of three core components: (1) a self-annotator that autonomously generates candidate examples; (2) a discriminator that leverages SNOMED CT to filter token-level examples by clinical relevance; and (3) a predictor that incorporates entity-type descriptions to enhance inference accuracy. Experimental results on two benchmark datasets, MTSamples and VAERS, demonstrate that OEMA achieves state-of-the-art performance under exact-match evaluation. Moreover, under related-match criteria, OEMA performs comparably to the supervised BioClinicalBERT model while significantly outperforming the traditional CRF method. OEMA improves zero-shot clinical NER, achieving near-supervised performance under related-match criteria. Future work will focus on continual learning and open-domain adaptation to expand its applicability in clinical NLP.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15204v1" target="_blank"><h2>Physics-Based Benchmarking Metrics for Multimodal Synthetic Images <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Kishor Datta Gupta, Marufa Kamal, Md. Mahfuzur Rahman, Fahad Rahman, Mohd Ariful Haque, Sunzida Siddique<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Current state of the art measures like BLEU, CIDEr, VQA score, SigLIP-2 and CLIPScore are often unable to capture semantic or structural accuracy, especially for domain-specific or context-dependent scenarios. For this, this paper proposes a Physics-Constrained Multimodal Data Evaluation (PCMDE) metric combining large language models with reasoning, knowledge based mapping and vision-language models to overcome these limitations. The architecture is comprised of three main stages: (1) feature extraction of spatial and semantic information with multimodal features through object detection and VLMs; (2) Confidence-Weighted Component Fusion for adaptive component-level validation; and (3) physics-guided reasoning using large language models for structural and relational constraints (e.g., alignment, position, consistency) enforcement.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15199v1" target="_blank"><h2>Learning Where, What and How to Transfer: A Multi-Role Reinforcement Learning Approach for Evolutionary Multitasking <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jiajun Zhan, Zeyuan Ma, Yue-Jiao Gong, Kay Chen Tan<br><strong><u>Categories:</u></strong> cs.NE, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Evolutionary multitasking (EMT) algorithms typically require tailored designs for knowledge transfer, in order to assure convergence and optimality in multitask optimization. In this paper, we explore designing a systematic and generalizable knowledge transfer policy through Reinforcement Learning. We first identify three major challenges: determining the task to transfer (where), the knowledge to be transferred (what) and the mechanism for the transfer (how). To address these challenges, we formulate a multi-role RL system where three (groups of) policy networks act as specialized agents: a task routing agent incorporates an attention-based similarity recognition module to determine source-target transfer pairs via attention scores; a knowledge control agent determines the proportion of elite solutions to transfer; and a group of strategy adaptation agents control transfer strength by dynamically controlling hyper-parameters in the underlying EMT framework. Through pre-training all network modules end-to-end over an augmented multitask problem distribution, a generalizable meta-policy is obtained. Comprehensive validation experiments show state-of-the-art performance of our method against representative baselines. Further in-depth analysis not only reveals the rationale behind our proposal but also provide insightful interpretations on what the system have learned.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15196v1" target="_blank"><h2>Particle Monte Carlo methods for Lattice Field Theory <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> David Yallup<br><strong><u>Categories:</u></strong> stat.ML, cs.LG, hep-lat<br><strong><u>Comments:</u></strong> To appear in the NeurIPS 2025 workshop, Frontiers in Probabilistic Inference: Sampling Meets Learning<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> High-dimensional multimodal sampling problems from lattice field theory (LFT) have become important benchmarks for machine learning assisted sampling methods. We show that GPU-accelerated particle methods, Sequential Monte Carlo (SMC) and nested sampling, provide a strong classical baseline that matches or outperforms state-of-the-art neural samplers in sample quality and wall-clock time on standard scalar field theory benchmarks, while also estimating the partition function. Using only a single data-driven covariance for tuning, these methods achieve competitive performance without problem-specific structure, raising the bar for when learned proposals justify their training cost.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15194v1" target="_blank"><h2>Eq.Bot: Enhance Robotic Manipulation Learning via Group Equivariant Canonicalization <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jian Deng, Yuandong Wang, Yangfu Zhu, Tao Feng, Tianyu Wo, Zhenzhou Shao<br><strong><u>Categories:</u></strong> cs.RO, cs.AI<br><strong><u>Comments:</u></strong> 12 pages, 4 figures and 3 tables<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multi-modal (abstract), transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Robotic manipulation systems are increasingly deployed across diverse domains. Yet existing multi-modal learning frameworks lack inherent guarantees of geometric consistency, struggling to handle spatial transformations such as rotations and translations. While recent works attempt to introduce equivariance through bespoke architectural modifications, these methods suffer from high implementation complexity, computational cost, and poor portability. Inspired by human cognitive processes in spatial reasoning, we propose Eq.Bot, a universal canonicalization framework grounded in SE(2) group equivariant theory for robotic manipulation learning. Our framework transforms observations into a canonical space, applies an existing policy, and maps the resulting actions back to the original space. As a model-agnostic solution, Eq.Bot aims to endow models with spatial equivariance without requiring architectural modifications. Extensive experiments demonstrate the superiority of Eq.Bot under both CNN-based (e.g., CLIPort) and Transformer-based (e.g., OpenVLA-OFT) architectures over existing methods on various robotic manipulation tasks, where the most significant improvement can reach 50.0%.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15191v1" target="_blank"><h2>HISE-KT: Synergizing Heterogeneous Information Networks and LLMs for Explainable Knowledge Tracing with Meta-Path Optimization <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zhiyi Duan, Zixing Shi, Hongyu Yuan, Qi Wang<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> explainable (title, abstract)<br><p><strong><u>Abstract:</u></strong> Knowledge Tracing (KT) aims to mine students' evolving knowledge states and predict their future question-answering performance. Existing methods based on heterogeneous information networks (HINs) are prone to introducing noises due to manual or random selection of meta-paths and lack necessary quality assessment of meta-path instances. Conversely, recent large language models (LLMs)-based methods ignore the rich information across students, and both paradigms struggle to deliver consistently accurate and evidence-based explanations. To address these issues, we propose an innovative framework, HIN-LLM Synergistic Enhanced Knowledge Tracing (HISE-KT), which seamlessly integrates HINs with LLMs. HISE-KT first builds a multi-relationship HIN containing diverse node types to capture the structural relations through multiple meta-paths. The LLM is then employed to intelligently score and filter meta-path instances and retain high-quality paths, pioneering automated meta-path quality assessment. Inspired by educational psychology principles, a similar student retrieval mechanism based on meta-paths is designed to provide a more valuable context for prediction. Finally, HISE-KT uses a structured prompt to integrate the target student's history with the retrieved similar trajectories, enabling the LLM to generate not only accurate predictions but also evidence-backed, explainable analysis reports. Experiments on four public datasets show that HISE-KT outperforms existing KT baselines in both prediction performance and interpretability.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15188v2" target="_blank"><h2>BrainRotViT: Transformer-ResNet Hybrid for Explainable Modeling of Brain Aging from 3D sMRI <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Wasif Jalal, Md Nafiu Rahman, Atif Hasan Rahman, M. Sohel Rahman<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> explainable (title), transformer (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Accurate brain age estimation from structural MRI is a valuable biomarker for studying aging and neurodegeneration. Traditional regression and CNN-based methods face limitations such as manual feature engineering, limited receptive fields, and overfitting on heterogeneous data. Pure transformer models, while effective, require large datasets and high computational cost. We propose Brain ResNet over trained Vision Transformer (BrainRotViT), a hybrid architecture that combines the global context modeling of vision transformers (ViT) with the local refinement of residual CNNs. A ViT encoder is first trained on an auxiliary age and sex classification task to learn slice-level features. The frozen encoder is then applied to all sagittal slices to generate a 2D matrix of embedding vectors, which is fed into a residual CNN regressor that incorporates subject sex at the final fully-connected layer to estimate continuous brain age. Our method achieves an MAE of 3.34 years (Pearson $r=0.98$, Spearman $ρ=0.97$, $R^2=0.95$) on validation across 11 MRI datasets encompassing more than 130 acquisition sites, outperforming baseline and state-of-the-art models. It also generalizes well across 4 independent cohorts with MAEs between 3.77 and 5.04 years. Analyses on the brain age gap (the difference between the predicted age and actual age) show that aging patterns are associated with Alzheimer's disease, cognitive impairment, and autism spectrum disorder. Model attention maps highlight aging-associated regions of the brain, notably the cerebellar vermis, precentral and postcentral gyri, temporal lobes, and medial superior frontal gyrus. Our results demonstrate that this method provides an efficient, interpretable, and generalizable framework for brain-age prediction, bridging the gap between CNN- and transformer-based approaches while opening new avenues for aging and neurodegeneration research.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15183v1" target="_blank"><h2>HinTel-AlignBench: A Framework and Benchmark for Hindi-Telugu with English-Aligned Samples <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Rishikant Chigrupaatii, Ponnada Sai Tulasi Kanishka, Lalit Chandra Routhu, Martin Patel Sama Supratheek Reddy, Divyam Gupta, Dasari Srikar, Krishna Teja Kuchimanchi, Rajiv Misra, Rohun Tripathi<br><strong><u>Categories:</u></strong> cs.CL, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> With nearly 1.5 billion people and more than 120 major languages, India represents one of the most diverse regions in the world. As multilingual Vision-Language Models (VLMs) gain prominence, robust evaluation methodologies are essential to drive progress toward equitable AI for low-resource languages. Current multilingual VLM evaluations suffer from four major limitations: reliance on unverified auto-translations, narrow task/domain coverage, limited sample sizes, and lack of cultural and natively sourced Question-Answering (QA). To address these gaps, we present a scalable framework to evaluate VLMs in Indian languages and compare it with performance in English. Using the framework, we generate HinTel-AlignBench, a benchmark that draws from diverse sources in Hindi and Telugu with English-aligned samples. Our contributions are threefold: (1) a semi-automated dataset creation framework combining back-translation, filtering, and human verification; (2) the most comprehensive vision-language benchmark for Hindi and and Telugu, including adapted English datasets (VQAv2, RealWorldQA, CLEVR-Math) and native novel Indic datasets (JEE for STEM, VAANI for cultural grounding) with approximately 4,000 QA pairs per language; and (3) a detailed performance analysis of various State-of-the-Art (SOTA) open-weight and closed-source VLMs. We find a regression in performance for tasks in English versus in Indian languages for 4 out of 5 tasks across all the models, with an average regression of 8.3 points in Hindi and 5.5 points for Telugu. We categorize common failure modes to highlight concrete areas of improvement in multilingual multimodal understanding.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15175v1" target="_blank"><h2>Vehicle Routing Problems via Quantum Graph Attention Network Deep Reinforcement Learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Le Tung Giang, Vu Hoang Viet, Nguyen Xuan Tung, Trinh Van Chien, Won-Joo Hwang<br><strong><u>Categories:</u></strong> cs.LG, cs.IT, quant-ph<br><strong><u>Comments:</u></strong> 11 pages, 3 figures, 2 tables. Accepted by SOICT 2025<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> The vehicle routing problem (VRP) is a fundamental NP-hard task in intelligent transportation systems with broad applications in logistics and distribution. Deep reinforcement learning (DRL) with Graph Neural Networks (GNNs) has shown promise, yet classical models rely on large multi-layer perceptrons (MLPs) that are parameter-heavy and memory-bound. We propose a Quantum Graph Attention Network (Q-GAT) within a DRL framework, where parameterized quantum circuits (PQCs) replace conventional MLPs at critical readout stages. The hybrid model maintains the expressive capacity of graph attention encoders while reducing trainable parameters by more than 50%. Using proximal policy optimization (PPO) with greedy and stochastic decoding, experiments on VRP benchmarks show that Q-GAT achieves faster convergence and reduces routing cost by about 5% compared with classical GAT baselines. These results demonstrate the potential of PQC-enhanced GNNs as compact and effective solvers for large-scale routing and logistics optimization.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15174v1" target="_blank"><h2>FaultDiffusion: Few-Shot Fault Time Series Generation with Diffusion Model <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yi Xu, Zhigang Chen, Rui Wang, Yangfan Li, Fengxiao Tang, Ming Zhao, Jiaqi Liu<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 4 figures, 5 tables ,8 pages<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> In industrial equipment monitoring, fault diagnosis is critical for ensuring system reliability and enabling predictive maintenance. However, the scarcity of fault data, due to the rarity of fault events and the high cost of data annotation, significantly hinders data-driven approaches. Existing time-series generation models, optimized for abundant normal data, struggle to capture fault distributions in few-shot scenarios, producing samples that lack authenticity and diversity due to the large domain gap and high intra-class variability of faults. To address this, we propose a novel few-shot fault time-series generation framework based on diffusion models. Our approach employs a positive-negative difference adapter, leveraging pre-trained normal data distributions to model the discrepancies between normal and fault domains for accurate fault synthesis. Additionally, a diversity loss is introduced to prevent mode collapse, encouraging the generation of diverse fault samples through inter-sample difference regularization. Experimental results demonstrate that our model significantly outperforms traditional methods in authenticity and diversity, achieving state-of-the-art performance on key benchmarks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15173v1" target="_blank"><h2>Data-driven Prediction of Species-Specific Plant Responses to Spectral-Shifting Films from Leaf Phenotypic and Photosynthetic Traits <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jun Hyeun Kang, Jung Eek Son, Tae In Ahn<br><strong><u>Categories:</u></strong> q-bio.QM, cs.CV, cs.LG, eess.IV<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> data-driven (title), variational autoencoder (abstract), neural network (abstract), data augmentation (abstract)<br><p><strong><u>Abstract:</u></strong> The application of spectral-shifting films in greenhouses to shift green light to red light has shown variable growth responses across crop species. However, the yield enhancement of crops under altered light quality is related to the collective effects of the specific biophysical characteristics of each species. Considering only one attribute of a crop has limitations in understanding the relationship between sunlight quality adjustments and crop growth performance. Therefore, this study aims to comprehensively link multiple plant phenotypic traits and daily light integral considering the physiological responses of crops to their growth outcomes under SF using artificial intelligence. Between 2021 and 2024, various leafy, fruiting, and root crops were grown in greenhouses covered with either PEF or SF, and leaf reflectance, leaf mass per area, chlorophyll content, daily light integral, and light saturation point were measured from the plants cultivated in each condition. 210 data points were collected, but there was insufficient data to train deep learning models, so a variational autoencoder was used for data augmentation. Most crop yields showed an average increase of 22.5% under SF. These data were used to train several models, including logistic regression, decision tree, random forest, XGBoost, and feedforward neural network (FFNN), aiming to binary classify whether there was a significant effect on yield with SF application. The FFNN achieved a high classification accuracy of 91.4% on a test dataset that was not used for training. This study provide insight into the complex interactions between leaf phenotypic and photosynthetic traits, environmental conditions, and solar spectral components by improving the ability to predict solar spectral shift effects using SF.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15172v2" target="_blank"><h2>Complex variational autoencoders admit Kähler structure <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Andrew Gracyk<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> variational autoencoder (title, abstract), VAE (abstract), latent space (abstract)<br><p><strong><u>Abstract:</u></strong> It has been discovered that latent-Euclidean variational autoencoders (VAEs) admit, in various capacities, Riemannian structure. We adapt these arguments but for complex VAEs with a complex latent stage. We show that complex VAEs reveal to some level Kähler geometric structure. Our methods will be tailored for decoder geometry. We derive the Fisher information metric in the complex case under a latent complex Gaussian regularization with trivial relation matrix. It is well known from statistical information theory that the Fisher information coincides with the Hessian of the Kullback-Leibler (KL) divergence. Thus, the metric Kähler potential relation is exactly achieved under relative entropy. We propose a Kähler potential derivative of complex Gaussian mixtures that has rough equivalence to the Fisher information metric while still being faithful to the underlying Kähler geometry. Computation of the metric via this potential is efficient, and through our potential, valid as a plurisubharmonic (PSH) function, large scale computational burden of automatic differentiation is displaced to small scale. We show that we can regularize the latent space with decoder geometry, and that we can sample in accordance with a weighted complex volume element. We demonstrate these strategies, at the exchange of sample variation, yield consistently smoother representations and fewer semantic outliers.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15167v1" target="_blank"><h2>Learning Depth from Past Selves: Self-Evolution Contrast for Robust Depth Estimation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jing Cao, Kui Jiang, Shenyi Li, Xiaocheng Feng, Yong Huang<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Self-supervised depth estimation has gained significant attention in autonomous driving and robotics. However, existing methods exhibit substantial performance degradation under adverse weather conditions such as rain and fog, where reduced visibility critically impairs depth prediction. To address this issue, we propose a novel self-evolution contrastive learning framework called SEC-Depth for self-supervised robust depth estimation tasks. Our approach leverages intermediate parameters generated during training to construct temporally evolving latency models. Using these, we design a self-evolution contrastive scheme to mitigate performance loss under challenging conditions. Concretely, we first design a dynamic update strategy of latency models for the depth estimation task to capture optimization states across training stages. To effectively leverage latency models, we introduce a self-evolution contrastive Loss (SECL) that treats outputs from historical latency models as negative samples. This mechanism adaptively adjusts learning objectives while implicitly sensing weather degradation severity, reducing the needs for manual intervention. Experiments show that our method integrates seamlessly into diverse baseline models and significantly enhances robustness in zero-shot evaluations.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15165v1" target="_blank"><h2>Can MLLMs Detect Phishing? A Comprehensive Security Benchmark Suite Focusing on Dynamic Threats and Multimodal Evaluation in Academic Environments <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jingzhuo Zhou<br><strong><u>Categories:</u></strong> cs.CR, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> The rapid proliferation of Multimodal Large Language Models (MLLMs) has introduced unprecedented security challenges, particularly in phishing detection within academic environments. Academic institutions and researchers are high-value targets, facing dynamic, multilingual, and context-dependent threats that leverage research backgrounds, academic collaborations, and personal information to craft highly tailored attacks. Existing security benchmarks largely rely on datasets that do not incorporate specific academic background information, making them inadequate for capturing the evolving attack patterns and human-centric vulnerability factors specific to academia. To address this gap, we present AdapT-Bench, a unified methodological framework and benchmark suite for systematically evaluating MLLM defense capabilities against dynamic phishing attacks in academic settings.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15162v1" target="_blank"><h2>Multimodal Wireless Foundation Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Ahmed Aboulfotouh, Hatem Abou-Zeid<br><strong><u>Categories:</u></strong> eess.SP, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Wireless foundation models (WFMs) have recently demonstrated promising capabilities, jointly performing multiple wireless functions and adapting effectively to new environments. However, while current WFMs process only one modality, depending on the task and operating conditions, the most informative modality changes and no single modality is best for all tasks. WFMs should therefore be designed to accept multiple modalities to enable a broader and more diverse range of tasks and scenarios. In this work, we propose and build the first multimodal wireless foundation model capable of processing both raw IQ streams and image-like wireless modalities (e.g., spectrograms and CSI) and performing multiple tasks across both. We introduce masked wireless modeling for the multimodal setting, a self-supervised objective and pretraining recipe that learns a joint representation from IQ streams and image-like wireless modalities. We evaluate the model on five tasks across both modality families: image-based (human activity sensing, RF signal classification, 5G NR positioning) and IQ-based (RF device fingerprinting, interference detection/classification). The multimodal WFM is competitive with single-modality WFMs, and in several cases surpasses their performance. Our results demonstrates the strong potential of developing multimodal WFMs that support diverse wireless tasks across different modalities. We believe this provides a concrete step toward both AI-native 6G and the vision of joint sensing, communication, and localization.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15158v1" target="_blank"><h2>Non-thermal processes in standard big bang nucleosynthesis: III. Reactions with slow nuclei and the overall effect <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Victor T. Voronchev<br><strong><u>Categories:</u></strong> astro-ph.CO<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> The present paper completes a series of our works on non-thermal nuclear processes in big bang nucleosynthesis (BBN) started in JCAP05(2008)010 (Part I) and 05(2009)001 (Part II). The processes are triggered by non-Maxwellian particles naturally born in the main BBN reactions. Half of these reactions generate fast particles k^+ (= n,p,t,3He,alpha). The other half, being radiative capture processes, produce slow nuclei k^- (= d,t,3He,7Li,7Be) which can undergo (k^-,n) reactions with neutrons having large cross sections. The particle production rate R_k, thermalization time tau_k, and effective number density n_k are determined. It is shown that the values of n_k at the Universe temperatures T > 65 keV can exceed the number densities of Maxwellian 7Li and 7Be ions. To clarify the overall non-Maxwellian effect on BBN, both types of the non-Maxwellian particles are taken into account in the reaction network. Particular attention is paid to two-step sequential processes like p(n,gamma)d^-(n,gamma)t, d(p,gamma)3He^-(n,p)t, t(alpha,gamma)7Li^-(n,gamma)8Li, 3He(alpha,gamma)7Be^-(n,p)7Li, d(t,alpha)n^+(A,n)a_1a_2, and d(3He,alpha)p^+(A,p)a_1a_2 with (A,a_1,a_2) = 7Li,t,alpha) and (7Be,3He,alpha. It is obtained that the non-Maxwellian particles can selectively affect the element abundances, e.g., improve the prediction on 7Li/H by ~1.5% and at the same time leave unchanged the 4He abundance. The main conclusion however is that these particles are unable to significantly change the standard picture of BBN in general, and provide a pathway toward a solution of the cosmological lithium problem in particular.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15151v1" target="_blank"><h2>DCL-SE: Dynamic Curriculum Learning for Spatiotemporal Encoding of Brain Imaging <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Meihua Zhou, Xinyu Tong, Jiarui Zhao, Min Cheng, Li Yang, Lei Tian, Nan Wan<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> High-dimensional neuroimaging analyses for clinical diagnosis are often constrained by compromises in spatiotemporal fidelity and by the limited adaptability of large-scale, general-purpose models. To address these challenges, we introduce Dynamic Curriculum Learning for Spatiotemporal Encoding (DCL-SE), an end-to-end framework centered on data-driven spatiotemporal encoding (DaSE). We leverage Approximate Rank Pooling (ARP) to efficiently encode three-dimensional volumetric brain data into information-rich, two-dimensional dynamic representations, and then employ a dynamic curriculum learning strategy, guided by a Dynamic Group Mechanism (DGM), to progressively train the decoder, refining feature extraction from global anatomical structures to fine pathological details. Evaluated across six publicly available datasets, including Alzheimer's disease and brain tumor classification, cerebral artery segmentation, and brain age prediction, DCL-SE consistently outperforms existing methods in accuracy, robustness, and interpretability. These findings underscore the critical importance of compact, task-specific architectures in the era of large-scale pretrained networks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15139v1" target="_blank"><h2>CASPER: Cross-modal Alignment of Spatial and single-cell Profiles for Expression Recovery <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Amit Kumar, Maninder Kaur, Raghvendra Mall, Sukrit Gupta<br><strong><u>Categories:</u></strong> q-bio.GN, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Spatial Transcriptomics enables mapping of gene expression within its native tissue context, but current platforms measure only a limited set of genes due to experimental constraints and excessive costs. To overcome this, computational models integrate Single-Cell RNA Sequencing data with Spatial Transcriptomics to predict unmeasured genes. We propose CASPER, a cross-attention based framework that predicts unmeasured gene expression in Spatial Transcriptomics by leveraging centroid-level representations from Single-Cell RNA Sequencing. We performed rigorous testing over four state-of-the-art Spatial Transcriptomics/Single-Cell RNA Sequencing dataset pairs across four existing baseline models. CASPER shows significant improvement in nine out of the twelve metrics for our experiments. This work paves the way for further work in Spatial Transcriptomics to Single-Cell RNA Sequencing modality translation. The code for CASPER is available at https://github.com/AI4Med-Lab/CASPER.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15138v1" target="_blank"><h2>Cross-Modal Consistency-Guided Active Learning for Affective BCI Systems <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Hyo-Jeong Jang, Hye-Bin Shin, Kang Yin<br><strong><u>Categories:</u></strong> cs.LG, cs.HC<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> latent space (abstract)<br><p><strong><u>Abstract:</u></strong> Deep learning models perform best with abundant, high-quality labels, yet such conditions are rarely achievable in EEG-based emotion recognition. Electroencephalogram (EEG) signals are easily corrupted by artifacts and individual variability, while emotional labels often stem from subjective and inconsistent reports-making robust affective decoding particularly difficult. We propose an uncertainty-aware active learning framework that enhances robustness to label noise by jointly leveraging model uncertainty and cross-modal consistency. Instead of relying solely on EEG-based uncertainty estimates, the method evaluates cross-modal alignment to determine whether uncertainty originates from cognitive ambiguity or sensor noise. A representation alignment module embeds EEG and face features into a shared latent space, enforcing semantic coherence between modalities. Residual discrepancies are treated as noise-induced inconsistencies, and these samples are selectively queried for oracle feedback during active learning. This feedback-driven process guides the network toward reliable, informative samples and reduces the impact of noisy labels. Experiments on the ASCERTAIN dataset examine the efficiency and robustness of ours, highlighting its potential as a data-efficient and noise-tolerant approach for EEG-based affective decoding in brain-computer interface systems.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15752v1" target="_blank"><h2>Build AI Assistants using Large Language Models and Agents to Enhance the Engineering Education of Biomechanics <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Hanzhi Yan, Qin Lu, Xianqiao Wang, Xiaoming Zhai, Tianming Liu, He Li<br><strong><u>Categories:</u></strong> cs.AI, cs.MA<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> explainable (abstract)<br><p><strong><u>Abstract:</u></strong> While large language models (LLMs) have demonstrated remarkable versatility across a wide range of general tasks, their effectiveness often diminishes in domain-specific applications due to inherent knowledge gaps. Moreover, their performance typically declines when addressing complex problems that require multi-step reasoning and analysis. In response to these challenges, we propose leveraging both LLMs and AI agents to develop education assistants aimed at enhancing undergraduate learning in biomechanics courses that focus on analyzing the force and moment in the musculoskeletal system of the human body. To achieve our goal, we construct a dual-module framework to enhance LLM performance in biomechanics educational tasks: 1) we apply Retrieval-Augmented Generation (RAG) to improve the specificity and logical consistency of LLM's responses to the conceptual true/false questions; 2) we build a Multi-Agent System (MAS) to solve calculation-oriented problems involving multi-step reasoning and code execution. Specifically, we evaluate the performance of several LLMs, i.e., Qwen-1.0-32B, Qwen-2.5-32B, and Llama-70B, on a biomechanics dataset comprising 100 true/false conceptual questions and problems requiring equation derivation and calculation. Our results demonstrate that RAG significantly enhances the performance and stability of LLMs in answering conceptual questions, surpassing those of vanilla models. On the other hand, the MAS constructed using multiple LLMs demonstrates its ability to perform multi-step reasoning, derive equations, execute code, and generate explainable solutions for tasks that require calculation. These findings demonstrate the potential of applying RAG and MAS to enhance LLM performance for specialized courses in engineering curricula, providing a promising direction for developing intelligent tutoring in engineering education.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15125v1" target="_blank"><h2>Efficient RF Passive Components Modeling with Bayesian Online Learning and Uncertainty Aware Sampling <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Huifan Zhang, Pingqiang Zhou<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Conventional radio frequency (RF) passive components modeling based on machine learning requires extensive electromagnetic (EM) simulations to cover geometric and frequency design spaces, creating computational bottlenecks. In this paper, we introduce an uncertainty-aware Bayesian online learning framework for efficient parametric modeling of RF passive components, which includes: 1) a Bayesian neural network with reconfigurable heads for joint geometric-frequency domain modeling while quantifying uncertainty; 2) an adaptive sampling strategy that simultaneously optimizes training data sampling across geometric parameters and frequency domain using uncertainty guidance. Validated on three RF passive components, the framework achieves accurate modeling while using only 2.86% EM simulation time compared to traditional ML-based flow, achieving a 35 times speedup.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15122v1" target="_blank"><h2>Multi-Aspect Cross-modal Quantization for Generative Recommendation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Fuwei Zhang, Xiaoyu Liu, Dongbo Xi, Jishen Yin, Huan Chen, Peng Yan, Fuzhen Zhuang, Zhao Zhang<br><strong><u>Categories:</u></strong> cs.IR, cs.AI<br><strong><u>Comments:</u></strong> Accepted by AAAI 2026 (Oral)<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Generative Recommendation (GR) has emerged as a new paradigm in recommender systems. This approach relies on quantized representations to discretize item features, modeling users' historical interactions as sequences of discrete tokens. Based on these tokenized sequences, GR predicts the next item by employing next-token prediction methods. The challenges of GR lie in constructing high-quality semantic identifiers (IDs) that are hierarchically organized, minimally conflicting, and conducive to effective generative model training. However, current approaches remain limited in their ability to harness multimodal information and to capture the deep and intricate interactions among diverse modalities, both of which are essential for learning high-quality semantic IDs and for effectively training GR models. To address this, we propose Multi-Aspect Cross-modal quantization for generative Recommendation (MACRec), which introduces multimodal information and incorporates it into both semantic ID learning and generative model training from different aspects. Specifically, we first introduce cross-modal quantization during the ID learning process, which effectively reduces conflict rates and thus improves codebook usability through the complementary integration of multimodal information. In addition, to further enhance the generative ability of our GR model, we incorporate multi-aspect cross-modal alignments, including the implicit and explicit alignments. Finally, we conduct extensive experiments on three well-known recommendation datasets to demonstrate the effectiveness of our proposed method.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15120v1" target="_blank"><h2>Neural Networks Learn Generic Multi-Index Models Near Information-Theoretic Limit <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Bohan Zhang, Zihao Wang, Hengyu Fu, Jason D. Lee<br><strong><u>Categories:</u></strong> stat.ML, cs.AI, cs.IT, cs.LG, math.ST<br><strong><u>Comments:</u></strong> 86 pages, 2 figures. The order of the first two authors was determined by a coin flip<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> In deep learning, a central issue is to understand how neural networks efficiently learn high-dimensional features. To this end, we explore the gradient descent learning of a general Gaussian Multi-index model $f(\boldsymbol{x})=g(\boldsymbol{U}\boldsymbol{x})$ with hidden subspace $\boldsymbol{U}\in \mathbb{R}^{r\times d}$, which is the canonical setup to study representation learning. We prove that under generic non-degenerate assumptions on the link function, a standard two-layer neural network trained via layer-wise gradient descent can agnostically learn the target with $o_d(1)$ test error using $\widetilde{\mathcal{O}}(d)$ samples and $\widetilde{\mathcal{O}}(d^2)$ time. The sample and time complexity both align with the information-theoretic limit up to leading order and are therefore optimal. During the first stage of gradient descent learning, the proof proceeds via showing that the inner weights can perform a power-iteration process. This process implicitly mimics a spectral start for the whole span of the hidden subspace and eventually eliminates finite-sample noise and recovers this span. It surprisingly indicates that optimal results can only be achieved if the first layer is trained for more than $\mathcal{O}(1)$ steps. This work demonstrates the ability of neural networks to effectively learn hierarchical functions with respect to both sample and time efficiency.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15097v1" target="_blank"><h2>MAIF: Enforcing AI Trust and Provenance with an Artifact-Centric Agentic Paradigm <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Vineeth Sai Narajala, Manish Bhatt, Idan Habler, Ronald F. Del Rosario<br><strong><u>Categories:</u></strong> cs.CR, cs.AI<br><strong><u>Comments:</u></strong> 7 Pages, 2 Figures, 6 Tables, Repo:this https URL<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> explainability (abstract), multimodal (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> The AI trustworthiness crisis threatens to derail the artificial intelligence revolution, with regulatory barriers, security vulnerabilities, and accountability gaps preventing deployment in critical domains. Current AI systems operate on opaque data structures that lack the audit trails, provenance tracking, or explainability required by emerging regulations like the EU AI Act. We propose an artifact-centric AI agent paradigm where behavior is driven by persistent, verifiable data artifacts rather than ephemeral tasks, solving the trustworthiness problem at the data architecture level. Central to this approach is the Multimodal Artifact File Format (MAIF), an AI-native container embedding semantic representations, cryptographic provenance, and granular access controls. MAIF transforms data from passive storage into active trust enforcement, making every AI operation inherently auditable. Our production-ready implementation demonstrates ultra-high-speed streaming (2,720.7 MB/s), optimized video processing (1,342 MB/s), and enterprise-grade security. Novel algorithms for cross-modal attention, semantic compression, and cryptographic binding achieve up to 225 compression while maintaining semantic fidelity. Advanced security features include stream-level access control, real-time tamper detection, and behavioral anomaly analysis with minimal overhead. This approach directly addresses the regulatory, security, and accountability challenges preventing AI deployment in sensitive domains, offering a viable path toward trustworthy AI systems at scale.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15090v1" target="_blank"><h2>BBox DocVQA: A Large Scale Bounding Box Grounded Dataset for Enhancing Reasoning in Document Visual Question Answer <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Wenhan Yu, Wang Chen, Guanqiang Qi, Weikang Li, Yang Li, Lei Sha, Deguo Xia, Jizhou Huang<br><strong><u>Categories:</u></strong> cs.DB, cs.AI, cs.CV<br><strong><u>Comments:</u></strong> 22 pages, 4 figures<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Document Visual Question Answering (DocVQA) is a fundamental task for multimodal document understanding and a key testbed for vision language reasoning. However, most existing DocVQA datasets are limited to the page level and lack fine grained spatial grounding, constraining the interpretability and reasoning capability of Vision Language Models (VLMs). To address this gap, we introduce BBox DocVQA a large scale, bounding box grounded dataset designed to enhance spatial reasoning and evidence localization in visual documents. We further present an automated construction pipeline, Segment Judge and Generate, which integrates a segment model for region segmentation, a VLM for semantic judgment, and another advanced VLM for question answer generation, followed by human verification for quality assurance. The resulting dataset contains 3.6 K diverse documents and 32 K QA pairs, encompassing single and multi region as well as single and multi page scenarios. Each QA instance is grounded on explicit bounding boxes, enabling fine grained evaluation of spatial semantic alignment. Benchmarking multiple state of the art VLMs (e.g., GPT 5, Qwen2.5 VL, and InternVL) on BBox DocVQA reveals persistent challenges in spatial grounding and reasoning accuracy. Furthermore, fine tuning on BBox DocVQA substantially improves both bounding box localization and answer generation, validating its effectiveness for enhancing the reasoning ability of VLMs. Our dataset and code will be publicly released to advance research on interpretable and spatially grounded vision language reasoning.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15083v1" target="_blank"><h2>Fourier-KAN-Mamba: A Novel State-Space Equation Approach for Time-Series Anomaly Detection <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Xiancheng Wang, Lin Wang, Rui Wang, Zhibo Zhang, Minghang Zhao<br><strong><u>Categories:</u></strong> cs.LG, eess.SP<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> anomaly detection (title, abstract)<br><p><strong><u>Abstract:</u></strong> Time-series anomaly detection plays a critical role in numerous real-world applications, including industrial monitoring and fault diagnosis. Recently, Mamba-based state-space models have shown remarkable efficiency in long-sequence modeling. However, directly applying Mamba to anomaly detection tasks still faces challenges in capturing complex temporal patterns and nonlinear dynamics. In this paper, we propose Fourier-KAN-Mamba, a novel hybrid architecture that integrates Fourier layer, Kolmogorov-Arnold Networks (KAN), and Mamba selective state-space model. The Fourier layer extracts multi-scale frequency features, KAN enhances nonlinear representation capability, and a temporal gating control mechanism further improves the model's ability to distinguish normal and anomalous patterns. Extensive experiments on MSL, SMAP, and SWaT datasets demonstrate that our method significantly outperforms existing state-of-the-art approaches.
  Keywords: time-series anomaly detection, state-space model, Mamba, Fourier transform, Kolmogorov-Arnold Network</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15062v1" target="_blank"><h2>Interpretable temporal fusion network of multi- and multi-class arrhythmia classification <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yun Kwan Kim<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> [Doctoral dissertation, Korea University, 2025]<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Clinical decision support systems (CDSSs) have been widely utilized to support the decisions made by cardiologists when detecting and classifying arrhythmia from electrocardiograms. However, forming a CDSS for the arrhythmia classification task is challenging due to the varying lengths of arrhythmias. Although the onset time of arrhythmia varies, previously developed methods have not considered such conditions. Thus, we propose a framework that consists of (i) local and global extraction and (ii) local-global information fusion with attention to enable arrhythmia detection and classification within a constrained input length. The framework's performance was evaluated in terms of 10-class and 4-class arrhythmia detection, focusing on identifying the onset and ending point of arrhythmia episodes and their duration using the MIT-BIH arrhythmia database (MITDB) and the MIT-BIH atrial fibrillation database (AFDB). Duration, episode, and Dice score performances resulted in overall F1-scores of 96.45%, 82.05%, and 96.31% on the MITDB and 97.57%, 98.31%, and 97.45% on the AFDB, respectively. The results demonstrated statistically superior performance compared to those of the benchmark models. To assess the generalization capability of the proposed method, an MITDB-trained model and MIT-BIH malignant ventricular arrhythmia database-trained model were tested AFDB and MITDB, respectively. Superior performance was attained compared with that of a state-of-the-art model. The proposed method effectively captures both local and global information and dynamics without significant information loss. Consequently, arrhythmias can be detected with greater accuracy, and their occurrence times can be precisely determined, enabling the clinical field to develop more accurate treatment plans based on the proposed method.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15055v1" target="_blank"><h2>Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jian-Ting Guo, Yu-Cheng Chen, Ping-Chun Hsieh, Kuo-Hao Ho, Po-Wei Huang, Ti-Rong Wu, I-Chen Wu<br><strong><u>Categories:</u></strong> cs.AI, cs.LG, cs.RO<br><strong><u>Comments:</u></strong> Accepted by the Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025)<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> VAE (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Human-like agents have long been one of the goals in pursuing artificial intelligence. Although reinforcement learning (RL) has achieved superhuman performance in many domains, relatively little attention has been focused on designing human-like RL agents. As a result, many reward-driven RL agents often exhibit unnatural behaviors compared to humans, raising concerns for both interpretability and trustworthiness. To achieve human-like behavior in RL, this paper first formulates human-likeness as trajectory optimization, where the objective is to find an action sequence that closely aligns with human behavior while also maximizing rewards, and adapts the classic receding-horizon control to human-like learning as a tractable and efficient implementation. To achieve this, we introduce Macro Action Quantization (MAQ), a human-like RL framework that distills human demonstrations into macro actions via Vector-Quantized VAE. Experiments on D4RL Adroit benchmarks show that MAQ significantly improves human-likeness, increasing trajectory similarity scores, and achieving the highest human-likeness rankings among all RL agents in the human evaluation study. Our results also demonstrate that MAQ can be easily integrated into various off-the-shelf RL algorithms, opening a promising direction for learning human-like RL agents. Our code is available at https://rlg.iis.sinica.edu.tw/papers/MAQ.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15048v1" target="_blank"><h2>Oversampling techniques for predicting COVID-19 patient length of stay <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zachariah Farahany, Jiawei Wu, K M Sajjadul Islam, Praveen Madiraju<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 10 pages, 2022 IEEE International Conference on Big Data (Big Data)<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> COVID-19 is a respiratory disease that caused a global pandemic in 2019. It is highly infectious and has the following symptoms: fever or chills, cough, shortness of breath, fatigue, muscle or body aches, headache, the new loss of taste or smell, sore throat, congestion or runny nose, nausea or vomiting, and diarrhea. These symptoms vary in severity; some people with many risk factors have been known to have lengthy hospital stays or die from the disease. In this paper, we analyze patients' electronic health records (EHR) to predict the severity of their COVID-19 infection using the length of stay (LOS) as our measurement of severity. This is an imbalanced classification problem, as many people have a shorter LOS rather than a longer one. To combat this problem, we synthetically create alternate oversampled training data sets. Once we have this oversampled data, we run it through an Artificial Neural Network (ANN), which during training has its hyperparameters tuned using Bayesian optimization. We select the model with the best F1 score and then evaluate it and discuss it.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15046v1" target="_blank"><h2>UniHOI: Unified Human-Object Interaction Understanding via Unified Token Space <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Panqi Yang, Haodong Jing, Nanning Zheng, Yongqiang Ma<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> Accepted by AAAI 2026,9 pages, 4 figures<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> In the field of human-object interaction (HOI), detection and generation are two dual tasks that have traditionally been addressed separately, hindering the development of comprehensive interaction understanding. To address this, we propose UniHOI, which jointly models HOI detection and generation via a unified token space, thereby effectively promoting knowledge sharing and enhancing generalization. Specifically, we introduce a symmetric interaction-aware attention module and a unified semi-supervised learning paradigm, enabling effective bidirectional mapping between images and interaction semantics even under limited annotations. Extensive experiments demonstrate that UniHOI achieves state-of-the-art performance in both HOI detection and generation. Specifically, UniHOI improves accuracy by 4.9% on long-tailed HOI detection and boosts interaction metrics by 42.0% on open-vocabulary generation tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15750v1" target="_blank"><h2>Writing With Machines and Peers: Designing for Critical Engagement with Generative AI <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Xinran Zhu, Cong Wang, Duane Searsmith<br><strong><u>Categories:</u></strong> cs.CY, cs.AI, cs.ET, cs.HC<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> literature review (abstract)<br><p><strong><u>Abstract:</u></strong> The growing integration of generative AI in higher education is transforming how students write, learn, and engage with knowledge. As AI tools become more integrated into classrooms, there is an urgent need for pedagogical approaches that help students use them critically and reflectively. This study proposes a pedagogical design that integrates AI and peer feedback in a graduate-level academic writing activity. Over eight weeks, students developed literature review projects through multiple writing and revision stages, receiving feedback from both a custom-built AI reviewer and human peers. We examine two questions: (1) How did students interact with and incorporate AI and peer feedback during the writing process? and (2) How did they reflect on and build relationships with both human and AI reviewers? Data sources include student writing artifacts, AI and peer feedback, AI chat logs, and student reflections. Findings show that students engaged differently with each feedback source-relying on AI for rubric alignment and surface-level edits, and on peer feedback for conceptual development and disciplinary relevance. Reflections revealed evolving relationships with AI, characterized by increasing confidence, strategic use, and critical awareness of its limitations. The pedagogical design supported writing development, AI literacy, and disciplinary understanding. This study offers a scalable pedagogical model for integrating AI into writing instruction and contributes insights for system-level approaches to fostering meaningful human-AI collaboration in higher education.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15010v1" target="_blank"><h2>Latent space analysis and generalization to out-of-distribution data <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Katie Rainey, Erin Hausmann, Donald Waagen, David Gray, Donald Hulsey<br><strong><u>Categories:</u></strong> stat.ML, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> latent space (title, abstract)<br><p><strong><u>Abstract:</u></strong> Understanding the relationships between data points in the latent decision space derived by the deep learning system is critical to evaluating and interpreting the performance of the system on real world data. Detecting \textit{out-of-distribution} (OOD) data for deep learning systems continues to be an active research topic. We investigate the connection between latent space OOD detection and classification accuracy of the model. Using open source simulated and measured Synthetic Aperture RADAR (SAR) datasets, we empirically demonstrate that the OOD detection cannot be used as a proxy measure for model performance. We hope to inspire additional research into the geometric properties of the latent space that may yield future insights into deep learning robustness and generalizability.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15006v1" target="_blank"><h2>Toward Complete Merger Identification at Cosmic Noon with Deep Learning <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Aimee Schechter, Aleksandra Ciprijanovic, Rebecca Nevin, Julie Comerford, Xuejian Shen, Aaron Stemo, Laura Blecha<br><strong><u>Categories:</u></strong> astro-ph.GA<br><strong><u>Comments:</u></strong> 5 pages, accepted to the NeurIPS 2025 Machine Learning for the Physical Sciences Workshop<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> latent space (abstract)<br><p><strong><u>Abstract:</u></strong> As we enter the era of large imaging surveys such as $\textit{Roman}$, Rubin, and $\textit{Euclid}$, a deeper understanding of potential biases and selection effects in optical astronomical catalogs created with the use of ML-based methods is paramount. This work focuses on a deeper understanding of the performance and limitations of deep learning-based classifiers as tools for galaxy merger identification. We train a ResNet18 model on mock Hubble Space Telescope CANDELS images from the IllustrisTNG50 simulation. Our focus is on a more challenging classification of galaxy mergers and nonmergers at higher redshifts $1<z<1.5$, including minor mergers and lower mass galaxies down to the stellar mass of $10^8 M_\odot$. We demonstrate, for the first time, that a deep learning model, such as the one developed in this work, can successfully identify even minor and low mass mergers even at these redshifts. Our model achieves overall accuracy, purity, and completeness of 73%. We show that some galaxy mergers can only be identified from certain observation angles, leading to a potential upper limit in overall accuracy. Using Grad-CAMs and UMAPs, we more deeply examine the performance and observe a visible gradient in the latent space with stellar mass and specific star formation rate, but no visible gradient with merger mass ratio or merger stage.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15003v1" target="_blank"><h2>Resource-Based Time and Cost Prediction in Project Networks: From Statistical Modeling to Graph Neural Networks <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Reza Mirjalili, Behrad Braghi, Shahram Shadrokh Sikari<br><strong><u>Categories:</u></strong> stat.AP, cs.LG<br><strong><u>Comments:</u></strong> 52 pages, 12 figures<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> explainable (abstract), neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> Accurate prediction of project duration and cost remains one of the most challenging aspects of project management, particularly in resource-constrained and interdependent task networks. Traditional analytical techniques such as the Critical Path Method (CPM) and Program Evaluation and Review Technique (PERT) rely on simplified and often static assumptions regarding task interdependencies and resource performance. This study proposes a novel resource-based predictive framework that integrates network representations of project activities with graph neural networks (GNNs) to capture structural and contextual relationships among tasks, resources, and time-cost dynamics. The model represents the project as a heterogeneous activity-resource graph in which nodes denote activities and resources, and edges encode temporal and resource dependencies.
  We evaluate multiple learning paradigms, including GraphSAGE and Temporal Graph Networks, on both synthetic and benchmark project datasets. Experimental results show that the proposed GNN framework achieves an average 23 to 31 percent reduction in mean absolute error compared to traditional regression and tree-based methods, while improving the coefficient of determination R2 from approximately 0.78 to 0.91 for large and complex project networks. Furthermore, the learned embeddings provide interpretable insights into resource bottlenecks and critical dependencies, enabling more explainable and adaptive scheduling decisions.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14997v1" target="_blank"><h2>A Neural Network Approach to Preferred Event Selection for Low-Latency Gravitational-Wave Alerts <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Pratyusava Baral, Cody Messick, Patrick Brady<br><strong><u>Categories:</u></strong> astro-ph.IM, astro-ph.HE, gr-qc<br><strong><u>Comments:</u></strong> 24 pages, 7 figures<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> The LIGO-Virgo-KAGRA collaboration uses multiple independent search pipelines to detect gravitational waves, often resulting in multiple triggers (g-events) for a single astrophysical source. These triggers are grouped into superevents, raising a critical question for multimessenger astronomy: which g-event provides the most accurate sky localization for electromagnetic follow-up? Currently, the g-event with the highest signal-to-noise ratio (SNR) is selected, under the assumption that it should provide the best estimators of the source's parameters, including its location on the sky. Analysis of simulated signals reveals systematic deviations from this expectation. In particular, a false-alarm rate (FAR)-based selector performs slightly better than the SNR-based method, but introduces pipeline biases. We present a neural network-based selector trained on simulated signals to identify the g-event with the minimum searched area -- a metric quantifying localization accuracy. The network uses information (detector SNRs, FAR, and chirp mass) from all of the triggers associated with each astrophysical source and is designed to be pipeline-agnostic. Our results show that the neural network outperforms both traditional selectors, achieving a mean searched area ~2% smaller than the SNR-based selector. Unlike FAR-based selection, the neural network preserves the underlying distribution of pipeline contributions, avoiding systematic biases toward specific pipelines. The network can be trained in approximately one minute on a few thousand events and performs event selection instantaneously, making it suitable for low-latency applications. These results demonstrate that machine learning can enhance multimessenger astronomy capabilities while maintaining fairness across detection pipelines. We recommend implementing this approach for future observing runs.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14981v1" target="_blank"><h2>Logit-Based Losses Limit the Effectiveness of Feature Knowledge Distillation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Nicholas Cooper, Lijun Chen, Sailesh Dwivedy, Danna Gurari<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> NeurIPS Workshop on Symmetry and Geometry in Neural Representations (NeurReps), December 2025<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract), transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Knowledge distillation (KD) methods can transfer knowledge of a parameter-heavy teacher model to a light-weight student model. The status quo for feature KD methods is to utilize loss functions based on logits (i.e., pre-softmax class scores) and intermediate layer features (i.e., latent representations). Unlike previous approaches, we propose a feature KD framework for training the student's backbone using feature-based losses exclusively (i.e., without logit-based losses such as cross entropy). Leveraging recent discoveries about the geometry of latent representations, we introduce a knowledge quality metric for identifying which teacher layers provide the most effective knowledge for distillation. Experiments on three image classification datasets with four diverse student-teacher pairs, spanning convolutional neural networks and vision transformers, demonstrate our KD method achieves state-of-the-art performance, delivering top-1 accuracy boosts of up to 15% over standard approaches. We publically share our code to facilitate future work at https://github.com/Thegolfingocto/KD_wo_CE.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14970v1" target="_blank"><h2>EGSA-PT:Edge-Guided Spatial Attention with Progressive Training for Monocular Depth Estimation and Segmentation of Transparent Objects <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Gbenga Omotara, Ramy Farag, Seyed Mohamad Ali Tousi, G. N. DeSouza<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.RO<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> multi-modal (abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Transparent object perception remains a major challenge in computer vision research, as transparency confounds both depth estimation and semantic segmentation. Recent work has explored multi-task learning frameworks to improve robustness, yet negative cross-task interactions often hinder performance. In this work, we introduce Edge-Guided Spatial Attention (EGSA), a fusion mechanism designed to mitigate destructive interactions by incorporating boundary information into the fusion between semantic and geometric features. On both Syn-TODD and ClearPose benchmarks, EGSA consistently improved depth accuracy over the current state of the art method (MODEST), while preserving competitive segmentation performance, with the largest improvements appearing in transparent regions. Besides our fusion design, our second contribution is a multi-modal progressive training strategy, where learning transitions from edges derived from RGB images to edges derived from predicted depth images. This approach allows the system to bootstrap learning from the rich textures contained in RGB images, and then switch to more relevant geometric content in depth maps, while it eliminates the need for ground-truth depth at training time. Together, these contributions highlight edge-guided fusion as a robust approach capable of improving transparent object perception.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14969v1" target="_blank"><h2>Quality-Controlled Multimodal Emotion Recognition in Conversations with Identity-Based Transfer Learning and MAMBA Fusion <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zanxu Wang, Homayoon Beigi<br><strong><u>Categories:</u></strong> eess.AS, cs.AI, cs.LG, eess.IV, eess.SP<br><strong><u>Comments:</u></strong> 8 pages, 14 images, 3 tables, Recognition Technologies, Inc. Technical Report RTI-20251118-01<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract), transfer learning (title, abstract)<br><p><strong><u>Abstract:</u></strong> This paper addresses data quality issues in multimodal emotion recognition in conversation (MERC) through systematic quality control and multi-stage transfer learning. We implement a quality control pipeline for MELD and IEMOCAP datasets that validates speaker identity, audio-text alignment, and face detection. We leverage transfer learning from speaker and face recognition, assuming that identity-discriminative embeddings capture not only stable acoustic and Facial traits but also person-specific patterns of emotional expression. We employ RecoMadeEasy(R) engines for extracting 512-dimensional speaker and face embeddings, fine-tune MPNet-v2 for emotion-aware text representations, and adapt these features through emotion-specific MLPs trained on unimodal datasets. MAMBA-based trimodal fusion achieves 64.8% accuracy on MELD and 74.3% on IEMOCAP. These results show that combining identity-based audio and visual embeddings with emotion-tuned text representations on a quality-controlled subset of data yields consistent competitive performance for multimodal emotion recognition in conversation and provides a basis for further improvement on challenging, low-frequency emotion classes.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14962v1" target="_blank"><h2>Reconstruction of three-dimensional shapes of normal and disease-related erythrocytes from partial observations using multi-fidelity neural networks <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Haizhou Wen, He Li, Zhen Li<br><strong><u>Categories:</u></strong> physics.comp-ph, cs.LG, eess.IV, physics.bio-ph, q-bio.QM<br><strong><u>Comments:</u></strong> 29 pages, 10 figures, 3 appendices<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> Reconstruction of 3D erythrocyte or red blood cell (RBC) morphology from partial observations, such as microscope images, is essential for understanding the physiology of RBC aging and the pathology of various RBC disorders. In this study, we propose a multi-fidelity neural network (MFNN) approach to fuse high-fidelity cross-sections of an RBC, with a morphologically similar low-fidelity reference 3D RBC shape to recover its full 3D surface. The MFNN predictor combines a convolutional neural network trained on low-fidelity reference RBC data with a feedforward neural network that captures nonlinear morphological correlations, and augments training with surface area and volume constraints for regularization in the low-fidelity branch. This approach is theoretically grounded by a topological homeomorphism between a sphere and 3D RBC surfaces, with training data generated by dissipative particle dynamics simulations of stomatocyte-discocyte-echinocyte transformation. Benchmarking across diverse RBC shapes observed in normal and aged populations, our results show that the MFNN predictor can reconstruct complex RBC morphologies with over 95% coordinate accuracy when provided with at least two orthogonal cross-sections. It is observed that informative oblique cross-sections intersecting spicule tips of echinocytes improve both local and global feature reconstruction, highlighting the value of feature-aware sampling. Our study further evaluates the influence of sampling strategies, shape dissimilarity, and noise, showing enhanced robustness under physically constrained training. Altogether, these results demonstrate the capability of MFNN to reconstruct the 3D shape of normal and aged RBCs from partial cross-sections as observed in conventional microscope images, which could facilitate the quantitative analysis of RBC morphological parameters in normal and disease-related RBC samples.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14961v1" target="_blank"><h2>Knowledge Graphs as Structured Memory for Embedding Spaces: From Training Clusters to Explainable Inference <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Artur A. Oliveira, Mateus Espadoto, Roberto M. Cesar, Roberto Hirata<br><strong><u>Categories:</u></strong> cs.LG, cs.CV<br><strong><u>Comments:</u></strong> Submitted to GRIVAPP 2026 (21st International Conference on Computer Graphics, Interaction, Visualization Theory and Applications), Marbella, Spain, March 9-11 2026<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> explainable (title)<br><p><strong><u>Abstract:</u></strong> We introduce Graph Memory (GM), a structured non-parametric framework that augments embedding-based inference with a compact, relational memory over region-level prototypes. Rather than treating each training instance in isolation, GM summarizes the embedding space into prototype nodes annotated with reliability indicators and connected by edges that encode geometric and contextual relations. This design unifies instance retrieval, prototype-based reasoning, and graph-based label propagation within a single inductive model that supports both efficient inference and faithful explanation. Experiments on synthetic and real datasets including breast histopathology (IDC) show that GM achieves accuracy competitive with $k$NN and Label Spreading while offering substantially better calibration and smoother decision boundaries, all with an order of magnitude fewer samples. By explicitly modeling reliability and relational structure, GM provides a principled bridge between local evidence and global consistency in non-parametric learning.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14953v1" target="_blank"><h2>Compiling to recurrent neurons <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Joey Velez-Ginorio, Nada Amin, Konrad Kording, Steve Zdancewic<br><strong><u>Categories:</u></strong> cs.PL, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Discrete structures are currently second-class in differentiable programming. Since functions over discrete structures lack overt derivatives, differentiable programs do not differentiate through them and limit where they can be used. For example, when programming a neural network, conditionals and iteration cannot be used everywhere; they can break the derivatives necessary for gradient-based learning to work. This limits the class of differentiable algorithms we can directly express, imposing restraints on how we build neural networks and differentiable programs more generally. However, these restraints are not fundamental. Recent work shows conditionals can be first-class, by compiling them into differentiable form as linear neurons. Similarly, this work shows iteration can be first-class -- by compiling to linear recurrent neurons. We present a minimal typed, higher-order and linear programming language with iteration called $\textsf{Cajal}\scriptstyle(\mathbb{\multimap}, \mathbb{2}, \mathbb{N})$. We prove its programs compile correctly to recurrent neurons, allowing discrete algorithms to be expressed in a differentiable form compatible with gradient-based learning. With our implementation, we conduct two experiments where we link these recurrent neurons against a neural network solving an iterative image transformation task. This determines part of its function prior to learning. As a result, the network learns faster and with greater data-efficiency relative to a neural network programmed without first-class iteration. A key lesson is that recurrent neurons enable a rich interplay between learning and the discrete structures of ordinary programming.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14952v1" target="_blank"><h2>Artificial intelligence approaches for energy-efficient laser cutting machines <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Mohamed Abdallah Salem, Hamdy Ahmed Ashour, Ahmed Elshenawy<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.LG, cs.RO<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract), transfer learning (abstract)<br><p><strong><u>Abstract:</u></strong> This research addresses the significant challenges of energy consumption and environmental impact in laser cutting by proposing novel deep learning (DL) methodologies to achieve energy reduction. Recognizing the current lack of adaptive control and the open-loop nature of CO2 laser suction pumps, this study utilizes closed-loop configurations that dynamically adjust pump power based on both the material being cut and the smoke level generated. To implement this adaptive system, diverse material classification methods are introduced, including techniques leveraging lens-less speckle sensing with a customized Convolutional Neural Network (CNN) and an approach using a USB camera with transfer learning via the pre-trained VGG16 CNN model. Furthermore, a separate DL model for smoke level detection is employed to simultaneously refine the pump's power output. This integration prompts the exhaust suction pump to automatically halt during inactive times and dynamically adjust power during operation, leading to experimentally proven and remarkable energy savings, with results showing a 20% to 50% reduction in the smoke suction pump's energy consumption, thereby contributing substantially to sustainable development in the manufacturing sector.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14922v1" target="_blank"><h2>Integrating Causal Inference with Graph Neural Networks for Alzheimer's Disease Analysis <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Pranay Kumar Peddi, Dhrubajyoti Ghosh<br><strong><u>Categories:</u></strong> cs.LG, stat.ME<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (title)<br><p><strong><u>Abstract:</u></strong> Deep graph learning has advanced Alzheimer's (AD) disease classification from MRI, but most models remain correlational, confounding demographic and genetic factors with disease specific features. We present Causal-GCN, an interventional graph convolutional framework that integrates do-calculus-based back-door adjustment to identify brain regions exerting stable causal influence on AD progression. Each subject's MRI is represented as a structural connectome where nodes denote cortical and subcortical regions and edges encode anatomical connectivity. Confounders such as age, sec, and APOE4 genotype are summarized via principal components and included in the causal adjustment set. After training, interventions on individual regions are simulated by serving their incoming edges and altering node features to estimate average causal effects on disease probability. Applied to 484 subjects from the ADNI cohort, Causal-GCN achieves performance comparable to baseline GNNs while providing interpretable causal effect rankings that highlight posterior, cingulate, and insular hubs consistent with established AD neuropathology.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14920v1" target="_blank"><h2>Structured Contrastive Learning for Interpretable Latent Representations <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zhengyang Shen, Hua Tu, Mayue Shi<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> Comments: 10 pages, 6 figures. Applications to medical signal retrieval and activity recognition. Correspondence: m.shi16@imperial.this http URL<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> latent space (abstract), neural network (abstract), data augmentation (abstract)<br><p><strong><u>Abstract:</u></strong> Neural networks exhibit severe brittleness to semantically irrelevant transformations. A mere 75ms electrocardiogram (ECG) phase shift degrades latent cosine similarity from 1.0 to 0.2, while sensor rotations collapse activity recognition performance with inertial measurement units (IMUs). We identify the root cause as "laissez-faire" representation learning, where latent spaces evolve unconstrained provided task performance is satisfied. We propose Structured Contrastive Learning (SCL), a framework that partitions latent space representations into three semantic groups: invariant features that remain consistent under given transformations (e.g., phase shifts or rotations), variant features that actively differentiate transformations via a novel variant mechanism, and free features that preserve task flexibility. This creates controllable push-pull dynamics where different latent dimensions serve distinct, interpretable purposes. The variant mechanism enhances contrastive learning by encouraging variant features to differentiate within positive pairs, enabling simultaneous robustness and interpretability. Our approach requires no architectural modifications and integrates seamlessly into existing training pipelines. Experiments on ECG phase invariance and IMU rotation robustness demonstrate superior performance: ECG similarity improves from 0.25 to 0.91 under phase shifts, while WISDM activity recognition achieves 86.65% accuracy with 95.38% rotation consistency, consistently outperforming traditional data augmentation. This work represents a paradigm shift from reactive data augmentation to proactive structural learning, enabling interpretable latent representations in neural networks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14916v1" target="_blank"><h2>SN 2023taz: Implications for the UV Diversity of Superluminous Supernovae <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Aysha Aamer, Matt Nicholl, Charlotte Angus, Shubham Srivastav, Jeff Cooke, Natasha Van Bemmel, Frédérick Poidevin, Stefan Geier, Joseph P. Anderson, Thomas de Boer, Kenneth C. Chambers, Ting-Wan Chen, Mariusz Gromadzki, Claudia P. Gutiérrez, Erkki Kankare, Réka Könyves-Tóth, Chien-Cheng Lin, Thomas B. Lowe, Eugene Magnier, Paolo Mazzali, Kyle Medler, Paloma Minguez, Tomás E. Müller-Bravo, Ben Warwick<br><strong><u>Categories:</u></strong> astro-ph.HE<br><strong><u>Comments:</u></strong> Submitted to ApJ<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> VAE (title, abstract)<br><p><strong><u>Abstract:</u></strong> Superluminous supernovae (SLSNe) are some of the brightest explosions in the Universe representing the extremes of stellar deaths. At the upper end of their distribution is SN\,2023taz, one of the most luminous SLSNe discovered to date with a peak absolute magnitude of $M_{g,\rm{peak}}=-22.75 \pm 0.03$ and a lower limit for energy radiated of $E=2.9 \times 10^{51}$\,erg. Magnetar model fits reveal individual parameter values typical of the SLSN population, but the combination of a low $B$-field and ejecta mass with a short spin period places SN\,2023taz in a unusual region of parameter space, accounting for its extreme luminosity. The optical data around peak are consistent with a temperature of $\sim$17\,000\,K but SN\,2023taz shows a surprising deficit in the UV compared to other events in this temperature range. We find no indication of dust extinction that could plausibly explain the UV deficit. The lower level of UV flux is reminiscent of the absorption seen in lower-luminosity events like SN\,2017dwh, where Fe-group elements are responsible for the effect. However, in the case of SN\,2023taz, there is no evidence for a larger amount of Fe-group elements which could contribute to line blanketing. Comparing to SLSNe with well-observed UV spectra, an underlying temperature of $8000-9000$\,K would match the UV spectral slope, but is not consistent with the optical colour temperatures of these events. The most likely explanation is enhanced absorption by intermediate-mass elements, challenging previous findings that SLSNe exhibit similar UV absorption line equivalent widths. This highlights the need for expanded UV spectroscopic coverage of SLSNe, especially at early times, to build a framework for interpreting their diversity and to enable classification at higher redshifts where optical observations will exclusively probe rest-frame UV emission.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14889v1" target="_blank"><h2>Bringing Federated Learning to Space <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Grace Kim, Filip Svoboda, Nicholas Lane<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 15 pages, 9 figures, 3 tables accepted to IEEE Aeroconf 2026<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> As Low Earth Orbit (LEO) satellite constellations rapidly expand to hundreds and thousands of spacecraft, the need for distributed on-board machine learning becomes critical to address downlink bandwidth limitations. Federated learning (FL) offers a promising framework to conduct collaborative model training across satellite networks. Realizing its benefits in space naturally requires addressing space-specific constraints, from intermittent connectivity to dynamics imposed by orbital motion. This work presents the first systematic feasibility analysis of adapting off-the-shelf FL algorithms for satellite constellation deployment. We introduce a comprehensive "space-ification" framework that adapts terrestrial algorithms (FedAvg, FedProx, FedBuff) to operate under orbital constraints, producing an orbital-ready suite of FL algorithms. We then evaluate these space-ified methods through extensive parameter sweeps across 768 constellation configurations that vary cluster sizes (1-10), satellites per cluster (1-10), and ground station networks (1-13). Our analysis demonstrates that space-adapted FL algorithms efficiently scale to constellations of up to 100 satellites, achieving performance close to the centralized ideal. Multi-month training cycles can be reduced to days, corresponding to a 9x speedup through orbital scheduling and local coordination within satellite clusters. These results provide actionable insights for future mission designers, enabling distributed on-board learning for more autonomous, resilient, and data-driven satellite operations.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15743v1" target="_blank"><h2>Connecting the Dots: A Machine Learning Ready Dataset for Ionospheric Forecasting Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Linnea M. Wolniewicz, Halil S. Kelebek, Simone Mestici, Michael D. Vergalla, Giacomo Acciarini, Bala Poduval, Olga Verkhoglyadova, Madhulika Guhathakurta, Thomas E. Berger, Atılım Güneş Baydin, Frank Soboczenski<br><strong><u>Categories:</u></strong> cs.LG, astro-ph.EP, astro-ph.IM<br><strong><u>Comments:</u></strong> 8 pages, 2 figures, 2 tables. Accepted as a poster presentation in the Machine Learning for the Physical Sciences workshop at NeurIPS 2025<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> Operational forecasting of the ionosphere remains a critical space weather challenge due to sparse observations, complex coupling across geospatial layers, and a growing need for timely, accurate predictions that support Global Navigation Satellite System (GNSS), communications, aviation safety, as well as satellite operations. As part of the 2025 NASA Heliolab, we present a curated, open-access dataset that integrates diverse ionospheric and heliospheric measurements into a coherent, machine learning-ready structure, designed specifically to support next-generation forecasting models and address gaps in current operational frameworks. Our workflow integrates a large selection of data sources comprising Solar Dynamic Observatory data, solar irradiance indices (F10.7), solar wind parameters (velocity and interplanetary magnetic field), geomagnetic activity indices (Kp, AE, SYM-H), and NASA JPL's Global Ionospheric Maps of Total Electron Content (GIM-TEC). We also implement geospatially sparse data such as the TEC derived from the World-Wide GNSS Receiver Network and crowdsourced Android smartphone measurements. This novel heterogeneous dataset is temporally and spatially aligned into a single, modular data structure that supports both physical and data-driven modeling. Leveraging this dataset, we train and benchmark several spatiotemporal machine learning architectures for forecasting vertical TEC under both quiet and geomagnetically active conditions. This work presents an extensive dataset and modeling pipeline that enables exploration of not only ionospheric dynamics but also broader Sun-Earth interactions, supporting both scientific inquiry and operational forecasting efforts.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14887v1" target="_blank"><h2>Transformer-Guided Deep Reinforcement Learning for Optimal Takeoff Trajectory Design of an eVTOL Drone <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Nathan M. Roberts, Xiaosong Du<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> Conference version with 12 pages and 2 figures<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> The rapid advancement of electric vertical take-off and landing (eVTOL) aircraft offers a promising opportunity to alleviate urban traffic congestion. Thus, developing optimal takeoff trajectories for minimum energy consumption becomes essential for broader eVTOL aircraft applications. Conventional optimal control methods (such as dynamic programming and linear quadratic regulator) provide highly efficient and well-established solutions but are limited by problem dimensionality and complexity. Deep reinforcement learning (DRL) emerges as a special type of artificial intelligence tackling complex, nonlinear systems; however, the training difficulty is a key bottleneck that limits DRL applications. To address these challenges, we propose the transformer-guided DRL to alleviate the training difficulty by exploring a realistic state space at each time step using a transformer. The proposed transformer-guided DRL was demonstrated on an optimal takeoff trajectory design of an eVTOL drone for minimal energy consumption while meeting takeoff conditions (i.e., minimum vertical displacement and minimum horizontal velocity) by varying control variables (i.e., power and wing angle to the vertical). Results presented that the transformer-guided DRL agent learned to take off with $4.57\times10^6$ time steps, representing 25% of the $19.79\times10^6$ time steps needed by a vanilla DRL agent. In addition, the transformer-guided DRL achieved 97.2% accuracy on the optimal energy consumption compared against the simulation-based optimal reference while the vanilla DRL achieved 96.3% accuracy. Therefore, the proposed transformer-guided DRL outperformed vanilla DRL in terms of both training efficiency as well as optimal design verification.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14868v1" target="_blank"><h2>Hierarchical Token Prepending: Enhancing Information Flow in Decoder-based LLM Embeddings <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Xueying Ding, Xingyue Huang, Mingxuan Ju, Liam Collins, Yozen Liu, Leman Akoglu, Neil Shah, Tong Zhao<br><strong><u>Categories:</u></strong> cs.CL, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Large language models produce powerful text embeddings, but their causal attention mechanism restricts the flow of information from later to earlier tokens, degrading representation quality. While recent methods attempt to solve this by prepending a single summary token, they over-compress information, hence harming performance on long documents. We propose Hierarchical Token Prepending (HTP), a method that resolves two critical bottlenecks. To mitigate attention-level compression, HTP partitions the input into blocks and prepends block-level summary tokens to subsequent blocks, creating multiple pathways for backward information flow. To address readout-level over-squashing, we replace last-token pooling with mean-pooling, a choice supported by theoretical analysis. HTP achieves consistent performance gains across 11 retrieval datasets and 30 general embedding benchmarks, especially in long-context settings. As a simple, architecture-agnostic method, HTP enhances both zero-shot and finetuned models, offering a scalable route to superior long-document embeddings.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14865v1" target="_blank"><h2>FinTRec: Transformer Based Unified Contextual Ads Targeting and Personalization for Financial Applications <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Dwipam Katariya, Snehita Varma, Akshat Shreemali, Benjamin Wu, Kalanand Mishra, Pranab Mohanty<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 10 pages, 7 figures, Accepted at CARS @ RecSys 2025<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> explainability (abstract), transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> Transformer-based architectures are widely adopted in sequential recommendation systems, yet their application in Financial Services (FS) presents distinct practical and modeling challenges for real-time recommendation. These include:a) long-range user interactions (implicit and explicit) spanning both digital and physical channels generating temporally heterogeneous context, b) the presence of multiple interrelated products require coordinated models to support varied ad placements and personalized feeds, while balancing competing business goals. We propose FinTRec, a transformer-based framework that addresses these challenges and its operational objectives in FS. While tree-based models have traditionally been preferred in FS due to their explainability and alignment with regulatory requirements, our study demonstrate that FinTRec offers a viable and effective shift toward transformer-based architectures. Through historic simulation and live A/B test correlations, we show FinTRec consistently outperforms the production-grade tree-based baseline. The unified architecture, when fine-tuned for product adaptation, enables cross-product signal sharing, reduces training cost and technical debt, while improving offline performance across all products. To our knowledge, this is the first comprehensive study of unified sequential recommendation modeling in FS that addresses both technical and business considerations.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14860v1" target="_blank"><h2>When CNNs Outperform Transformers and Mambas: Revisiting Deep Architectures for Dental Caries Segmentation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Aashish Ghimire, Jun Zeng, Roshan Paudel, Nikhil Kumar Tomar, Deepak Ranjan Nayak, Harshith Reddy Nalla, Vivek Jha, Glenda Reynolds, Debesh Jha<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> 8 pages, 4 figures<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract), transformer (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Accurate identification and segmentation of dental caries in panoramic radiographs are critical for early diagnosis and effective treatment planning. Automated segmentation remains challenging due to low lesion contrast, morphological variability, and limited annotated data. In this study, we present the first comprehensive benchmarking of convolutional neural networks, vision transformers and state-space mamba architectures for automated dental caries segmentation on panoramic radiographs through a DC1000 dataset. Twelve state-of-the-art architectures, including VMUnet, MambaUNet, VMUNetv2, RMAMamba-S, TransNetR, PVTFormer, DoubleU-Net, and ResUNet++, were trained under identical configurations. Results reveal that, contrary to the growing trend toward complex attention based architectures, the CNN-based DoubleU-Net achieved the highest dice coefficient of 0.7345, mIoU of 0.5978, and precision of 0.8145, outperforming all transformer and Mamba variants. In the study, the top 3 results across all performance metrics were achieved by CNN-based architectures. Here, Mamba and transformer-based methods, despite their theoretical advantage in global context modeling, underperformed due to limited data and weaker spatial priors. These findings underscore the importance of architecture-task alignment in domain-specific medical image segmentation more than model complexity. Our code is available at: https://github.com/JunZengz/dental-caries-segmentation.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14858v1" target="_blank"><h2>Large Language Model Driven Analysis of General Coordinates Network (GCN) Circulars <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Vidushi Sharma, Ronit Agarwala, Judith L. Racusin, Leo P. Singer, Tyler Barna, Eric Burns, Michael W. Coughlin, Dakota Dutko, Courey Elliott, Rahul Gupta, Ashish Mahabal, Nikhil Mukund<br><strong><u>Categories:</u></strong> astro-ph.HE, astro-ph.IM<br><strong><u>Comments:</u></strong> 61 pages, 11 figures, 7 tables. Accepted for publication in ApJS<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> time-domain (abstract)<br><p><strong><u>Abstract:</u></strong> The General Coordinates Network (GCN) is NASA's time-domain and multi-messenger alert system. GCN distributes two data products - automated ``Notices,'' and human-generated ``Circulars,'' that report the observations of high-energy and multi-messenger astronomical transients. The flexible and non-structured format of GCN Circulars, comprising of more than 40500 Circulars accumulated over three decades, makes it challenging to manually extract observational information, such as redshift or observed wavebands. In this work, we employ large language models (LLMs) to facilitate the automated parsing of transient reports. We develop a neural topic modeling pipeline with open-source tools for the automatic clustering and summarization of astrophysical topics in the Circulars database. Using neural topic modeling and contrastive fine-tuning, we classify Circulars based on their observation wavebands and messengers. Additionally, we separate gravitational wave (GW) event clusters and their electromagnetic (EM) counterparts from the Circulars database. Finally, using the open-source Mistral model, we implement a system to automatically extract gamma-ray burst (GRB) redshift information from the Circulars archive, without the need for any training. Evaluation against the manually curated Neil Gehrels Swift Observatory GRB table shows that our simple system, with the help of prompt-tuning, output parsing, and retrieval augmented generation (RAG), can achieve an accuracy of 97.2 % for redshift-containing Circulars. Our neural search enhanced RAG pipeline accurately retrieved 96.8 % of redshift circulars from the manually curated database. Our study demonstrates the potential of LLMs, to automate and enhance astronomical text mining, and provides a foundation work for future advances in transient alert analysis.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14857v1" target="_blank"><h2>511 keV Gamma Ray Echo from Particle Decays in Supernovae <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Garv Chauhan, Cecilia Lunardini<br><strong><u>Categories:</u></strong> hep-ph, astro-ph.HE<br><strong><u>Comments:</u></strong> 5+3 pages, 1 table, 2+2 figures<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> VAE (title)<br><p><strong><u>Abstract:</u></strong> The formation of a hot and dense core in a core-collapse supernova (SN) can produce massive Beyond Standard Model (BSM) particles. These particles can decay in the stellar envelope, generating positrons either directly or through secondary processes involving neutrinos or photons. We show for the first time that such positrons regardless of their production channel, can thermalize and annihilate at rest with ambient electrons in the outer SN envelope, producing a characteristic echo of 511 keV gamma rays. For axion-like particles (ALPs), we derive bounds on the ALP-photon coupling ($G_{a γ}$) using Pioneer Venus Orbiter observations of SN 1987A. We also evaluate the sensitivity of upcoming MeV gap gamma-ray telescopes in the 511 keV range, such as COSI and AMEGO, for future Galactic SNe, which can improve existing constraints or enable ALP discovery. The echo signal is a generic prediction for any particle species that efficiently produces positrons near the stellar surface.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14844v1" target="_blank"><h2>J-HERTz: J-PLUS Heritage Exploration of Radio Targets at z $<$ 5 <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> D. Fernández Gil, J. A. Fernández-Ontiveros, C. López-Sanjuan, F. Arizo-Borillo, A. del Pino, A. Hernán-Caballero, A. Lumbreras-Calle, Rahna P. T., David Sobral, H. Vázquez Ramió, A. J. Cenarro, A. Marín-Franch, R. E. Angulo, A. Ederoclite, D. Cristóbal-Hornillos, R. A. Dupke, C. Hernández-Monteagudo, M. Moles, L. Sodré, J. Varela<br><strong><u>Categories:</u></strong> astro-ph.GA<br><strong><u>Comments:</u></strong> 27 pages, 15 figures, 4 tables. Accepted for publication in ApJS. DOI:https://doi.org/10.3847/1538-4365/ae2016<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> We introduce J-HERTz (J-PLUS Heritage Exploration of Radio Targets at $z < 5$), a new multi-wavelength catalog that combines optical narrow-band photometry from J-PLUS, infrared observations from WISE, and deep low-frequency radio data from LoTSS for nearly half a million sources across 2,100 deg$^2$ of the northern sky. Key innovations of J-HERTz include Bayesian neural network classifications for 390,000 galaxies, 31,000 quasars, and 20,000 stars, along with significantly improved photometric redshifts for 235,000 galaxies compared to previous J-PLUS DR3 and LoTSS DR2 estimates. We identify 831 candidate Galactic radio stars, which, if confirmed, would constitute a significant addition to the number of radio-emitting stars identified to date. Among radio-loud galaxies with spectroscopic observations, $\gtrsim$20% lack Seyfert or LINER signatures, indicating a substantial population of optically quiescent radio galaxies, in agreement with previous works. Spectral energy distribution fitting of their host galaxies using J-PLUS photospectra reveals systematically low specific star formation rates, consistent with quenched stellar populations. J-HERTz thus provides a powerful dataset to exploit radio-optical synergies, enabling studies that span from the origin of stellar radio emission to the AGN life cycle and the role of jet activity in shaping host galaxy evolution.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14832v1" target="_blank"><h2>How to pick the best anomaly detector? <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Marie Hein, Gregor Kasieczka, Michael Krämer, Louis Moureaux, Alexander Mück, David Shih<br><strong><u>Categories:</u></strong> hep-ph, cs.LG, hep-ex, physics.data-an<br><strong><u>Comments:</u></strong> 12 pages, 7 figures<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), anomaly detection (abstract)<br><p><strong><u>Abstract:</u></strong> Anomaly detection has the potential to discover new physics in unexplored regions of the data. However, choosing the best anomaly detector for a given data set in a model-agnostic way is an important challenge which has hitherto largely been neglected. In this paper, we introduce the data-driven ARGOS metric, which has a sound theoretical foundation and is empirically shown to robustly select the most sensitive anomaly detection model given the data. Focusing on weakly-supervised, classifier-based anomaly detection methods, we show that the ARGOS metric outperforms other model selection metrics previously used in the literature, in particular the binary cross-entropy loss. We explore several realistic applications, including hyperparameter tuning as well as architecture and feature selection, and in all cases we demonstrate that ARGOS is robust to the noisy conditions of anomaly detection.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14761v1" target="_blank"><h2>ARC Is a Vision Problem! <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Keya Hu, Ali Cy, Linlu Qiu, Xiaoman Delores Ding, Runqian Wang, Yeyin Eva Zhu, Jacob Andreas, Kaiming He<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> Technical Report. Project webpage:this https URL<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> The Abstraction and Reasoning Corpus (ARC) is designed to promote research on abstract reasoning, a fundamental aspect of human intelligence. Common approaches to ARC treat it as a language-oriented problem, addressed by large language models (LLMs) or recurrent reasoning models. However, although the puzzle-like tasks in ARC are inherently visual, existing research has rarely approached the problem from a vision-centric perspective. In this work, we formulate ARC within a vision paradigm, framing it as an image-to-image translation problem. To incorporate visual priors, we represent the inputs on a "canvas" that can be processed like natural images. It is then natural for us to apply standard vision architectures, such as a vanilla Vision Transformer (ViT), to perform image-to-image mapping. Our model is trained from scratch solely on ARC data and generalizes to unseen tasks through test-time training. Our framework, termed Vision ARC (VARC), achieves 60.4% accuracy on the ARC-1 benchmark, substantially outperforming existing methods that are also trained from scratch. Our results are competitive with those of leading LLMs and close the gap to average human performance.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14753v1" target="_blank"><h2>SparseST: Exploiting Data Sparsity in Spatiotemporal Modeling and Prediction <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Junfeng Wu, Hadjer Benmeziane, Kaoutar El Maghraoui, Liu Liu, Yinan Wang<br><strong><u>Categories:</u></strong> cs.LG, cs.CE<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract)<br><p><strong><u>Abstract:</u></strong> Spatiotemporal data mining (STDM) has a wide range of applications in various complex physical systems (CPS), i.e., transportation, manufacturing, healthcare, etc. Among all the proposed methods, the Convolutional Long Short-Term Memory (ConvLSTM) has proved to be generalizable and extendable in different applications and has multiple variants achieving state-of-the-art performance in various STDM applications. However, ConvLSTM and its variants are computationally expensive, which makes them inapplicable in edge devices with limited computational resources. With the emerging need for edge computing in CPS, efficient AI is essential to reduce the computational cost while preserving the model performance. Common methods of efficient AI are developed to reduce redundancy in model capacity (i.e., model pruning, compression, etc.). However, spatiotemporal data mining naturally requires extensive model capacity, as the embedded dependencies in spatiotemporal data are complex and hard to capture, which limits the model redundancy. Instead, there is a fairly high level of data and feature redundancy that introduces an unnecessary computational burden, which has been largely overlooked in existing research. Therefore, we developed a novel framework SparseST, that pioneered in exploiting data sparsity to develop an efficient spatiotemporal model. In addition, we explore and approximate the Pareto front between model performance and computational efficiency by designing a multi-objective composite loss function, which provides a practical guide for practitioners to adjust the model according to computational resource constraints and the performance requirements of downstream tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14744v1" target="_blank"><h2>Measuring AI Progress in Drug Discovery: A Reproducible Leaderboard for the Tox21 Challenge <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Antonia Ebner, Christoph Bartmann, Sonja Topf, Sohvi Luukkonen, Johannes Schimunek, Günter Klambauer<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Deep learning's rise since the early 2010s has transformed fields like computer vision and natural language processing and strongly influenced biomedical research. For drug discovery specifically, a key inflection - akin to vision's "ImageNet moment" - arrived in 2015, when deep neural networks surpassed traditional approaches on the Tox21 Data Challenge. This milestone accelerated the adoption of deep learning across the pharmaceutical industry, and today most major companies have integrated these methods into their research pipelines. After the Tox21 Challenge concluded, its dataset was included in several established benchmarks, such as MoleculeNet and the Open Graph Benchmark. However, during these integrations, the dataset was altered and labels were imputed or manufactured, resulting in a loss of comparability across studies. Consequently, the extent to which bioactivity and toxicity prediction methods have improved over the past decade remains unclear. To this end, we introduce a reproducible leaderboard, hosted on Hugging Face with the original Tox21 Challenge dataset, together with a set of baseline and representative methods. The current version of the leaderboard indicates that the original Tox21 winner - the ensemble-based DeepTox method - and the descriptor-based self-normalizing neural networks introduced in 2017, continue to perform competitively and rank among the top methods for toxicity prediction, leaving it unclear whether substantial progress in toxicity prediction has been achieved over the past decade. As part of this work, we make all baselines and evaluated models publicly accessible for inference via standardized API calls to Hugging Face Spaces.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14727v1" target="_blank"><h2>The first data-driven bounds on the quantum decoherence of inflationary gravitational waves <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jessie de Kruijf, Giacomo Galloni, Nicola Bartolo<br><strong><u>Categories:</u></strong> astro-ph.CO<br><strong><u>Comments:</u></strong> Prepared for submission to JCAP<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> data-driven (title)<br><p><strong><u>Abstract:</u></strong> The (large-scale) structures we observe in the Universe are classical, but within the inflationary scenario they do originate from quantum fluctuations. This leads to the question: ''How did this quantum-to-classical transition occur?''. A potential explanation is quantum decoherence due to interactions between different fields present during inflation. The tensor modes (i.e. primordial gravitational waves) can interact with a scalar sector, causing their quantum decoherence to occur and inducing a change in the gravitational wave (GW) background. The power spectrum of these GWs can be constrained using the upper bounds found by Planck, BICEP/Keck Array, LIGO-Virgo-KAGRA, Big Bang Nucleosynthesis, and the Pulsar Timing Array detections. These impose constraints on the interaction between the fields. We find that the observational upper bounds mainly constrain scenarios with a strong interaction, especially if the interaction is also strongly time dependent. Furthermore, we find which observationally allowed scenarios have not completed decoherence by the end of inflation, thus possibly leaving quantum signatures in the GW background. Lastly, we show that, interestingly enough, there are decoherence scenarios corresponding to the signal observed by PTA experiments. This highlights the importance of the quantum decoherence effect on GWs.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14721v1" target="_blank"><h2>AdamHD: Decoupled Huber Decay Regularization for Language Model Pre-Training <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Fu-Ming Guo, Yingfang Fan<br><strong><u>Categories:</u></strong> cs.LG, math.OC<br><strong><u>Comments:</u></strong> 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: GPU-Accelerated and Scalable Optimization (ScaleOpt)<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Adaptive optimizers with decoupled weight decay, such as AdamW, are the de facto standard for pre-training large transformer-based generative models. Yet the quadratic nature of the $\ell_2$ penalty embedded in weight decay drives all parameters toward the origin at the same rate, making the update vulnerable to rare but extreme gradient directions and often over-penalizing well-conditioned coordinates. We propose AdamHuberDecay, a drop-in replacement for AdamW that substitutes the $\ell_2$ penalty with a decoupled smooth Huber regularizer. The resulting update decays parameters quadratically while their magnitude remains below a threshold $δ$, and linearly ($\ell_1$-like) once they exceed $δ$, yielding (i) bounded regularization gradients, (ii) invariance to per-coordinate second-moment rescaling, and (iii) stronger sparsity pressure on overgrown weights.
  We derive the closed-form decoupled Huber decay step and show how to integrate it with any Adam-family optimizer at $O(1)$ extra cost. Extensive experiments on GPT-2 and GPT-3 pre-training demonstrate that AdamHuberDecay (a) converges 10-15% faster in wall-clock time, (b) reduces validation perplexity by up to 4 points, (c) delivers performance improvements of 2.5-4.7% across downstream tasks, and (d) yields visibly sparser weight histograms that translate into 20-30% memory savings after magnitude pruning, without tuning the decay coefficient beyond the default grid used for AdamW. Ablations confirm robustness to outlier gradients and large-batch regimes, together with theoretical analyses that bound the expected parameter norm under noisy updates. AdamHuberDecay therefore provides a simple, principled path toward more efficient and resilient training of next-generation foundational generative transformers.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14710v1" target="_blank"><h2>Towards a Unified Analysis of Neural Networks in Nonparametric Instrumental Variable Regression: Optimization and Generalization <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zonghao Chen, Atsushi Nitanda, Arthur Gretton, Taiji Suzuki<br><strong><u>Categories:</u></strong> stat.ML, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> We establish the first global convergence result of neural networks for two stage least squares (2SLS) approach in nonparametric instrumental variable regression (NPIV). This is achieved by adopting a lifted perspective through mean-field Langevin dynamics (MFLD), unlike standard MFLD, however, our setting of 2SLS entails a \emph{bilevel} optimization problem in the space of probability measures. To address this challenge, we leverage the penalty gradient approach recently developed for bilevel optimization which formulates bilevel optimization as a Lagrangian problem. This leads to a novel fully first-order algorithm, termed \texttt{F$^2$BMLD}. Apart from the convergence bound, we further provide a generalization bound, revealing an inherent trade-off in the choice of the Lagrange multiplier between optimization and statistical guarantees. Finally, we empirically validate the effectiveness of the proposed method on an offline reinforcement learning benchmark.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14702v1" target="_blank"><h2>Seeing Beyond the Image: ECG and Anatomical Knowledge-Guided Myocardial Scar Segmentation from Late Gadolinium-Enhanced Images <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Farheen Ramzan, Yusuf Kiberu, Nikesh Jathanna, Meryem Jabrane, Vicente Grau, Shahnaz Jamil-Copley, Richard H. Clayton, Chen, Chen<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Accurate segmentation of myocardial scar from late gadolinium enhanced (LGE) cardiac MRI is essential for evaluating tissue viability, yet remains challenging due to variable contrast and imaging artifacts. Electrocardiogram (ECG) signals provide complementary physiological information, as conduction abnormalities can help localize or suggest scarred myocardial regions. In this work, we propose a novel multimodal framework that integrates ECG-derived electrophysiological information with anatomical priors from the AHA-17 atlas for physiologically consistent LGE-based scar segmentation. As ECGs and LGE-MRIs are not acquired simultaneously, we introduce a Temporal Aware Feature Fusion (TAFF) mechanism that dynamically weights and fuses features based on their acquisition time difference. Our method was evaluated on a clinical dataset and achieved substantial gains over the state-of-the-art image-only baseline (nnU-Net), increasing the average Dice score for scars from 0.6149 to 0.8463 and achieving high performance in both precision (0.9115) and sensitivity (0.9043). These results show that integrating physiological and anatomical knowledge allows the model to "see beyond the image", setting a new direction for robust and physiologically grounded cardiac scar segmentation.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14698v1" target="_blank"><h2>HyMAD: A Hybrid Multi-Activity Detection Approach for Border Surveillance and Monitoring <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Sriram Srinivasan, Srinivasan Aruchamy, Siva Ram Krisha Vadali<br><strong><u>Categories:</u></strong> cs.CV, cs.LG, eess.SP<br><strong><u>Comments:</u></strong> Multi-label seismic signal classification using novel attention-based feature fusion. Submitting to cs.CV due to relevance to general pattern recognition and time-frequency (spectrogram) analysis<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Seismic sensing has emerged as a promising solution for border surveillance and monitoring; the seismic sensors that are often buried underground are small and cannot be noticed easily, making them difficult for intruders to detect, avoid, or vandalize. This significantly enhances their effectiveness compared to highly visible cameras or fences. However, accurately detecting and distinguishing between overlapping activities that are happening simultaneously, such as human intrusions, animal movements, and vehicle rumbling, remains a major challenge due to the complex and noisy nature of seismic signals. Correctly identifying simultaneous activities is critical because failing to separate them can lead to misclassification, missed detections, and an incomplete understanding of the situation, thereby reducing the reliability of surveillance systems. To tackle this problem, we propose HyMAD (Hybrid Multi-Activity Detection), a deep neural architecture based on spatio-temporal feature fusion. The framework integrates spectral features extracted with SincNet and temporal dependencies modeled by a recurrent neural network (RNN). In addition, HyMAD employs self-attention layers to strengthen intra-modal representations and a cross-modal fusion module to achieve robust multi-label classification of seismic events. e evaluate our approach on a dataset constructed from real-world field recordings collected in the context of border surveillance and monitoring, demonstrating its ability to generalize to complex, simultaneous activity scenarios involving humans, animals, and vehicles. Our method achieves competitive performance and offers a modular framework for extending seismic-based activity recognition in real-world security applications.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14694v1" target="_blank"><h2>Near-Lossless Model Compression Enables Longer Context Inference in DNA Large Language Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Rui Zhu, Xiaopu Zhou, Haixu Tang, Stephen W. Scherer, Lucila Ohno-Machado<br><strong><u>Categories:</u></strong> q-bio.GN, cs.AI, cs.LG, q-bio.PE<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Trained on massive cross-species DNA corpora, DNA large language models (LLMs) learn the fundamental "grammar" and evolutionary patterns of genomic sequences. This makes them powerful priors for DNA sequence modeling, particularly over long ranges. However, two major constraints hinder their use in practice: the quadratic computational cost of self-attention and the growing memory required for key-value (KV) caches during autoregressive decoding. These constraints force the use of heuristics such as fixed-window truncation or sliding windows, which compromise fidelity on ultra-long sequences by discarding distant information. We introduce FOCUS (Feature-Oriented Compression for Ultra-long Self-attention), a progressive context-compression module that can be plugged into pretrained DNA LLMs. FOCUS combines the established k-mer representation in genomics with learnable hierarchical compression: it inserts summary tokens at k-mer granularity and progressively compresses attention key and value activations across multiple Transformer layers, retaining only the summary KV states across windows while discarding ordinary-token KV. A shared-boundary windowing scheme yields a stationary cross-window interface that propagates long-range information with minimal loss. We validate FOCUS on an Evo-2-based DNA LLM fine-tuned on GRCh38 chromosome 1 with self-supervised training and randomized compression schedules to promote robustness across compression ratios. On held-out human chromosomes, FOCUS achieves near-lossless fidelity: compressing a 1 kb context into only 10 summary tokens (about 100x) shifts the average per-nucleotide probability by only about 0.0004. Compared to a baseline without compression, FOCUS reduces KV-cache memory and converts effective inference scaling from O(N^2) to near-linear O(N), enabling about 100x longer inference windows on commodity GPUs with near-lossless fidelity.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14691v1" target="_blank"><h2>Attention via Synaptic Plasticity is All You Need: A Biologically Inspired Spiking Neuromorphic Transformer <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Kallol Mondal, Ankush Kumar<br><strong><u>Categories:</u></strong> cs.NE, cs.AI, cs.CV, cs.ET, stat.ML<br><strong><u>Comments:</u></strong> 21 Pages, 5 Figures, 3 Table<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> explainable (abstract), neural network (abstract), transformer (title, abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Attention is the brain's ability to selectively focus on a few specific aspects while ignoring irrelevant ones. This biological principle inspired the attention mechanism in modern Transformers. Transformers now underpin large language models (LLMs) such as GPT, but at the cost of massive training and inference energy, leading to a large carbon footprint. While brain attention emerges from neural circuits, Transformer attention relies on dot-product similarity to weight elements in the input sequence. Neuromorphic computing, especially spiking neural networks (SNNs), offers a brain-inspired path to energy-efficient intelligence. Despite recent work on attention-based spiking Transformers, the core attention layer remains non-neuromorphic. Current spiking attention (i) relies on dot-product or element-wise similarity suited to floating-point operations, not event-driven spikes; (ii) keeps attention matrices that suffer from the von Neumann bottleneck, limiting in-memory computing; and (iii) still diverges from brain-like computation. To address these issues, we propose the Spiking STDP Transformer (S$^{2}$TDPT), a neuromorphic Transformer that implements self-attention through spike-timing-dependent plasticity (STDP), embedding query--key correlations in synaptic weights. STDP, a core mechanism of memory and learning in the brain and widely studied in neuromorphic devices, naturally enables in-memory computing and supports non-von Neumann hardware. On CIFAR-10 and CIFAR-100, our model achieves 94.35\% and 78.08\% accuracy with only four timesteps and 0.49 mJ on CIFAR-100, an 88.47\% energy reduction compared to a standard ANN Transformer. Grad-CAM shows that the model attends to semantically relevant regions, enhancing interpretability. Overall, S$^{2}$TDPT illustrates how biologically inspired attention can yield energy-efficient, hardware-friendly, and explainable neuromorphic models.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14654v1" target="_blank"><h2>Improving segmentation of retinal arteries and veins using cardiac signal in doppler holograms <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Marius Dubosc, Yann Fischer, Zacharie Auray, Nicolas Boutry, Edwin Carlinet, Michael Atlan, Thierry Geraud<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> 5 pages, 3 figures, 1 table. Submitted to ISBI2026<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Doppler holography is an emerging retinal imaging technique that captures the dynamic behavior of blood flow with high temporal resolution, enabling quantitative assessment of retinal hemodynamics. This requires accurate segmentation of retinal arteries and veins, but traditional segmentation methods focus solely on spatial information and overlook the temporal richness of holographic data. In this work, we propose a simple yet effective approach for artery-vein segmentation in temporal Doppler holograms using standard segmentation architectures. By incorporating features derived from a dedicated pulse analysis pipeline, our method allows conventional U-Nets to exploit temporal dynamics and achieve performance comparable to more complex attention- or iteration-based models. These findings demonstrate that time-resolved preprocessing can unlock the full potential of deep learning for Doppler holography, opening new perspectives for quantitative exploration of retinal hemodynamics. The dataset is publicly available at https://huggingface.co/datasets/DigitalHolography/</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14640v1" target="_blank"><h2>Doppler Invariant CNN for Signal Classification <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Avi Bagchi, Dwight Hutchenson<br><strong><u>Categories:</u></strong> eess.SP, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Radio spectrum monitoring in contested environments motivates the need for reliable automatic signal classification technology. Prior work highlights deep learning as a promising approach, but existing models depend on brute-force Doppler augmentation to achieve real-world generalization, which undermines both training efficiency and interpretability. In this paper, we propose a convolutional neural network (CNN) architecture with complex-valued layers that exploits convolutional shift equivariance in the frequency domain. To establish provable frequency bin shift invariance, we use adaptive polyphase sampling (APS) as pooling layers followed by a global average pooling layer at the end of the network. Using a synthetic dataset of common interference signals, experimental results demonstrate that unlike a vanilla CNN, our model maintains consistent classification accuracy with and without random Doppler shifts despite being trained on no Doppler-shifted examples. Overall, our method establishes an invariance-driven framework for signal classification that offers provable robustness against real-world effects.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14632v1" target="_blank"><h2>Adapformer: Adaptive Channel Management for Multivariate Time Series Forecasting <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yuchen Luo, Xinyu Li, Liuhua Peng, Mingming Gong<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> In multivariate time series forecasting (MTSF), accurately modeling the intricate dependencies among multiple variables remains a significant challenge due to the inherent limitations of traditional approaches. Most existing models adopt either \textbf{channel-independent} (CI) or \textbf{channel-dependent} (CD) strategies, each presenting distinct drawbacks. CI methods fail to leverage the potential insights from inter-channel interactions, resulting in models that may not fully exploit the underlying statistical dependencies present in the data. Conversely, CD approaches often incorporate too much extraneous information, risking model overfitting and predictive inefficiency. To address these issues, we introduce the Adaptive Forecasting Transformer (\textbf{Adapformer}), an advanced Transformer-based framework that merges the benefits of CI and CD methodologies through effective channel management. The core of Adapformer lies in its dual-stage encoder-decoder architecture, which includes the \textbf{A}daptive \textbf{C}hannel \textbf{E}nhancer (\textbf{ACE}) for enriching embedding processes and the \textbf{A}daptive \textbf{C}hannel \textbf{F}orecaster (\textbf{ACF}) for refining the predictions. ACE enhances token representations by selectively incorporating essential dependencies, while ACF streamlines the decoding process by focusing on the most relevant covariates, substantially reducing noise and redundancy. Our rigorous testing on diverse datasets shows that Adapformer achieves superior performance over existing models, enhancing both predictive accuracy and computational efficiency, thus making it state-of-the-art in MTSF.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14631v1" target="_blank"><h2>Enhancing Agentic Autonomous Scientific Discovery with Vision-Language Model Capabilities <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Kahaan Gandhi, Boris Bolliet, Inigo Zubeldia<br><strong><u>Categories:</u></strong> cs.CL, cs.AI, cs.CV, cs.MA<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> We show that multi-agent systems guided by vision-language models (VLMs) improve end-to-end autonomous scientific discovery. By treating plots as verifiable checkpoints, a VLM-as-a-judge evaluates figures against dynamically generated domain-specific rubrics, enabling agents to correct their own errors and steer exploratory data analysis in real-time. Case studies in cosmology and astrochemistry demonstrate recovery from faulty reasoning paths and adaptation to new datasets without human intervention. On a 10-task benchmark for data-driven discovery, VLM-augmented systems achieve pass at 1 scores of 0.7-0.8, compared to 0.2-0.3 for code-only and 0.4-0.5 for code-and-text baselines, while also providing auditable reasoning traces that improve interpretability. Code available here: https://github.com/CMBAgents/cmbagent</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14606v1" target="_blank"><h2>Bridging Human and Model Perspectives: A Comparative Analysis of Political Bias Detection in News Media Using Large Language Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Shreya Adrita Banik, Niaz Nafi Rahman, Tahsina Moiukh, Farig Sadeque<br><strong><u>Categories:</u></strong> cs.CL, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Detecting political bias in news media is a complex task that requires interpreting subtle linguistic and contextual cues. Although recent advances in Natural Language Processing (NLP) have enabled automatic bias classification, the extent to which large language models (LLMs) align with human judgment still remains relatively underexplored and not yet well understood. This study aims to present a comparative framework for evaluating the detection of political bias across human annotations and multiple LLMs, including GPT, BERT, RoBERTa, and FLAN. We construct a manually annotated dataset of news articles and assess annotation consistency, bias polarity, and inter-model agreement to quantify divergence between human and model perceptions of bias. Experimental results show that among traditional transformer-based models, RoBERTa achieves the highest alignment with human labels, whereas generative models such as GPT demonstrate the strongest overall agreement with human annotations in a zero-shot setting. Among all transformer-based baselines, our fine-tuned RoBERTa model acquired the highest accuracy and the strongest alignment with human-annotated labels. Our findings highlight systematic differences in how humans and LLMs perceive political slant, underscoring the need for hybrid evaluation frameworks that combine human interpretability with model scalability in automated media bias detection.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14603v1" target="_blank"><h2>A Method for Characterizing Disease Progression from Acute Kidney Injury to Chronic Kidney Disease <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yilu Fang, Jordan G. Nestor, Casey N. Ta, Jerard Z. Kneifati-Hayek, Chunhua Weng<br><strong><u>Categories:</u></strong> cs.CL, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> Patients with acute kidney injury (AKI) are at high risk of developing chronic kidney disease (CKD), but identifying those at greatest risk remains challenging. We used electronic health record (EHR) data to dynamically track AKI patients' clinical evolution and characterize AKI-to-CKD progression. Post-AKI clinical states were identified by clustering patient vectors derived from longitudinal medical codes and creatinine measurements. Transition probabilities between states and progression to CKD were estimated using multi-state modeling. After identifying common post-AKI trajectories, CKD risk factors in AKI subpopulations were identified through survival analysis. Of 20,699 patients with AKI at admission, 3,491 (17%) developed CKD. We identified fifteen distinct post-AKI states, each with different probabilities of CKD development. Most patients (75%, n=15,607) remained in a single state or made only one transition during the study period. Both established (e.g., AKI severity, diabetes, hypertension, heart failure, liver disease) and novel CKD risk factors, with their impact varying across these clinical states. This study demonstrates a data-driven approach for identifying high-risk AKI patients, supporting the development of decision-support tools for early CKD detection and intervention.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14601v1" target="_blank"><h2>MRI Embeddings Complement Clinical Predictors for Cognitive Decline Modeling in Alzheimer's Disease Cohorts <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Nathaniel Putera, Daniel Vilet Rodríguez, Noah Videcrantz, Julia Machnio, Mostafa Mehdipour Ghazi<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> Accepted at SPIE - Medical Imaging Conference 2026<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), multimodal (abstract), transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Accurate modeling of cognitive decline in Alzheimer's disease is essential for early stratification and personalized management. While tabular predictors provide robust markers of global risk, their ability to capture subtle brain changes remains limited. In this study, we evaluate the predictive contributions of tabular and imaging-based representations, with a focus on transformer-derived Magnetic Resonance Imaging (MRI) embeddings. We introduce a trajectory-aware labeling strategy based on Dynamic Time Warping clustering to capture heterogeneous patterns of cognitive change, and train a 3D Vision Transformer (ViT) via unsupervised reconstruction on harmonized and augmented MRI data to obtain anatomy-preserving embeddings without progression labels. The pretrained encoder embeddings are subsequently assessed using both traditional machine learning classifiers and deep learning heads, and compared against tabular representations and convolutional network baselines. Results highlight complementary strengths across modalities. Clinical and volumetric features achieved the highest AUCs of around 0.70 for predicting mild and severe progression, underscoring their utility in capturing global decline trajectories. In contrast, MRI embeddings from the ViT model were most effective in distinguishing cognitively stable individuals with an AUC of 0.71. However, all approaches struggled in the heterogeneous moderate group. These findings indicate that clinical features excel in identifying high-risk extremes, whereas transformer-based MRI embeddings are more sensitive to subtle markers of stability, motivating multimodal fusion strategies for AD progression modeling.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14599v1" target="_blank"><h2>CCSD: Cross-Modal Compositional Self-Distillation for Robust Brain Tumor Segmentation with Missing Modalities <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Dongqing Xie, Yonghuang Wu, Zisheng Ai, Jun Min, Zhencun Jiang, Shaojin Geng, Lei Wang<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> 9 pages, 5 figures<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> multi-modal (abstract)<br><p><strong><u>Abstract:</u></strong> The accurate segmentation of brain tumors from multi-modal MRI is critical for clinical diagnosis and treatment planning. While integrating complementary information from various MRI sequences is a common practice, the frequent absence of one or more modalities in real-world clinical settings poses a significant challenge, severely compromising the performance and generalizability of deep learning-based segmentation models. To address this challenge, we propose a novel Cross-Modal Compositional Self-Distillation (CCSD) framework that can flexibly handle arbitrary combinations of input modalities. CCSD adopts a shared-specific encoder-decoder architecture and incorporates two self-distillation strategies: (i) a hierarchical modality self-distillation mechanism that transfers knowledge across modality hierarchies to reduce semantic discrepancies, and (ii) a progressive modality combination distillation approach that enhances robustness to missing modalities by simulating gradual modality dropout during training. Extensive experiments on public brain tumor segmentation benchmarks demonstrate that CCSD achieves state-of-the-art performance across various missing-modality scenarios, with strong generalization and stability.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15741v1" target="_blank"><h2>Uncertainty-Resilient Multimodal Learning via Consistency-Guided Cross-Modal Transfer <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Hyo-Jeong Jang<br><strong><u>Categories:</u></strong> cs.AI, cs.HC, cs.LG<br><strong><u>Comments:</u></strong> Master's thesis, Korea University, 2025<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> latent space (abstract), multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Multimodal learning systems often face substantial uncertainty due to noisy data, low-quality labels, and heterogeneous modality characteristics. These issues become especially critical in human-computer interaction settings, where data quality, semantic reliability, and annotation consistency vary across users and recording conditions. This thesis tackles these challenges by exploring uncertainty-resilient multimodal learning through consistency-guided cross-modal transfer. The central idea is to use cross-modal semantic consistency as a basis for robust representation learning. By projecting heterogeneous modalities into a shared latent space, the proposed framework mitigates modality gaps and uncovers structural relations that support uncertainty estimation and stable feature learning. Building on this foundation, the thesis investigates strategies to enhance semantic robustness, improve data efficiency, and reduce the impact of noise and imperfect supervision without relying on large, high-quality annotations. Experiments on multimodal affect-recognition benchmarks demonstrate that consistency-guided cross-modal transfer significantly improves model stability, discriminative ability, and robustness to noisy or incomplete supervision. Latent space analyses further show that the framework captures reliable cross-modal structure even under challenging conditions. Overall, this thesis offers a unified perspective on resilient multimodal learning by integrating uncertainty modeling, semantic alignment, and data-efficient supervision, providing practical insights for developing reliable and adaptive brain-computer interface systems.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14577v1" target="_blank"><h2>Estimating differential pistons for the Extremely Large Telescope using focal plane imaging and a residual network <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> P. Janin-Potiron, M. Gray, B. Neichel, M. Dumont, J. -F. Sauvage, C. T. Heritier, P. Jouve, R. Fetick, T. Fusco<br><strong><u>Categories:</u></strong> astro-ph.IM<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> As the Extremely Large Telescope (ELT) approaches operational status, optimising its imaging performance is critical. A differential piston, arising from either the adaptive optics (AO) control loop, thermomechanical effects, or other sources, significantly degrades the image quality and is detrimental to the telescope's overall performance. In a numerical simulation set-up, we propose a method for estimating the differential piston between the petals of the ELT's M4 mirror using images from a 2x2 Shack-Hartmann wavefront sensor (SH-WFS), commonly used in the ELT's tomographic AO mode. We aim to identify the limitations of this approach by evaluating its sensitivity to various observing conditions and sources of noise. Using a deep learning model based on a ResNet architecture, we trained a neural network (NN) on simulated datasets to estimate the differential piston. We assessed the robustness of the method under various conditions, including variations in Strehl ratio, polychromaticity, and detector noise. The performance was quantified using the root mean square error (RMSE) of the estimated differential piston aberration. This method demonstrates the ability to extract differential piston information from 2x2 SH-WFS images. Temporal averaging of frames makes the differential piston signal emerge from the turbulence-induced speckle field and leads to a significant improvement in the RMSE calculation. As expected, better seeing conditions result in improved accuracy. Polychromaticity only degrades the performance by less than 5% compared to the monochromatic case. In a realistic scenario, detector noise is not a limiting factor, as the primary limitation rather arises from the need for sufficient speckle averaging. The network was also shown to be applicable to input images other than the 2x2 SH-WFS data.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14569v1" target="_blank"><h2>Task Addition and Weight Disentanglement in Closed-Vocabulary Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Adam Hazimeh, Alessandro Favero, Pascal Frossard<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Task arithmetic has recently emerged as a promising method for editing pre-trained \textit{open-vocabulary} models, offering a cost-effective alternative to standard multi-task fine-tuning. However, despite the abundance of \textit{closed-vocabulary} models that are not pre-trained with language supervision, applying task arithmetic to these models remains unexplored. In this paper, we deploy and study task addition in closed-vocabulary image classification models. We consider different pre-training schemes and find that \textit{weight disentanglement} -- the property enabling task arithmetic -- is a general consequence of pre-training, as it appears in different pre-trained closed-vocabulary models. In fact, we find that pre-trained closed-vocabulary vision transformers can also be edited with task arithmetic, achieving high task addition performance and enabling the efficient deployment of multi-task models. Finally, we demonstrate that simple linear probing is a competitive baseline to task addition. Overall, our findings expand the applicability of task arithmetic to a broader class of pre-trained models and open the way for more efficient use of pre-trained models in diverse settings.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14566v1" target="_blank"><h2>Examining the Metrics for Document-Level Claim Extraction in Czech and Slovak <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Lucia Makaiová, Martin Fajčík, Antonín Jarolím<br><strong><u>Categories:</u></strong> cs.CL, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Document-level claim extraction remains an open challenge in the field of fact-checking, and subsequently, methods for evaluating extracted claims have received limited attention. In this work, we explore approaches to aligning two sets of claims pertaining to the same source document and computing their similarity through an alignment score. We investigate techniques to identify the best possible alignment and evaluation method between claim sets, with the aim of providing a reliable evaluation framework. Our approach enables comparison between model-extracted and human-annotated claim sets, serving as a metric for assessing the extraction performance of models and also as a possible measure of inter-annotator agreement. We conduct experiments on newly collected dataset-claims extracted from comments under Czech and Slovak news articles-domains that pose additional challenges due to the informal language, strong local context, and subtleties of these closely related languages. The results draw attention to the limitations of current evaluation approaches when applied to document-level claim extraction and highlight the need for more advanced methods-ones able to correctly capture semantic similarity and evaluate essential claim properties such as atomicity, checkworthiness, and decontextualization.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14554v1" target="_blank"><h2>ForensicFlow: A Tri-Modal Adaptive Network for Robust Deepfake Detection <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Mohammad Romani<br><strong><u>Categories:</u></strong> cs.CV, cs.CR, cs.LG<br><strong><u>Comments:</u></strong> 11 pages, 4 figures, 2 tables. Preprint. Submitted on November 18, 2025<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Deepfakes generated by advanced GANs and autoencoders severely threaten information integrity and societal stability. Single-stream CNNs fail to capture multi-scale forgery artifacts across spatial, texture, and frequency domains, limiting robustness and generalization. We introduce the ForensicFlow, a tri-modal forensic framework that synergistically fuses RGB, texture, and frequency evidence for video Deepfake detection. The RGB branch (ConvNeXt-tiny) extracts global visual inconsistencies; the texture branch (Swin Transformer-tiny) detects fine-grained blending artifacts; the frequency branch (CNN + SE) identifies periodic spectral noise. Attention-based temporal pooling dynamically prioritizes high-evidence frames, while adaptive attention fusion balances branch contributions.Trained on Celeb-DF (v2) with Focal Loss, ForensicFlow achieves AUC 0.9752, F1-Score 0.9408, and accuracy 0.9208, outperforming single-stream baselines. Ablation validates branch synergy; Grad-CAM confirms forensic focus. This comprehensive feature fusion provides superior resilience against subtle forgeries.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14545v1" target="_blank"><h2>DeepBlip: Estimating Conditional Average Treatment Effects Over Time <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Haorui Ma, Dennis Frauen, Stefan Feuerriegel<br><strong><u>Categories:</u></strong> stat.ML, cs.LG, stat.ME<br><strong><u>Comments:</u></strong> 42 pages<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Structural nested mean models (SNMMs) are a principled approach to estimate the treatment effects over time. A particular strength of SNMMs is to break the joint effect of treatment sequences over time into localized, time-specific ``blip effects''. This decomposition promotes interpretability through the incremental effects and enables the efficient offline evaluation of optimal treatment policies without re-computation. However, neural frameworks for SNMMs are lacking, as their inherently sequential g-estimation scheme prevents end-to-end, gradient-based training. Here, we propose DeepBlip, the first neural framework for SNMMs, which overcomes this limitation with a novel double optimization trick to enable simultaneous learning of all blip functions. Our DeepBlip seamlessly integrates sequential neural networks like LSTMs or transformers to capture complex temporal dependencies. By design, our method correctly adjusts for time-varying confounding to produce unbiased estimates, and its Neyman-orthogonal loss function ensures robustness to nuisance model misspecification. Finally, we evaluate our DeepBlip across various clinical datasets, where it achieves state-of-the-art performance.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14544v1" target="_blank"><h2>Mind the Gaps: Measuring Visual Artifacts in Dimensionality Reduction <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jaume Ros, Alessio Arleo, Fernando Paulovich<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> dimensionality reduction (title, abstract)<br><p><strong><u>Abstract:</u></strong> Dimensionality Reduction (DR) techniques are commonly used for the visual exploration and analysis of high-dimensional data due to their ability to project datasets of high-dimensional points onto the 2D plane. However, projecting datasets in lower dimensions often entails some distortion, which is not necessarily easy to recognize but can lead users to misleading conclusions. Several Projection Quality Metrics (PQMs) have been developed as tools to quantify the goodness-of-fit of a DR projection; however, they mostly focus on measuring how well the projection captures the global or local structure of the data, without taking into account the visual distortion of the resulting plots, thus often ignoring the presence of outliers or artifacts that can mislead a visual analysis of the projection. In this work, we introduce the Warping Index (WI), a new metric for measuring the quality of DR projections onto the 2D plane, based on the assumption that the correct preservation of empty regions between points is of crucial importance towards a faithful visual representation of the data.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14533v1" target="_blank"><h2>A Neuro-Symbolic Framework for Reasoning under Perceptual Uncertainty: Bridging Continuous Perception and Discrete Symbolic Planning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jiahao Wu, Shengwen Yu<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> 29 pages, 10 figures, 12 tables<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Bridging continuous perceptual signals and discrete symbolic reasoning is a fundamental challenge in AI systems that must operate under uncertainty. We present a neuro-symbolic framework that explicitly models and propagates uncertainty from perception to planning, providing a principled connection between these two abstraction levels. Our approach couples a transformer-based perceptual front-end with graph neural network (GNN) relational reasoning to extract probabilistic symbolic states from visual observations, and an uncertainty-aware symbolic planner that actively gathers information when confidence is low. We demonstrate the framework's effectiveness on tabletop robotic manipulation as a concrete application: the translator processes 10,047 PyBullet-generated scenes (3--10 objects) and outputs probabilistic predicates with calibrated confidences (overall F1=0.68). When embedded in the planner, the system achieves 94\%/90\%/88\% success on Simple Stack, Deep Stack, and Clear+Stack benchmarks (90.7\% average), exceeding the strongest POMDP baseline by 10--14 points while planning within 15\,ms. A probabilistic graphical-model analysis establishes a quantitative link between calibrated uncertainty and planning convergence, providing theoretical guarantees that are validated empirically. The framework is general-purpose and can be applied to any domain requiring uncertainty-aware reasoning from perceptual input to symbolic planning.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14530v1" target="_blank"><h2>DeCo-VAE: Learning Compact Latents for Video Reconstruction via Decoupled Representation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Xiangchen Yin, Jiahui Yuan, Zhangchi Hu, Wenzhang Sun, Jie Chen, Xiaozhen Qiao, Hao Li, Xiaoyan Sun<br><strong><u>Categories:</u></strong> cs.CV, cs.LG, cs.MM<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> variational autoencoder (abstract), VAE (title, abstract)<br><p><strong><u>Abstract:</u></strong> Existing video Variational Autoencoders (VAEs) generally overlook the similarity between frame contents, leading to redundant latent modeling. In this paper, we propose decoupled VAE (DeCo-VAE) to achieve compact latent representation. Instead of encoding RGB pixels directly, we decompose video content into distinct components via explicit decoupling: keyframe, motion and residual, and learn dedicated latent representation for each. To avoid cross-component interference, we design dedicated encoders for each decoupled component and adopt a shared 3D decoder to maintain spatiotemporal consistency during reconstruction. We further utilize a decoupled adaptation strategy that freezes partial encoders while training the others sequentially, ensuring stable training and accurate learning of both static and dynamic features. Extensive quantitative and qualitative experiments demonstrate that DeCo-VAE achieves superior video reconstruction performance.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14516v2" target="_blank"><h2>Full-Atom Peptide Design via Riemannian-Euclidean Bayesian Flow Networks <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Hao Qian, Shikui Tu, Lei Xu<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> AAAI2026<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Diffusion and flow matching models have recently emerged as promising approaches for peptide binder design. Despite their progress, these models still face two major challenges. First, categorical sampling of discrete residue types collapses their continuous parameters into onehot assignments, while continuous variables (e.g., atom positions) evolve smoothly throughout the generation process. This mismatch disrupts the update dynamics and results in suboptimal performance. Second, current models assume unimodal distributions for side-chain torsion angles, which conflicts with the inherently multimodal nature of side chain rotameric states and limits prediction accuracy. To address these limitations, we introduce PepBFN, the first Bayesian flow network for full atom peptide design that directly models parameter distributions in fully continuous space. Specifically, PepBFN models discrete residue types by learning their continuous parameter distributions, enabling joint and smooth Bayesian updates with other continuous structural parameters. It further employs a novel Gaussian mixture based Bayesian flow to capture the multimodal side chain rotameric states and a Matrix Fisher based Riemannian flow to directly model residue orientations on the $\mathrm{SO}(3)$ manifold. Together, these parameter distributions are progressively refined via Bayesian updates, yielding smooth and coherent peptide generation. Experiments on side chain packing, reverse folding, and binder design tasks demonstrate the strong potential of PepBFN in computational peptide design.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14515v1" target="_blank"><h2>IMSE: Efficient U-Net-based Speech Enhancement using Inception Depthwise Convolution and Amplitude-Aware Linear Attention <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Xinxin Tang, Bin Qin, Yufang Li<br><strong><u>Categories:</u></strong> cs.SD, cs.AI, cs.CV<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Achieving a balance between lightweight design and high performance remains a significant challenge for speech enhancement (SE) tasks on resource-constrained devices. Existing state-of-the-art methods, such as MUSE, have established a strong baseline with only 0.51M parameters by introducing a Multi-path Enhanced Taylor (MET) transformer and Deformable Embedding (DE). However, an in-depth analysis reveals that MUSE still suffers from efficiency bottlenecks: the MET module relies on a complex "approximate-compensate" mechanism to mitigate the limitations of Taylor-expansion-based attention, while the offset calculation for deformable embedding introduces additional computational burden. This paper proposes IMSE, a systematically optimized and ultra-lightweight network. We introduce two core innovations: 1) Replacing the MET module with Amplitude-Aware Linear Attention (MALA). MALA fundamentally rectifies the "amplitude-ignoring" problem in linear attention by explicitly preserving the norm information of query vectors in the attention calculation, achieving efficient global modeling without an auxiliary compensation branch. 2) Replacing the DE module with Inception Depthwise Convolution (IDConv). IDConv borrows the Inception concept, decomposing large-kernel operations into efficient parallel branches (square, horizontal, and vertical strips), thereby capturing spectrogram features with extremely low parameter redundancy. Extensive experiments on the VoiceBank+DEMAND dataset demonstrate that, compared to the MUSE baseline, IMSE significantly reduces the parameter count by 16.8\% (from 0.513M to 0.427M) while achieving competitive performance comparable to the state-of-the-art on the PESQ metric (3.373). This study sets a new benchmark for the trade-off between model size and speech quality in ultra-lightweight speech enhancement.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14510v1" target="_blank"><h2>CLO: Efficient LLM Inference System with CPU-Light KVCache Offloading via Algorithm-System Co-Design <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jiawei Yi, Ping Gong, Youhui Bai, Jiaqi Ruan, Shengnan Wang, Pengcheng Wang, Haibo Wang, Weiguang Wang, Xia Zhu, Feng Wu, Cheng Li<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> The growth of million-token LLMs exposes the scalability limits of inference systems, where the KVCache dominates memory usage and data transfer overhead. Recent offloading systems migrate the KVCache to CPU memory and incorporate top-k attention to reduce the volume of data transferred from the CPU, while further applying system-level optimizations such as on-GPU caching and prefetching to lower transfer overhead. However, they overlook the CPU bottleneck in three aspects: (1) substantial overhead of fine-grained dynamic cache management performed on the CPU side, (2) significant transfer overhead from poor PCIe bandwidth utilization caused by heavy gathering operations at the CPU side, and (3) GPU runtime bubbles introduced by coarse-grained CPU-centric synchronization. To address these challenges, we propose CLO, a CPU-light KVCache offloading system via algorithm-system co-design. CLO features: (1) a coarse-grained head-wise approximate on-GPU caching strategy with negligible cache management cost, (2) seamless combination of data prefetching and on-GPU persistent caching for lower transfer overhead, (3) a zero-copy transfer engine to fully exploit PCIe bandwidth, and a GPU-centric synchronization method to eliminate GPU stalls. Evaluation on two widely-used LLMs demonstrates that CLO achieves comparable accuracy to state-of-the-art systems, while substantially minimizing CPU overhead, fully utilizing PCIe bandwidth, thus improving decoding throughput by 9.3%-66.6%. Our results highlight that algorithm-system co-design is essential for memory-constrained LLM inference on modern GPU platforms. We open source CLO at https://github.com/CommediaJW/CLO.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14501v1" target="_blank"><h2>Improved Convergence in Parameter-Agnostic Error Feedback through Momentum <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Abdurakhmon Sadiev, Yury Demidovich, Igor Sokolov, Grigory Malinovsky, Sarit Khirirat, Peter Richtárik<br><strong><u>Categories:</u></strong> math.OC, cs.LG<br><strong><u>Comments:</u></strong> 50 pages, 12 figures<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Communication compression is essential for scalable distributed training of modern machine learning models, but it often degrades convergence due to the noise it introduces. Error Feedback (EF) mechanisms are widely adopted to mitigate this issue of distributed compression algorithms. Despite their popularity and training efficiency, existing distributed EF algorithms often require prior knowledge of problem parameters (e.g., smoothness constants) to fine-tune stepsizes. This limits their practical applicability especially in large-scale neural network training. In this paper, we study normalized error feedback algorithms that combine EF with normalized updates, various momentum variants, and parameter-agnostic, time-varying stepsizes, thus eliminating the need for problem-dependent tuning. We analyze the convergence of these algorithms for minimizing smooth functions, and establish parameter-agnostic complexity bounds that are close to the best-known bounds with carefully-tuned problem-dependent stepsizes. Specifically, we show that normalized EF21 achieve the convergence rate of near ${O}(1/T^{1/4})$ for Polyak's heavy-ball momentum, ${O}(1/T^{2/7})$ for Iterative Gradient Transport (IGT), and ${O}(1/T^{1/3})$ for STORM and Hessian-corrected momentum. Our results hold with decreasing stepsizes and small mini-batches. Finally, our empirical experiments confirm our theoretical insights.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14482v1" target="_blank"><h2>Gradient-Based Join Ordering <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Tim Schwabe, Maribel Acosta<br><strong><u>Categories:</u></strong> cs.DB, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Join ordering is the NP-hard problem of selecting the most efficient sequence in which to evaluate joins (conjunctive, binary operators) in a database query. As the performance of query execution critically depends on this choice, join ordering lies at the core of query optimization. Traditional approaches cast this problem as a discrete combinatorial search over binary trees guided by a cost model, but they often suffer from high computational complexity and limited scalability. We show that, when the cost model is differentiable, the query plans can be continuously relaxed into a soft adjacency matrix representing a superposition of plans. This continuous relaxation, together with a Gumbel-Softmax parameterization of the adjacency matrix and differentiable constraints enforcing plan validity, enables gradient-based search for plans within this relaxed space. Using a learned Graph Neural Network as the cost model, we demonstrate that this gradient-based approach can find comparable and even lower-cost plans compared to traditional discrete local search methods on two different graph datasets. Furthermore, we empirically show that the runtime of this approach scales linearly with query size, in contrast to quadratic or exponential runtimes of classical approaches. We believe this first step towards gradient-based join ordering can lead to more effective and efficient query optimizers in the future.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14465v1" target="_blank"><h2>nnterp: A Standardized Interface for Mechanistic Interpretability of Transformers <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Clément Dumas<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 7 pages, 1 figure, accepted at the mechanistic interpretability workshop of NeurIPS 2025<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Mechanistic interpretability research requires reliable tools for analyzing transformer internals across diverse architectures. Current approaches face a fundamental tradeoff: custom implementations like TransformerLens ensure consistent interfaces but require coding a manual adaptation for each architecture, introducing numerical mismatch with the original models, while direct HuggingFace access through NNsight preserves exact behavior but lacks standardization across models. To bridge this gap, we develop nnterp, a lightweight wrapper around NNsight that provides a unified interface for transformer analysis while preserving original HuggingFace implementations. Through automatic module renaming and comprehensive validation testing, nnterp enables researchers to write intervention code once and deploy it across 50+ model variants spanning 16 architecture families. The library includes built-in implementations of common interpretability methods (logit lens, patchscope, activation steering) and provides direct access to attention probabilities for models that support it. By packaging validation tests with the library, researchers can verify compatibility with custom models locally. nnterp bridges the gap between correctness and usability in mechanistic interpretability tooling.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14456v1" target="_blank"><h2>Analyzing the Impact of Participant Failures in Cross-Silo Federated Learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Fabian Stricker, David Bermbach, Christian Zirpins<br><strong><u>Categories:</u></strong> cs.DC, cs.AI<br><strong><u>Comments:</u></strong> Accepted for publication in 3rd IEEE International Conference on Federated Learning Applications and Technologies (FLTA2025)<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Federated learning (FL) is a new paradigm for training machine learning (ML) models without sharing data. While applying FL in cross-silo scenarios, where organizations collaborate, it is necessary that the FL system is reliable; however, participants can fail due to various reasons (e.g., communication issues or misconfigurations). In order to provide a reliable system, it is necessary to analyze the impact of participant failures. While this problem received attention in cross-device FL where mobile devices with limited resources participate, there is comparatively little research in cross-silo FL.
  Therefore, we conduct an extensive study for analyzing the impact of participant failures on the model quality in the context of inter-organizational cross-silo FL with few participants. In our study, we focus on analyzing generally influential factors such as the impact of the timing and the data as well as the impact on the evaluation, which is important for deciding, if the model should be deployed. We show that under high skews the evaluation is optimistic and hides the real impact. Furthermore, we demonstrate that the timing impacts the quality of the trained model. Our results offer insights for researchers and software architects aiming to build robust FL systems.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14455v2" target="_blank"><h2>Nonparametric estimation of conditional probability distributions using a generative approach based on conditional push-forward neural networks <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Nicola Rares Franco, Lorenzo Tedesco<br><strong><u>Categories:</u></strong> cs.LG, stat.ME<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> We introduce conditional push-forward neural networks (CPFN), a generative framework for conditional distribution estimation. Instead of directly modeling the conditional density $f_{Y|X}$, CPFN learns a stochastic map $\varphi=\varphi(x,u)$ such that $\varphi(x,U)$ and $Y|X=x$ follow approximately the same law, with $U$ a suitable random vector of pre-defined latent variables. This enables efficient conditional sampling and straightforward estimation of conditional statistics through Monte Carlo methods. The model is trained via an objective function derived from a Kullback-Leibler formulation, without requiring invertibility or adversarial training. We establish a near-asymptotic consistency result and demonstrate experimentally that CPFN can achieve performance competitive with, or even superior to, state-of-the-art methods, including kernel estimators, tree-based algorithms, and popular deep learning techniques, all while remaining lightweight and easy to train.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14452v1" target="_blank"><h2>Hybrid Modeling of Photoplethysmography for Non-invasive Monitoring of Cardiovascular Parameters <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Emanuele Palumbo, Sorawit Saengkyongam, Maria R. Cervera, Jens Behrmann, Andrew C. Miller, Guillermo Sapiro, Christina Heinze-Deml, Antoine Wehenkel<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> variational autoencoder (abstract)<br><p><strong><u>Abstract:</u></strong> Continuous cardiovascular monitoring can play a key role in precision health. However, some fundamental cardiac biomarkers of interest, including stroke volume and cardiac output, require invasive measurements, e.g., arterial pressure waveforms (APW). As a non-invasive alternative, photoplethysmography (PPG) measurements are routinely collected in hospital settings. Unfortunately, the prediction of key cardiac biomarkers from PPG instead of APW remains an open challenge, further complicated by the scarcity of annotated PPG measurements. As a solution, we propose a hybrid approach that uses hemodynamic simulations and unlabeled clinical data to estimate cardiovascular biomarkers directly from PPG signals. Our hybrid model combines a conditional variational autoencoder trained on paired PPG-APW data with a conditional density estimator of cardiac biomarkers trained on labeled simulated APW segments. As a key result, our experiments demonstrate that the proposed approach can detect fluctuations of cardiac output and stroke volume and outperform a supervised baseline in monitoring temporal changes in these biomarkers.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14445v1" target="_blank"><h2>Tell Me: An LLM-powered Mental Well-being Assistant with RAG, Synthetic Dialogue Generation, and Agentic Planning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Trishala Jayesh Ahalpara<br><strong><u>Categories:</u></strong> cs.CL, cs.AI, cs.HC, cs.LG<br><strong><u>Comments:</u></strong> 8 pages, 2 figures, 1 Table. Submitted to the Computation and Language (cs.CL) category. Uses the ACL-style template. Code and demo will be released at:this https URL<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> data augmentation (abstract)<br><p><strong><u>Abstract:</u></strong> We present Tell Me, a mental well-being system that leverages advances in large language models to provide accessible, context-aware support for users and researchers. The system integrates three components: (i) a retrieval-augmented generation (RAG) assistant for personalized, knowledge-grounded dialogue; (ii) a synthetic client-therapist dialogue generator conditioned on client profiles to facilitate research on therapeutic language and data augmentation; and (iii) a Well-being AI crew, implemented with CrewAI, that produces weekly self-care plans and guided meditation audio. The system is designed as a reflective space for emotional processing rather than a substitute for professional therapy. It illustrates how conversational assistants can lower barriers to support, complement existing care, and broaden access to mental health resources. To address the shortage of confidential therapeutic data, we introduce synthetic client-therapist dialogue generation conditioned on client profiles. Finally, the planner demonstrates an innovative agentic workflow for dynamically adaptive, personalized self-care, bridging the limitations of static well-being tools. We describe the architecture, demonstrate its functionalities, and report evaluation of the RAG assistant in curated well-being scenarios using both automatic LLM-based judgments and a human-user study. This work highlights opportunities for interdisciplinary collaboration between NLP researchers and mental health professionals to advance responsible innovation in human-AI interaction for well-being.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14428v1" target="_blank"><h2>Context-aware, Ante-hoc Explanations of Driving Behaviour <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Dominik Grundt, Ishan Saxena, Malte Petersen, Bernd Westphal, Eike Möhlmann<br><strong><u>Categories:</u></strong> cs.LO, cs.AI, cs.CY<br><strong><u>Comments:</u></strong> In Proceedings FMAS 2025,arXiv:2511.13245<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> explainability (abstract)<br><p><strong><u>Abstract:</u></strong> Autonomous vehicles (AVs) must be both safe and trustworthy to gain social acceptance and become a viable option for everyday public transportation. Explanations about the system behaviour can increase safety and trust in AVs. Unfortunately, explaining the system behaviour of AI-based driving functions is particularly challenging, as decision-making processes are often opaque. The field of Explainability Engineering tackles this challenge by developing explanation models at design time. These models are designed from system design artefacts and stakeholder needs to develop correct and good explanations. To support this field, we propose an approach that enables context-aware, ante-hoc explanations of (un)expectable driving manoeuvres at runtime. The visual yet formal language Traffic Sequence Charts is used to formalise explanation contexts, as well as corresponding (un)expectable driving manoeuvres. A dedicated runtime monitoring enables context-recognition and ante-hoc presentation of explanations at runtime. In combination, we aim to support the bridging of correct and good explanations. Our method is demonstrated in a simulated overtaking.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14427v1" target="_blank"><h2>Self-Supervised Multisensory Pretraining for Contact-Rich Robot Reinforcement Learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Rickmer Krohn, Vignesh Prasad, Gabriele Tiboni, Georgia Chalvatzaki<br><strong><u>Categories:</u></strong> cs.RO, cs.LG<br><strong><u>Comments:</u></strong> 9 pages, 10 figures, preprint<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Effective contact-rich manipulation requires robots to synergistically leverage vision, force, and proprioception. However, Reinforcement Learning agents struggle to learn in such multisensory settings, especially amidst sensory noise and dynamic changes. We propose MultiSensory Dynamic Pretraining (MSDP), a novel framework for learning expressive multisensory representations tailored for task-oriented policy learning. MSDP is based on masked autoencoding and trains a transformer-based encoder by reconstructing multisensory observations from only a subset of sensor embeddings, leading to cross-modal prediction and sensor fusion. For downstream policy learning, we introduce a novel asymmetric architecture, where a cross-attention mechanism allows the critic to extract dynamic, task-specific features from the frozen embeddings, while the actor receives a stable pooled representation to guide its actions. Our method demonstrates accelerated learning and robust performance under diverse perturbations, including sensor noise, and changes in object dynamics. Evaluations in multiple challenging, contact-rich robot manipulation tasks in simulation and the real world showcase the effectiveness of MSDP. Our approach exhibits strong robustness to perturbations and achieves high success rates on the real robot with as few as 6,000 online interactions, offering a simple yet powerful solution for complex multisensory robotic control.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14422v1" target="_blank"><h2>Sigil: Server-Enforced Watermarking in U-Shaped Split Federated Learning via Gradient Injection <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zhengchunmin Dai, Jiaxiong Tang, Peng Sun, Honglong Chen, Liantao Wu<br><strong><u>Categories:</u></strong> cs.CR, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> 18 pages,8 figures<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> anomaly detection (abstract)<br><p><strong><u>Abstract:</u></strong> In decentralized machine learning paradigms such as Split Federated Learning (SFL) and its variant U-shaped SFL, the server's capabilities are severely restricted. Although this enhances client-side privacy, it also leaves the server highly vulnerable to model theft by malicious clients. Ensuring intellectual property protection for such capability-limited servers presents a dual challenge: watermarking schemes that depend on client cooperation are unreliable in adversarial settings, whereas traditional server-side watermarking schemes are technically infeasible because the server lacks access to critical elements such as model parameters or labels.
  To address this challenge, this paper proposes Sigil, a mandatory watermarking framework designed specifically for capability-limited servers. Sigil defines the watermark as a statistical constraint on the server-visible activation space and embeds the watermark into the client model via gradient injection, without requiring any knowledge of the data. Besides, we design an adaptive gradient clipping mechanism to ensure that our watermarking process remains both mandatory and stealthy, effectively countering existing gradient anomaly detection methods and a specifically designed adaptive subspace removal attack. Extensive experiments on multiple datasets and models demonstrate Sigil's fidelity, robustness, and stealthiness.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14396v1" target="_blank"><h2>Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Xiuxiu Qi, Yu Yang, Jiannong Cao, Luyao Bai, Chongshan Fan, Chengtai Cao, Hongpeng Wang<br><strong><u>Categories:</u></strong> cs.RO, cs.AI, cs.CV<br><strong><u>Comments:</u></strong> Accepted at AAAI 2026, the Project website is available atthis https URL<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> attention (abstract), data augmentation (abstract)<br><p><strong><u>Abstract:</u></strong> Language-conditioned manipulation facilitates human-robot interaction via behavioral cloning (BC), which learns control policies from human demonstrations and serves as a cornerstone of embodied AI. Overcoming compounding errors in sequential action decisions remains a central challenge to improving BC performance. Existing approaches mitigate compounding errors through data augmentation, expressive representation, or temporal abstraction. However, they suffer from physical discontinuities and semantic-physical misalignment, leading to inaccurate action cloning and intermittent execution. In this paper, we present Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL), a novel BC framework that ensures temporally consistent execution and fine-grained semantic grounding. It generates robust and smooth action execution trajectories through continuous co-learning across vision, language, and proprioceptive inputs (e.g., robot internal states). Meanwhile, we anchor language semantics to visuomotor representations by a bidirectional cross-attention to learn contextual information for action generation, successfully overcoming the problem of semantic-physical misalignment. Extensive experiments show that CCoL achieves an average 8.0% relative improvement across three simulation suites, with up to 19.2% relative gain in human-demonstrated bimanual insertion tasks. Real-world tests on a 7-DoF robot further confirm CCoL's generalization under unseen and noisy object states.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14348v1" target="_blank"><h2>Enforcing hidden physics in physics-informed neural networks <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Nanxi Chen, Sifan Wang, Rujin Ma, Airong Chen, Chuanjie Cui<br><strong><u>Categories:</u></strong> cs.LG, physics.comp-ph<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> Physics-informed neural networks (PINNs) represent a new paradigm for solving partial differential equations (PDEs) by integrating physical laws into the learning process of neural networks. However, despite their foundational role, the hidden irreversibility implied by the Second Law of Thermodynamics is often neglected during training, leading to unphysical solutions or even training failures in conventional PINNs. In this paper, we identify this critical gap and introduce a simple, generalized, yet robust irreversibility-regularized strategy that enforces hidden physical laws as soft constraints during training. This approach ensures that the learned solutions consistently respect the intrinsic one-way nature of irreversible physical processes. Across a wide range of benchmarks spanning traveling wave propagation, steady combustion, ice melting, corrosion evolution, and crack propagation, we demonstrate that our regularization scheme reduces predictive errors by more than an order of magnitude, while requiring only minimal modification to existing PINN frameworks. We believe that the proposed framework is broadly applicable to a wide class of PDE-governed physical systems and will have significant impact within the scientific machine learning community.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14341v1" target="_blank"><h2>Going Places: Place Recognition in Artificial and Natural Systems <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Michael Milford, Tobias Fischer<br><strong><u>Categories:</u></strong> cs.RO, cs.AI, cs.CV<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Place recognition, the ability to identify previously visited locations, is critical for both biological navigation and autonomous systems. This review synthesizes findings from robotic systems, animal studies, and human research to explore how different systems encode and recall place. We examine the computational and representational strategies employed across artificial systems, animals, and humans, highlighting convergent solutions such as topological mapping, cue integration, and memory management. Animal systems reveal evolved mechanisms for multimodal navigation and environmental adaptation, while human studies provide unique insights into semantic place concepts, cultural influences, and introspective capabilities. Artificial systems showcase scalable architectures and data-driven models. We propose a unifying set of concepts by which to consider and develop place recognition mechanisms and identify key challenges such as generalization, robustness, and environmental variability. This review aims to foster innovations in artificial localization by connecting future developments in artificial place recognition systems to insights from both animal navigation research and human spatial cognition studies.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14322v1" target="_blank"><h2>LSP-YOLO: A Lightweight Single-Stage Network for Sitting Posture Recognition on Embedded Devices <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Nanjun Li, Ziyue Hao, Quanqiang Wang, Xuanyin Wang<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> Submitted to Engineering Applications of Artificial Intelligence (EAAI)<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> With the rise in sedentary behavior, health problems caused by poor sitting posture have drawn increasing attention. Most existing methods, whether using invasive sensors or computer vision, rely on two-stage pipelines, which result in high intrusiveness, intensive computation, and poor real-time performance on embedded edge devices. Inspired by YOLOv11-Pose, a lightweight single-stage network for sitting posture recognition on embedded edge devices termed LSP-YOLO was proposed. By integrating partial convolution(PConv) and Similarity-Aware Activation Module(SimAM), a lightweight module, Light-C3k2, was designed to reduce computational cost while maintaining feature extraction capability. In the recognition head, keypoints were directly mapped to posture classes through pointwise convolution, and intermediate supervision was employed to enable efficient fusion of pose estimation and classification. Furthermore, a dataset containing 5,000 images across six posture categories was constructed for model training and testing. The smallest trained model, LSP-YOLO-n, achieved 94.2% accuracy and 251 Fps on personal computer(PC) with a model size of only 1.9 MB. Meanwhile, real-time and high-accuracy inference under constrained computational resources was demonstrated on the SV830C + GC030A platform. The proposed approach is characterized by high efficiency, lightweight design and deployability, making it suitable for smart classrooms, rehabilitation, and human-computer interaction applications.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14312v1" target="_blank"><h2>H-LDM: Hierarchical Latent Diffusion Models for Controllable and Interpretable PCG Synthesis from Clinical Metadata <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Chenyang Xu, Siming Li, Hao Wang<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> This paper was accepted by IEEE BIBM 2025 conference<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> VAE (abstract), latent space (abstract), attention (abstract), data augmentation (abstract)<br><p><strong><u>Abstract:</u></strong> Phonocardiogram (PCG) analysis is vital for cardiovascular disease diagnosis, yet the scarcity of labeled pathological data hinders the capability of AI systems. To bridge this, we introduce H-LDM, a Hierarchical Latent Diffusion Model for generating clinically accurate and controllable PCG signals from structured metadata. Our approach features: (1) a multi-scale VAE that learns a physiologically-disentangled latent space, separating rhythm, heart sounds, and murmurs; (2) a hierarchical text-to-biosignal pipeline that leverages rich clinical metadata for fine-grained control over 17 distinct conditions; and (3) an interpretable diffusion process guided by a novel Medical Attention module. Experiments on the PhysioNet CirCor dataset demonstrate state-of-the-art performance, achieving a Fréchet Audio Distance of 9.7, a 92% attribute disentanglement score, and 87.1% clinical validity confirmed by cardiologists. Augmenting diagnostic models with our synthetic data improves the accuracy of rare disease classification by 11.3\%. H-LDM establishes a new direction for data augmentation in cardiac diagnostics, bridging data scarcity with interpretable clinical insights.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14301v1" target="_blank"><h2>Steganographic Backdoor Attacks in NLP: Ultra-Low Poisoning and Defense Evasion <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Eric Xue, Ruiyi Zhang, Zijun Zhang, Pengtao Xie<br><strong><u>Categories:</u></strong> cs.CR, cs.CL, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Transformer models are foundational to natural language processing (NLP) applications, yet remain vulnerable to backdoor attacks introduced through poisoned data, which implant hidden behaviors during training. To strengthen the ability to prevent such compromises, recent research has focused on designing increasingly stealthy attacks to stress-test existing defenses, pairing backdoor behaviors with stylized artifact or token-level perturbation triggers. However, this trend diverts attention from the harder and more realistic case: making the model respond to semantic triggers such as specific names or entities, where a successful backdoor could manipulate outputs tied to real people or events in deployed systems. Motivated by this growing disconnect, we introduce SteganoBackdoor, bringing stealth techniques back into line with practical threat models. Leveraging innocuous properties from natural-language steganography, SteganoBackdoor applies a gradient-guided data optimization process to transform semantic trigger seeds into steganographic carriers that embed a high backdoor payload, remain fluent, and exhibit no representational resemblance to the trigger. Across diverse experimental settings, SteganoBackdoor achieves over 99% attack success at an order-of-magnitude lower data-poisoning rate than prior approaches while maintaining unparalleled evasion against a comprehensive suite of data-level defenses. By revealing this practical and covert attack, SteganoBackdoor highlights an urgent blind spot in current defenses and demands immediate attention to adversarial data defenses and real-world threat modeling.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14299v1" target="_blank"><h2>DataSage: Multi-agent Collaboration for Insight Discovery with External Knowledge Retrieval, Multi-role Debating, and Multi-path Reasoning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Xiaochuan Liu, Yuanfeng Song, Xiaoming Yin, Xing Chen<br><strong><u>Categories:</u></strong> cs.AI, cs.CL, cs.MA<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> In today's data-driven era, fully automated end-to-end data analytics, particularly insight discovery, is critical for discovering actionable insights that assist organizations in making effective decisions. With the rapid advancement of large language models (LLMs), LLM-driven agents have emerged as a promising paradigm for automating data analysis and insight discovery. However, existing data insight agents remain limited in several key aspects, often failing to deliver satisfactory results due to: (1) insufficient utilization of domain knowledge, (2) shallow analytical depth, and (3) error-prone code generation during insight generation. To address these issues, we propose DataSage, a novel multi-agent framework that incorporates three innovative features including external knowledge retrieval to enrich the analytical context, a multi-role debating mechanism to simulate diverse analytical perspectives and deepen analytical depth, and multi-path reasoning to improve the accuracy of the generated code and insights. Extensive experiments on InsightBench demonstrate that DataSage consistently outperforms existing data insight agents across all difficulty levels, offering an effective solution for automated data insight discovery.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14283v1" target="_blank"><h2>NeuralSSD: A Neural Solver for Signed Distance Surface Reconstruction <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zi-Chen Xi, Jiahui Huang, Hao-Xiang Chen, Francis Williams, Qun-Ce Xu, Tai-Jiang Mu, Shi-Min Hu<br><strong><u>Categories:</u></strong> cs.CV, cs.GR, cs.LG<br><strong><u>Comments:</u></strong> Under review<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract)<br><p><strong><u>Abstract:</u></strong> We proposed a generalized method, NeuralSSD, for reconstructing a 3D implicit surface from the widely-available point cloud data. NeuralSSD is a solver-based on the neural Galerkin method, aimed at reconstructing higher-quality and accurate surfaces from input point clouds. Implicit method is preferred due to its ability to accurately represent shapes and its robustness in handling topological changes. However, existing parameterizations of implicit fields lack explicit mechanisms to ensure a tight fit between the surface and input data. To address this, we propose a novel energy equation that balances the reliability of point cloud information. Additionally, we introduce a new convolutional network that learns three-dimensional information to achieve superior optimization results. This approach ensures that the reconstructed surface closely adheres to the raw input points and infers valuable inductive biases from point clouds, resulting in a highly accurate and stable surface reconstruction. NeuralSSD is evaluated on a variety of challenging datasets, including the ShapeNet and Matterport datasets, and achieves state-of-the-art results in terms of both surface reconstruction accuracy and generalizability.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14282v1" target="_blank"><h2>Weight Variance Amplifier Improves Accuracy in High-Sparsity One-Shot Pruning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Vincent-Daniel Yun, Junhyuk Jo, Sunwoo Lee<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Deep neural networks achieve outstanding performance in visual recognition tasks, yet their large number of parameters makes them less practical for real-world applications. Recently, one-shot pruning has emerged as an effective strategy for reducing model size without additional training. However, models trained with standard objective functions often suffer a significant drop in accuracy after aggressive pruning. Some existing pruning-robust optimizers, such as SAM, and CrAM, mitigate this accuracy drop by guiding the model toward flatter regions of the parameter space, but they inevitably incur non-negligible additional computations. We propose a Variance Amplifying Regularizer (VAR) that deliberately increases the variance of model parameters during training. Our study reveals an intriguing finding that parameters with higher variance exhibit greater pruning robustness. VAR exploits this property by promoting such variance in the weight distribution, thereby mitigating the adverse effects of pruning. We further provide a theoretical analysis of its convergence behavior, supported by extensive empirical results demonstrating the superior pruning robustness of VAR.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14268v1" target="_blank"><h2>Statistically controllable microstructure reconstruction framework for heterogeneous materials using sliced-Wasserstein metric and neural networks <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zhenchuan Ma, Qizhi Teng, Pengcheng Yan, Lindong Li, Kirill M. Gerke, Marina V. Karsanina, Xiaohai He<br><strong><u>Categories:</u></strong> physics.comp-ph, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> Heterogeneous porous materials play a crucial role in various engineering systems. Microstructure characterization and reconstruction provide effective means for modeling these materials, which are critical for conducting physical property simulations, structure-property linkage studies, and enhancing their performance across different applications. To achieve superior controllability and applicability with small sample sizes, we propose a statistically controllable microstructure reconstruction framework that integrates neural networks with sliced-Wasserstein metric. Specifically, our approach leverages local pattern distribution for microstructure characterization and employs a controlled sampling strategy to generate target distributions that satisfy given conditional parameters. A neural network-based model establishes the mapping from the input distribution to the target local pattern distribution, enabling microstructure reconstruction. Combinations of sliced-Wasserstein metric and gradient optimization techniques minimize the distance between these distributions, leading to a stable and reliable model. Our method can perform stochastic and controllable reconstruction tasks even with small sample sizes. Additionally, it can generate large-size (e.g. 512 and 1024) 3D microstructures using a chunking strategy. By introducing spatial location masks, our method excels at generating spatially heterogeneous and complex microstructures. We conducted experiments on stochastic reconstruction, controllable reconstruction, heterogeneous reconstruction, and large-size microstructure reconstruction across various materials. Comparative analysis through visualization, statistical measures, and physical property simulations demonstrates the effectiveness, providing new insights and possibilities for research on structure-property linkage and material inverse design.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14265v1" target="_blank"><h2>Unified Multimodal Vessel Trajectory Prediction with Explainable Navigation Intention <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Rui Zhang, Chao Li, Kezhong Liu, Chen Wang, Bolong Zheng, Hongbo Jiang<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> variational autoencoder (abstract), VAE (abstract), explainability (abstract), explainable (title, abstract), multimodal (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Vessel trajectory prediction is fundamental to intelligent maritime systems. Within this domain, short-term prediction of rapid behavioral changes in complex maritime environments has established multimodal trajectory prediction (MTP) as a promising research area. However, existing vessel MTP methods suffer from limited scenario applicability and insufficient explainability. To address these challenges, we propose a unified MTP framework incorporating explainable navigation intentions, which we classify into sustained and transient categories. Our method constructs sustained intention trees from historical trajectories and models dynamic transient intentions using a Conditional Variational Autoencoder (CVAE), while using a non-local attention mechanism to maintain global scenario consistency. Experiments on real Automatic Identification System (AIS) datasets demonstrates our method's broad applicability across diverse scenarios, achieving significant improvements in both ADE and FDE. Furthermore, our method improves explainability by explicitly revealing the navigational intentions underlying each predicted trajectory.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14263v1" target="_blank"><h2>Algebraformer: A Neural Approach to Linear Systems <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Pietro Sittoni, Francesco Tudisco<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Recent work in deep learning has opened new possibilities for solving classical algorithmic tasks using end-to-end learned models. In this work, we investigate the fundamental task of solving linear systems, particularly those that are ill-conditioned. Existing numerical methods for ill-conditioned systems often require careful parameter tuning, preconditioning, or domain-specific expertise to ensure accuracy and stability. In this work, we propose Algebraformer, a Transformer-based architecture that learns to solve linear systems end-to-end, even in the presence of severe ill-conditioning. Our model leverages a novel encoding scheme that enables efficient representation of matrix and vector inputs, with a memory complexity of $O(n^2)$, supporting scalable inference. We demonstrate its effectiveness on application-driven linear problems, including interpolation tasks from spectral methods for boundary value problems and acceleration of the Newton method. Algebraformer achieves competitive accuracy with significantly lower computational overhead at test time, demonstrating that general-purpose neural architectures can effectively reduce complexity in traditional scientific computing pipelines.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14262v1" target="_blank"><h2>Object-Centric World Models for Causality-Aware Reinforcement Learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yosuke Nishimoto, Takashi Matsubara<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> Accepted by AAAI-26<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (abstract), causality (title, abstract)<br><p><strong><u>Abstract:</u></strong> World models have been developed to support sample-efficient deep reinforcement learning agents. However, it remains challenging for world models to accurately replicate environments that are high-dimensional, non-stationary, and composed of multiple objects with rich interactions since most world models learn holistic representations of all environmental components. By contrast, humans perceive the environment by decomposing it into discrete objects, facilitating efficient decision-making. Motivated by this insight, we propose \emph{Slot Transformer Imagination with CAusality-aware reinforcement learning} (STICA), a unified framework in which object-centric Transformers serve as the world model and causality-aware policy and value networks. STICA represents each observation as a set of object-centric tokens, together with tokens for the agent action and the resulting reward, enabling the world model to predict token-level dynamics and interactions. The policy and value networks then estimate token-level cause--effect relations and use them in the attention layers, yielding causality-guided decision-making. Experiments on object-rich benchmarks demonstrate that STICA consistently outperforms state-of-the-art agents in both sample efficiency and final performance.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14250v1" target="_blank"><h2>Count The Notes: Histogram-Based Supervision for Automatic Music Transcription <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jonathan Yaffe, Ben Maman, Meinard Müller, Amit H. Bermano<br><strong><u>Categories:</u></strong> cs.SD, cs.LG<br><strong><u>Comments:</u></strong> ISMIR 2025<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Automatic Music Transcription (AMT) converts audio recordings into symbolic musical representations. Training deep neural networks (DNNs) for AMT typically requires strongly aligned training pairs with precise frame-level annotations. Since creating such datasets is costly and impractical for many musical contexts, weakly aligned approaches using segment-level annotations have gained traction. However, existing methods often rely on Dynamic Time Warping (DTW) or soft alignment loss functions, both of which still require local semantic correspondences, making them error-prone and computationally expensive. In this article, we introduce CountEM, a novel AMT framework that eliminates the need for explicit local alignment by leveraging note event histograms as supervision, enabling lighter computations and greater flexibility. Using an Expectation-Maximization (EM) approach, CountEM iteratively refines predictions based solely on note occurrence counts, significantly reducing annotation efforts while maintaining high transcription accuracy. Experiments on piano, guitar, and multi-instrument datasets demonstrate that CountEM matches or surpasses existing weakly supervised methods, improving AMT's robustness, scalability, and efficiency. Our project page is available at https://yoni-yaffe.github.io/count-the-notes.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14248v1" target="_blank"><h2>Enhancing Regional Airbnb Trend Forecasting Using LLM-Based Embeddings of Accessibility and Human Mobility <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Hongju Lee, Youngjun Park, Jisun An, Dongman Lee<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> Accepted at ASONAM 2025<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), transformer (abstract)<br><p><strong><u>Abstract:</u></strong> The expansion of short-term rental platforms, such as Airbnb, has significantly disrupted local housing markets, often leading to increased rental prices and housing affordability issues. Accurately forecasting regional Airbnb market trends can thus offer critical insights for policymakers and urban planners aiming to mitigate these impacts. This study proposes a novel time-series forecasting framework to predict three key Airbnb indicators -- Revenue, Reservation Days, and Number of Reservations -- at the regional level. Using a sliding-window approach, the model forecasts trends 1 to 3 months ahead. Unlike prior studies that focus on individual listings at fixed time points, our approach constructs regional representations by integrating listing features with external contextual factors such as urban accessibility and human mobility. We convert structured tabular data into prompt-based inputs for a Large Language Model (LLM), producing comprehensive regional embeddings. These embeddings are then fed into advanced time-series models (RNN, LSTM, Transformer) to better capture complex spatio-temporal dynamics. Experiments on Seoul's Airbnb dataset show that our method reduces both average RMSE and MAE by approximately 48% compared to conventional baselines, including traditional statistical and machine learning models. Our framework not only improves forecasting accuracy but also offers practical insights for detecting oversupplied regions and supporting data-driven urban policy decisions.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14235v1" target="_blank"><h2>Spontaneous Symmetry Breaking as a Late-Time Trigger for Interacting Dark Energy <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Pradosh Keshav MV, NS Kavya, Kenath Arun<br><strong><u>Categories:</u></strong> astro-ph.CO, gr-qc<br><strong><u>Comments:</u></strong> Prepared for submission to JCAP<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> Persistent tensions in the Hubble constant (H0) and the matter clustering parameter (S8) motivate late-time new physics that suppresses structure growth without significantly altering the background expansion history of the LambdaCDM model. We study a class of dark-sector dynamics in which a scalar dark energy field, governed by a Z2-symmetric quartic potential, interacts with dark matter through Yukawa and portal couplings. When the matter density drops below a critical threshold, a cosmological spontaneous symmetry breaking mechanism generates a time-dependent vacuum expectation value v(a) and activates an effective coupling eta(a). This creates a symmetric phase (a <= ac) identical to LambdaCDM at early times, and a broken phase (a > ac) in which eta(a) > 0 transfers energy from dark matter to dark energy, suppressing linear structure growth. Using RSD, BAO, cosmic chronometers, Pantheon+SH0ES supernovae, and compressed Planck distance priors, we compare a fixed LambdaCDM background with a self-consistent coupled-scalar evolution. The RSD-only analysis shows a strong shift: the dynamical background gives Omega_m ~ 0.31 +/- 0.10 and sigma8,0 ~ 0.59 +/- 0.01, while the fixed-background case gives Omega_m ~ 0.20 +/- 0.09 and sigma8,0 ~ 0.75 +/- 0.05. In the full joint fit, we obtain Omega_m = 0.29 +/- 0.01, H0 = 69.7 +/- 0.6 km s^-1 Mpc^-1, and sigma8,0 = 0.78 +/- 0.01. A late-time interaction triggered by spontaneous symmetry breaking can therefore damp structure growth and ease the S8 tension while leaving the expansion history and the inferred H0 essentially unchanged, suggesting distinct physical origins for the two tensions.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14229v1" target="_blank"><h2>EBind: a practical approach to space binding <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jim Broadbent, Felix Cohen, Frederik Hvilshøj, Eric Landau, Eren Sasoglu<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> We simplify space binding by focusing on two core components, a single encoder per modality and high-quality data; enabling training state-of-the-art models on a single GPU in a few hours as opposed to multiple days. We present EBind, an Easy, data-centric, and parameter-efficient method to Bind the embedding spaces of multiple contrastive models. We demonstrate that a simple 1.8B-parameter image-text-video-audio-3D model can outperform models 4 to 17x the size. The key to achieving this is a carefully curated dataset of three complementary data sources: i) 6.7M fully-automated multimodal quintuples sourced via SOTA retrieval models, ii) 1M diverse, semi-automated triples annotated by humans as negative, partial, or positive matches, and iii) 3.4M pre-existing captioned data items. We use 13 different evaluations to demonstrate the value of each data source. Due to limitations with existing benchmarks, we further introduce the first high-quality, consensus-annotated zero-shot classification benchmark between audio and PCs. In contrast to related work, we will open-source our code, model weights, and datasets.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14219v1" target="_blank"><h2>Listen Like a Teacher: Mitigating Whisper Hallucinations using Adaptive Layer Attention and Knowledge Distillation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Kumud Tripathi, Aditya Srinivas Menon, Aman Gaurav, Raj Prakash Gohil, Pankaj Wasnik<br><strong><u>Categories:</u></strong> cs.AI, cs.SD<br><strong><u>Comments:</u></strong> Accepted at AAAI 2026 - Main Technical Track<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> The Whisper model, an open-source automatic speech recognition system, is widely adopted for its strong performance across multilingual and zero-shot settings. However, it frequently suffers from hallucination errors, especially under noisy acoustic conditions. Previous works to reduce hallucinations in Whisper-style ASR systems have primarily focused on audio preprocessing or post-processing of transcriptions to filter out erroneous content. However, modifications to the Whisper model itself remain largely unexplored to mitigate hallucinations directly. To address this challenge, we present a two-stage architecture that first enhances encoder robustness through Adaptive Layer Attention (ALA) and further suppresses hallucinations using a multi-objective knowledge distillation (KD) framework. In the first stage, ALA groups encoder layers into semantically coherent blocks via inter-layer correlation analysis. A learnable multi-head attention module then fuses these block representations, enabling the model to jointly exploit low- and high-level features for more robust encoding. In the second stage, our KD framework trains the student model on noisy audio to align its semantic and attention distributions with a teacher model processing clean inputs. Our experiments on noisy speech benchmarks show notable reductions in hallucinations and word error rates, while preserving performance on clean speech. Together, ALA and KD offer a principled strategy to improve Whisper's reliability under real-world noisy conditions.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14210v2" target="_blank"><h2>Orion: A Unified Visual Agent for Multimodal Perception, Advanced Visual Reasoning and Execution <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> N Dinesh Reddy, Dylan Snyder, Lona Kiragu, Mirajul Mohin, Shahrear Bin Amin, Sudeep Pillai<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> multimodal (title)<br><p><strong><u>Abstract:</u></strong> We introduce Orion, a visual agent that integrates vision-based reasoning with tool-augmented execution to achieve powerful, precise, multi-step visual intelligence across images, video, and documents. Unlike traditional vision-language models that generate descriptive outputs, Orion orchestrates a suite of specialized computer vision tools, including object detection, keypoint localization, panoptic segmentation, Optical Character Recognition (OCR), and geometric analysis, to execute complex multi-step visual workflows. The system achieves competitive performance across MMMU, MMBench, DocVQA, and MMLongBench while extending monolithic VLM capabilities to production-grade visual intelligence. Through its agentic, tool-augmented approach, Orion enables autonomous visual reasoning that bridges neural perception with symbolic execution, marking the transition from passive visual understanding to active, tool-driven visual intelligence.
  Try Orion for free at: https://chat.vlm.run
  Learn more at: https://www.vlm.run/orion</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14206v1" target="_blank"><h2>Causal Discovery on Higher-Order Interactions <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Alessio Zanga, Marco Scutari, Fabio Stella<br><strong><u>Categories:</u></strong> stat.ML, cs.LG<br><strong><u>Comments:</u></strong> 16 pages, 2 figures<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Causal discovery combines data with knowledge provided by experts to learn the DAG representing the causal relationships between a given set of variables. When data are scarce, bagging is used to measure our confidence in an average DAG obtained by aggregating bootstrapped DAGs. However, the aggregation step has received little attention from the specialized literature: the average DAG is constructed using only the confidence in the individual edges of the bootstrapped DAGs, thus disregarding complex higher-order edge structures. In this paper, we introduce a novel theoretical framework based on higher-order structures and describe a new DAG aggregation algorithm. We perform a simulation study, discussing the advantages and limitations of the proposed approach. Our proposal is both computationally efficient and effective, outperforming state-of-the-art solutions, especially in low sample size regimes and under high dimensionality settings.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14203v1" target="_blank"><h2>Multi-Scale Correlation-Aware Transformer for Maritime Vessel Re-Identification <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yunhe Liu<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> Maritime vessel re-identification (Re-ID) plays a crucial role in advancing maritime monitoring and intelligent situational awareness systems. However, some existing vessel Re-ID methods are directly adapted from pedestrian-focused algorithms, making them ill-suited for mitigating the unique problems present in vessel images, particularly the greater intra-identity variations and more severe missing of local parts, which lead to the emergence of outlier samples within the same identity. To address these challenges, we propose the Multi-scale Correlation-aware Transformer Network (MCFormer), which explicitly models multi-scale correlations across the entire input set to suppress the adverse effects of outlier samples with intra-identity variations or local missing, incorporating two novel modules, the Global Correlation Module (GCM), and the Local Correlation Module (LCM). Specifically, GCM constructs a global similarity affinity matrix across all input images to model global correlations through feature aggregation based on inter-image consistency, rather than solely learning features from individual images as in most existing approaches. Simultaneously, LCM mines and aligns local features of positive samples with contextual similarity to extract local correlations by maintaining a dynamic memory bank, effectively compensating for missing or occluded regions in individual images. To further enhance feature robustness, MCFormer integrates global and local features that have been respectively correlated across multiple scales, effectively capturing latent relationships among image features. Experiments on three benchmarks demonstrate that MCFormer achieves state-of-the-art performance.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14186v1" target="_blank"><h2>Few-Shot Precise Event Spotting via Unified Multi-Entity Graph and Distillation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zhaoyu Liu, Kan Jiang, Murong Ma, Zhe Hou, Yun Lin, Jin Song Dong<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> The 40th Annual AAAI Conference on Artificial Intelligence (AAAI 2026)<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Precise event spotting (PES) aims to recognize fine-grained events at exact moments and has become a key component of sports analytics. This task is particularly challenging due to rapid succession, motion blur, and subtle visual differences. Consequently, most existing methods rely on domain-specific, end-to-end training with large labeled datasets and often struggle in few-shot conditions due to their dependence on pixel- or pose-based inputs alone. However, obtaining large labeled datasets is practically hard. We propose a Unified Multi-Entity Graph Network (UMEG-Net) for few-shot PES. UMEG-Net integrates human skeletons and sport-specific object keypoints into a unified graph and features an efficient spatio-temporal extraction module based on advanced GCN and multi-scale temporal shift. To further enhance performance, we employ multimodal distillation to transfer knowledge from keypoint-based graphs to visual representations. Our approach achieves robust performance with limited labeled data and significantly outperforms baseline models in few-shot settings, providing a scalable and effective solution for few-shot PES. Code is publicly available at https://github.com/LZYAndy/UMEG-Net.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14172v1" target="_blank"><h2>SymLoc: Symbolic Localization of Hallucination across HaluEval and TruthfulQA <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Naveen Lamba, Sanju Tiwari, Manas Gaur<br><strong><u>Categories:</u></strong> cs.CL, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> LLMs still struggle with hallucination, especially when confronted with symbolic triggers like modifiers, negation, numbers, exceptions, and named entities. Yet, we lack a clear understanding of where these symbolic hallucinations originate, making it crucial to systematically handle such triggers and localize the emergence of hallucination inside the model. While prior work explored localization using statistical techniques like LSC and activation variance analysis, these methods treat all tokens equally and overlook the role symbolic linguistic knowledge plays in triggering hallucinations. So far, no approach has investigated how symbolic elements specifically drive hallucination failures across model layers, nor has symbolic linguistic knowledge been used as the foundation for a localization framework. We propose the first symbolic localization framework that leverages symbolic linguistic and semantic knowledge to meaningfully trace the development of hallucinations across all model layers. By focusing on how models process symbolic triggers, we analyze five models using HaluEval and TruthfulQA. Our symbolic knowledge approach reveals that attention variance for these linguistic elements explodes to critical instability in early layers (2-4), with negation triggering catastrophic variance levels, demonstrating that symbolic semantic processing breaks down from the very beginning. Through the lens of symbolic linguistic knowledge, despite larger model sizes, hallucination rates remain consistently high (78.3%-83.7% across Gemma variants), with steep attention drops for symbolic semantic triggers throughout deeper layers. Our findings demonstrate that hallucination is fundamentally a symbolic linguistic processing failure, not a general generation problem, revealing that symbolic semantic knowledge provides the key to understanding and localizing hallucination mechanisms in LLMs.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14169v1" target="_blank"><h2>AdaTok: Adaptive Token Compression with Object-Aware Representations for Efficient Multimodal LLMs <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Xinliang Zhang, Lei Zhu, Hangzhou He, Shuang Zeng, Ourui Fu, Jiakui Hu, Zhengjian Yao, Yanye Lu<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Multimodal Large Language Models (MLLMs) have demonstrated substantial value in unified text-image understanding and reasoning, primarily by converting images into sequences of patch-level tokens that align with their architectural paradigm. However, patch-level tokenization leads to a quadratic growth in image tokens, burdening MLLMs' understanding and reasoning with enormous computation and memory. Additionally, the traditional patch-wise scanning tokenization workflow misaligns with the human vision cognition system, further leading to hallucination and computational redundancy. To address this issue, we propose an object-level token merging strategy for Adaptive Token compression, revealing the consistency with human vision system. The experiments are conducted on multiple comprehensive benchmarks, which show that our approach averagely, utilizes only 10% tokens while achieving almost 96% of the vanilla model's performance. More extensive experimental results in comparison with relevant works demonstrate the superiority of our method in balancing compression ratio and performance. Our code will be available.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14168v1" target="_blank"><h2>Certified Signed Graph Unlearning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Junpeng Zhao, Lin Li, Kaixi Hu, Kaize Shi, Jingling Yuan<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Signed graphs model complex relationships through positive and negative edges, with widespread real-world applications. Given the sensitive nature of such data, selective removal mechanisms have become essential for privacy protection. While graph unlearning enables the removal of specific data influences from Graph Neural Networks (GNNs), existing methods are designed for conventional GNNs and overlook the unique heterogeneous properties of signed graphs. When applied to Signed Graph Neural Networks (SGNNs), these methods lose critical sign information, degrading both model utility and unlearning effectiveness. To address these challenges, we propose Certified Signed Graph Unlearning (CSGU), which provides provable privacy guarantees while preserving the sociological principles underlying SGNNs. CSGU employs a three-stage method: (1) efficiently identifying minimal influenced neighborhoods via triangular structures, (2) applying sociological theories to quantify node importance for optimal privacy budget allocation, and (3) performing importance-weighted parameter updates to achieve certified modifications with minimal utility degradation. Extensive experiments demonstrate that CSGU outperforms existing methods, achieving superior performance in both utility preservation and unlearning effectiveness on SGNNs.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14153v1" target="_blank"><h2>A Comprehensive Study of Implicit and Explicit Biases in Large Language Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Fatima Kazi, Alex Young, Yash Inani, Setareh Rafatirad<br><strong><u>Categories:</u></strong> cs.LG, cs.CY<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> data augmentation (abstract)<br><p><strong><u>Abstract:</u></strong> Large Language Models (LLMs) inherit explicit and implicit biases from their training datasets. Identifying and mitigating biases in LLMs is crucial to ensure fair outputs, as they can perpetuate harmful stereotypes and misinformation. This study highlights the need to address biases in LLMs amid growing generative AI. We studied bias-specific benchmarks such as StereoSet and CrowSPairs to evaluate the existence of various biases in multiple generative models such as BERT and GPT 3.5. We proposed an automated Bias-Identification Framework to recognize various social biases in LLMs such as gender, race, profession, and religion. We adopted a two-pronged approach to detect explicit and implicit biases in text data. Results indicated fine-tuned models struggle with gender biases but excelled at identifying and avoiding racial biases. Our findings illustrated that despite having some success, LLMs often over-relied on keywords. To illuminate the capability of the analyzed LLMs in detecting implicit biases, we employed Bag-of-Words analysis and unveiled indications of implicit stereotyping within the vocabulary. To bolster the model performance, we applied an enhancement strategy involving fine-tuning models using prompting techniques and data augmentation of the bias benchmarks. The fine-tuned models exhibited promising adaptability during cross-dataset testing and significantly enhanced performance on implicit bias benchmarks, with performance gains of up to 20%.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14143v1" target="_blank"><h2>SMART: Shot-Aware Multimodal Video Moment Retrieval with Audio-Enhanced MLLM <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> An Yu, Weiheng Lu, Jian Li, Zhenfei Zhang, Yunhang Shen, Felix X. -F. Ye, Ming-Ching Chang<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Video Moment Retrieval is a task in video understanding that aims to localize a specific temporal segment in an untrimmed video based on a natural language query. Despite recent progress in moment retrieval from videos using both traditional techniques and Multimodal Large Language Models (MLLM), most existing methods still rely on coarse temporal understanding and a single visual modality, limiting performance on complex videos. To address this, we introduce \textit{S}hot-aware \textit{M}ultimodal \textit{A}udio-enhanced \textit{R}etrieval of \textit{T}emporal \textit{S}egments (SMART), an MLLM-based framework that integrates audio cues and leverages shot-level temporal structure. SMART enriches multimodal representations by combining audio and visual features while applying \textbf{Shot-aware Token Compression}, which selectively retains high-information tokens within each shot to reduce redundancy and preserve fine-grained temporal details. We also refine prompt design to better utilize audio-visual cues. Evaluations on Charades-STA and QVHighlights show that SMART achieves significant improvements over state-of-the-art methods, including a 1.61\% increase in R1@0.5 and 2.59\% gain in R1@0.7 on Charades-STA.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14131v1" target="_blank"><h2>Run, Ruminate, and Regulate: A Dual-process Thinking System for Vision-and-Language Navigation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yu Zhong, Zihao Zhang, Rui Zhang, Lingdong Huang, Haihan Gao, Shuo Wang, Da Li, Ruijian Han, Jiaming Guo, Shaohui Peng, Di Huang, Yunji Chen<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract), transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Vision-and-Language Navigation (VLN) requires an agent to dynamically explore complex 3D environments following human instructions. Recent research underscores the potential of harnessing large language models (LLMs) for VLN, given their commonsense knowledge and general reasoning capabilities. Despite their strengths, a substantial gap in task completion performance persists between LLM-based approaches and domain experts, as LLMs inherently struggle to comprehend real-world spatial correlations precisely. Additionally, introducing LLMs is accompanied with substantial computational cost and inference latency. To address these issues, we propose a novel dual-process thinking framework dubbed R3, integrating LLMs' generalization capabilities with VLN-specific expertise in a zero-shot manner. The framework comprises three core modules: Runner, Ruminator, and Regulator. The Runner is a lightweight transformer-based expert model that ensures efficient and accurate navigation under regular circumstances. The Ruminator employs a powerful multimodal LLM as the backbone and adopts chain-of-thought (CoT) prompting to elicit structured reasoning. The Regulator monitors the navigation progress and controls the appropriate thinking mode according to three criteria, integrating Runner and Ruminator harmoniously. Experimental results illustrate that R3 significantly outperforms other state-of-the-art methods, exceeding 3.28% and 3.30% in SPL and RGSPL respectively on the REVERIE benchmark. This pronounced enhancement highlights the effectiveness of our method in handling challenging VLN tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14119v1" target="_blank"><h2>Real-Time Mobile Video Analytics for Pre-arrival Emergency Medical Services <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Liuyi Jin, Amran Haroon, Radu Stoleru, Pasan Gunawardena, Michael Middleton, Jeeeun Kim<br><strong><u>Categories:</u></strong> cs.MM, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Timely and accurate pre-arrival video streaming and analytics are critical for emergency medical services (EMS) to deliver life-saving interventions. Yet, current-generation EMS infrastructure remains constrained by one-to-one video streaming and limited analytics capabilities, leaving dispatchers and EMTs to manually interpret overwhelming, often noisy or redundant information in high-stress environments. We present TeleEMS, a mobile live video analytics system that enables pre-arrival multimodal inference by fusing audio and video into a unified decision-making pipeline before EMTs arrive on scene.
  TeleEMS comprises two key components: TeleEMS Client and TeleEMS Server. The TeleEMS Client runs across phones, smart glasses, and desktops to support bystanders, EMTs en route, and 911 dispatchers. The TeleEMS Server, deployed at the edge, integrates EMS-Stream, a communication backbone that enables smooth multi-party video streaming. On top of EMSStream, the server hosts three real-time analytics modules: (1) audio-to-symptom analytics via EMSLlama, a domain-specialized LLM for robust symptom extraction and normalization; (2) video-to-vital analytics using state-of-the-art rPPG methods for heart rate estimation; and (3) joint text-vital analytics via PreNet, a multimodal multitask model predicting EMS protocols, medication types, medication quantities, and procedures.
  Evaluation shows that EMSLlama outperforms GPT-4o (exact-match 0.89 vs. 0.57) and that text-vital fusion improves inference robustness, enabling reliable pre-arrival intervention recommendations. TeleEMS demonstrates the potential of mobile live video analytics to transform EMS operations, bridging the gap between bystanders, dispatchers, and EMTs, and paving the way for next-generation intelligent EMS infrastructure.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14112v1" target="_blank"><h2>Synthetic Clinical Notes for Rare ICD Codes: A Data-Centric Framework for Long-Tail Medical Coding <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Truong Vo, Weiyi Wu, Kaize Ding<br><strong><u>Categories:</u></strong> cs.CL, cs.AI<br><strong><u>Comments:</u></strong> 4 page-short paper<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Automatic ICD coding from clinical text is a critical task in medical NLP but remains hindered by the extreme long-tail distribution of diagnostic codes. Thousands of rare and zero-shot ICD codes are severely underrepresented in datasets like MIMIC-III, leading to low macro-F1 scores. In this work, we propose a data-centric framework that generates high-quality synthetic discharge summaries to mitigate this imbalance. Our method constructs realistic multi-label code sets anchored on rare codes by leveraging real-world co-occurrence patterns, ICD descriptions, synonyms, taxonomy, and similar clinical notes. Using these structured prompts, we generate 90,000 synthetic notes covering 7,902 ICD codes, significantly expanding the training distribution. We fine-tune two state-of-the-art transformer-based models, PLM-ICD and GKI-ICD, on both the original and extended datasets. Experiments show that our approach modestly improves macro-F1 while maintaining strong micro-F1, outperforming prior SOTA. While the gain may seem marginal relative to the computational cost, our results demonstrate that carefully crafted synthetic data can enhance equity in long-tail ICD code prediction.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14111v1" target="_blank"><h2>CascadedViT: Cascaded Chunk-FeedForward and Cascaded Group Attention Vision Transformer <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Srivathsan Sivakumar, Faisal Z. Qureshi<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract), attention (title)<br><p><strong><u>Abstract:</u></strong> Vision Transformers (ViTs) have demonstrated remarkable performance across a range of computer vision tasks; however, their high computational, memory, and energy demands hinder deployment on resource-constrained platforms. In this paper, we propose \emph{Cascaded-ViT (CViT)}, a lightweight and compute-efficient vision transformer architecture featuring a novel feedforward network design called \emph{Cascaded-Chunk Feed Forward Network (CCFFN)}. By splitting input features, CCFFN improves parameter and FLOP efficiency without sacrificing accuracy. Experiments on ImageNet-1K show that our \emph{CViT-XL} model achieves 75.5\% Top-1 accuracy while reducing FLOPs by 15\% and energy consumption by 3.3\% compared to EfficientViT-M5. Across various model sizes, the CViT family consistently exhibits the lowest energy consumption, making it suitable for deployment on battery-constrained devices such as mobile phones and drones. Furthermore, when evaluated using a new metric called \emph{Accuracy-Per-FLOP (APF)}, which quantifies compute efficiency relative to accuracy, CViT models consistently achieve top-ranking efficiency. Particularly, CViT-L is 2.2\% more accurate than EfficientViT-M2 while having comparable APF scores.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14110v1" target="_blank"><h2>A Patient-Independent Neonatal Seizure Prediction Model Using Reduced Montage EEG and ECG <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Sithmini Ranasingha, Agasthi Haputhanthri, Hansa Marasinghe, Nima Wickramasinghe, Kithmin Wickremasinghe, Jithangi Wanigasinghe, Chamira U. S. Edussooriya, Joshua P. Kulasingham<br><strong><u>Categories:</u></strong> eess.SP, cs.LG<br><strong><u>Comments:</u></strong> 10 pages, 4 figures<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), explainable (abstract), neural network (abstract), transfer learning (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Neonates are highly susceptible to seizures, often leading to short or long-term neurological impairments. However, clinical manifestations of neonatal seizures are subtle and often lead to misdiagnoses. This increases the risk of prolonged, untreated seizure activity and subsequent brain injury. Continuous video electroencephalogram (cEEG) monitoring is the gold standard for seizure detection. However, this is an expensive evaluation that requires expertise and time. In this study, we propose a convolutional neural network-based model for early prediction of neonatal seizures by distinguishing between interictal and preictal states of the EEG. Our model is patient-independent, enabling generalization across multiple subjects, and utilizes mel-frequency cepstral coefficient matrices extracted from multichannel EEG and electrocardiogram (ECG) signals as input features. Trained and validated on the Helsinki neonatal EEG dataset with 10-fold cross-validation, the proposed model achieved an average accuracy of 97.52%, sensitivity of 98.31%, specificity of 96.39%, and F1-score of 97.95%, enabling accurate seizure prediction up to 30 minutes before onset. The inclusion of ECG alongside EEG improved the F1-score by 1.42%, while the incorporation of an attention mechanism yielded an additional 0.5% improvement. To enhance transparency, we incorporated SHapley Additive exPlanations (SHAP) as an explainable artificial intelligence method to interpret the model and provided localization of seizure focus using scalp plots. The overall results demonstrate the model's potential for minimally supervised deployment in neonatal intensive care units, enabling timely and reliable prediction of neonatal seizures, while demonstrating strong generalization capability across unseen subjects through transfer learning.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14099v1" target="_blank"><h2>FAPE-IR: Frequency-Aware Planning and Execution Framework for All-in-One Image Restoration <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jingren Liu, Shuning Xu, Qirui Yang, Yun Wang, Xiangyu Chen, Zhong Ji<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> All-in-One Image Restoration (AIO-IR) aims to develop a unified model that can handle multiple degradations under complex conditions. However, existing methods often rely on task-specific designs or latent routing strategies, making it hard to adapt to real-world scenarios with various degradations. We propose FAPE-IR, a Frequency-Aware Planning and Execution framework for image restoration. It uses a frozen Multimodal Large Language Model (MLLM) as a planner to analyze degraded images and generate concise, frequency-aware restoration plans. These plans guide a LoRA-based Mixture-of-Experts (LoRA-MoE) module within a diffusion-based executor, which dynamically selects high- or low-frequency experts, complemented by frequency features of the input image. To further improve restoration quality and reduce artifacts, we introduce adversarial training and a frequency regularization loss. By coupling semantic planning with frequency-based restoration, FAPE-IR offers a unified and interpretable solution for all-in-one image restoration. Extensive experiments show that FAPE-IR achieves state-of-the-art performance across seven restoration tasks and exhibits strong zero-shot generalization under mixed degradations.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14087v1" target="_blank"><h2>GCA-ResUNet:Image segmentation in medical images using grouped coordinate attention <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jun Ding, Shang Gao<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract), transformer (abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Medical image segmentation underpins computer-aided diagnosis and therapy by supporting clinical diagnosis, preoperative planning, and disease monitoring. While U-Net style convolutional neural networks perform well due to their encoder-decoder structures with skip connections, they struggle to capture long-range dependencies. Transformer-based variants address global context but often require heavy computation and large training datasets. This paper proposes GCA-ResUNet, an efficient segmentation network that integrates Grouped Coordinate Attention (GCA) into ResNet-50 residual blocks. GCA uses grouped coordinate modeling to jointly encode global dependencies across channels and spatial locations, strengthening feature representation and boundary delineation while adding minimal parameter and FLOP overhead compared with self-attention. On the Synapse dataset, GCA-ResUNet achieves a Dice score of 86.11%, and on the ACDC dataset, it reaches 92.64%, surpassing several state-of-the-art baselines while maintaining fast inference and favorable computational efficiency. These results indicate that GCA offers a practical way to enhance convolutional architectures with global modeling capability, enabling high-accuracy and resource-efficient medical image segmentation.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14082v1" target="_blank"><h2>Zero-Training Task-Specific Model Synthesis for Few-Shot Medical Image Classification <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yao Qin, Yangyang Yan, YuanChao Yang, Jinhua Pang, Huanyong Bi, Yuan Liu, HaiHua Wang<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> multi-modal (abstract)<br><p><strong><u>Abstract:</u></strong> Deep learning models have achieved remarkable success in medical image analysis but are fundamentally constrained by the requirement for large-scale, meticulously annotated datasets. This dependency on "big data" is a critical bottleneck in the medical domain, where patient data is inherently difficult to acquire and expert annotation is expensive, particularly for rare diseases where samples are scarce by definition. To overcome this fundamental challenge, we propose a novel paradigm: Zero-Training Task-Specific Model Synthesis (ZS-TMS). Instead of adapting a pre-existing model or training a new one, our approach leverages a large-scale, pre-trained generative engine to directly synthesize the entire set of parameters for a task-specific classifier. Our framework, the Semantic-Guided Parameter Synthesizer (SGPS), takes as input minimal, multi-modal task information as little as a single example image (1-shot) and a corresponding clinical text description to directly synthesize the entire set of parameters for a task-specific classifier.
  The generative engine interprets these inputs to generate the weights for a lightweight, efficient classifier (e.g., an EfficientNet-V2), which can be deployed for inference immediately without any task-specific training or fine-tuning. We conduct extensive evaluations on challenging few-shot classification benchmarks derived from the ISIC 2018 skin lesion dataset and a custom rare disease dataset. Our results demonstrate that SGPS establishes a new state-of-the-art, significantly outperforming advanced few-shot and zero-shot learning methods, especially in the ultra-low data regimes of 1-shot and 5-shot classification. This work paves the way for the rapid development and deployment of AI-powered diagnostic tools, particularly for the long tail of rare diseases where data is critically limited.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14076v1" target="_blank"><h2>Meta-SimGNN: Adaptive and Robust WiFi Localization Across Dynamic Configurations and Diverse Scenarios <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Qiqi Xiao, Ziqi Ye, Yinghui He, Jianwei Liu, Guanding Yu<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> To promote the practicality of deep learning-based localization, existing studies aim to address the issue of scenario dependence through meta-learning. However, these studies primarily focus on variations in environmental layouts while overlooking the impact of changes in device configurations, such as bandwidth, the number of access points (APs), and the number of antennas used. Unlike environmental changes, variations in device configurations affect the dimensionality of channel state information (CSI), thereby compromising neural network usability. To address this issue, we propose Meta-SimGNN, a novel WiFi localization system that integrates graph neural networks with meta-learning to improve localization generalization and robustness. First, we introduce a fine-grained CSI graph construction scheme, where each AP is treated as a graph node, allowing for adaptability to changes in the number of APs. To structure the features of each node, we propose an amplitude-phase fusion method and a feature extraction method. The former utilizes both amplitude and phase to construct CSI images, enhancing data reliability, while the latter extracts dimension-consistent features to address variations in bandwidth and the number of antennas. Second, a similarity-guided meta-learning strategy is developed to enhance adaptability in diverse scenarios. The initial model parameters for the fine-tuning stage are determined by comparing the similarity between the new scenario and historical scenarios, facilitating rapid adaptation of the model to the new localization scenario. Extensive experimental results over commodity WiFi devices in different scenarios show that Meta-SimGNN outperforms the baseline methods in terms of localization generalization and accuracy.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14064v1" target="_blank"><h2>CafeMed: Causal Attention Fusion Enhanced Medication Recommendation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Kelin Ren, Chan-Yang Ju, Dong-Ho Lee<br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ME<br><strong><u>Comments:</u></strong> Accepted by BIBM 2025<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Medication recommendation systems play a crucial role in assisting clinicians with personalized treatment decisions. While existing approaches have made significant progress in learning medication representations, they suffer from two fundamental limitations: (i) treating medical entities as independent features without modeling their synergistic effects on medication selection; (ii) employing static causal relationships that fail to adapt to patient-specific contexts and health states. To address these challenges, we propose CafeMed, a framework that integrates dynamic causal reasoning with cross-modal attention for safe and accurate medication recommendation. CafeMed introduces two key components: the Causal Weight Generator (CWG) that transforms static causal effects into dynamic modulation weights based on individual patient states, and the Channel Harmonized Attention Refinement Module (CHARM) that captures complex interdependencies between diagnoses and procedures. This design enables CafeMed to model how different medical conditions jointly influence treatment decisions while maintaining medication safety constraints. Extensive experiments on MIMIC-III and MIMIC-IV datasets demonstrate that CafeMed significantly outperforms state-of-the-art baselines, achieving superior accuracy in medication prediction while maintaining the lower drug--drug interaction rates. Our results indicate that incorporating dynamic causal relationships and cross-modal synergies leads to more clinically-aligned and personalized medication recommendations. Our code is released publicly at https://github.com/rkl71/CafeMed.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14062v1" target="_blank"><h2>LogPurge: Log Data Purification for Anomaly Detection via Rule-Enhanced Filtering <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Shenglin Zhang, Ziang Chen, Zijing Que, Yilun Liu, Yongqian Sun, Sicheng Wei, Dan Pei, Hailin Li<br><strong><u>Categories:</u></strong> cs.SE, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> anomaly detection (title, abstract)<br><p><strong><u>Abstract:</u></strong> Log anomaly detection, which is critical for identifying system failures and preempting security breaches, detects irregular patterns within large volumes of log data, and impacts domains such as service reliability, performance optimization, and database log analysis. Modern log anomaly detection methods rely on training deep learning models on clean, anomaly-free log sequences. However, obtaining such clean log data requires costly and tedious human labeling, and existing automatic cleaning methods fail to fully integrate the specific characteristics and actual semantics of logs in their purification process. In this paper, we propose a cost-aware, rule-enhanced purification framework, LogPurge, that automatically selects a sufficient subset of normal log sequences from contamination log sequences to train a anomaly detection model. Our approach involves a two-stage filtering algorithm: In the first stage, we use a large language model (LLM) to remove clustered anomalous patterns and enhance system rules to improve LLM's understanding of system logs; in the second stage, we utilize a divide-and-conquer strategy that decomposes the remaining contaminated regions into smaller subproblems, allowing each to be effectively purified through the first stage procedure. Our experiments, conducted on two public datasets and one industrial dataset, show that our method significantly removes an average of 98.74% of anomalies while retaining 82.39% of normal samples. Compared to the latest unsupervised log sample selection algorithms, our method achieves F-1 score improvements of 35.7% and 84.11% on the public datasets, and an impressive 149.72% F-1 improvement on the private dataset, demonstrating the effectiveness of our approach.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14057v1" target="_blank"><h2>A Machine Learning-Based Multimodal Framework for Wearable Sensor-Based Archery Action Recognition and Stress Estimation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Xianghe Liu, Jiajia Liu, Chuxian Xu, Minghan Wang, Hongbo Peng, Tao Sun, Jiaqi Xu<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> In precision sports such as archery, athletes' performance depends on both biomechanical stability and psychological resilience. Traditional motion analysis systems are often expensive and intrusive, limiting their use in natural training environments. To address this limitation, we propose a machine learning-based multimodal framework that integrates wearable sensor data for simultaneous action recognition and stress estimation. Using a self-developed wrist-worn device equipped with an accelerometer and photoplethysmography (PPG) sensor, we collected synchronized motion and physiological data during real archery sessions. For motion recognition, we introduce a novel feature--Smoothed Differential Acceleration (SmoothDiff)--and employ a Long Short-Term Memory (LSTM) model to identify motion phases, achieving 96.8% accuracy and 95.9% F1-score. For stress estimation, we extract heart rate variability (HRV) features from PPG signals and apply a Multi-Layer Perceptron (MLP) classifier, achieving 80% accuracy in distinguishing high- and low-stress levels. The proposed framework demonstrates that integrating motion and physiological sensing can provide meaningful insights into athletes' technical and mental states. This approach offers a foundation for developing intelligent, real-time feedback systems for training optimization in archery and other precision sports.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14056v1" target="_blank"><h2>Radial Compensation: Stable and Semantically Decoupled Generative Models on Riemannian Manifolds <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Marios Papamichals, Regina Ruane<br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.IT, math.DG, stat.ML<br><strong><u>Comments:</u></strong> This is the first version of the paper<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> Generative models on curved spaces rely on charts to map Euclidean spaces to manifolds. Exponential maps preserve geodesics but have stiff, radius-dependent Jacobians, while volume-preserving charts maintain densities but distort geodesic distances. Both approaches entangle curvature with model parameters, inflating gradient variance. In high-dimensional latent normalizing flows, the wrapped exponential prior can stretch radii far beyond the curvature scale, leading to poor test likelihoods and stiff solvers. We introduce Radial Compensation (RC), an information-geometric method that selects the base density in the tangent space so that the likelihood depends only on geodesic distance from a pole, decoupling parameter semantics from curvature. RC lets radial parameters retain their usual meaning in geodesic units, while the chart can be tuned as a numerical preconditioner. We extend RC to manifolds with known geodesic polar volume and show that RC is the only construction for geodesic-radial likelihoods with curvature-invariant Fisher information. We derive the Balanced-Exponential (bExp) chart family, balancing volume distortion and geodesic error. Under RC, all bExp settings preserve the same manifold density and Fisher information, with smaller dial values reducing gradient variance and flow cost. Empirically, RC yields stable generative models across densities, VAEs, flows on images and graphs, and protein models. RC improves likelihoods, restores clean geodesic radii, and prevents radius blow-ups in high-dimensional flows, making RC-bExp a robust default for likelihood-trained generative models on manifolds.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14052v1" target="_blank"><h2>Making Evidence Actionable in Adaptive Learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Amirreza Mehrabi, Jason W. Morphew, Breejha Quezada, N. Sanjay Rebello<br><strong><u>Categories:</u></strong> cs.AI, cs.CE, stat.AP, stat.OT<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Adaptive learning often diagnoses precisely yet intervenes weakly, yielding help that is mistimed or misaligned. This study presents evidence supporting an instructor-governed feedback loop that converts concept-level assessment evidence into vetted micro-interventions. The adaptive learning algorithm contains three safeguards: adequacy as a hard guarantee of gap closure, attention as a budgeted constraint for time and redundancy, and diversity as protection against overfitting to a single resource. We formalize intervention assignment as a binary integer program with constraints for coverage, time, difficulty windows informed by ability estimates, prerequisites encoded by a concept matrix, and anti-redundancy enforced through diversity. Greedy selection serves low-richness and tight-latency regimes, gradient-based relaxation serves rich repositories, and a hybrid method transitions along a richness-latency frontier. In simulation and in an introductory physics deployment with one thousand two hundred four students, both solvers achieved full skill coverage for essentially all learners within bounded watch time. The gradient-based method reduced redundant coverage by approximately twelve percentage points relative to greedy and harmonized difficulty across slates, while greedy delivered comparable adequacy with lower computational cost in scarce settings. Slack variables localized missing content and supported targeted curation, sustaining sufficiency across subgroups. The result is a tractable and auditable controller that closes the diagnostic-pedagogical loop and delivers equitable, load-aware personalization at classroom scale.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14049v1" target="_blank"><h2>SmallML: Bayesian Transfer Learning for Small-Data Predictive Analytics <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Semen Leontev<br><strong><u>Categories:</u></strong> cs.LG, stat.ML<br><strong><u>Comments:</u></strong> 64 pages, 5 figures, 15 tables<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transfer learning (title, abstract)<br><p><strong><u>Abstract:</u></strong> Small and medium-sized enterprises (SMEs) represent 99.9% of U.S. businesses yet remain systematically excluded from AI due to a mismatch between their operational scale and modern machine learning's data requirements. This paper introduces SmallML, a Bayesian transfer learning framework achieving enterprise-level prediction accuracy with datasets as small as 50-200 observations.
  We develop a three-layer architecture integrating transfer learning, hierarchical Bayesian modeling, and conformal prediction. Layer 1 extracts informative priors from 22,673 public records using a SHAP-based procedure transferring knowledge from gradient boosting to logistic regression. Layer 2 implements hierarchical pooling across J=5-50 SMEs with adaptive shrinkage, balancing population patterns with entity-specific characteristics. Layer 3 provides conformal sets with finite-sample coverage guarantees P(y in C(x)) >= 1-alpha for distribution-free uncertainty quantification.
  Validation on customer churn data demonstrates 96.7% +/- 4.2% AUC with 100 observations per business -- a +24.2 point improvement over independent logistic regression (72.5% +/- 8.1%), with p < 0.000001. Conformal prediction achieves 92% empirical coverage at 90% target. Training completes in 33 minutes on standard CPU hardware. By enabling enterprise-grade predictions for 33 million U.S. SMEs previously excluded from machine learning, SmallML addresses a critical gap in AI democratization.
  Keywords: Bayesian transfer learning, hierarchical models, conformal prediction, small-data analytics, SME machine learning</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15732v1" target="_blank"><h2>Just Asking Questions: Doing Our Own Research on Conspiratorial Ideation by Generative AI Chatbots <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Katherine M. FitzGerald, Michelle Riedlinger, Axel Bruns, Stephen Harrington, Timothy Graham, Daniel Angus<br><strong><u>Categories:</u></strong> cs.CY, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Interactive chat systems that build on artificial intelligence frameworks are increasingly ubiquitous and embedded into search engines, Web browsers, and operating systems, or are available on websites and apps. Researcher efforts have sought to understand the limitations and potential for harm of generative AI, which we contribute to here. Conducting a systematic review of six AI-powered chat systems (ChatGPT 3.5; ChatGPT 4 Mini; Microsoft Copilot in Bing; Google Search AI; Perplexity; and Grok in Twitter/X), this study examines how these leading products respond to questions related to conspiracy theories. This follows the platform policy implementation audit approach established by Glazunova et al. (2023). We select five well-known and comprehensively debunked conspiracy theories and four emerging conspiracy theories that relate to breaking news events at the time of data collection. Our findings demonstrate that the extent of safety guardrails against conspiratorial ideation in generative AI chatbots differs markedly, depending on chatbot model and conspiracy theory. Our observations indicate that safety guardrails in AI chatbots are often very selectively designed: generative AI companies appear to focus especially on ensuring that their products are not seen to be racist; they also appear to pay particular attention to conspiracy theories that address topics of substantial national trauma such as 9/11 or relate to well-established political issues. Future work should include an ongoing effort extended to further platforms, multiple languages, and a range of conspiracy theories extending well beyond the United States.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14044v1" target="_blank"><h2>The CHASM-SWPC Dataset for Coronal Hole Detection & Analysis <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Cutter Beck, Evan Smith, Khagendra Katuwal, Rudra Kafle, Jacob Whitehill<br><strong><u>Categories:</u></strong> astro-ph.IM, cs.CV, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Coronal holes (CHs) are low-activity, low-density solar coronal regions with open magnetic field lines (Cranmer 2009). In the extreme ultraviolet (EUV) spectrum, CHs appear as dark patches. Using daily hand-drawn maps from the Space Weather Prediction Center (SWPC), we developed a semi-automated pipeline to digitize the SWPC maps into binary segmentation masks. The resulting masks constitute the CHASM-SWPC dataset, a high-quality dataset to train and test automated CH detection models, which is released with this paper. We developed CHASM (Coronal Hole Annotation using Semi-automatic Methods), a software tool for semi-automatic annotation that enables users to rapidly and accurately annotate SWPC maps. The CHASM tool enabled us to annotate 1,111 CH masks, comprising the CHASM-SWPC-1111 dataset. We then trained multiple CHRONNOS (Coronal Hole RecOgnition Neural Network Over multi-Spectral-data) architecture (Jarolim et al. 2021) neural networks using the CHASM-SWPC dataset and compared their performance. Training the CHRONNOS neural network on these data achieved an accuracy of 0.9805, a True Skill Statistic (TSS) of 0.6807, and an intersection-over-union (IoU) of 0.5668, which is higher than the original pretrained CHRONNOS model Jarolim et al. (2021) achieved an accuracy of 0.9708, a TSS of 0.6749, and an IoU of 0.4805, when evaluated on the CHASM-SWPC-1111 test set.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14023v1" target="_blank"><h2>Syn-STARTS: Synthesized START Triage Scenario Generation Framework for Scalable LLM Evaluation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Chiharu Hagiwara, Naoki Nonaka, Yuhta Hashimoto, Ryu Uchimido, Jun Seita<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> Introducing an open dataset<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Triage is a critically important decision-making process in mass casualty incidents (MCIs) to maximize victim survival rates. While the role of AI in such situations is gaining attention for making optimal decisions within limited resources and time, its development and performance evaluation require benchmark datasets of sufficient quantity and quality. However, MCIs occur infrequently, and sufficient records are difficult to accumulate at the scene, making it challenging to collect large-scale realworld data for research use. Therefore, we developed Syn-STARTS, a framework that uses LLMs to generate triage cases, and verified its effectiveness. The results showed that the triage cases generated by Syn-STARTS were qualitatively indistinguishable from the TRIAGE open dataset generated by manual curation from training materials. Furthermore, when evaluating the LLM accuracy using hundreds of cases each from the green, yellow, red, and black categories defined by the standard triage method START, the results were found to be highly stable. This strongly indicates the possibility of synthetic data in developing high-performance AI models for severe and critical medical situations.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13987v1" target="_blank"><h2>Artificial Intelligence Agents in Music Analysis: An Integrative Perspective Based on Two Use Cases <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Antonio Manuel Martínez-Heredia, Dolores Godrid Rodríguez, Andrés Ortiz García<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> Extended version of the conference paper presented at SATMUS 2025<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> explainable (abstract)<br><p><strong><u>Abstract:</u></strong> This paper presents an integrative review and experimental validation of artificial intelligence (AI) agents applied to music analysis and education. We synthesize the historical evolution from rule-based models to contemporary approaches involving deep learning, multi-agent architectures, and retrieval-augmented generation (RAG) frameworks. The pedagogical implications are evaluated through a dual-case methodology: (1) the use of generative AI platforms in secondary education to foster analytical and creative skills; (2) the design of a multiagent system for symbolic music analysis, enabling modular, scalable, and explainable workflows.
  Experimental results demonstrate that AI agents effectively enhance musical pattern recognition, compositional parameterization, and educational feedback, outperforming traditional automated methods in terms of interpretability and adaptability. The findings highlight key challenges concerning transparency, cultural bias, and the definition of hybrid evaluation metrics, emphasizing the need for responsible deployment of AI in educational environments.
  This research contributes to a unified framework that bridges technical, pedagogical, and ethical considerations, offering evidence-based guidance for the design and application of intelligent agents in computational musicology and music education.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13981v1" target="_blank"><h2>Data Whitening Improves Sparse Autoencoder Learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Ashwin Saraswatula, David Klindt<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> Accepted to the AAAI 2026 XAI4Science Workshop<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Sparse autoencoders (SAEs) have emerged as a promising approach for learning interpretable features from neural network activations. However, the optimization landscape for SAE training can be challenging due to correlations in the input data. We demonstrate that applying PCA Whitening to input activations -- a standard preprocessing technique in classical sparse coding -- improves SAE performance across multiple metrics. Through theoretical analysis and simulation, we show that whitening transforms the optimization landscape, making it more convex and easier to navigate. We evaluate both ReLU and Top-K SAEs across diverse model architectures, widths, and sparsity regimes. Empirical evaluation on SAEBench, a comprehensive benchmark for sparse autoencoders, reveals that whitening consistently improves interpretability metrics, including sparse probing accuracy and feature disentanglement, despite minor drops in reconstruction quality. Our results challenge the assumption that interpretability aligns with an optimal sparsity--fidelity trade-off and suggest that whitening should be considered as a default preprocessing step for SAE training, particularly when interpretability is prioritized over perfect reconstruction.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13977v1" target="_blank"><h2>Efficient reconstruction of multidimensional random field models with heterogeneous data using stochastic neural networks <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Mingtao Xia, Qijing Shen<br><strong><u>Categories:</u></strong> cs.LG, math.NA, math.PR<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> In this paper, we analyze the scalability of a recent Wasserstein-distance approach for training stochastic neural networks (SNNs) to reconstruct multidimensional random field models. We prove a generalization error bound for reconstructing multidimensional random field models on training stochastic neural networks with a limited number of training data. Our results indicate that when noise is heterogeneous across dimensions, the convergence rate of the generalization error may not depend explicitly on the model's dimensionality, partially alleviating the "curse of dimensionality" for learning multidimensional random field models from a finite number of data points. Additionally, we improve the previous Wasserstein-distance SNN training approach and showcase the robustness of the SNN. Through numerical experiments on different multidimensional uncertainty quantification tasks, we show that our Wasserstein-distance approach can successfully train stochastic neural networks to learn multidimensional uncertainty models.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13954v1" target="_blank"><h2>A Brain Wave Encodes a Thousand Tokens: Modeling Inter-Cortical Neural Interactions for Effective EEG-based Emotion Recognition <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Nilay Kumar, Priyansh Bhandari, G. Maragatham<br><strong><u>Categories:</u></strong> q-bio.NC, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> latent space (abstract), neural network (abstract), transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Human emotions are difficult to convey through words and are often abstracted in the process; however, electroencephalogram (EEG) signals can offer a more direct lens into emotional brain activity. Recent studies show that deep learning models can process these signals to perform emotion recognition with high accuracy. However, many existing approaches overlook the dynamic interplay between distinct brain regions, which can be crucial to understanding how emotions unfold and evolve over time, potentially aiding in more accurate emotion recognition. To address this, we propose RBTransformer, a Transformer-based neural network architecture that models inter-cortical neural dynamics of the brain in latent space to better capture structured neural interactions for effective EEG-based emotion recognition. First, the EEG signals are converted into Band Differential Entropy (BDE) tokens, which are then passed through Electrode Identity embeddings to retain spatial provenance. These tokens are processed through successive inter-cortical multi-head attention blocks that construct an electrode x electrode attention matrix, allowing the model to learn the inter-cortical neural dependencies. The resulting features are then passed through a classification head to obtain the final prediction. We conducted extensive experiments, specifically under subject-dependent settings, on the SEED, DEAP, and DREAMER datasets, over all three dimensions, Valence, Arousal, and Dominance (for DEAP and DREAMER), under both binary and multi-class classification settings. The results demonstrate that the proposed RBTransformer outperforms all previous state-of-the-art methods across all three datasets, over all three dimensions under both classification settings. The source code is available at: https://github.com/nnilayy/RBTransformer.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13937v1" target="_blank"><h2>Complex-Weighted Convolutional Networks: Provable Expressiveness via Complex Diffusion <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Cristina López Amado, Tassilo Schwarz, Yu Tian, Renaud Lambiotte<br><strong><u>Categories:</u></strong> cs.LG, cs.SI, math.DS, physics.soc-ph<br><strong><u>Comments:</u></strong> 19 pages, 6 figures. Learning on Graphs Conference 2025<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> convolutional (title, abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Graph Neural Networks (GNNs) have achieved remarkable success across diverse applications, yet they remain limited by oversmoothing and poor performance on heterophilic graphs. To address these challenges, we introduce a novel framework that equips graphs with a complex-weighted structure, assigning each edge a complex number to drive a diffusion process that extends random walks into the complex domain. We prove that this diffusion is highly expressive: with appropriately chosen complex weights, any node-classification task can be solved in the steady state of a complex random walk. Building on this insight, we propose the Complex-Weighted Convolutional Network (CWCN), which learns suitable complex-weighted structures directly from data while enriching diffusion with learnable matrices and nonlinear activations. CWCN is simple to implement, requires no additional hyperparameters beyond those of standard GNNs, and achieves competitive performance on benchmark datasets. Our results demonstrate that complex-weighted diffusion provides a principled and general mechanism for enhancing GNN expressiveness, opening new avenues for models that are both theoretically grounded and practically effective.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13935v2" target="_blank"><h2>Weather Maps as Tokens: Transformers for Renewable Energy Forecasting <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Federico Battini<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract), transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> Accurate renewable energy forecasting is essential to reduce dependence on fossil fuels and enabling grid decarbonization. However, current approaches fail to effectively integrate the rich spatial context of weather patterns with their temporal evolution. This work introduces a novel approach that treats weather maps as tokens in transformer sequences to predict renewable energy. Hourly weather maps are encoded as spatial tokens using a lightweight convolutional neural network, and then processed by a transformer to capture temporal dynamics across a 45-hour forecast horizon. Despite disadvantages in input initialization, evaluation against ENTSO-E operational forecasts shows a reduction in RMSE of about 60% and 20% for wind and solar respectively. A live dashboard showing daily forecasts is available at: https://www.sardiniaforecast.ifabfoundation.it.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13912v1" target="_blank"><h2>Compute-in-Memory Implementation of State Space Models for Event Sequence Processing <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Xiaoyu Zhang, Mingtao Hu, Sen Lu, Soohyeon Kim, Eric Yeu-Jer Lee, Yuyang Liu, Wei D. Lu<br><strong><u>Categories:</u></strong> eess.SP, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> Xiaoyu Zhang and Mingtao Hu contributed equally to this work<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract)<br><p><strong><u>Abstract:</u></strong> State space models (SSMs) have recently emerged as a powerful framework for long sequence processing, outperforming traditional methods on diverse benchmarks. Fundamentally, SSMs can generalize both recurrent and convolutional networks and have been shown to even capture key functions of biological systems. Here we report an approach to implement SSMs in energy-efficient compute-in-memory (CIM) hardware to achieve real-time, event-driven processing. Our work re-parameterizes the model to function with real-valued coefficients and shared decay constants, reducing the complexity of model mapping onto practical hardware systems. By leveraging device dynamics and diagonalized state transition parameters, the state evolution can be natively implemented in crossbar-based CIM systems combined with memristors exhibiting short-term memory effects. Through this algorithm and hardware co-design, we show the proposed system offers both high accuracy and high energy efficiency while supporting fully asynchronous processing for event-based vision and audio tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13899v1" target="_blank"><h2>A Disentangled Low-Rank RNN Framework for Uncovering Neural Connectivity and Dynamics <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Chengrui Li, Yunmiao Wang, Yule Wang, Weihan Li, Dieter Jaeger, Anqi Wu<br><strong><u>Categories:</u></strong> q-bio.NC, cs.CE, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> variational autoencoder (abstract), VAE (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Low-rank recurrent neural networks (lrRNNs) are a class of models that uncover low-dimensional latent dynamics underlying neural population activity. Although their functional connectivity is low-rank, it lacks disentanglement interpretations, making it difficult to assign distinct computational roles to different latent dimensions. To address this, we propose the Disentangled Recurrent Neural Network (DisRNN), a generative lrRNN framework that assumes group-wise independence among latent dynamics while allowing flexible within-group entanglement. These independent latent groups allow latent dynamics to evolve separately, but are internally rich for complex computation. We reformulate the lrRNN under a variational autoencoder (VAE) framework, enabling us to introduce a partial correlation penalty that encourages disentanglement between groups of latent dimensions. Experiments on synthetic, monkey M1, and mouse voltage imaging data show that DisRNN consistently improves the disentanglement and interpretability of learned neural latent trajectories in low-dimensional space and low-rank connectivity over baseline lrRNNs that do not encourage partial disentanglement.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13893v1" target="_blank"><h2>Beyond One-Size-Fits-All: Neural Networks for Differentially Private Tabular Data Synthesis <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Kai Chen, Chen Gong, Tianhao Wang<br><strong><u>Categories:</u></strong> cs.LG, cs.CR<br><strong><u>Comments:</u></strong> 18 pages. Github Link provided:this https URL<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> In differentially private (DP) tabular data synthesis, the consensus is that statistical models are better than neural network (NN)-based methods. However, we argue that this conclusion is incomplete and overlooks the challenge of densely correlated datasets, where intricate dependencies can overwhelm statistical models. In such complex scenarios, neural networks are more suitable due to their capacity to fit complex distributions by learning directly from samples. Despite this potential, existing NN-based algorithms still suffer from significant limitations. We therefore propose MargNet, incorporating successful algorithmic designs of statistical models into neural networks. MargNet applies an adaptive marginal selection strategy and trains the neural networks to generate data that conforms to the selected marginals. On sparsely correlated datasets, our approach achieves utility close to the best statistical method while offering an average 7$\times$ speedup over it. More importantly, on densely correlated datasets, MargNet establishes a new state-of-the-art, reducing fidelity error by up to 26\% compared to the previous best. We release our code on GitHub.\footnote{https://github.com/KaiChen9909/margnet}</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13892v1" target="_blank"><h2>Jailbreaking Large Vision Language Models in Intelligent Transportation Systems <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Badhan Chandra Das, Md Tasnim Jawad, Md Jueal Mia, M. Hadi Amini, Yanzhao Wu<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Large Vision Language Models (LVLMs) demonstrate strong capabilities in multimodal reasoning and many real-world applications, such as visual question answering. However, LVLMs are highly vulnerable to jailbreaking attacks. This paper systematically analyzes the vulnerabilities of LVLMs integrated in Intelligent Transportation Systems (ITS) under carefully crafted jailbreaking attacks. First, we carefully construct a dataset with harmful queries relevant to transportation, following OpenAI's prohibited categories to which the LVLMs should not respond. Second, we introduce a novel jailbreaking attack that exploits the vulnerabilities of LVLMs through image typography manipulation and multi-turn prompting. Third, we propose a multi-layered response filtering defense technique to prevent the model from generating inappropriate responses. We perform extensive experiments with the proposed attack and defense on the state-of-the-art LVLMs (both open-source and closed-source). To evaluate the attack method and defense technique, we use GPT-4's judgment to determine the toxicity score of the generated responses, as well as manual verification. Further, we compare our proposed jailbreaking method with existing jailbreaking techniques and highlight severe security risks involved with jailbreaking attacks with image typography manipulation and multi-turn prompting in the LVLMs integrated in ITS.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13889v2" target="_blank"><h2>Uni-Hema: Unified Model for Digital Hematopathology <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Abdul Rehman, Iqra Rasool, Ayisha Imran, Mohsen Ali, Waqas Sultani<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract), multi-modal (abstract)<br><p><strong><u>Abstract:</u></strong> Digital hematopathology requires cell-level analysis across diverse disease categories, including malignant disorders (e.g., leukemia), infectious conditions (e.g., malaria), and non-malignant red blood cell disorders (e.g., sickle cell disease). Whether single-task, vision-language, WSI-optimized, or single-cell hematology models, these approaches share a key limitation, they cannot provide unified, multi-task, multi-modal reasoning across the complexities of digital hematopathology. To overcome these limitations, we propose Uni-Hema, a multi-task, unified model for digital hematopathology integrating detection, classification, segmentation, morphology prediction, and reasoning across multiple diseases. Uni-Hema leverages 46 publicly available datasets, encompassing over 700K images and 21K question-answer pairs, and is built upon Hema-Former, a multimodal module that bridges visual and textual representations at the hierarchy level for the different tasks (detection, classification, segmentation, morphology, mask language modeling and visual question answer) at different granularity. Extensive experiments demonstrate that Uni-Hema achieves comparable or superior performance to train on a single-task and single dataset models, across diverse hematological tasks, while providing interpretable, morphologically relevant insights at the single-cell level. Our framework establishes a new standard for multi-task and multi-modal digital hematopathology. The code will be made publicly available.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13877v1" target="_blank"><h2>Hybrid Convolution Neural Network Integrated with Pseudo-Newton Boosting for Lumbar Spine Degeneration Detection <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Pandiyaraju V, Abishek Karthik, Jaspin K, Kannan A, Jaime Lloret<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (title), transfer learning (abstract)<br><p><strong><u>Abstract:</u></strong> This paper proposes a new enhanced model architecture to perform classification of lumbar spine degeneration with DICOM images while using a hybrid approach, integrating EfficientNet and VGG19 together with custom-designed components. The proposed model is differentiated from traditional transfer learning methods as it incorporates a Pseudo-Newton Boosting layer along with a Sparsity-Induced Feature Reduction Layer that forms a multi-tiered framework, further improving feature selection and representation. The Pseudo-Newton Boosting layer makes smart variations of feature weights, with more detailed anatomical features, which are mostly left out in a transfer learning setup. In addition, the Sparsity-Induced Layer removes redundancy for learned features, producing lean yet robust representations for pathology in the lumbar spine. This architecture is novel as it overcomes the constraints in the traditional transfer learning approach, especially in the high-dimensional context of medical images, and achieves a significant performance boost, reaching a precision of 0.9, recall of 0.861, F1 score of 0.88, loss of 0.18, and an accuracy of 88.1%, compared to the baseline model, EfficientNet. This work will present the architectures, preprocessing pipeline, and experimental results. The results contribute to the development of automated diagnostic tools for medical images.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13869v2" target="_blank"><h2>H-CNN-ViT: A Hierarchical Gated Attention Multi-Branch Model for Bladder Cancer Recurrence Prediction <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Xueyang Li, Zongren Wang, Yuliang Zhang, Zixuan Pan, Yu-Jen Chen, Nishchal Sapkota, Gelei Xu, Danny Z. Chen, Yiyu Shi<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multi-modal (abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Bladder cancer is one of the most prevalent malignancies worldwide, with a recurrence rate of up to 78%, necessitating accurate post-operative monitoring for effective patient management. Multi-sequence contrast-enhanced MRI is commonly used for recurrence detection; however, interpreting these scans remains challenging, even for experienced radiologists, due to post-surgical alterations such as scarring, swelling, and tissue remodeling. AI-assisted diagnostic tools have shown promise in improving bladder cancer recurrence prediction, yet progress in this field is hindered by the lack of dedicated multi-sequence MRI datasets for recurrence assessment study. In this work, we first introduce a curated multi-sequence, multi-modal MRI dataset specifically designed for bladder cancer recurrence prediction, establishing a valuable benchmark for future research. We then propose H-CNN-ViT, a new Hierarchical Gated Attention Multi-Branch model that enables selective weighting of features from the global (ViT) and local (CNN) paths based on contextual demands, achieving a balanced and targeted feature fusion. Our multi-branch architecture processes each modality independently, ensuring that the unique properties of each imaging channel are optimally captured and integrated. Evaluated on our dataset, H-CNN-ViT achieves an AUC of 78.6%, surpassing state-of-the-art models. Our model is publicly available at https://github.com/XLIAaron/H-CNN-ViT.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14808v1" target="_blank"><h2>Transformer Injectivity & Geometric Robustness - Analytic Margins and Bi-Lipschitz Uniformity of Sequence-Level Hidden States <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Mikael von Strauss<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 22 pages, 5 figures<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> Under real-analytic assumptions on decoder-only Transformers, recent work shows that the map from discrete prompts to last-token hidden states is generically injective on finite prompt sets. We refine this picture: for each layer $\ell$ we define a collision discriminant $Δ^\ell \subset Θ$ and injective stratum $U^\ell = Θ\setminus Δ^\ell$, and prove a dichotomy -- either the model is nowhere injective on the set, or $U^\ell$ is open and dense and every $F^\ell_θ$ is injective. Under mild non-singularity assumptions on the optimizer and an absolutely continuous initialization, generic injectivity persists along smooth training trajectories over any fixed horizon. We also treat symmetry groups $G$, showing that discriminants and injective strata descend to the quotient $Θ/G$, so injectivity is naturally a property of functional equivalence classes.
  We complement these results with an empirical study of layerwise geometric diagnostics. We define a separation margin and a co-Lipschitz (lower Lipschitz) constant between prompt space and last-token representation space, estimated via nearest-neighbor statistics on large prompt sets. Applying these diagnostics to pretrained LLaMA-3 and Qwen models, we study behavior across layers, sequence lengths, model scales, and 8- and 4-bit activation quantization. On our sampled prompts we see no collisions in full precision or at 8 bits, while 4-bit quantization induces a small number of collisions and markedly shrinks co-Lipschitz estimates. For a small GPT-2 trained from scratch, normalized metrics remain stable over training. Overall, the results suggest that Transformer representations are generically and persistently injective in the continuous-parameter idealization, while their practical invertibility can be probed using simple geometric diagnostics.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14806v1" target="_blank"><h2>MergeDNA: Context-aware Genome Modeling with Dynamic Tokenization through Token Merging <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Siyuan Li, Kai Yu, Anna Wang, Zicheng Liu, Chang Yu, Jingbo Zhou, Qirong Yang, Yucheng Guo, Xiaoming Zhang, Stan Z. Li<br><strong><u>Categories:</u></strong> q-bio.GN, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> AAAI 2026 (Oral Presentation) Preprint<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Modeling genomic sequences faces two unsolved challenges: the information density varies widely across different regions, while there is no clearly defined minimum vocabulary unit. Relying on either four primitive bases or independently designed DNA tokenizers, existing approaches with naive masked language modeling pre-training often fail to adapt to the varying complexities of genomic sequences. Leveraging Token Merging techniques, this paper introduces a hierarchical architecture that jointly optimizes a dynamic genomic tokenizer and latent Transformers with context-aware pre-training tasks. As for network structures, the tokenization module automatically chunks adjacent bases into words by stacking multiple layers of the differentiable token merging blocks with local-window constraints, then a Latent Encoder captures the global context of these merged words by full-attention blocks. Symmetrically employing a Latent Decoder and a Local Decoder, MergeDNA learns with two pre-training tasks: Merged Token Reconstruction simultaneously trains the dynamic tokenization module and adaptively filters important tokens, while Adaptive Masked Token Modeling learns to predict these filtered tokens to capture informative contents. Extensive experiments show that MergeDNA achieves superior performance on three popular DNA benchmarks and several multi-omics tasks with fine-tuning or zero-shot evaluation, outperforming typical tokenization methods and large-scale DNA foundation models.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13860v1" target="_blank"><h2>Randomized Controlled Trials for Phishing Triage Agent <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> James Bono<br><strong><u>Categories:</u></strong> econ.GN, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Security operations centers (SOCs) face a persistent challenge: efficiently triaging a high volume of user-reported phishing emails while maintaining robust protection against threats. This paper presents the first randomized controlled trial (RCT) evaluating the impact of a domain-specific AI agent - the Microsoft Security Copilot Phishing Triage Agent - on analyst productivity and accuracy. Our results demonstrate that agent-augmented analysts achieved up to 6.5 times as many true positives per analyst minute and a 77% improvement in verdict accuracy compared to a control group. The agent's queue prioritization and verdict explanations were both significant drivers of efficiency. Behavioral analysis revealed that agent-augmented analysts reallocated their attention, spending 53% more time on malicious emails, and were not prone to rubber-stamping the agent's malicious verdicts. These findings offer actionable insights for SOC leaders considering AI adoption, including the potential for agents to fundamentally change the optimal allocation of SOC resources.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13840v1" target="_blank"><h2>Inferring Planet and Disk Parameters from Protoplanetary Disk Images Using a Variational Autoencoder <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Sayed Shafaat Mahmud, Sayantan Auddy, Neal Turner, Jeffrey S. Bary<br><strong><u>Categories:</u></strong> astro-ph.EP, astro-ph.IM<br><strong><u>Comments:</u></strong> 29 Pages, 20 Figures, Accepted at the Astrophysical Journal<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> variational autoencoder (title, abstract), VAE (abstract)<br><p><strong><u>Abstract:</u></strong> Dust-continuum observations of many protoplanetary disks reveal rings and gaps that are widely interpreted as evidence of ongoing planet formation. Here we present the first framework for inferring planet and disk parameters from such images using variational autoencoder (VAE) based generative machine learning (ML). The new framework is called VADER (Variational Autoencoder for Disks with Embedded Rings). We train VADER on synthetic images of dust continuum emission, generated from \texttt{FARGO3D} hydrodynamic simulations post-processed with Monte Carlo radiative transfer calculations. VADER infers the masses of up to three embedded planets as well as the disk parameters viscous $α$, dust-to-gas ratio, Stokes number, and flaring index. VADER returns a full posterior distribution for each of these quantities. We demonstrate that VADER reconstructs disk morphologies with high structural similarity (index $>$ 0.99), accurately recovers planet parameters with $R^2 > 0.9$ across planet masses, and reliably predicts disk parameters. Applied to ALMA dust continuum images of 23 protoplanetary disks, our model returns mass estimates for embedded planets of 0.3-2~$M_{\mathrm{Jup}}$ that agree to within $1σ$ of published values in most cases, and infers disk parameters consistent with current literature. Once trained, the VAE performs full posterior parameter inference in a matter of minutes, offering statistical rigor with enough computational speed for application to large-scale ALMA surveys. These results establish VAE-based models as powerful tools for inferring from disk structure the masses of embedded planets and the global disk parameters, with their associated uncertainties.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13815v1" target="_blank"><h2>Stripped-Envelope Supernovae for QCD Axion Detection <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Francisco R. Candón, Damiano F. G. Fiorillo, Ángel Gil Muyor, Hans-Thomas Janka, Georg G. Raffelt, Edoardo Vitagliano<br><strong><u>Categories:</u></strong> hep-ph, astro-ph.CO, astro-ph.HE<br><strong><u>Comments:</u></strong> 7 pages, 4 figures<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> VAE (title)<br><p><strong><u>Abstract:</u></strong> QCD axions would be copiously produced in the proto-neutron star formed in a core-collapse supernova (SN). After escaping, they would convert into gamma rays in the Galactic magnetic field and, as recently shown, in that of the progenitor star itself. Here, we show that Type Ibc SNe -- whose progenitors have lost their hydrogen or even helium envelopes -- are the optimal targets for this search. The stripped progenitors are much more compact, and show larger magnetic fields than both red and blue supergiants, the progenitors of Type IIP/L SNe. If the next galactic SN is of Type Ibc, Fermi-LAT or a similar gamma-ray satellite might be able to discover the QCD axion down to masses as small as $m_a\simeq 10^{-4}\,\rm eV$ (Peccei-Quinn scale $f_a\simeq 10^{11} \,\rm GeV$).</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13719v1" target="_blank"><h2>Scaling Spatial Intelligence with Multimodal Foundation Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zhongang Cai, Ruisi Wang, Chenyang Gu, Fanyi Pu, Junxiang Xu, Yubo Wang, Wanqi Yin, Zhitao Yang, Chen Wei, Qingping Sun, Tongxi Zhou, Jiaqi Li, Hui En Pang, Oscar Qian, Yukun Wei, Zhiqian Lin, Xuanke Shi, Kewang Deng, Xiaoyang Han, Zukai Chen, Xiangyu Fan, Hanming Deng, Lewei Lu, Liang Pan, Bo Li, Ziwei Liu, Quan Wang, Dahua Lin, Lei Yang<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.LG, cs.MM, cs.RO<br><strong><u>Comments:</u></strong> Model:this https URLCode:this https URL<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial intelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13712v1" target="_blank"><h2>From Black Box to Insight: Explainable AI for Extreme Event Preparedness <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Kiana Vu, İsmet Selçuk Özer, Phung Lai, Zheng Wu, Thilanka Munasinghe, Jennifer Wei<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> explainability (abstract), explainable (title, abstract)<br><p><strong><u>Abstract:</u></strong> As climate change accelerates the frequency and severity of extreme events such as wildfires, the need for accurate, explainable, and actionable forecasting becomes increasingly urgent. While artificial intelligence (AI) models have shown promise in predicting such events, their adoption in real-world decision-making remains limited due to their black-box nature, which limits trust, explainability, and operational readiness. This paper investigates the role of explainable AI (XAI) in bridging the gap between predictive accuracy and actionable insight for extreme event forecasting. Using wildfire prediction as a case study, we evaluate various AI models and employ SHapley Additive exPlanations (SHAP) to uncover key features, decision pathways, and potential biases in model behavior. Our analysis demonstrates how XAI not only clarifies model reasoning but also supports critical decision-making by domain experts and response teams. In addition, we provide supporting visualizations that enhance the interpretability of XAI outputs by contextualizing feature importance and temporal patterns in seasonality and geospatial characteristics. This approach enhances the usability of AI explanations for practitioners and policymakers. Our findings highlight the need for AI systems that are not only accurate but also interpretable, accessible, and trustworthy, essential for effective use in disaster preparedness, risk mitigation, and climate resilience planning.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13705v1" target="_blank"><h2>Rare Genomic Subtype Discovery from RNA-seq via Autoencoder Embeddings and Stability-Aware Clustering <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Alaa Mezghiche<br><strong><u>Categories:</u></strong> cs.LG, q-bio.GN<br><strong><u>Comments:</u></strong> 16 pages<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> latent space (abstract)<br><p><strong><u>Abstract:</u></strong> Unsupervised learning on high-dimensional RNA-seq data can reveal molecular subtypes beyond standard labels. We combine an autoencoder-based representation with clustering and stability analysis to search for rare but reproducible genomic subtypes. On the UCI "Gene Expression Cancer RNA-Seq" dataset (801 samples, 20,531 genes; BRCA, COAD, KIRC, LUAD, PRAD), a pan-cancer analysis shows clusters aligning almost perfectly with tissue of origin (Cramer's V = 0.887), serving as a negative control. We therefore reframe the problem within KIRC (n = 146): we select the top 2,000 highly variable genes, standardize them, train a feed-forward autoencoder (128-dimensional latent space), and run k-means for k = 2-10. While global indices favor small k, scanning k with a pre-specified discovery rule (rare < 10 percent and stable with Jaccard >= 0.60 across 20 seeds after Hungarian alignment) yields a simple solution at k = 5 (silhouette = 0.129, DBI = 2.045) with a rare cluster C0 (6.85 percent of patients) that is highly stable (Jaccard = 0.787). Cluster-vs-rest differential expression (Welch's t-test, Benjamini-Hochberg FDR) identifies coherent markers. Overall, pan-cancer clustering is dominated by tissue of origin, whereas a stability-aware within-cancer approach reveals a rare, reproducible KIRC subtype.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13699v1" target="_blank"><h2>Efficient Calibration for Decision Making <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Parikshit Gopalan, Konstantinos Stavropoulos, Kunal Talwar, Pranay Tankala<br><strong><u>Categories:</u></strong> cs.LG, cs.DS, stat.ML<br><strong><u>Comments:</u></strong> 50 pages, 3 figures<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> A decision-theoretic characterization of perfect calibration is that an agent seeking to minimize a proper loss in expectation cannot improve their outcome by post-processing a perfectly calibrated predictor. Hu and Wu (FOCS'24) use this to define an approximate calibration measure called calibration decision loss ($\mathsf{CDL}$), which measures the maximal improvement achievable by any post-processing over any proper loss. Unfortunately, $\mathsf{CDL}$ turns out to be intractable to even weakly approximate in the offline setting, given black-box access to the predictions and labels.
  We suggest circumventing this by restricting attention to structured families of post-processing functions $K$. We define the calibration decision loss relative to $K$, denoted $\mathsf{CDL}_K$ where we consider all proper losses but restrict post-processings to a structured family $K$. We develop a comprehensive theory of when $\mathsf{CDL}_K$ is information-theoretically and computationally tractable, and use it to prove both upper and lower bounds for natural classes $K$. In addition to introducing new definitions and algorithmic techniques to the theory of calibration for decision making, our results give rigorous guarantees for some widely used recalibration procedures in machine learning.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13685v1" target="_blank"><h2>Protein Secondary Structure Prediction Using 3D Graphs and Relation-Aware Message Passing Transformers <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Disha Varshney, Samarth Garg, Sarthak Tyagi, Deeksha Varshney, Nayan Deep, Asif Ekbal<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 40 pages<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract), transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> In this study, we tackle the challenging task of predicting secondary structures from protein primary sequences, a pivotal initial stride towards predicting tertiary structures, while yielding crucial insights into protein activity, relationships, and functions. Existing methods often utilize extensive sets of unlabeled amino acid sequences. However, these approaches neither explicitly capture nor harness the accessible protein 3D structural data, which is recognized as a decisive factor in dictating protein functions. To address this, we utilize protein residue graphs and introduce various forms of sequential or structural connections to capture enhanced spatial information. We adeptly combine Graph Neural Networks (GNNs) and Language Models (LMs), specifically utilizing a pre-trained transformer-based protein language model to encode amino acid sequences and employing message-passing mechanisms like GCN and R-GCN to capture geometric characteristics of protein structures. Employing convolution within a specific node's nearby region, including relations, we stack multiple convolutional layers to efficiently learn combined insights from the protein's spatial graph, revealing intricate interconnections and dependencies in its structural arrangement. To assess our model's performance, we employed the training dataset provided by NetSurfP-2.0, which outlines secondary structure in 3-and 8-states. Extensive experiments show that our proposed model, SSRGNet surpasses the baseline on f1-scores.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13679v1" target="_blank"><h2>QUILL: An Algorithm-Architecture Co-Design for Cache-Local Deformable Attention <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Hyunwoo Oh, Hanning Chen, Sanggeon Yun, Yang Ni, Wenjun Huang, Tamoghno Das, Suyeon Jang, Mohsen Imani<br><strong><u>Categories:</u></strong> cs.AR, cs.CV, cs.LG<br><strong><u>Comments:</u></strong> Accepted to DATE 2026<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Deformable transformers deliver state-of-the-art detection but map poorly to hardware due to irregular memory access and low arithmetic intensity. We introduce QUILL, a schedule-aware accelerator that turns deformable attention into cache-friendly, single-pass work. At its core, Distance-based Out-of-Order Querying (DOOQ) orders queries by spatial proximity; the look-ahead drives a region prefetch into an alternate buffer--forming a schedule-aware prefetch loop that overlaps memory and compute. A fused MSDeformAttn engine executes interpolation, Softmax, aggregation, and the final projection (W''m) in one pass without spilling intermediates, while small tensors are kept on-chip and surrounding dense layers run on integrated GEMMs. Implemented as RTL and evaluated end-to-end, QUILL achieves up to 7.29x higher throughput and 47.3x better energy efficiency than an RTX 4090, and exceeds prior accelerators by 3.26-9.82x in throughput and 2.01-6.07x in energy efficiency. With mixed-precision quantization, accuracy tracks FP32 within <=0.9 AP across Deformable and Sparse DETR variants. By converting sparsity into locality--and locality into utilization--QUILL delivers consistent, end-to-end speedups.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13666v1" target="_blank"><h2>A model-independent assessment of the late-time dark energy density evolution <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Rayff de Souza, Agripino Sousa-Neto, Javier E. González, Jailson Alcaniz<br><strong><u>Categories:</u></strong> astro-ph.CO, gr-qc, hep-ph, hep-th<br><strong><u>Comments:</u></strong> 13 pages, 5 figures, 3 tables<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> Combined measurements of Baryon Acoustic Oscillations (BAO) from the Dark Energy Spectroscopic Survey (DESI), the Cosmic Microwave Background (CMB) and Type Ia Supernovae (SN Ia), have recently challenged the $Λ$-Cold Dark Matter ($Λ$CDM) paradigm, indicating potential evidence for a dynamical dark energy component. These results are usually obtained in the context of the dark energy equation-of-state (EoS) parameterizations, generally implying in phantom-crossing at intermediate redshifts. However, a general mapping between these parameterizations that yields approximately the same background observables clouds the inference of the true nature of dark energy in the context of these parametric methods. In this work, we propose a model-independent reconstruction of the dark energy density, which is more directly constrained than its EoS, based on the Gaussian Process (GP) regression method with the use of DESI DR2 BAO data and the Pantheon+, Union3 and DESY5 SN Ia samples. In addition, we perform a statistical comparison between the energy densities of $Λ$, a non-phantom thawing quintessence-type dark energy, and the Chevallier-Polarski-Linder parameterization with the reconstructed function. We find that all models agree with the GP reconstruction at 95\% C.L., with the largest discrepancy coming from $Λ$CDM with DESY5 at low redshifts. Even in this case, our findings suggest that it may be premature to claim statistically significant evidence for evolving or phantom dark energy with current DESI and SN Ia measurements.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13663v1" target="_blank"><h2>Cost-Driven Synthesis of Sound Abstract Interpreters <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Qiuhan Gu, Avaljot Singh, Gagandeep Singh<br><strong><u>Categories:</u></strong> cs.PL, cs.LG<br><strong><u>Comments:</u></strong> 37 pages, 20 figures<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Constructing abstract interpreters that provide global soundness guarantees remains a major obstacle in abstract interpretation. We investigate whether modern LLMs can reduce this burden by leveraging them to synthesize sound, non-trivial abstract interpreters across multiple abstract domains in the setting of neural network verification. We formulate synthesis as a constrained optimization problem and introduce a novel mathematically grounded cost function for measuring unsoundness under strict syntactic and semantic constraints. Based on this formulation, we develop a unified framework that unifies LLM-based generation with syntactic and semantic validation and a quantitative cost-guided feedback mechanism. Empirical results demonstrate that our framework not only matches the quality of handcrafted transformers, but more importantly, discovers sound, high-precision transformers for complex nonlinear operators that are absent from existing literature.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13809v1" target="_blank"><h2>ScoresActivation: A New Activation Function for Model Agnostic Global Explainability by Design <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Emanuel Covaci, Fabian Galis, Radu Balan, Daniela Zaharie, Darian Onchis<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> Paper submitted to ECAI 2025 Conference<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> explainability (title, abstract), explainable (abstract)<br><p><strong><u>Abstract:</u></strong> Understanding the decision of large deep learning models is a critical challenge for building transparent and trustworthy systems. Although the current post hoc explanation methods offer valuable insights into feature importance, they are inherently disconnected from the model training process, limiting their faithfulness and utility. In this work, we introduce a novel differentiable approach to global explainability by design, integrating feature importance estimation directly into model training. Central to our method is the ScoresActivation function, a feature-ranking mechanism embedded within the learning pipeline. This integration enables models to prioritize features according to their contribution to predictive performance in a differentiable and end-to-end trainable manner. Evaluations across benchmark datasets show that our approach yields globally faithful, stable feature rankings aligned with SHAP values and ground-truth feature importance, while maintaining high predictive performance. Moreover, feature scoring is 150 times faster than the classical SHAP method, requiring only 2 seconds during training compared to SHAP's 300 seconds for feature ranking in the same configuration. Our method also improves classification accuracy by 11.24% with 10 features (5 relevant) and 29.33% with 16 features (5 relevant, 11 irrelevant), demonstrating robustness to irrelevant inputs. This work bridges the gap between model accuracy and interpretability, offering a scalable framework for inherently explainable machine learning.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13655v1" target="_blank"><h2>OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Henry Herzog, Favyen Bastani, Yawen Zhang, Gabriel Tseng, Joseph Redmon, Hadrien Sablon, Ryan Park, Jacob Morrison, Alexandra Buraczynski, Karen Farley, Joshua Hansen, Andrew Howe, Patrick Alan Johnson, Mark Otterlee, Ted Schmitt, Hunter Pitelka, Stephen Daspit, Rachel Ratner, Christopher Wilhelm, Sebastian Wood, Mike Jacobi, Hannah Kerner, Evan Shelhamer, Ali Farhadi, Ranjay Krishna, Patrick Beukema<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Earth observation data presents a unique challenge: it is spatial like images, sequential like video or text, and highly multimodal. We present OlmoEarth: a multimodal, spatio-temporal foundation model that employs a novel self-supervised learning formulation, masking strategy, and loss all designed for the Earth observation domain. OlmoEarth achieves state-of-the-art performance compared to 12 other foundation models across a variety of research benchmarks and real-world tasks from external partners. When evaluating embeddings OlmoEarth achieves the best performance on 15 out of 24 tasks, and with full fine-tuning it is the best on 19 of 29 tasks. We deploy OlmoEarth as the backbone of an end-to-end platform for data collection, labeling, training, and inference of Earth observation models. The OlmoEarth Platform puts frontier foundation models and powerful data management tools into the hands of non-profits and NGOs working to solve the world's biggest problems. OlmoEarth source code, training data, and pre-trained weights are available at $\href{https://github.com/allenai/olmoearth_pretrain}{\text{https://github.com/allenai/olmoearth_pretrain}}$.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13653v1" target="_blank"><h2>Weight-sparse transformers have interpretable circuits <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Leo Gao, Achyuta Rajaram, Jacob Coxon, Soham V. Govande, Bowen Baker, Dan Mossing<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> transformer (title)<br><p><strong><u>Abstract:</u></strong> Finding human-understandable circuits in language models is a central goal of the field of mechanistic interpretability. We train models to have more understandable circuits by constraining most of their weights to be zeros, so that each neuron only has a few connections. To recover fine-grained circuits underlying each of several hand-crafted tasks, we prune the models to isolate the part responsible for the task. These circuits often contain neurons and residual channels that correspond to natural concepts, with a small number of straightforwardly interpretable connections between them. We study how these models scale and find that making weights sparser trades off capability for interpretability, and scaling model size improves the capability-interpretability frontier. However, scaling sparse models beyond tens of millions of nonzero parameters while preserving interpretability remains a challenge. In addition to training weight-sparse models de novo, we show preliminary results suggesting our method can also be adapted to explain existing dense models. Our work produces circuits that achieve an unprecedented level of human understandability and validates them with considerable rigor.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13637v1" target="_blank"><h2>Towards Multimodal Representation Learning in Paediatric Kidney Disease <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Ana Durica, John Booth, Ivana Drobnjak<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 4 pages, 3 figures. EurIPS 2025 Multimodal Representation Learning for Healthcare (MMRL4H) workshop paper<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Paediatric kidney disease varies widely in its presentation and progression, which calls for continuous monitoring of renal function. Using electronic health records collected between 2019 and 2025 at Great Ormond Street Hospital, a leading UK paediatric hospital, we explored a temporal modelling approach that integrates longitudinal laboratory sequences with demographic information. A recurrent neural model trained on these data was used to predict whether a child would record an abnormal serum creatinine value within the following thirty days. Framed as a pilot study, this work provides an initial demonstration that simple temporal representations can capture useful patterns in routine paediatric data and lays the groundwork for future multimodal extensions using additional clinical signals and more detailed renal outcomes.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13626v1" target="_blank"><h2>CreBench: Human-Aligned Creativity Evaluation from Idea to Process to Product <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Kaiwen Xue, Chenglong Li, Zhonghong Ou, Guoxin Zhang, Kaoyan Lu, Shuai Lyu, Yifan Zhu, Ping Zong Junpeng Ding, Xinyu Liu, Qunlin Chen, Weiwei Qin, Yiran Shen, Jiayi Cen<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> 13 pages, 3 figures,The 40th Annual AAAI Conference on Artificial Intelligence(AAAI 2026),Paper has been accepted for a poster presentation<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Human-defined creativity is highly abstract, posing a challenge for multimodal large language models (MLLMs) to comprehend and assess creativity that aligns with human judgments. The absence of an existing benchmark further exacerbates this dilemma. To this end, we propose CreBench, which consists of two key components: 1) an evaluation benchmark covering the multiple dimensions from creative idea to process to products; 2) CreMIT (Creativity Multimodal Instruction Tuning dataset), a multimodal creativity evaluation dataset, consisting of 2.2K diverse-sourced multimodal data, 79.2K human feedbacks and 4.7M multi-typed instructions. Specifically, to ensure MLLMs can handle diverse creativity-related queries, we prompt GPT to refine these human feedbacks to activate stronger creativity assessment capabilities. CreBench serves as a foundation for building MLLMs that understand human-aligned creativity. Based on the CreBench, we fine-tune open-source general MLLMs, resulting in CreExpert, a multimodal creativity evaluation expert model. Extensive experiments demonstrate that the proposed CreExpert models achieve significantly better alignment with human creativity evaluation compared to state-of-the-art MLLMs, including the most advanced GPT-4V and Gemini-Pro-Vision.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13609v1" target="_blank"><h2>AtlasMorph: Learning conditional deformable templates for brain MRI <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Marianne Rakic, Andrew Hoopes, S. Mazdak Abulnaga, Mert R. Sabuncu, John V. Guttag, Adrian V. Dalca<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Deformable templates, or atlases, are images that represent a prototypical anatomy for a population, and are often enhanced with probabilistic anatomical label maps. They are commonly used in medical image analysis for population studies and computational anatomy tasks such as registration and segmentation. Because developing a template is a computationally expensive process, relatively few templates are available. As a result, analysis is often conducted with sub-optimal templates that are not truly representative of the study population, especially when there are large variations within this population. We propose a machine learning framework that uses convolutional registration neural networks to efficiently learn a function that outputs templates conditioned on subject-specific attributes, such as age and sex. We also leverage segmentations, when available, to produce anatomical segmentation maps for the resulting templates. The learned network can also be used to register subject images to the templates. We demonstrate our method on a compilation of 3D brain MRI datasets, and show that it can learn high-quality templates that are representative of populations. We find that annotated conditional templates enable better registration than their unlabeled unconditional counterparts, and outperform other templates construction methods.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13595v2" target="_blank"><h2>Physics-Informed Neural Networks for Nonlinear Output Regulation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Sebastiano Mengozzi, Giovanni B. Esposito, Michelangelo Bin, Andrea Acquaviva, Andrea Bartolini, Lorenzo Marconi<br><strong><u>Categories:</u></strong> eess.SY, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> This work addresses the full-information output regulation problem for nonlinear systems, assuming the states of both the plant and the exosystem are known. In this setting, perfect tracking or rejection is achieved by constructing a zero-regulation-error manifold $π(w)$ and a feedforward input $c(w)$ that render such manifold invariant. The pair $(π(w), c(w))$ is characterized by the regulator equations, i.e., a system of PDEs with an algebraic constraint. We focus on accurately solving the regulator equations introducing a physics-informed neural network (PINN) approach that directly approximates $π(w)$ and $c(w)$ by minimizing the residuals under boundary and feasibility conditions, without requiring precomputed trajectories or labeled data. The learned operator maps exosystem states to steady state plant states and inputs, enables real-time inference and, critically, generalizes across families of the exosystem with varying initial conditions and parameters. The framework is validated on a regulation task that synchronizes a helicopter's vertical dynamics with a harmonically oscillating platform. The resulting PINN-based solver reconstructs the zero-error manifold with high fidelity and sustains regulation performance under exosystem variations, highlighting the potential of learning-enabled solvers for nonlinear output regulation. The proposed approach is broadly applicable to nonlinear systems that admit a solution to the output regulation problem.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13591v1" target="_blank"><h2>A Fleeting GLIMPSE of N/O Enrichment at Cosmic Dawn: Evidence for Wolf Rayet N Stars in a z = 6.1 Galaxy <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Danielle A. Berg, Rohan P. Naidu, John Chisholm, Hakim Atek, Seiji Fujimoto, Vasily Kokorev, Lukas J. Furtak, Chiaki Kobayashi, Daniel Schaerer, Angela Adamo, Qinyue Fei, Damien Korber, Jorryt Matthee, Rui Marques-Chaves, Zorayda Martinez, Kristen B. W. Mcquinn, Julian B. Muñoz, Pascal A. Oesch, Daniel P. Stark, Mabel G. Stephenson, Tiger Yu-Yang Hsiao<br><strong><u>Categories:</u></strong> astro-ph.GA<br><strong><u>Comments:</u></strong> 18 pages, 6 figures, submitted to ApJ<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> We present the discovery of extreme nitrogen enrichment by Wolf Rayet nitrogen stars (WN) in the metal-poor ($\sim10\%Z_\odot$), lensed, compact ($R_{\rm eff}\sim20$ pc) galaxy RXCJ2248 at $z=6.1$, revealed by unprecedentedly deep JWST/NIRSpec medium-resolution spectroscopy from the GLIMPSE-D Survey. The exquisite S/N reveals multiple high-ionization nebular lines and broad Balmer and [OIII] components (FWHM$\sim700-3000$ km s$^{-1}$). We detect broadened HeII $λ$1640 and $λ$4687 (FWHM$\sim530$ km s$^{-1}$) and strong NIII] $λ$4642 emission consistent with a population of WN stars, making RXCJ2248 the most distant galaxy with confirmed WR features to date. We measure the multi-phase nebular density across five ions, the direct-method metallicity ($12+\log(\rm O/H)= 7.749\pm0.023$), and a non-uniform elemental enrichment pattern of extreme N/O enhancement ($\log(\rm N/O)=-0.390\pm0.035$ from N$^+$, N$^{+2}$, and N$^{+3}$) and suppressed C/O relative to empirical C/N trends. We show that this abundance pattern can be explained by enrichment from a dual-burst with a low WC/WN ratio, as expected at low metallicities. Crucially, these signatures can only arise during a brief, rare evolutionary window shortly after a burst ($\sim3-6$ Myr), when WN stars dominate chemical feedback but before dilution by later yields (e.g., supernovae). The observed frequency of strong N emitters at high$-z$ implies a $\sim50$ Myr burst duty cycle, suggesting that N/O outliers may represent a brief but ubiquitous phase in the evolution of highly star-forming early galaxies. The detection in RXCJ2248, therefore, provides the first direct evidence of WN-driven chemical enrichment in the early Universe and a novel timing argument for the bursty star formation cycles that shaped galaxies at cosmic dawn.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13588v1" target="_blank"><h2>Data-driven Acceleration of MPC with Guarantees <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Agustin Castellano, Shijie Pan, Enrique Mallada<br><strong><u>Categories:</u></strong> eess.SY, cs.AI, math.DS<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> data-driven (title, abstract)<br><p><strong><u>Abstract:</u></strong> Model Predictive Control (MPC) is a powerful framework for optimal control but can be too slow for low-latency applications. We present a data-driven framework to accelerate MPC by replacing online optimization with a nonparametric policy constructed from offline MPC solutions. Our policy is greedy with respect to a constructed upper bound on the optimal cost-to-go, and can be implemented as a nonparametric lookup rule that is orders of magnitude faster than solving MPC online. Our analysis shows that under sufficient coverage condition of the offline data, the policy is recursively feasible and admits provable, bounded optimality gap. These conditions establish an explicit trade-off between the amount of data collected and the tightness of the bounds. Our experiments show that this policy is between 100 and 1000 times faster than standard MPC, with only a modest hit to optimality, showing potential for real-time control tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13575v1" target="_blank"><h2>Hierarchical Prompt Learning for Image- and Text-Based Person Re-Identification <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Linhan Zhou, Shuang Li, Neng Dong, Yonghang Tai, Yafei Zhang, Huafeng Li<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> 9 pages, 4 figures, accepted by AAAI 2026<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Person re-identification (ReID) aims to retrieve target pedestrian images given either visual queries (image-to-image, I2I) or textual descriptions (text-to-image, T2I). Although both tasks share a common retrieval objective, they pose distinct challenges: I2I emphasizes discriminative identity learning, while T2I requires accurate cross-modal semantic alignment. Existing methods often treat these tasks separately, which may lead to representation entanglement and suboptimal performance. To address this, we propose a unified framework named Hierarchical Prompt Learning (HPL), which leverages task-aware prompt modeling to jointly optimize both tasks. Specifically, we first introduce a Task-Routed Transformer, which incorporates dual classification tokens into a shared visual encoder to route features for I2I and T2I branches respectively. On top of this, we develop a hierarchical prompt generation scheme that integrates identity-level learnable tokens with instance-level pseudo-text tokens. These pseudo-tokens are derived from image or text features via modality-specific inversion networks, injecting fine-grained, instance-specific semantics into the prompts. Furthermore, we propose a Cross-Modal Prompt Regularization strategy to enforce semantic alignment in the prompt token space, ensuring that pseudo-prompts preserve source-modality characteristics while enhancing cross-modal transferability. Extensive experiments on multiple ReID benchmarks validate the effectiveness of our method, achieving state-of-the-art performance on both I2I and T2I tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13565v1" target="_blank"><h2>Artificial Intelligence-driven Intelligent Wearable Systems: A full-stack Integration from Material Design to Personalized Interaction <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jingyi Zhao, Daqian Shi, Zhengda Wang, Xiongfeng Tang, Yanguo Qin<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> 5 pages, l figure, l table. Accepted at AI4RWC@WI-IAT 2025<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multi-modal (abstract)<br><p><strong><u>Abstract:</u></strong> Intelligent wearable systems are at the forefront of precision medicine and play a crucial role in enhancing human-machine interaction. Traditional devices often encounter limitations due to their dependence on empirical material design and basic signal processing techniques. To overcome these issues, we introduce the concept of Human-Symbiotic Health Intelligence (HSHI), which is a framework that integrates multi-modal sensor networks with edge-cloud collaborative computing and a hybrid approach to data and knowledge modeling. HSHI is designed to adapt dynamically to both inter-individual and intra-individual variability, transitioning health management from passive monitoring to an active collaborative evolution. The framework incorporates AI-driven optimization of materials and micro-structures, provides robust interpretation of multi-modal signals, and utilizes a dual mechanism that merges population-level insights with personalized adaptations. Moreover, the integration of closed-loop optimization through reinforcement learning and digital twins facilitates customized interventions and feedback. In general, HSHI represents a significant shift in healthcare, moving towards a model that emphasizes prevention, adaptability, and a harmonious relationship between technology and health management.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13561v1" target="_blank"><h2>RAC-DMVC: Reliability-Aware Contrastive Deep Multi-View Clustering under Multi-Source Noise <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Shihao Dong, Yue Liu, Xiaotong Zhou, Yuhui Zheng, Huiying Xu, Xinzhong Zhu<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Multi-view clustering (MVC), which aims to separate the multi-view data into distinct clusters in an unsupervised manner, is a fundamental yet challenging task. To enhance its applicability in real-world scenarios, this paper addresses a more challenging task: MVC under multi-source noises, including missing noise and observation noise. To this end, we propose a novel framework, Reliability-Aware Contrastive Deep Multi-View Clustering (RAC-DMVC), which constructs a reliability graph to guide robust representation learning under noisy environments. Specifically, to address observation noise, we introduce a cross-view reconstruction to enhances robustness at the data level, and a reliability-aware noise contrastive learning to mitigates bias in positive and negative pairs selection caused by noisy representations. To handle missing noise, we design a dual-attention imputation to capture shared information across views while preserving view-specific features. In addition, a self-supervised cluster distillation module further refines the learned representations and improves the clustering performance. Extensive experiments on five benchmark datasets demonstrate that RAC-DMVC outperforms SOTA methods on multiple evaluation metrics and maintains excellent performance under varying ratios of noise.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13545v1" target="_blank"><h2>Robust Defense Strategies for Multimodal Contrastive Learning: Efficient Fine-tuning Against Backdoor Attacks <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Md. Iqbal Hossain, Afia Sajeeda, Neeresh Kumar Perla, Ming Shao<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> The advent of multimodal deep learning models, such as CLIP, has unlocked new frontiers in a wide range of applications, from image-text understanding to classification tasks. However, these models are not safe for adversarial attacks, particularly backdoor attacks, which can subtly manipulate model behavior. Moreover, existing defense methods typically involve training from scratch or fine-tuning using a large dataset without pinpointing the specific labels that are affected. In this study, we introduce an innovative strategy to enhance the robustness of multimodal contrastive learning models against such attacks. In particular, given a poisoned CLIP model, our approach can identify the backdoor trigger and pinpoint the victim samples and labels in an efficient manner. To that end, an image segmentation ``oracle'' is introduced as the supervisor for the output of the poisoned CLIP. We develop two algorithms to rectify the poisoned model: (1) differentiating between CLIP and Oracle's knowledge to identify potential triggers; (2) pinpointing affected labels and victim samples, and curating a compact fine-tuning dataset. With this knowledge, we are allowed to rectify the poisoned CLIP model to negate backdoor effects. Extensive experiments on visual recognition benchmarks demonstrate our strategy is effective in CLIP-based backdoor defense.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13542v2" target="_blank"><h2>Making Evidence Actionable in Adaptive Learning Closing the Diagnostic Pedagogical Loop <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Amirreza Mehrabi, Jason Wade Morphew, Breejha Quezada, N. Sanjay Rebello<br><strong><u>Categories:</u></strong> cs.CE, cs.AI, cs.CY, stat.AP<br><strong><u>Comments:</u></strong> We have submitted the same article with another title: Making Evidence Actionable in Adaptive Learning (arXiv:2511.14052)<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Adaptive learning often diagnoses precisely yet intervenes weakly, producing help that is mistimed or misaligned. This study presents evidence supporting an instructor-governed feedback loop that converts concept-level assessment evidence into vetted microinterventions. The adaptive learning algorithm includes three safeguards: adequacy as a hard guarantee of gap closure, attention as a budgeted limit for time and redundancy, and diversity as protection against overfitting to a single resource. We formulate intervention assignment as a binary integer program with constraints for coverage, time, difficulty windows derived from ability estimates, prerequisites encoded by a concept matrix, and anti-redundancy with diversity. Greedy selection serves low-richness and tight-latency settings, gradient-based relaxation serves rich repositories, and a hybrid switches along a richness-latency frontier. In simulation and in an introductory physics deployment with 1204 students, both solvers achieved full skill coverage for nearly all learners within bounded watch time. The gradient-based method reduced redundant coverage by about 12 percentage points relative to greedy and produced more consistent difficulty alignment, while greedy delivered comparable adequacy at lower computational cost in resource-scarce environments. Slack variables localized missing content and guided targeted curation, sustaining sufficiency across student subgroups. The result is a tractable and auditable controller that closes the diagnostic pedagogical loop and enables equitable, load-aware personalization at the classroom scale.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13541v1" target="_blank"><h2>Graph Out-of-Distribution Detection via Test-Time Calibration with Dual Dynamic Dictionaries <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yue Hou, Ruomei Liu, Yingke Su, Junran Wu, Ke Xu<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> Accepted by AAAI 2026 (The 40th Annual AAAI Conference on Artificial Intelligence)<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> A key challenge in graph out-of-distribution (OOD) detection lies in the absence of ground-truth OOD samples during training. Existing methods are typically optimized to capture features within the in-distribution (ID) data and calculate OOD scores, which often limits pre-trained models from representing distributional boundaries, leading to unreliable OOD detection. Moreover, the latent structure of graph data is often governed by multiple underlying factors, which remains less explored. To address these challenges, we propose a novel test-time graph OOD detection method, termed BaCa, that calibrates OOD scores using dual dynamically updated dictionaries without requiring fine-tuning the pre-trained model. Specifically, BaCa estimates graphons and applies a mix-up strategy solely with test samples to generate diverse boundary-aware discriminative topologies, eliminating the need for exposing auxiliary datasets as outliers. We construct dual dynamic dictionaries via priority queues and attention mechanisms to adaptively capture latent ID and OOD representations, which are then utilized for boundary-aware OOD score calibration. To the best of our knowledge, extensive experiments on real-world datasets show that BaCa significantly outperforms existing state-of-the-art methods in OOD detection.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13540v2" target="_blank"><h2>Fairness-Aware Graph Representation Learning with Limited Demographic Information <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zichong Wang, Zhipeng Yin, Liping Yang, Jun Zhuang, Rui Yu, Qingzhao Kong, Wenbin Zhang<br><strong><u>Categories:</u></strong> cs.LG, cs.CY<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Ensuring fairness in Graph Neural Networks is fundamental to promoting trustworthy and socially responsible machine learning systems. In response, numerous fair graph learning methods have been proposed in recent years. However, most of them assume full access to demographic information, a requirement rarely met in practice due to privacy, legal, or regulatory restrictions. To this end, this paper introduces a novel fair graph learning framework that mitigates bias in graph learning under limited demographic information. Specifically, we propose a mechanism guided by partial demographic data to generate proxies for demographic information and design a strategy that enforces consistent node embeddings across demographic groups. In addition, we develop an adaptive confidence strategy that dynamically adjusts each node's contribution to fairness and utility based on prediction confidence. We further provide theoretical analysis demonstrating that our framework, FairGLite, achieves provable upper bounds on group fairness metrics, offering formal guarantees for bias mitigation. Through extensive experiments on multiple datasets and fair graph learning frameworks, we demonstrate the framework's effectiveness in both mitigating bias and maintaining model utility.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13530v1" target="_blank"><h2>Towards Affect-Adaptive Human-Robot Interaction: A Protocol for Multimodal Dataset Collection on Social Anxiety <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Vesna Poprcova, Iulia Lefter, Matthias Wieser, Martijn Warnier, Frances Brazier<br><strong><u>Categories:</u></strong> cs.RO, cs.AI<br><strong><u>Comments:</u></strong> Accepted at the Workshop on Benefits of pErsonalization and behAvioral adaptation in assistive Robots (BEAR 2025), held at the IEEE RO-MAN Conference 2025<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Social anxiety is a prevalent condition that affects interpersonal interactions and social functioning. Recent advances in artificial intelligence and social robotics offer new opportunities to examine social anxiety in the human-robot interaction context. Accurate detection of affective states and behaviours associated with social anxiety requires multimodal datasets, where each signal modality provides complementary insights into its manifestations. However, such datasets remain scarce, limiting progress in both research and applications. To address this, this paper presents a protocol for multimodal dataset collection designed to reflect social anxiety in a human-robot interaction context. The dataset will consist of synchronised audio, video, and physiological recordings acquired from at least 70 participants, grouped according to their level of social anxiety, as they engage in approximately 10-minute interactive Wizard-of-Oz role-play scenarios with the Furhat social robot under controlled experimental conditions. In addition to multimodal data, the dataset will be enriched with contextual data providing deeper insight into individual variability in social anxiety responses. This work can contribute to research on affect-adaptive human-robot interaction by providing support for robust multimodal detection of social anxiety.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13527v1" target="_blank"><h2>Mitigating Spurious Correlations in Patch-wise Tumor Classification on High-Resolution Multimodal Images <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Ihab Asaad, Maha Shadaydeh, Joachim Denzler<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> Accepted at EurIPS 2025 Workshop: Unifying Perspectives on Learning Biases (UPLB)<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Patch-wise multi-label classification provides an efficient alternative to full pixel-wise segmentation on high-resolution images, particularly when the objective is to determine the presence or absence of target objects within a patch rather than their precise spatial extent. This formulation substantially reduces annotation cost, simplifies training, and allows flexible patch sizing aligned with the desired level of decision granularity. In this work, we focus on a special case, patch-wise binary classification, applied to the detection of a single class of interest (tumor) on high-resolution multimodal nonlinear microscopy images. We show that, although this simplified formulation enables efficient model development, it can introduce spurious correlations between patch composition and labels: tumor patches tend to contain larger tissue regions, whereas non-tumor patches often consist mostly of background with small tissue areas. We further quantify the bias in model predictions caused by this spurious correlation, and propose to use a debiasing strategy to mitigate its effect. Specifically, we apply GERNE, a debiasing method that can be adapted to maximize worst-group accuracy (WGA). Our results show an improvement in WGA by approximately 7% compared to ERM for two different thresholds used to binarize the spurious feature. This enhancement boosts model performance on critical minority cases, such as tumor patches with small tissues and non-tumor patches with large tissues, and underscores the importance of spurious correlation-aware learning in patch-wise classification problems.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13514v1" target="_blank"><h2>A Quantum Tensor Network-Based Viewpoint for Modeling and Analysis of Time Series Data <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Pragatheeswaran Vipulananthan, Kamal Premaratne, Dilip Sarkar, Manohar N. Murthi<br><strong><u>Categories:</u></strong> cs.LG, cs.IT<br><strong><u>Comments:</u></strong> IEEE International Conference on Knowledge Graph (ICKG), 378-387, 2024<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Accurate uncertainty quantification is a critical challenge in machine learning. While neural networks are highly versatile and capable of learning complex patterns, they often lack interpretability due to their ``black box'' nature. On the other hand, probabilistic ``white box'' models, though interpretable, often suffer from a significant performance gap when compared to neural networks. To address this, we propose a novel quantum physics-based ``white box'' method that offers both accurate uncertainty quantification and enhanced interpretability. By mapping the kernel mean embedding (KME) of a time series data vector to a reproducing kernel Hilbert space (RKHS), we construct a tensor network-inspired 1D spin chain Hamiltonian, with the KME as one of its eigen-functions or eigen-modes. We then solve the associated Schr{ö}dinger equation and apply perturbation theory to quantify uncertainty, thereby improving the interpretability of tasks performed with the quantum tensor network-based model. We demonstrate the effectiveness of this methodology, compared to state-of-the-art ``white box" models, in change point detection and time series clustering, providing insights into the uncertainties associated with decision-making throughout the process.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13487v2" target="_blank"><h2>Systematic Evaluation of Time-Frequency Features for Binaural Sound Source Localization <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Davoud Shariat Panah, Alessandro Ragano, Dan Barry, Jan Skoglund, Andrew Hines<br><strong><u>Categories:</u></strong> eess.AS, cs.LG, cs.SD<br><strong><u>Comments:</u></strong> Submitted to ICASSP 2026<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> This study presents a systematic evaluation of time-frequency feature design for binaural sound source localization (SSL), focusing on how feature selection influences model performance across diverse conditions. We investigate the performance of a convolutional neural network (CNN) model using various combinations of amplitude-based features (magnitude spectrogram, interaural level difference - ILD) and phase-based features (phase spectrogram, interaural phase difference - IPD). Evaluations on in-domain and out-of-domain data with mismatched head-related transfer functions (HRTFs) reveal that carefully chosen feature combinations often outperform increases in model complexity. While two-feature sets such as ILD + IPD are sufficient for in-domain SSL, generalization to diverse content requires richer inputs combining channel spectrograms with both ILD and IPD. Using the optimal feature sets, our low-complexity CNN model achieves competitive performance. Our findings underscore the importance of feature design in binaural SSL and provide practical guidance for both domain-specific and general-purpose localization.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13476v1" target="_blank"><h2>Multi-Agent Multimodal Large Language Model Framework for Automated Interpretation of Fuel Efficiency Analytics in Public Transportation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zhipeng Ma, Ali Rida Bahja, Andreas Burgdorf, André Pomp, Tobias Meisen, Bo Nørregaard Jørgensen, Zheng Grace Ma<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Enhancing fuel efficiency in public transportation requires the integration of complex multimodal data into interpretable, decision-relevant insights. However, traditional analytics and visualization methods often yield fragmented outputs that demand extensive human interpretation, limiting scalability and consistency. This study presents a multi-agent framework that leverages multimodal large language models (LLMs) to automate data narration and energy insight generation. The framework coordinates three specialized agents, including a data narration agent, an LLM-as-a-judge agent, and an optional human-in-the-loop evaluator, to iteratively transform analytical artifacts into coherent, stakeholder-oriented reports. The system is validated through a real-world case study on public bus transportation in Northern Jutland, Denmark, where fuel efficiency data from 4006 trips are analyzed using Gaussian Mixture Model clustering. Comparative experiments across five state-of-the-art LLMs and three prompting paradigms identify GPT-4.1 mini with Chain-of-Thought prompting as the optimal configuration, achieving 97.3% narrative accuracy while balancing interpretability and computational cost. The findings demonstrate that multi-agent orchestration significantly enhances factual precision, coherence, and scalability in LLM-based reporting. The proposed framework establishes a replicable and domain-adaptive methodology for AI-driven narrative generation and decision support in energy informatics.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13469v1" target="_blank"><h2>GREAT: Generalizable Representation Enhancement via Auxiliary Transformations for Zero-Shot Environmental Prediction <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Shiyuan Luo, Chonghao Qiu, Runlong Yu, Yiqun Xie, Xiaowei Jia<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Environmental modeling faces critical challenges in predicting ecosystem dynamics across unmonitored regions due to limited and geographically imbalanced observation data. This challenge is compounded by spatial heterogeneity, causing models to learn spurious patterns that fit only local data. Unlike conventional domain generalization, environmental modeling must preserve invariant physical relationships and temporal coherence during augmentation. In this paper, we introduce Generalizable Representation Enhancement via Auxiliary Transformations (GREAT), a framework that effectively augments available datasets to improve predictions in completely unseen regions. GREAT guides the augmentation process to ensure that the original governing processes can be recovered from the augmented data, and the inclusion of the augmented data leads to improved model generalization. Specifically, GREAT learns transformation functions at multiple layers of neural networks to augment both raw environmental features and temporal influence. They are refined through a novel bi-level training process that constrains augmented data to preserve key patterns of the original source data. We demonstrate GREAT's effectiveness on stream temperature prediction across six ecologically diverse watersheds in the eastern U.S., each containing multiple stream segments. Experimental results show that GREAT significantly outperforms existing methods in zero-shot scenarios. This work provides a practical solution for environmental applications where comprehensive monitoring is infeasible.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13463v1" target="_blank"><h2>Multi-task GINN-LP for Multi-target Symbolic Regression <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Hussein Rajabu, Lijun Qian, Xishuang Dong<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> explainable (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> In the area of explainable artificial intelligence, Symbolic Regression (SR) has emerged as a promising approach by discovering interpretable mathematical expressions that fit data. However, SR faces two main challenges: most methods are evaluated on scientific datasets with well-understood relationships, limiting generalization, and SR primarily targets single-output regression, whereas many real-world problems involve multi-target outputs with interdependent variables. To address these issues, we propose multi-task regression GINN-LP (MTRGINN-LP), an interpretable neural network for multi-target symbolic regression. By integrating GINN-LP with a multi-task deep learning, the model combines a shared backbone including multiple power-term approximator blocks with task-specific output layers, capturing inter-target dependencies while preserving interpretability. We validate multi-task GINN-LP on practical multi-target applications, including energy efficiency prediction and sustainable agriculture. Experimental results demonstrate competitive predictive performance alongside high interpretability, effectively extending symbolic regression to broader real-world multi-output tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13457v1" target="_blank"><h2>Artificial Intelligence-Enabled Spirometry for Early Detection of Right Heart Failure <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Bin Liu, Qinghao Zhao, Yuxi Zhou, Zhejun Sun, Kaijie Lei, Deyun Zhang, Shijia Geng, Shenda Hong<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 19 pages, 5 figures<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> variational autoencoder (abstract), VAE (abstract)<br><p><strong><u>Abstract:</u></strong> Right heart failure (RHF) is a disease characterized by abnormalities in the structure or function of the right ventricle (RV), which is associated with high morbidity and mortality. Lung disease often causes increased right ventricular load, leading to RHF. Therefore, it is very important to screen out patients with cor pulmonale who develop RHF from people with underlying lung diseases. In this work, we propose a self-supervised representation learning method to early detecting RHF from patients with cor pulmonale, which uses spirogram time series to predict patients with RHF at an early stage. The proposed model is divided into two stages. The first stage is the self-supervised representation learning-based spirogram embedding (SLSE) network training process, where the encoder of the Variational autoencoder (VAE-encoder) learns a robust low-dimensional representation of the spirogram time series from the data-augmented unlabeled data. Second, this low-dimensional representation is fused with demographic information and fed into a CatBoost classifier for the downstream RHF prediction task. Trained and tested on a carefully selected subset of 26,617 individuals from the UK Biobank, our model achieved an AUROC of 0.7501 in detecting RHF, demonstrating strong population-level distinction ability. We further evaluated the model on high-risk clinical subgroups, achieving AUROC values of 0.8194 on a test set of 74 patients with chronic kidney disease (CKD) and 0.8413 on a set of 64 patients with valvular heart disease (VHD). These results highlight the model's potential utility in predicting RHF among clinically elevated-risk populations. In conclusion, this study presents a self-supervised representation learning approach combining spirogram time series and demographic data, demonstrating promising potential for early RHF detection in clinical practice.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13444v1" target="_blank"><h2>Discovering Operational Patterns Using Image-Based Convolutional Clustering and Composite Evaluation: A Case Study in Foundry Melting Processes <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zhipeng Ma, Bo Nørregaard Jørgensen, Zheng Grace Ma<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), convolutional (title, abstract), explainability (abstract), explainable (abstract)<br><p><strong><u>Abstract:</u></strong> Industrial process monitoring increasingly relies on sensor-generated time-series data, yet the lack of labels, high variability, and operational noise make it difficult to extract meaningful patterns using conventional methods. Existing clustering techniques either rely on fixed distance metrics or deep models designed for static data, limiting their ability to handle dynamic, unstructured industrial sequences. Addressing this gap, this paper proposes a novel framework for unsupervised discovery of operational modes in univariate time-series data using image-based convolutional clustering with composite internal evaluation. The proposed framework improves upon existing approaches in three ways: (1) raw time-series sequences are transformed into grayscale matrix representations via overlapping sliding windows, allowing effective feature extraction using a deep convolutional autoencoder; (2) the framework integrates both soft and hard clustering outputs and refines the selection through a two-stage strategy; and (3) clustering performance is objectively evaluated by a newly developed composite score, S_eva, which combines normalized Silhouette, Calinski-Harabasz, and Davies-Bouldin indices. Applied to over 3900 furnace melting operations from a Nordic foundry, the method identifies seven explainable operational patterns, revealing significant differences in energy consumption, thermal dynamics, and production duration. Compared to classical and deep clustering baselines, the proposed approach achieves superior overall performance, greater robustness, and domain-aligned explainability. The framework addresses key challenges in unsupervised time-series analysis, such as sequence irregularity, overlapping modes, and metric inconsistency, and provides a generalizable solution for data-driven diagnostics and energy optimization in industrial systems.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13442v2" target="_blank"><h2>Unlocking the Forgery Detection Potential of Vanilla MLLMs: A Novel Training-Free Pipeline <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Rui Zuo, Qinyue Tong, Zhe-Ming Lu, Ziqian Lu<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> With the rapid advancement of artificial intelligence-generated content (AIGC) technologies, including multimodal large language models (MLLMs) and diffusion models, image generation and manipulation have become remarkably effortless. Existing image forgery detection and localization (IFDL) methods often struggle to generalize across diverse datasets and offer limited interpretability. Nowadays, MLLMs demonstrate strong generalization potential across diverse vision-language tasks, and some studies introduce this capability to IFDL via large-scale training. However, such approaches cost considerable computational resources, while failing to reveal the inherent generalization potential of vanilla MLLMs to address this problem. Inspired by this observation, we propose Foresee, a training-free MLLM-based pipeline tailored for image forgery analysis. It eliminates the need for additional training and enables a lightweight inference process, while surpassing existing MLLM-based methods in both tamper localization accuracy and the richness of textual explanations. Foresee employs a type-prior-driven strategy and utilizes a Flexible Feature Detector (FFD) module to specifically handle copy-move manipulations, thereby effectively unleashing the potential of vanilla MLLMs in the forensic domain. Extensive experiments demonstrate that our approach simultaneously achieves superior localization accuracy and provides more comprehensive textual explanations. Moreover, Foresee exhibits stronger generalization capability, outperforming existing IFDL methods across various tampering types, including copy-move, splicing, removal, local enhancement, deepfake, and AIGC-based editing. The code will be released in the final version.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13419v1" target="_blank"><h2>MMWSTM-ADRAN+: A Novel Hybrid Deep Learning Architecture for Enhanced Climate Time Series Forecasting and Extreme Event Prediction <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Shaheen Mohammed Saleh Ahmed, Hakan Hakan Guneyli<br><strong><u>Categories:</u></strong> cs.LG, physics.ao-ph<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multi-modal (abstract), attention (abstract), data augmentation (abstract)<br><p><strong><u>Abstract:</u></strong> Accurate short-range prediction of extreme air temperature events remains a fundamental challenge in operational climate-risk management. We present Multi-Modal Weather State Transition Model with Anomaly-Driven Recurrent Attention Network Plus (MMWSTM-ADRAN+), a dual-stream deep learning architecture that couples a regime-aware dynamics model with an anomaly-focused attention mechanism to forecast daily maximum temperature and its extremes. The first stream, MMWSTM, combines bidirectional Long Short-Term Memory (BiLSTM) units with a learnable Markov state transition matrix to capture synoptic-scale weather regime changes. The second stream, ADRAN, integrates bidirectional Gated Recurrent Units (BiGRUs), multi-head self-attention, and a novel anomaly amplification layer to enhance sensitivity to low-probability signals. A lightweight attentive fusion gate adaptively determines the contribution of each stream to the final prediction. Model optimization employs a custom ExtremeWeatherLoss function that up-weights errors on the upper 5% and lower 5% of the temperature distribution, and a time-series data augmentation suite (jittering, scaling, time/magnitude warping) that effectively quadruples the training data</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13394v1" target="_blank"><h2>Fast and Robust Simulation-Based Inference With Optimization Monte Carlo <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Vasilis Gkolemis, Christos Diou, Michael Gutmann<br><strong><u>Categories:</u></strong> cs.LG, stat.ML<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Bayesian parameter inference for complex stochastic simulators is challenging due to intractable likelihood functions. Existing simulation-based inference methods often require large number of simulations and become costly to use in high-dimensional parameter spaces or in problems with partially uninformative outputs. We propose a new method for differentiable simulators that delivers accurate posterior inference with substantially reduced runtimes. Building on the Optimization Monte Carlo framework, our approach reformulates stochastic simulation as deterministic optimization problems. Gradient-based methods are then applied to efficiently navigate toward high-density posterior regions and avoid wasteful simulations in low-probability areas. A JAX-based implementation further enhances the performance through vectorization of key method components. Extensive experiments, including high-dimensional parameter spaces, uninformative outputs, multiple observations and multimodal posteriors show that our method consistently matches, and often exceeds, the accuracy of state-of-the-art approaches, while reducing the runtime by a substantial margin.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13393v1" target="_blank"><h2>Learning Cosmology from Nearest Neighbour Statistics <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Atrideb Chatterjee, Arka Banerjee, Francisco Villaescusa-Navarro, Tom Abel<br><strong><u>Categories:</u></strong> astro-ph.CO<br><strong><u>Comments:</u></strong> Submitted for publication to A&A<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Extracting cosmological parameters from galaxy/halo catalogues with sub-percent level accuracy is an important aspect of modern cosmology, especially in view of ongoing and upcoming surveys such as Euclid, DESI, and LSST. While traditional two-point statistics have been known to be suboptimal for this task, recently proposed k-Nearest Neighbour (kNN) based summary statistics have demonstrated tighter constraining power. Building on the kNN statistics, we introduce a new field-level representation of discrete halo catalogues - NN distance maps. We employ this technique on the halo catalogues obtained from Quijote N-body simulation suites. By combining these maps with kNN-based summary statistics, we train a hybrid neural network to infer cosmological parameters, showing that the resulting constraints achieve state-of-the-art, if not the best, accuracy. In addition, our hybrid framework is 5-10 times more computationally efficient than some of the existing point-cloud-based ML methods.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13392v1" target="_blank"><h2>Cosmic Expansion Driven by Gravitational Particle Production: Toward a Complete Cosmological Scenario <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> P. W. R. Lima, J. A. S. Lima<br><strong><u>Categories:</u></strong> astro-ph.CO<br><strong><u>Comments:</u></strong> 23 pages and 5 figures<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> A dark-energy-free cosmological model ($Ω_{DE} \equiv 0$) based on gravitationally induced adiabatic particle creation is proposed. The thermodynamics of particle production yields an effective negative pressure that drives both primordial inflation and late-time cosmic acceleration. The model, characterized by four components and two free parameters ($α$, $β$), reproduces a $Λ$CDM-like expansion for suitable $α$, while $β$ introduces small but testable deviations from the cosmic concordance model. Constraints from type Ia Supernovae (Pantheon+SH0ES) and H(z) data indicate $β\simeq 0.13$, suggesting a mild departure from standard cosmology and possible relief of the $H_0$ and $S_8$ tensions. The resulting classical cosmology evolves smoothly between two extreme de Sitter phases, offering a singularity-free, unified scenario that beyond solving old cosmological puzzles opens a new perspective to handle the tensions plaguing the current cosmic concordance model.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13373v1" target="_blank"><h2>A Novel Hierarchical Integration Method for Efficient Model Merging in Medical LLMs <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Prakrit Timilsina, Anuj Nepal, Rajan Kadel, Robin Doss<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Large Language Models (LLMs) face significant challenges in distributed healthcare, including consolidating specialized domain knowledge across institutions while maintaining privacy, reducing computational overhead, and preventing catastrophic forgetting during model updates.This paper presents a systematic evaluation of six parameter-space merging techniques applied to two architecturally compatible medical LLMs derived from the Mistral-7B base model. We introduce a novel hierarchical method that combines selective Optimal Transport (OT) alignment for attention layers with cosine similarity-weighted interpolation, designed to address permutation variance while minimizing computational overhead for edge deployment scenarios. Our study evaluates Task Arithmetic, Linear Averaging, DARE-TIES, DELLA, Breadcrumbs, and our Hierarchical approach across five medical benchmarks. Results demonstrate that architecturally compatible models benefit significantly from simple averaging methods, with Task Arithmetic achieving 45.80% accuracy on MedQA, outperforming complex pruning-based approaches. These findings offer critical insights for the deployment of distributed medical AI in resource-constrained IoT environments, where computational efficiency and model compatibility are paramount. Our work establishes that for architecturally compatible models, simple averaging provides a robust and computationally efficient baseline for knowledge consolidation, offering a pragmatic path forward for scalable medical AI systems.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13372v1" target="_blank"><h2>Constraining r-process nucleosynthesis with multi-objective Galactic chemical evolution models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> M. Molero, A. Arcones, F. Montes, C. J. Hansen<br><strong><u>Categories:</u></strong> astro-ph.GA, astro-ph.HE, astro-ph.SR<br><strong><u>Comments:</u></strong> 21 pages, 16 figures, submitted to Astronomy&Astrophysics<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), VAE (abstract)<br><p><strong><u>Abstract:</u></strong> The astrophysical site(s) of the r-process are uncertain, with candidates such as neutron star mergers and magneto-rotational supernovae predicting different event rates, delay times, and heavy-element yields. Galactic chemical evolution models constrain these properties by comparing model predictions with observed abundances. We explore, in a systematic and data-driven way, the astrophysical conditions under which r-process enrichment can reproduce the observed trends of multiple neutron-capture elements in the Milky Way. Rather than assuming a fixed site, we adopt a flexible, parametric approach to test whether a common set of r-process parameters can explain the chemical evolution of several heavy elements. We compute a grid of one-infall, homogeneous models varying: Eu yield per event, r-process event rate, enrichment delay time, and progenitor mass range. For each of the $\sim 1.5 \times 10^5$ models, we predict [X/Fe] vs. [Fe/H] trends by scaling Eu yields with the solar r-process pattern. A multi-objective optimisation based on Pareto fronts identifies models that best reproduce the abundance trends. Best-fitting models favour short delay times ($\leq 30\ \rm Myr$), low-mass progenitors ($\sim 20-25\ \rm M_\odot$), and an effective Eu injection of $\sim 2 \times 10^{-7}\ \rm M_\odot$ per event. Stars more massive than $\sim 80\ \rm M_\odot$ are too rare to dominate the enrichment. While heavy elements can be reproduced, lighter ones show stronger conflicts with Eu, reflecting that the solar r-process scaling relation becomes less valid toward lighter elements. No single class of r-process events, under solar-scaled yields, can explain light and heavy neutron-capture elements; at least two components are required: a main r-process consistent with solar and r-rich stars, and a weaker component producing enhanced light r-process elements, similar to that observed in r-poor stars.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13371v1" target="_blank"><h2>Cognitive Maps in Language Models: A Mechanistic Analysis of Spatial Planning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Caroline Baumgartner, Eleanor Spens, Neil Burgess, Petru Manescu<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> How do large language models solve spatial navigation tasks? We investigate this by training GPT-2 models on three spatial learning paradigms in grid environments: passive exploration (Foraging Model- predicting steps in random walks), goal-directed planning (generating optimal shortest paths) on structured Hamiltonian paths (SP-Hamiltonian), and a hybrid model fine-tuned with exploratory data (SP-Random Walk). Using behavioural, representational and mechanistic analyses, we uncover two fundamentally different learned algorithms. The Foraging model develops a robust, map-like representation of space, akin to a 'cognitive map'. Causal interventions reveal that it learns to consolidate spatial information into a self-sufficient coordinate system, evidenced by a sharp phase transition where its reliance on historical direction tokens vanishes by the middle layers of the network. The model also adopts an adaptive, hierarchical reasoning system, switching between a low-level heuristic for short contexts and map-based inference for longer ones. In contrast, the goal-directed models learn a path-dependent algorithm, remaining reliant on explicit directional inputs throughout all layers. The hybrid model, despite demonstrating improved generalisation over its parent, retains the same path-dependent strategy. These findings suggest that the nature of spatial intelligence in transformers may lie on a spectrum, ranging from generalisable world models shaped by exploratory data to heuristics optimised for goal-directed tasks. We provide a mechanistic account of this generalisation-optimisation trade-off and highlight how the choice of training regime influences the strategies that emerge.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13351v1" target="_blank"><h2>Dual-LoRA and Quality-Enhanced Pseudo Replay for Multimodal Continual Food Learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Xinlan Wu, Bin Zhu, Feng Han, Pengkun Jiao, Jingjing Chen<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Food analysis has become increasingly critical for health-related tasks such as personalized nutrition and chronic disease prevention. However, existing large multimodal models (LMMs) in food analysis suffer from catastrophic forgetting when learning new tasks, requiring costly retraining from scratch. To address this, we propose a novel continual learning framework for multimodal food learning, integrating a Dual-LoRA architecture with Quality-Enhanced Pseudo Replay. We introduce two complementary low-rank adapters for each task: a specialized LoRA that learns task-specific knowledge with orthogonal constraints to previous tasks' subspaces, and a cooperative LoRA that consolidates shared knowledge across tasks via pseudo replay. To improve the reliability of replay data, our Quality-Enhanced Pseudo Replay strategy leverages self-consistency and semantic similarity to reduce hallucinations in generated samples. Experiments on the comprehensive Uni-Food dataset show superior performance in mitigating forgetting, representing the first effective continual learning approach for complex food tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13339v1" target="_blank"><h2>Statistically Accurate and Robust Generative Prediction of Rock Discontinuities with A Tabular Foundation Model <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Han Meng, Gang Mei, Hong Tian, Nengxiong Xu, Jianbing Peng<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> Rock discontinuities critically govern the mechanical behavior and stability of rock masses. Their internal distributions remain largely unobservable and are typically inferred from surface-exposed discontinuities using generative prediction approaches. However, surface-exposed observations are inherently sparse, and existing generative prediction approaches either fail to capture the underlying complex distribution patterns or lack robustness under data-sparse conditions. Here, we proposed a simple yet robust approach for statistically accurate generative prediction of rock discontinuities by utilizing a tabular foundation model. By leveraging the powerful sample learning capability of the foundation model specifically designed for small data, our approach can effectively capture the underlying complex distribution patterns within limited measured discontinuities. Comparative experiments on ten datasets with diverse scales and distribution patterns of discontinuities demonstrate superior accuracy and robustness over conventional statistical models and deep generative approaches. This work advances quantitative characterization of rock mass structures, supporting safer and more reliable data-driven geotechnical design.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13338v1" target="_blank"><h2>Tab-PET: Graph-Based Positional Encodings for Tabular Transformers <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yunze Leng, Rohan Ghosh, Mehul Motani<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract), attention (abstract), causality (abstract)<br><p><strong><u>Abstract:</u></strong> Supervised learning with tabular data presents unique challenges, including low data sizes, the absence of structural cues, and heterogeneous features spanning both categorical and continuous domains. Unlike vision and language tasks, where models can exploit inductive biases in the data, tabular data lacks inherent positional structure, hindering the effectiveness of self-attention mechanisms. While recent transformer-based models like TabTransformer, SAINT, and FT-Transformer (which we refer to as 3T) have shown promise on tabular data, they typically operate without leveraging structural cues such as positional encodings (PEs), as no prior structural information is usually available. In this work, we find both theoretically and empirically that structural cues, specifically PEs can be a useful tool to improve generalization performance for tabular transformers. We find that PEs impart the ability to reduce the effective rank (a form of intrinsic dimensionality) of the features, effectively simplifying the task by reducing the dimensionality of the problem, yielding improved generalization. To that end, we propose Tab-PET (PEs for Tabular Transformers), a graph-based framework for estimating and inculcating PEs into embeddings. Inspired by approaches that derive PEs from graph topology, we explore two paradigms for graph estimation: association-based and causality-based. We empirically demonstrate that graph-derived PEs significantly improve performance across 50 classification and regression datasets for 3T. Notably, association-based graphs consistently yield more stable and pronounced gains compared to causality-driven ones. Our work highlights an unexpected role of PEs in tabular transformers, revealing how they can be harnessed to improve generalization.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13326v2" target="_blank"><h2>TacEleven: generative tactic discovery for football open play <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Siyao Zhao, Hao Ma, Zhiqiang Pu, Jingjing Huang, Yi Pan, Shijie Wang, Zhi Ming<br><strong><u>Categories:</u></strong> stat.AP, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Creating offensive advantages during open play is fundamental to football success. However, due to the highly dynamic and long-sequence nature of open play, the potential tactic space grows exponentially as the sequence progresses, making automated tactic discovery extremely challenging. To address this, we propose TacEleven, a generative framework for football open-play tactic discovery developed in close collaboration with domain experts from AJ Auxerre, designed to assist coaches and analysts in tactical decision-making. TacEleven consists of two core components: a language-controlled tactical generator that produces diverse tactical proposals, and a multimodal large language model-based tactical critic that selects the optimal proposal aligned with a high-level stylistic tactical instruction. The two components enables rapid exploration of tactical proposals and discovery of alternative open-play offensive tactics. We evaluate TacEleven across three tasks with progressive tactical complexity: counterfactual exploration, single-step discovery, and multi-step discovery, through both quantitative metrics and a questionnaire-based qualitative assessment. The results show that the TacEleven-discovered tactics exhibit strong realism and tactical creativity, with 52.50% of the multi-step tactical alternatives rated adoptable in real-world elite football scenarios, highlighting the framework's ability to rapidly generate numerous high-quality tactics for complex long-sequence open-play situations. TacEleven demonstrates the potential of creatively leveraging domain data and generative models to advance tactical analysis in sports.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13322v1" target="_blank"><h2>Explainable RL Policies by Distilling to Locally-Specialized Linear Policies with Voronoi State Partitioning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Senne Deproost, Dennis Steckelmacher, Ann Nowé<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> Accepted for BNAIC/BeNeLearn 2025<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> explainable (title, abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Deep Reinforcement Learning is one of the state-of-the-art methods for producing near-optimal system controllers. However, deep RL algorithms train a deep neural network, that lacks transparency, which poses challenges when the controller has to meet regulations, or foster trust. To alleviate this, one could transfer the learned behaviour into a model that is human-readable by design using knowledge distilla- tion. Often this is done with a single model which mimics the original model on average but could struggle in more dynamic situations. A key challenge is that this simpler model should have the right balance be- tween flexibility and complexity or right balance between balance bias and accuracy. We propose a new model-agnostic method to divide the state space into regions where a simplified, human-understandable model can operate in. In this paper, we use Voronoi partitioning to find regions where linear models can achieve similar performance to the original con- troller. We evaluate our approach on a gridworld environment and a classic control task. We observe that our proposed distillation to locally- specialized linear models produces policies that are explainable and show that the distillation matches or even slightly outperforms the black-box policy they are distilled from.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13315v1" target="_blank"><h2>Computer Vision based group activity detection and action spotting <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Narthana Sivalingam, Santhirarajah Sivasthigan, Thamayanthi Mahendranathan, G. M. R. I. Godaliyadda, M. P. B. Ekanayake, H. M. V. R. Herath<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Group activity detection in multi-person scenes is challenging due to complex human interactions, occlusions, and variations in appearance over time. This work presents a computer vision based framework for group activity recognition and action spotting using a combination of deep learning models and graph based relational reasoning. The system first applies Mask R-CNN to obtain accurate actor localization through bounding boxes and instance masks. Multiple backbone networks, including Inception V3, MobileNet, and VGG16, are used to extract feature maps, and RoIAlign is applied to preserve spatial alignment when generating actor specific features. The mask information is then fused with the feature maps to obtain refined masked feature representations for each actor. To model interactions between individuals, we construct Actor Relation Graphs that encode appearance similarity and positional relations using methods such as normalized cross correlation, sum of absolute differences, and dot product. Graph Convolutional Networks operate on these graphs to reason about relationships and predict both individual actions and group level activities. Experiments on the Collective Activity dataset demonstrate that the combination of mask based feature refinement, robust similarity search, and graph neural network reasoning leads to improved recognition performance across both crowded and non crowded scenarios. This approach highlights the potential of integrating segmentation, feature extraction, and relational graph reasoning for complex video understanding tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13314v1" target="_blank"><h2>Unveiling the nature of the Einstein Probe transient EP 241021a <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> J. Quirola-Vásquez, P. G. Jonker, A. J. Levan, D. B. Malesani, F. E. Bauer, N. Sarin, G. P. Lamb, A. Martin-Carrillo, J. Sánchez-Sierras, M. Fraser, L. Izzo, M. E. Ravasio, D. Mata Sánchez, M. A. P. Torres, J. N. D. van Dalen, A. P. C. van Hoof, J. A. Chacón, S. Littlefair, V. S. Dhillon, L. Cotter, G. Corcoran, R. A. J. Eyles-Ferris, P. T. O'Brien, D. Stern, V. D'Elia, D. H. Hartmann<br><strong><u>Categories:</u></strong> astro-ph.HE<br><strong><u>Comments:</u></strong> 21 pages, 15 Figures (+3 in an Appendix), accepted for publication in MNRAS<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> We present a multi-wavelength analysis of the fast X-ray transient EP 241021a, discovered by the Wide-field X-ray Telescope aboard the \emph{Einstein Probe} satellite on 2024 October 21. The event was not detected in gamma-rays. Follow-up observations from $\sim$1.5 to 100 days post-trigger were obtained across X-ray, UV, optical, near-infrared, and radio bands with ground- and space-based facilities. The redshift is constrained to $z = 0.7485$ from prominent optical spectral features. The optical light curve shows complex evolution: an initial $\sim t^{-0.7}$ decay, followed by a rapid re-brightening peaking at day 7.7 with $\sim t^{-1.7}$ decay, and a third phase peaking near day 19 with $\sim t^{-1.3}$ decay. The spectral energy distribution (SED) and its temporal evolution are consistent with a mix of non-thermal and thermal components. Early optical-to-X-ray spectral indices agree with optically thin synchrotron emission, while steepening of the optical SED after $\sim$20 days indicates either a shift in emission mechanism or the emergence of an additional component. Although broad-lined absorption features are absent, comparisons with type Ic-BL supernovae suggest a SN contribution at late times, suggesting a collapsar origin for EP 241021a. The likely SN in EP 241021a appears to require an additional energy source beyond $^{56}$Ni decay. These results support the view that some fast X-ray transients detected by the \emph{Einstein Probe} arise from massive stellar explosions.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13295v1" target="_blank"><h2>Causal Inference, Biomarker Discovery, Graph Neural Network, Feature Selection <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Chaowang Lan, Jingxin Wu, Yulong Yuan, Chuxun Liu, Huangyi Kang, Caihua Liu<br><strong><u>Categories:</u></strong> q-bio.QM, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> Biomarker discovery from high-throughput transcriptomic data is crucial for advancing precision medicine. However, existing methods often neglect gene-gene regulatory relationships and lack stability across datasets, leading to conflation of spurious correlations with genuine causal effects. To address these issues, we develop a causal graph neural network (Causal-GNN) method that integrates causal inference with multi-layer graph neural networks (GNNs). The key innovation is the incorporation of causal effect estimation for identifying stable biomarkers, coupled with a GNN-based propensity scoring mechanism that leverages cross-gene regulatory networks. Experimental results demonstrate that our method achieves consistently high predictive accuracy across four distinct datasets and four independent classifiers. Moreover, it enables the identification of more stable biomarkers compared to traditional methods. Our work provides a robust, efficient, and biologically interpretable tool for biomarker discovery, demonstrating strong potential for broad application across medical disciplines.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13265v1" target="_blank"><h2>TransFit-CSM: A Fast, Physically Consistent Framework for Interaction-Powered Transients <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Yu-Hao Zhang, Liang-Duan Liu, Ze-Xin Du, Guang-Lei Wu, Jing-Yao Li, Yun-Wei Yu<br><strong><u>Categories:</u></strong> astro-ph.HE<br><strong><u>Comments:</u></strong> 19 pages 10 figures<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> time-domain (abstract)<br><p><strong><u>Abstract:</u></strong> We present TransFit-CSM, a fast and physically consistent framework for modeling interaction-powered transients. The method self-consistently couples the ejecta circumstellar medium (CSM) shock dynamics to radiative diffusion from a moving heating boundary tied to the shocks, so that both the photon escape path and the effective diffusion time evolve with radius and time. We solve the mass and momentum equations for the forward and reverse shocks together with the diffusion equation in the unshocked CSM. TransFit-CSM reproduces the canonical sequence of an early dark phase, a diffusion-mediated rise and peak, and a post-interaction cooling tail, and it clarifies why Arnett-like peak scalings break down in optically thick CSM. The framework is well suited for Bayesian inference and constrains physical parameters of the ejecta and CSM from bolometric or joint multi-band light curves. Applications to SN 2006gy and SN 2010jl yield accurate fits and physically interpretable posteriors, highlighting the dominant role of pre-supernova mass loss in shaping the observables. Because it is both computationally efficient and physically grounded, TransFit-CSM bridges simple analytic prescriptions and radiation-hydrodynamic simulations, enabling population-level inference for current and future time-domain surveys.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13259v1" target="_blank"><h2>GeoX-Bench: Benchmarking Cross-View Geo-Localization and Pose Estimation Capabilities of Large Multimodal Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yushuo Zheng, Jiangyong Ying, Huiyu Duan, Chunyi Li, Zicheng Zhang, Jing Liu, Xiaohong Liu, Guangtao Zhai<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Large multimodal models (LMMs) have demonstrated remarkable capabilities across a wide range of tasks, however their knowledge and abilities in the cross-view geo-localization and pose estimation domains remain unexplored, despite potential benefits for navigation, autonomous driving, outdoor robotics, \textit{etc}. To bridge this gap, we introduce \textbf{GeoX-Bench}, a comprehensive \underline{Bench}mark designed to explore and evaluate the capabilities of LMMs in \underline{cross}-view \underline{Geo}-localization and pose estimation. Specifically, GeoX-Bench contains 10,859 panoramic-satellite image pairs spanning 128 cities in 49 countries, along with corresponding 755,976 question-answering (QA) pairs. Among these, 42,900 QA pairs are designated for benchmarking, while the remaining are intended to enhance the capabilities of LMMs. Based on GeoX-Bench, we evaluate the capabilities of 25 state-of-the-art LMMs on cross-view geo-localization and pose estimation tasks, and further explore the empowered capabilities of instruction-tuning. Our benchmark demonstrate that while current LMMs achieve impressive performance in geo-localization tasks, their effectiveness declines significantly on the more complex pose estimation tasks, highlighting a critical area for future improvement, and instruction-tuning LMMs on the training data of GeoX-Bench can significantly improve the cross-view geo-sense abilities. The GeoX-Bench is available at \textcolor{magenta}{https://github.com/IntMeGroup/GeoX-Bench}.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13243v1" target="_blank"><h2>Uncovering and Mitigating Transient Blindness in Multimodal Model Editing <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Xiaoqi Han, Ru Li, Ran Yi, Hongye Tan, Zhuomin Liang, Víctor Gutiérrez-Basulto, Jeff Z. Pan<br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.CV<br><strong><u>Comments:</u></strong> Accepted at AAAI'26<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Multimodal Model Editing (MMED) aims to correct erroneous knowledge in multimodal models. Existing evaluation methods, adapted from textual model editing, overstate success by relying on low-similarity or random inputs, obscure overfitting. We propose a comprehensive locality evaluation framework, covering three key dimensions: random-image locality, no-image locality, and consistent-image locality, operationalized through seven distinct data types, enabling a detailed and structured analysis of multimodal edits. We introduce De-VQA, a dynamic evaluation for visual question answering, uncovering a phenomenon we term transient blindness, overfitting to edit-similar text while ignoring visuals. Token analysis shows edits disproportionately affect textual tokens. We propose locality-aware adversarial losses to balance cross-modal representations. Empirical results demonstrate that our approach consistently outperforms existing baselines, reducing transient blindness and improving locality by 17% on average.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13238v1" target="_blank"><h2>Computational Measurement of Political Positions: A Review of Text-Based Ideal Point Estimation Algorithms <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Patrick Parschan, Charlott Jakob<br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.CL, cs.CY<br><strong><u>Comments:</u></strong> 46 pages, 8 figures, 2 tables, accepted for publication in Quality & Quantity<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> literature review (abstract)<br><p><strong><u>Abstract:</u></strong> This article presents the first systematic review of unsupervised and semi-supervised computational text-based ideal point estimation (CT-IPE) algorithms, methods designed to infer latent political positions from textual data. These algorithms are widely used in political science, communication, computational social science, and computer science to estimate ideological preferences from parliamentary speeches, party manifestos, and social media. Over the past two decades, their development has closely followed broader NLP trends -- beginning with word-frequency models and most recently turning to large language models (LLMs). While this trajectory has greatly expanded the methodological toolkit, it has also produced a fragmented field that lacks systematic comparison and clear guidance for applied use. To address this gap, we identified 25 CT-IPE algorithms through a systematic literature review and conducted a manual content analysis of their modeling assumptions and development contexts. To compare them meaningfully, we introduce a conceptual framework that distinguishes how algorithms generate, capture, and aggregate textual variance. On this basis, we identify four methodological families -- word-frequency, topic modeling, word embedding, and LLM-based approaches -- and critically assess their assumptions, interpretability, scalability, and limitations. Our review offers three contributions. First, it provides a structured synthesis of two decades of algorithm development, clarifying how diverse methods relate to one another. Second, it translates these insights into practical guidance for applied researchers, highlighting trade-offs in transparency, technical requirements, and validation strategies that shape algorithm choice. Third, it emphasizes that differences in estimation outcomes across algorithms are themselves informative, underscoring the need for systematic benchmarking.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13237v1" target="_blank"><h2>Counterfactual Explainable AI (XAI) Method for Deep Learning-Based Multivariate Time Series Classification <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Alan G. Paredes Cetina, Kaouther Benguessoum, Raoni Lourenço, Sylvain Kubler<br><strong><u>Categories:</u></strong> cs.LG, stat.ML<br><strong><u>Comments:</u></strong> Accepted in AAAI 2026 Technical Main Track<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> explainable (title, abstract)<br><p><strong><u>Abstract:</u></strong> Recent advances in deep learning have improved multivariate time series (MTS) classification and regression by capturing complex patterns, but their lack of transparency hinders decision-making. Explainable AI (XAI) methods offer partial insights, yet often fall short of conveying the full decision space. Counterfactual Explanations (CE) provide a promising alternative, but current approaches typically prioritize either accuracy, proximity or sparsity -- rarely all -- limiting their practical value. To address this, we propose CONFETTI, a novel multi-objective CE method for MTS. CONFETTI identifies key MTS subsequences, locates a counterfactual target, and optimally modifies the time series to balance prediction confidence, proximity and sparsity. This method provides actionable insights with minimal changes, improving interpretability, and decision support. CONFETTI is evaluated on seven MTS datasets from the UEA archive, demonstrating its effectiveness in various domains. CONFETTI consistently outperforms state-of-the-art CE methods in its optimization objectives, and in six other metrics from the literature, achieving $\geq10\%$ higher confidence while improving sparsity in $\geq40\%$.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13224v1" target="_blank"><h2>Natural gradient descent for improving variational inference based classification of radio galaxies <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Devina Mohan, Anna M. M. Scaife<br><strong><u>Categories:</u></strong> astro-ph.IM<br><strong><u>Comments:</u></strong> Accepted to Machine Learning and the Physical Sciences Workshop, NeurIPS 2025<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Bayesian neural networks (BNNs) are most commonly optimised with first-order optimisers such as stochastic gradient descent. However, when optimising for parameters of probabilistic models, incorporating second order information during optimisation can lead to a more direct path in the distribution space and faster convergence. In this work we examine whether using natural gradient descent can improve the performance of variational inference based classification of radio galaxies. We use the Improved Variational Online Newton (iVON) algorithm and compare its performance against a recent benchmark for BNNs for radio galaxy classification. We find that iVON results in better uncertainty calibration out of all the methods previously considered while providing similar predictive performance to the best performing inference methods such as Hamiltonian Monte Carlo and Bayes by Backprop based variational inference. Models trained with iVON can distinguish far out-of-distribution optical galaxy data, but they cannot reliably detect radio galaxy images from a telescope with different resolution and sensitivity. We find that the cold posterior effect persists in the models trained with iVON. Our results suggest that the choice of the optimiser can lead to qualitatively different solutions and future work using probabilistic neural network models should carefully consider the inductive biases being encoded through the optimisation process, in addition to the data, architecture and inference method.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13221v1" target="_blank"><h2>Likelihood-guided Regularization in Attention Based Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Mohamed Salem, Inyoung Kim<br><strong><u>Categories:</u></strong> stat.ML, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> The transformer architecture has demonstrated strong performance in classification tasks involving structured and high-dimensional data. However, its success often hinges on large- scale training data and careful regularization to prevent overfitting. In this paper, we intro- duce a novel likelihood-guided variational Ising-based regularization framework for Vision Transformers (ViTs), which simultaneously enhances model generalization and dynamically prunes redundant parameters. The proposed variational Ising-based regularization approach leverages Bayesian sparsification techniques to impose structured sparsity on model weights, allowing for adaptive architecture search during training. Unlike traditional dropout-based methods, which enforce fixed sparsity patterns, the variational Ising-based regularization method learns task-adaptive regularization, improving both efficiency and interpretability. We evaluate our approach on benchmark vision datasets, including MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100, demonstrating improved generalization under sparse, complex data and allowing for principled uncertainty quantification on both weights and selection parameters. Additionally, we show that the Ising regularizer leads to better-calibrated probability estimates and structured feature selection through uncertainty-aware attention mechanisms. Our results highlight the effectiveness of structured Bayesian sparsification in enhancing transformer-based architectures, offering a principled alternative to standard regularization techniques.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13214v1" target="_blank"><h2>Learning to Solve Resource-Constrained Project Scheduling Problems with Duration Uncertainty using Graph Neural Networks <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Guillaume Infantes, Stéphanie Roussel, Antoine Jacquet, Emmanuel Benazera<br><strong><u>Categories:</u></strong> cs.AI, cs.LG<br><strong><u>Comments:</u></strong> Accepted at ICTAI 2025 Conference<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> The Resource-Constrained Project Scheduling Problem (RCPSP) is a classical scheduling problem that has received significant attention due to of its numerous applications in industry. However, in practice, task durations are subject to uncertainty that must be considered in order to propose resilient scheduling. In this paper, we address the RCPSP variant with uncertain tasks duration (modeled using known probabilities) and aim to minimize the overall expected project duration. Our objective is to produce a baseline schedule that can be reused multiple times in an industrial setting regardless of the actual duration scenario. We leverage Graph Neural Networks in conjunction with Deep Reinforcement Learning (DRL) to develop an effective policy for task scheduling. This policy operates similarly to a priority dispatch rule and is paired with a Serial Schedule Generation Scheme to produce a schedule. Our empirical evaluation on standard benchmarks demonstrates the approach's superiority in terms of performance and its ability to generalize. The developed framework, Wheatley, is made publicly available online to facilitate further research and reproducibility.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13198v1" target="_blank"><h2>ParaDySe: A Parallel-Strategy Switching Framework for Dynamic Sequence Lengths in Transformer <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zhixin Ou, Peng Liang, Jianchen Han, Baihui Liu, Linbo Qiao<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> Dynamic sequences with varying lengths have been widely used in the training of Transformer-based large language models (LLMs). However, current training frameworks adopt a pre-defined static parallel strategy for these sequences, causing neither communication-parallelization cancellation on short sequences nor out-of-memory on long sequences. To mitigate these issues, we propose ParaDySe, a novel adaptive Parallel strategy switching framework for Dynamic Sequences. ParaDySe enables on-the-fly optimal strategy adoption according to the immediate input sequence. It first implements the modular function libraries for parallel strategies with unified tensor layout specifications, and then builds sequence-aware memory and time cost models with hybrid methods. Guided by cost models, ParaDySe selects optimal layer-wise strategies for dynamic sequences via an efficient heuristic algorithm. By integrating these techniques together, ParaDySe achieves seamless hot-switching of optimal strategies through its well-designed function libraries. We compare ParaDySe with baselines on representative LLMs under datasets with sequence lengths up to 624K. Experimental results indicate that ParaDySe addresses OOM and CPC bottlenecks in LLM training by systematically integrating long-sequence optimizations with existing frameworks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13186v1" target="_blank"><h2>DiffFP: Learning Behaviors from Scratch via Diffusion-based Fictitious Play <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Akash Karthikeyan, Yash Vardhan Pant<br><strong><u>Categories:</u></strong> cs.LG, eess.SY<br><strong><u>Comments:</u></strong> Initial results presented at the IJCAI 2025 Workshop on User-Aligned Assessment of Adaptive AI Systems. Project page:this https URL<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Self-play reinforcement learning has demonstrated significant success in learning complex strategic and interactive behaviors in competitive multi-agent games. However, achieving such behaviors in continuous decision spaces remains challenging. Ensuring adaptability and generalization in self-play settings is critical for achieving competitive performance in dynamic multi-agent environments. These challenges often cause methods to converge slowly or fail to converge at all to a Nash equilibrium, making agents vulnerable to strategic exploitation by unseen opponents. To address these challenges, we propose DiffFP, a fictitious play (FP) framework that estimates the best response to unseen opponents while learning a robust and multimodal behavioral policy. Specifically, we approximate the best response using a diffusion policy that leverages generative modeling to learn adaptive and diverse strategies. Through empirical evaluation, we demonstrate that the proposed FP framework converges towards $ε$-Nash equilibria in continuous- space zero-sum games. We validate our method on complex multi-agent environments, including racing and multi-particle zero-sum games. Simulation results show that the learned policies are robust against diverse opponents and outperform baseline reinforcement learning policies. Our approach achieves up to 3$\times$ faster convergence and 30$\times$ higher success rates on average against RL-based baselines, demonstrating its robustness to opponent strategies and stability across training iterations</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13185v1" target="_blank"><h2>Uncertainty-aware Physics-informed Neural Networks for Robust CARS-to-Raman Signal Reconstruction <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Aishwarya Venkataramanan, Sai Karthikeya Vemuri, Adithya Ashok Chalain Valapil, Joachim Denzler<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> EurIPS DiffSys workshop 2025<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (title)<br><p><strong><u>Abstract:</u></strong> Coherent anti-Stokes Raman scattering (CARS) spectroscopy is a powerful and rapid technique widely used in medicine, material science, and chemical analyses. However, its effectiveness is hindered by the presence of a non-resonant background that interferes with and distorts the true Raman signal. Deep learning methods have been employed to reconstruct the true Raman spectrum from measured CARS data using labeled datasets. A more recent development integrates the domain knowledge of Kramers-Kronig relationships and smoothness constraints in the form of physics-informed loss functions. However, these deterministic models lack the ability to quantify uncertainty, an essential feature for reliable deployment in high-stakes scientific and biomedical applications. In this work, we evaluate and compare various uncertainty quantification (UQ) techniques within the context of CARS-to-Raman signal reconstruction. Furthermore, we demonstrate that incorporating physics-informed constraints into these models improves their calibration, offering a promising path toward more trustworthy CARS data analysis.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13178v1" target="_blank"><h2>Real-time distortion prediction in metallic additive manufacturing via a physics-informed neural operator approach <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Mingxuan Tian, Haochen Mu, Donghong Ding, Mengjiao Li, Yuhan Ding, Jianping Zhao<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> With the development of digital twins and smart manufacturing systems, there is an urgent need for real-time distortion field prediction to control defects in metal Additive Manufacturing (AM). However, numerical simulation methods suffer from high computational cost, long run-times that prevent real-time use, while conventional Machine learning (ML) models struggle to extract spatiotemporal features for long-horizon prediction and fail to decouple thermo-mechanical fields. This paper proposes a Physics-informed Neural Operator (PINO) to predict z and y-direction distortion for the future 15 s. Our method, Physics-informed Deep Operator Network-Recurrent Neural Network (PIDeepONet-RNN) employs trunk and branch network to process temperature history and encode distortion fields, respectively, enabling decoupling of thermo-mechanical responses. By incorporating the heat conduction equation as a soft constraint, the model ensures physical consistency and suppresses unphysical artifacts, thereby establishing a more physically consistent mapping between the thermal history and distortion. This is important because such a basis function, grounded in physical laws, provides a robust and interpretable foundation for predictions. The proposed models are trained and tested using datasets generated from experimentally validated Finite Element Method (FEM). Evaluation shows that the model achieves high accuracy, low error accumulation, time efficiency. The max absolute errors in the z and y-directions are as low as 0.9733 mm and 0.2049 mm, respectively. The error distribution shows high errors in the molten pool but low gradient norms in the deposited and key areas. The performance of PINO surrogate model highlights its potential for real-time long-horizon physics field prediction in controlling defects.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13174v1" target="_blank"><h2>Warm-starting active-set solvers using graph neural networks <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Ella J. Schmidtobreick, Daniel Arnström, Paul Häusner, Jens Sjölund<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> Under review, 15 pages, 8 figures<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> Quadratic programming (QP) solvers are widely used in real-time control and optimization, but their computational cost often limits applicability in time-critical settings. We propose a learning-to-optimize approach using graph neural networks (GNNs) to predict active sets in the dual active-set solver DAQP. The method exploits the structural properties of QPs by representing them as bipartite graphs and learning to identify the optimal active set for efficiently warm-starting the solver. Across varying problem sizes, the GNN consistently reduces the number of solver iterations compared to cold-starting, while performance is comparable to a multilayer perceptron (MLP) baseline. Furthermore, a GNN trained on varying problem sizes generalizes effectively to unseen dimensions, demonstrating flexibility and scalability. These results highlight the potential of structure-aware learning to accelerate optimization in real-time applications such as model predictive control.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13168v1" target="_blank"><h2>SOMA: Feature Gradient Enhanced Affine-Flow Matching for SAR-Optical Registration <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Haodong Wang, Tao Zhuo, Xiuwei Zhang, Hanlin Yin, Wencong Wu, Yanning Zhang<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Achieving pixel-level registration between SAR and optical images remains a challenging task due to their fundamentally different imaging mechanisms and visual characteristics. Although deep learning has achieved great success in many cross-modal tasks, its performance on SAR-Optical registration tasks is still unsatisfactory. Gradient-based information has traditionally played a crucial role in handcrafted descriptors by highlighting structural differences. However, such gradient cues have not been effectively leveraged in deep learning frameworks for SAR-Optical image matching. To address this gap, we propose SOMA, a dense registration framework that integrates structural gradient priors into deep features and refines alignment through a hybrid matching strategy. Specifically, we introduce the Feature Gradient Enhancer (FGE), which embeds multi-scale, multi-directional gradient filters into the feature space using attention and reconstruction mechanisms to boost feature distinctiveness. Furthermore, we propose the Global-Local Affine-Flow Matcher (GLAM), which combines affine transformation and flow-based refinement within a coarse-to-fine architecture to ensure both structural consistency and local accuracy. Experimental results demonstrate that SOMA significantly improves registration precision, increasing the CMR@1px by 12.29% on the SEN1-2 dataset and 18.50% on the GFGE_SO dataset. In addition, SOMA exhibits strong robustness and generalizes well across diverse scenes and resolutions.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13160v1" target="_blank"><h2>InteractiveGNNExplainer: A Visual Analytics Framework for Multi-Faceted Understanding and Probing of Graph Neural Network Predictions <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> TC Singh, Sougata Mukherjea<br><strong><u>Categories:</u></strong> cs.AI, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> explainability (abstract), neural network (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Graph Neural Networks (GNNs) excel in graph-based learning tasks, but their complex, non-linear operations often render them as opaque "black boxes". This opacity hinders user trust, complicates debugging, bias detection, and adoption in critical domains requiring explainability. This paper introduces InteractiveGNNExplainer, a visual analytics framework to enhance GNN explainability, focusing on node classification. Our system uniquely integrates coordinated interactive views (dynamic graph layouts, embedding projections, feature inspection, neighborhood analysis) with established post-hoc (GNNExplainer) and intrinsic (GAT attention) explanation techniques. Crucially, it incorporates interactive graph editing, allowing users to perform a "what-if" analysis by perturbing graph structures and observing immediate impacts on GNN predictions and explanations. We detail the system architecture and, through case studies on Cora and CiteSeer datasets, demonstrate how InteractiveGNNExplainer facilitates in-depth misclassification diagnosis, comparative analysis of GCN versus GAT behaviors, and rigorous probing of model sensitivity. These capabilities foster a deeper, multifaceted understanding of GNN predictions, contributing to more transparent, trustworthy, and robust graph analysis.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13154v1" target="_blank"><h2>Towards an anomaly detection pipeline for gravitational waves at the Einstein Telescope <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Gianluca Inguglia, Huw Haigh, Kristyna Vitulova, Ulyana Dupletsa<br><strong><u>Categories:</u></strong> gr-qc, astro-ph.IM<br><strong><u>Comments:</u></strong> 10 pages, 12 figures<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), anomaly detection (title, abstract)<br><p><strong><u>Abstract:</u></strong> We present the implementation of an anomaly-detection algorithm based on a deep convolutional autoencoder for the search for gravitational waves (GWs) in time-frequency spectrograms. Our method targets short-duration ($\lesssim 2\,\text{s}$) GW signals, exemplified by mergers of compact objects forming or involving an intermediate-mass black hole (IMBH). Such short signals are difficult to distinguish from background noise; yet their brevity makes them well-suited to machine-learning analyses with modest computational requirements. Using the data from the Einstein Telescope Mock Data Challenge as a benchmark, we demonstrate that the approach can successfully flag GW-like transients as anomalies in interferometer data of a single detector, achieving an initial detection efficiency of 23% for injected signals corresponding to IMBH-forming mergers. After introducing weak supervision, the model exhibits excellent generalisation and recovers all injected IMBH-forming mergers, independent of their total mass or signal-to-noise ratio, with a false-alarm rate due to statistical noise fluctuations of approximately 4.5 events per year for a single interferometer operating with a 100% duty cycle. The method also successfully identifies lower-mass mergers leading to the formation of black holes with mass larger than $\simeq 20\,M_\odot$. Our pipeline does not yet classify anomalies, distinguishing between actual GW signals and noise artefacts; however, it highlights any deviation from the learned background noise distribution for further scrutiny. These results demonstrate that anomaly detection offers a powerful, model-independent framework for future GW searches, paving the way toward fully automated and adaptive analysis pipelines.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13145v1" target="_blank"><h2>Automated Road Distress Detection Using Vision Transformersand Generative Adversarial Networks <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Cesar Portocarrero Rodriguez, Laura Vandeweyen, Yosuke Yamamoto<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract), transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> The American Society of Civil Engineers has graded Americas infrastructure condition as a C, with the road system receiving a dismal D. Roads are vital to regional economic viability, yet their management, maintenance, and repair processes remain inefficient, relying on outdated manual or laser-based inspection methods that are both costly and time-consuming. With the increasing availability of real-time visual data from autonomous vehicles, there is an opportunity to apply computer vision (CV) methods for advanced road monitoring, providing insights to guide infrastructure rehabilitation efforts. This project explores the use of state-of-the-art CV techniques for road distress segmentation. It begins by evaluating synthetic data generated with Generative Adversarial Networks (GANs) to assess its usefulness for model training. The study then applies Convolutional Neural Networks (CNNs) for road distress segmentation and subsequently examines the transformer-based model MaskFormer. Results show that GAN-generated data improves model performance and that MaskFormer outperforms the CNN model in two metrics: mAP50 and IoU.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13143v1" target="_blank"><h2>SoK: The Last Line of Defense: On Backdoor Defense Evaluation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Gorka Abad, Marina Krček, Stefanos Koffas, Behrad Tajalli, Marco Arazzi, Roberto Riaño, Xiaoyun Xu, Zhuoran Liu, Antonino Nocera, Stjepan Picek<br><strong><u>Categories:</u></strong> cs.CR, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> literature review (abstract)<br><p><strong><u>Abstract:</u></strong> Backdoor attacks pose a significant threat to deep learning models by implanting hidden vulnerabilities that can be activated by malicious inputs. While numerous defenses have been proposed to mitigate these attacks, the heterogeneous landscape of evaluation methodologies hinders fair comparison between defenses. This work presents a systematic (meta-)analysis of backdoor defenses through a comprehensive literature review and empirical evaluation. We analyzed 183 backdoor defense papers published between 2018 and 2025 across major AI and security venues, examining the properties and evaluation methodologies of these defenses.
  Our analysis reveals significant inconsistencies in experimental setups, evaluation metrics, and threat model assumptions in the literature. Through extensive experiments involving three datasets (MNIST, CIFAR-100, ImageNet-1K), four model architectures (ResNet-18, VGG-19, ViT-B/16, DenseNet-121), 16 representative defenses, and five commonly used attacks, totaling over 3\,000 experiments, we demonstrate that defense effectiveness varies substantially across different evaluation setups. We identify critical gaps in current evaluation practices, including insufficient reporting of computational overhead and behavior under benign conditions, bias in hyperparameter selection, and incomplete experimentation. Based on our findings, we provide concrete challenges and well-motivated recommendations to standardize and improve future defense evaluations. Our work aims to equip researchers and industry practitioners with actionable insights for developing, assessing, and deploying defenses to different systems.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13137v1" target="_blank"><h2>Conditional Diffusion Model for Multi-Agent Dynamic Task Decomposition <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yanda Zhu, Yuanyang Zhu, Daoyi Dong, Caihua Chen, Chunlin Chen<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> AAAI 2026<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Task decomposition has shown promise in complex cooperative multi-agent reinforcement learning (MARL) tasks, which enables efficient hierarchical learning for long-horizon tasks in dynamic and uncertain environments. However, learning dynamic task decomposition from scratch generally requires a large number of training samples, especially exploring the large joint action space under partial observability. In this paper, we present the Conditional Diffusion Model for Dynamic Task Decomposition (C$\text{D}^\text{3}$T), a novel two-level hierarchical MARL framework designed to automatically infer subtask and coordination patterns. The high-level policy learns subtask representation to generate a subtask selection strategy based on subtask effects. To capture the effects of subtasks on the environment, C$\text{D}^\text{3}$T predicts the next observation and reward using a conditional diffusion model. At the low level, agents collaboratively learn and share specialized skills within their assigned subtasks. Moreover, the learned subtask representation is also used as additional semantic information in a multi-head attention mixing network to enhance value decomposition and provide an efficient reasoning bridge between individual and joint value functions. Experimental results on various benchmarks demonstrate that C$\text{D}^\text{3}$T achieves better performance than existing baselines.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13133v1" target="_blank"><h2>Soft Conflict-Resolution Decision Transformer for Offline Multi-Task Reinforcement Learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Shudong Wang, Xinfei Wang, Chenhao Zhang, Shanchen Pang, Haiyuan Gui, Wenhao Ji, Xiaojian Liao<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> transformer (title)<br><p><strong><u>Abstract:</u></strong> Multi-task reinforcement learning (MTRL) seeks to learn a unified policy for diverse tasks, but often suffers from gradient conflicts across tasks. Existing masking-based methods attempt to mitigate such conflicts by assigning task-specific parameter masks. However, our empirical study shows that coarse-grained binary masks have the problem of over-suppressing key conflicting parameters, hindering knowledge sharing across tasks. Moreover, different tasks exhibit varying conflict levels, yet existing methods use a one-size-fits-all fixed sparsity strategy to keep training stability and performance, which proves inadequate. These limitations hinder the model's generalization and learning efficiency.
  To address these issues, we propose SoCo-DT, a Soft Conflict-resolution method based by parameter importance. By leveraging Fisher information, mask values are dynamically adjusted to retain important parameters while suppressing conflicting ones. In addition, we introduce a dynamic sparsity adjustment strategy based on the Interquartile Range (IQR), which constructs task-specific thresholding schemes using the distribution of conflict and harmony scores during training. To enable adaptive sparsity evolution throughout training, we further incorporate an asymmetric cosine annealing schedule to continuously update the threshold. Experimental results on the Meta-World benchmark show that SoCo-DT outperforms the state-of-the-art method by 7.6% on MT50 and by 10.5% on the suboptimal dataset, demonstrating its effectiveness in mitigating gradient conflicts and improving overall multi-task performance.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13800v1" target="_blank"><h2>Synergizing Multigrid Algorithms with Vision Transformer: A Novel Approach to Enhance the Seismic Foundation Model <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Huiwen Wu, Shuo Zhang, Yi Liu, Hongbin Ye<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, math.NA<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> Due to the emergency and homogenization of Artificial Intelligence (AI) technology development, transformer-based foundation models have revolutionized scientific applications, such as drug discovery, materials research, and astronomy. However, seismic data presents unique characteristics that require specialized processing techniques for pretraining foundation models in seismic contexts with high- and low-frequency features playing crucial roles. Existing vision transformers (ViTs) with sequential tokenization ignore the intrinsic pattern and fail to grasp both the high- and low-frequency seismic information efficiently and effectively. This work introduces a novel adaptive two-grid foundation model training strategy (ADATG) with Hilbert encoding specifically tailored for seismogram data, leveraging the hierarchical structures inherent in seismic data. Specifically, our approach employs spectrum decomposition to separate high- and low-frequency components and utilizes hierarchical Hilbert encoding to represent the data effectively. Moreover, observing the frequency principle observed in ViTs, we propose an adaptive training strategy that initially emphasizes coarse-level information and then progressively refines the model's focus on fine-level features. Our extensive experiments demonstrate the effectiveness and efficiency of our training methods. This research highlights the importance of data encoding and training strategies informed by the distinct characteristics of high- and low-frequency features in seismic images, ultimately contributing to the enhancement of visual seismic foundation models pretraining.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13131v1" target="_blank"><h2>MM-Telco: Benchmarks and Multimodal Large Language Models for Telecom Applications <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Gagan Raj Gupta, Anshul Kumar, Manish Rai, Apu Chakraborty, Ashutosh Modi, Abdelaali Chaoub, Soumajit Pramanik, Moyank Giri, Yashwanth Holla, Sunny Kumar, M. V. Kiran Sooraj<br><strong><u>Categories:</u></strong> cs.AI, cs.CV, cs.ET, cs.NI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Large Language Models (LLMs) have emerged as powerful tools for automating complex reasoning and decision-making tasks. In telecommunications, they hold the potential to transform network optimization, automate troubleshooting, enhance customer support, and ensure regulatory compliance. However, their deployment in telecom is hindered by domain-specific challenges that demand specialized adaptation. To overcome these challenges and to accelerate the adaptation of LLMs for telecom, we propose MM-Telco, a comprehensive suite of multimodal benchmarks and models tailored for the telecom domain. The benchmark introduces various tasks (both text based and image based) that address various practical real-life use cases such as network operations, network management, improving documentation quality, and retrieval of relevant text and images. Further, we perform baseline experiments with various LLMs and VLMs. The models fine-tuned on our dataset exhibit a significant boost in performance. Our experiments also help analyze the weak areas in the working of current state-of-art multimodal LLMs, thus guiding towards further development and research.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13125v1" target="_blank"><h2>Region-Point Joint Representation for Effective Trajectory Similarity Learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Hao Long, Silin Zhou, Lisi Chen, Shuo Shang<br><strong><u>Categories:</u></strong> cs.CV, cs.IR, cs.LG<br><strong><u>Comments:</u></strong> This paper is accepted by AAAI2026<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Recent learning-based methods have reduced the computational complexity of traditional trajectory similarity computation, but state-of-the-art (SOTA) methods still fail to leverage the comprehensive spectrum of trajectory information for similarity modeling. To tackle this problem, we propose \textbf{RePo}, a novel method that jointly encodes \textbf{Re}gion-wise and \textbf{Po}int-wise features to capture both spatial context and fine-grained moving patterns. For region-wise representation, the GPS trajectories are first mapped to grid sequences, and spatial context are captured by structural features and semantic context enriched by visual features. For point-wise representation, three lightweight expert networks extract local, correlation, and continuous movement patterns from dense GPS sequences. Then, a router network adaptively fuses the learned point-wise features, which are subsequently combined with region-wise features using cross-attention to produce the final trajectory embedding. To train RePo, we adopt a contrastive loss with hard negative samples to provide similarity ranking supervision. Experiment results show that RePo achieves an average accuracy improvement of 22.2\% over SOTA baselines across all evaluation metrics.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13103v1" target="_blank"><h2>Transformer-Based Scalable Multi-Agent Reinforcement Learning for Networked Systems with Long-Range Interactions <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Vidur Sinha, Muhammed Ustaomeroglu, Guannan Qu<br><strong><u>Categories:</u></strong> cs.LG, cs.MA, eess.SY<br><strong><u>Comments:</u></strong> 8 pages, 7 figures, submitted for review<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> Multi-agent reinforcement learning (MARL) has shown promise for large-scale network control, yet existing methods face two major limitations. First, they typically rely on assumptions leading to decay properties of local agent interactions, limiting their ability to capture long-range dependencies such as cascading power failures or epidemic outbreaks. Second, most approaches lack generalizability across network topologies, requiring retraining when applied to new graphs. We introduce STACCA (Shared Transformer Actor-Critic with Counterfactual Advantage), a unified transformer-based MARL framework that addresses both challenges. STACCA employs a centralized Graph Transformer Critic to model long-range dependencies and provide system-level feedback, while its shared Graph Transformer Actor learns a generalizable policy capable of adapting across diverse network structures. Further, to improve credit assignment during training, STACCA integrates a novel counterfactual advantage estimator that is compatible with state-value critic estimates. We evaluate STACCA on epidemic containment and rumor-spreading network control tasks, demonstrating improved performance, network generalization, and scalability. These results highlight the potential of transformer-based MARL architectures to achieve scalable and generalizable control in large-scale networked systems.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13082v1" target="_blank"><h2>Real-time prediction of breast cancer sites using deformation-aware graph neural network <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Kyunghyun Lee, Yong-Min Shin, Minwoo Shin, Jihun Kim, Sunghwan Lim, Won-Yong Shin, Kyungho Yoon<br><strong><u>Categories:</u></strong> cs.LG, cs.CV<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> Early diagnosis of breast cancer is crucial, enabling the establishment of appropriate treatment plans and markedly enhancing patient prognosis. While direct magnetic resonance imaging-guided biopsy demonstrates promising performance in detecting cancer lesions, its practical application is limited by prolonged procedure times and high costs. To overcome these issues, an indirect MRI-guided biopsy that allows the procedure to be performed outside of the MRI room has been proposed, but it still faces challenges in creating an accurate real-time deformable breast model. In our study, we tackled this issue by developing a graph neural network (GNN)-based model capable of accurately predicting deformed breast cancer sites in real time during biopsy procedures. An individual-specific finite element (FE) model was developed by incorporating magnetic resonance (MR) image-derived structural information of the breast and tumor to simulate deformation behaviors. A GNN model was then employed, designed to process surface displacement and distance-based graph data, enabling accurate prediction of overall tissue displacement, including the deformation of the tumor region. The model was validated using phantom and real patient datasets, achieving an accuracy within 0.2 millimeters (mm) for cancer node displacement (RMSE) and a dice similarity coefficient (DSC) of 0.977 for spatial overlap with actual cancerous regions. Additionally, the model enabled real-time inference and achieved a speed-up of over 4,000 times in computational cost compared to conventional FE simulations. The proposed deformation-aware GNN model offers a promising solution for real-time tumor displacement prediction in breast biopsy, with high accuracy and real-time capability. Its integration with clinical procedures could significantly enhance the precision and efficiency of breast cancer diagnosis.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13078v1" target="_blank"><h2>A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Liuyi Jin, Pasan Gunawardena, Amran Haroon, Runzhi Wang, Sangwoo Lee, Radu Stoleru, Michael Middleton, Zepeng Huo, Jeeeun Kim, Jason Moats<br><strong><u>Categories:</u></strong> cs.LG, eess.AS, eess.IV<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Emergency Medical Technicians (EMTs) operate in high-pressure environments, making rapid, life-critical decisions under heavy cognitive and operational loads. We present EMSGlass, a smart-glasses system powered by EMSNet, the first multimodal multitask model for Emergency Medical Services (EMS), and EMSServe, a low-latency multimodal serving framework tailored to EMS scenarios. EMSNet integrates text, vital signs, and scene images to construct a unified real-time understanding of EMS incidents. Trained on real-world multimodal EMS datasets, EMSNet simultaneously supports up to five critical EMS tasks with superior accuracy compared to state-of-the-art unimodal baselines. Built on top of PyTorch, EMSServe introduces a modality-aware model splitter and a feature caching mechanism, achieving adaptive and efficient inference across heterogeneous hardware while addressing the challenge of asynchronous modality arrival in the field. By optimizing multimodal inference execution in EMS scenarios, EMSServe achieves 1.9x -- 11.7x speedup over direct PyTorch multimodal inference. A user study evaluation with six professional EMTs demonstrates that EMSGlass enhances real-time situational awareness, decision-making speed, and operational efficiency through intuitive on-glass interaction. In addition, qualitative insights from the user study provide actionable directions for extending EMSGlass toward next-generation AI-enabled EMS systems, bridging multimodal intelligence with real-world emergency response workflows.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13798v1" target="_blank"><h2>KANGURA: Kolmogorov-Arnold Network-Based Geometry-Aware Learning with Unified Representation Attention for 3D Modeling of Complex Structures <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Mohammad Reza Shafie, Morteza Hajiabadi, Hamed Khosravi, Mobina Noori, Imtiaz Ahmed<br><strong><u>Categories:</u></strong> cs.AI, cs.CV, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Microbial Fuel Cells (MFCs) offer a promising pathway for sustainable energy generation by converting organic matter into electricity through microbial processes. A key factor influencing MFC performance is the anode structure, where design and material properties play a crucial role. Existing predictive models struggle to capture the complex geometric dependencies necessary to optimize these structures. To solve this problem, we propose KANGURA: Kolmogorov-Arnold Network-Based Geometry-Aware Learning with Unified Representation Attention. KANGURA introduces a new approach to three-dimensional (3D) machine learning modeling. It formulates prediction as a function decomposition problem, where Kolmogorov-Arnold Network (KAN)- based representation learning reconstructs geometric relationships without a conventional multi- layer perceptron (MLP). To refine spatial understanding, geometry-disentangled representation learning separates structural variations into interpretable components, while unified attention mechanisms dynamically enhance critical geometric regions. Experimental results demonstrate that KANGURA outperforms over 15 state-of-the-art (SOTA) models on the ModelNet40 benchmark dataset, achieving 92.7% accuracy, and excels in a real-world MFC anode structure problem with 97% accuracy. This establishes KANGURA as a robust framework for 3D geometric modeling, unlocking new possibilities for optimizing complex structures in advanced manufacturing and quality-driven engineering applications.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13071v1" target="_blank"><h2>Orientation-Free Neural Network-Based Bias Estimation for Low-Cost Stationary Accelerometers <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Michal Levin, Itzik Klein<br><strong><u>Categories:</u></strong> cs.RO, cs.LG<br><strong><u>Comments:</u></strong> 22 pages, 10 figures<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (title)<br><p><strong><u>Abstract:</u></strong> Low-cost micro-electromechanical accelerometers are widely used in navigation, robotics, and consumer devices for motion sensing and position estimation. However, their performance is often degraded by bias errors. To eliminate deterministic bias terms a calibration procedure is applied under stationary conditions. It requires accelerom- eter leveling or complex orientation-dependent calibration procedures. To overcome those requirements, in this paper we present a model-free learning-based calibration method that estimates accelerometer bias under stationary conditions, without requiring knowledge of the sensor orientation and without the need to rotate the sensors. The proposed approach provides a fast, practical, and scalable solution suitable for rapid field deployment. Experimental validation on a 13.39-hour dataset collected from six accelerometers shows that the proposed method consistently achieves error levels more than 52% lower than traditional techniques. On a broader scale, this work contributes to the advancement of accurate calibration methods in orientation-free scenarios. As a consequence, it improves the reliability of low-cost inertial sensors in diverse scientific and industrial applications and eliminates the need for leveled calibration.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13062v1" target="_blank"><h2>Self-Adaptive Graph Mixture of Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Mohit Meena, Yash Punjabi, Abhishek A, Vishal Sharma, Mahesh Chandran<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 17 pages, 5 figures<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Graph Neural Networks (GNNs) have emerged as powerful tools for learning over graph-structured data, yet recent studies have shown that their performance gains are beginning to plateau. In many cases, well-established models such as GCN and GAT, when appropriately tuned, can match or even exceed the performance of more complex, state-of-the-art architectures. This trend highlights a key limitation in the current landscape: the difficulty of selecting the most suitable model for a given graph task or dataset. To address this, we propose Self-Adaptive Graph Mixture of Models (SAGMM), a modular and practical framework that learns to automatically select and combine the most appropriate GNN models from a diverse pool of architectures. Unlike prior mixture-of-experts approaches that rely on variations of a single base model, SAGMM leverages architectural diversity and a topology-aware attention gating mechanism to adaptively assign experts to each node based on the structure of the input graph. To improve efficiency, SAGMM includes a pruning mechanism that reduces the number of active experts during training and inference without compromising performance. We also explore a training-efficient variant in which expert models are pretrained and frozen, and only the gating and task-specific layers are trained. We evaluate SAGMM on 16 benchmark datasets covering node classification, graph classification, regression, and link prediction tasks, and demonstrate that it consistently outperforms or matches leading GNN baselines and prior mixture-based methods, offering a robust and adaptive solution for real-world graph learning.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13797v1" target="_blank"><h2>MAT-MPNN: A Mobility-Aware Transformer-MPNN Model for Dynamic Spatiotemporal Prediction of HIV Diagnoses in California, Florida, and New England <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zhaoxuan Wang, Weichen Kang, Yutian Han, Lingyuan Zhao, Bo Li<br><strong><u>Categories:</u></strong> q-bio.QM, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> 21 pages, 20 figures,1 table. Preprint<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> Human Immunodeficiency Virus (HIV) has posed a major global health challenge for decades, and forecasting HIV diagnoses continues to be a critical area of research. However, capturing the complex spatial and temporal dependencies of HIV transmission remains challenging. Conventional Message Passing Neural Network (MPNN) models rely on a fixed binary adjacency matrix that only encodes geographic adjacency, which is unable to represent interactions between non-contiguous counties. Our study proposes a deep learning architecture Mobility-Aware Transformer-Message Passing Neural Network (MAT-MPNN) framework to predict county-level HIV diagnosis rates across California, Florida, and the New England region. The model combines temporal features extracted by a Transformer encoder with spatial relationships captured through a Mobility Graph Generator (MGG). The MGG improves conventional adjacency matrices by combining geographic and demographic information. Compared with the best-performing hybrid baseline, the Transformer MPNN model, MAT-MPNN reduced the Mean Squared Prediction Error (MSPE) by 27.9% in Florida, 39.1% in California, and 12.5% in New England, and improved the Predictive Model Choice Criterion (PMCC) by 7.7%, 3.5%, and 3.9%, respectively. MAT-MPNN also achieved better results than the Spatially Varying Auto-Regressive (SVAR) model in Florida and New England, with comparable performance in California. These results demonstrate that applying mobility-aware dynamic spatial structures substantially enhances predictive accuracy and calibration in spatiotemporal epidemiological prediction.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13057v2" target="_blank"><h2>Dimension vs. Precision: A Comparative Analysis of Autoencoders and Quantization for Efficient Vector Retrieval on BEIR SciFact <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Satyanarayan Pati<br><strong><u>Categories:</u></strong> cs.IR, cs.AI<br><strong><u>Comments:</u></strong> 16 pages, 9 figures, 1 table<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> dimensionality reduction (abstract), latent space (abstract)<br><p><strong><u>Abstract:</u></strong> Dense retrieval models have become a standard for state-of-the-art information retrieval. However, their high-dimensional, high-precision (float32) vector embeddings create significant storage and memory challenges for real-world deployment. To address this, we conduct a rigorous empirical study on the BEIR SciFact benchmark, evaluating the trade-offs between two primary compression strategies: (1) Dimensionality Reduction via deep Autoencoders (AE), reducing original 384-dim vectors to latent spaces from 384 down to 12, and (2) Precision Reduction via Quantization (float16, int8, and binary). We systematically compare each method by measuring the "performance loss" (or gain) relative to a float32 baseline across a full suite of retrieval metrics (NDCG, MAP, MRR, Recall, Precision) at various k cutoffs. Our results show that int8 scalar quantization provides the most effective "sweet spot," achieving a 4x compression with a negligible [~1-2%] drop in nDCG@10. In contrast, Autoencoders show a graceful degradation but suffer a more significant performance loss at equivalent 4x compression ratios (AE-96). binary quantization was found to be unsuitable for this task due to catastrophic performance drops. This work provides a practical guide for deploying efficient, high-performance retrieval systems.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13052v1" target="_blank"><h2>Learning from the Undesirable: Robust Adaptation of Language Models without Forgetting <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yunhun Nam, Jaehyung Kim, Jongheon Jeong<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 17 pages; AAAI 2026; Code is available atthis https URL<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> data augmentation (abstract)<br><p><strong><u>Abstract:</u></strong> Language models (LMs) are often adapted through supervised fine-tuning (SFT) to specialize their capabilities for downstream tasks. However, in typical scenarios where the fine-tuning data is limited, e.g., compared to pre-training, SFT can lead LMs to overfit, causing them to rely on spurious patterns within the target task or to compromise other broadly useful capabilities as a side effect of narrow specialization. In this paper, we propose Learning-from-the-Undesirable (LfU), a simple yet effective regularization scheme for SFT to mitigate overfitting issues when fine-tuning LMs with limited data. Specifically, we aim to regularize the fine-tuning process to favor solutions that are resilient to "undesirable" model updates, e.g., gradient ascent steps that steer the model toward undesirable behaviors. To this end, we propose a novel form of consistency regularization that directly aligns internal representations of the model with those after an undesirable update. By leveraging representation-level data augmentation through undesirable updates, LfU effectively promotes generalization under limited data. Our experiments on diverse LM downstream tasks show that LfU serves as an effective prior that enhances adaptability while preserving pretrained knowledge. For example, our LM from LfU achieves a 16.8% average improvement on math tasks compared to vanilla SFT on the same dataset, where the latter even leads to degraded performance on those tasks. Furthermore, LfU exhibits improved robustness to prompt variations, e.g., yielding a 92.1% lower standard deviation in output performances compared to SFT, highlighting its versatile effects.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13035v1" target="_blank"><h2>One-Step Generative Policies with Q-Learning: A Reformulation of MeanFlow <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zeyuan Wang, Da Li, Yulin Chen, Ye Shi, Liang Bai, Tianyuan Yu, Yanwei Fu<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> Accepted in AAAI 2026 Poster<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> We introduce a one-step generative policy for offline reinforcement learning that maps noise directly to actions via a residual reformulation of MeanFlow, making it compatible with Q-learning. While one-step Gaussian policies enable fast inference, they struggle to capture complex, multimodal action distributions. Existing flow-based methods improve expressivity but typically rely on distillation and two-stage training when trained with Q-learning. To overcome these limitations, we propose to reformulate MeanFlow to enable direct noise-to-action generation by integrating the velocity field and noise-to-action transformation into a single policy network-eliminating the need for separate velocity estimation. We explore several reformulation variants and identify an effective residual formulation that supports expressive and stable policy learning. Our method offers three key advantages: 1) efficient one-step noise-to-action generation, 2) expressive modelling of multimodal action distributions, and 3) efficient and stable policy learning via Q-learning in a single-stage training setup. Extensive experiments on 73 tasks across the OGBench and D4RL benchmarks demonstrate that our method achieves strong performance in both offline and offline-to-online reinforcement learning settings. Code is available at https://github.com/HiccupRL/MeanFlowQL.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13021v1" target="_blank"><h2>PragWorld: A Benchmark Evaluating LLMs' Local World Model under Minimal Linguistic Alterations and Conversational Dynamics <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Sachin Vashistha, Aryan Bibhuti, Atharva Naik, Martin Tutek, Somak Aditya<br><strong><u>Categories:</u></strong> cs.AI, cs.CL<br><strong><u>Comments:</u></strong> 23 pages, 15 tables, 10 figures; AAAI 2026 Conference Main Track (oral)<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Real-world conversations are rich with pragmatic elements, such as entity mentions, references, and implicatures. Understanding such nuances is a requirement for successful natural communication, and often requires building a local world model which encodes such elements and captures the dynamics of their evolving states. However, it is not well-understood whether language models (LMs) construct or maintain a robust implicit representation of conversations. In this work, we evaluate the ability of LMs to encode and update their internal world model in dyadic conversations and test their malleability under linguistic alterations. To facilitate this, we apply seven minimal linguistic alterations to conversations sourced from popular datasets and construct two benchmarks comprising yes-no questions. We evaluate a wide range of open and closed source LMs and observe that they struggle to maintain robust accuracy. Our analysis unveils that LMs struggle to memorize crucial details, such as tracking entities under linguistic alterations to conversations. We then propose a dual-perspective interpretability framework which identifies transformer layers that are useful or harmful and highlights linguistic alterations most influenced by harmful layers, typically due to encoding spurious signals or relying on shortcuts. Inspired by these insights, we propose two layer-regularization based fine-tuning strategies that suppress the effect of the harmful layers.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13020v1" target="_blank"><h2>SpectralAdapt: Semi-Supervised Domain Adaptation with Spectral Priors for Human-Centered Hyperspectral Image Reconstruction <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yufei Wen, Yuting Zhang, Jingdan Kang, Hao Ren, Weibin Cheng, Jintai Chen, Kaishun Wu<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> domain adaptation (title, abstract)<br><p><strong><u>Abstract:</u></strong> Hyperspectral imaging (HSI) holds great potential for healthcare due to its rich spectral information. However, acquiring HSI data remains costly and technically demanding. Hyperspectral image reconstruction offers a practical solution by recovering HSI data from accessible modalities, such as RGB. While general domain datasets are abundant, the scarcity of human HSI data limits progress in medical applications. To tackle this, we propose SpectralAdapt, a semi-supervised domain adaptation (SSDA) framework that bridges the domain gap between general and human-centered HSI datasets. To fully exploit limited labels and abundant unlabeled data, we enhance spectral reasoning by introducing Spectral Density Masking (SDM), which adaptively masks RGB channels based on their spectral complexity, encouraging recovery of informative regions from complementary cues during consistency training. Furthermore, we introduce Spectral Endmember Representation Alignment (SERA), which derives physically interpretable endmembers from valuable labeled pixels and employs them as domain-invariant anchors to guide unlabeled predictions, with momentum updates ensuring adaptability and stability. These components are seamlessly integrated into SpectralAdapt, a spectral prior-guided framework that effectively mitigates domain shift, spectral degradation, and data scarcity in HSI reconstruction. Experiments on benchmark datasets demonstrate consistent improvements in spectral fidelity, cross-domain generalization, and training stability, highlighting the promise of SSDA as an efficient solution for hyperspectral imaging in healthcare.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13019v1" target="_blank"><h2>MeanFlow Transformers with Representation Autoencoders <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zheyuan Hu, Chieh-Hsin Lai, Ge Wu, Yuki Mitsufuji, Stefano Ermon<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> Code is available atthis https URL<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> variational autoencoder (abstract), VAE (abstract), latent space (abstract), transformer (title)<br><p><strong><u>Abstract:</u></strong> MeanFlow (MF) is a diffusion-motivated generative model that enables efficient few-step generation by learning long jumps directly from noise to data. In practice, it is often used as a latent MF by leveraging the pre-trained Stable Diffusion variational autoencoder (SD-VAE) for high-dimensional data modeling. However, MF training remains computationally demanding and is often unstable. During inference, the SD-VAE decoder dominates the generation cost, and MF depends on complex guidance hyperparameters for class-conditional generation. In this work, we develop an efficient training and sampling scheme for MF in the latent space of a Representation Autoencoder (RAE), where a pre-trained vision encoder (e.g., DINO) provides semantically rich latents paired with a lightweight decoder. We observe that naive MF training in the RAE latent space suffers from severe gradient explosion. To stabilize and accelerate training, we adopt Consistency Mid-Training for trajectory-aware initialization and use a two-stage scheme: distillation from a pre-trained flow matching teacher to speed convergence and reduce variance, followed by an optional bootstrapping stage with a one-point velocity estimator to further reduce deviation from the oracle mean flow. This design removes the need for guidance, simplifies training configurations, and reduces computation in both training and sampling. Empirically, our method achieves a 1-step FID of 2.03, outperforming vanilla MF's 3.43, while reducing sampling GFLOPS by 38% and total training cost by 83% on ImageNet 256. We further scale our approach to ImageNet 512, achieving a competitive 1-step FID of 3.23 with the lowest GFLOPS among all baselines. Code is available at https://github.com/sony/mf-rae.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13010v1" target="_blank"><h2>Are Graph Transformers Necessary? Efficient Long-Range Message Passing with Fractal Nodes in MPNNs <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jeongwhan Choi, Seungjun Park, Sumin Park, Sung-Bae Cho, Noseong Park<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> Accepted in AAAI 2026 for Oral Representation. This is the extended version including the appendix<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> Graph Neural Networks (GNNs) have emerged as powerful tools for learning on graph-structured data, but often struggle to balance local and global information. While graph Transformers aim to address this by enabling long-range interactions, they often overlook the inherent locality and efficiency of Message Passing Neural Networks (MPNNs). We propose a new concept called fractal nodes, inspired by the fractal structure observed in real-world networks. Our approach is based on the intuition that graph partitioning naturally induces fractal structure, where subgraphs often reflect the connectivity patterns of the full graph. Fractal nodes are designed to coexist with the original nodes and adaptively aggregate subgraph-level feature representations, thereby enforcing feature similarity within each subgraph. We show that fractal nodes alleviate the over-squashing problem by providing direct shortcut connections that enable long-range propagation of subgraph-level representations. Experiment results show that our method improves the expressive power of MPNNs and achieves comparable or better performance to graph Transformers while maintaining the computational efficiency of MPNN by improving the long-range dependencies of MPNN.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13005v1" target="_blank"><h2>SAGE: Spuriousness-Aware Guided Prompt Exploration for Mitigating Multimodal Bias <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Wenqian Ye, Di Wang, Guangtao Zheng, Bohan Liu, Aidong Zhang<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> Accepted at AAAI 2026<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Large vision-language models, such as CLIP, have shown strong zero-shot classification performance by aligning images and text in a shared embedding space. However, CLIP models often develop multimodal spurious biases, which is the undesirable tendency to rely on spurious features. For example, CLIP may infer object types in images based on frequently co-occurring backgrounds rather than the object's core features. This bias significantly impairs the robustness of pre-trained CLIP models on out-of-distribution data, where such cross-modal associations no longer hold. Existing methods for mitigating multimodal spurious bias typically require fine-tuning on downstream data or prior knowledge of the bias, which undermines the out-of-the-box usability of CLIP. In this paper, we first theoretically analyze the impact of multimodal spurious bias in zero-shot classification. Based on this insight, we propose Spuriousness-Aware Guided Exploration (SAGE), a simple and effective method that mitigates spurious bias through guided prompt selection. SAGE requires no training, fine-tuning, or external annotations. It explores a space of prompt templates and selects the prompts that induce the largest semantic separation between classes, thereby improving worst-group robustness. Extensive experiments on four real-world benchmark datasets and five popular backbone models demonstrate that SAGE consistently improves zero-shot performance and generalization, outperforming previous zero-shot approaches without any external knowledge or model updates.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12997v1" target="_blank"><h2>WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Genglin Liu, Shijie Geng, Sha Li, Hejie Cui, Sarah Zhang, Xin Liu, Tianyi Liu<br><strong><u>Categories:</u></strong> cs.AI, cs.CL<br><strong><u>Comments:</u></strong> 18 pages; work in progress<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Multimodal LLM-powered agents have recently demonstrated impressive capabilities in web navigation, enabling agents to complete complex browsing tasks across diverse domains. However, current agents struggle with repetitive errors and lack the ability to learn from past experiences across sessions, limiting their long-term robustness and sample efficiency. We introduce WebCoach, a model-agnostic self-evolving framework that equips web browsing agents with persistent cross-session memory, enabling improved long-term planning, reflection, and continual learning without retraining. WebCoach consists of three key components: (1) a WebCondenser, which standardizes raw navigation logs into concise summaries; (2) an External Memory Store, which organizes complete trajectories as episodic experiences; and (3) a Coach, which retrieves relevant experiences based on similarity and recency, and decides whether to inject task-specific advice into the agent via runtime hooks. This design empowers web agents to access long-term memory beyond their native context window, improving robustness in complex browsing tasks. Moreover, WebCoach achieves self-evolution by continuously curating episodic memory from new navigation trajectories, enabling agents to improve over time without retraining. Evaluations on the WebVoyager benchmark demonstrate that WebCoach consistently improves the performance of browser-use agents across three different LLM backbones. With a 38B model, it increases task success rates from 47% to 61% while reducing or maintaining the average number of steps. Notably, smaller base models with WebCoach achieve performance comparable to the same web agent using GPT-4o.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12986v1" target="_blank"><h2>Learning Branching Policies for MILPs with Proximal Policy Optimization <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Abdelouahed Ben Mhamed, Assia Kamal-Idrissi, Amal El Fallah Seghrouchni<br><strong><u>Categories:</u></strong> cs.LG, cs.AI, math.OC<br><strong><u>Comments:</u></strong> 11 pages, 3 figures, AAAI conference<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> Branch-and-Bound (B\&B) is the dominant exact solution method for Mixed Integer Linear Programs (MILP), yet its exponential time complexity poses significant challenges for large-scale instances. The growing capabilities of machine learning have spurred efforts to improve B\&B by learning data-driven branching policies. However, most existing approaches rely on Imitation Learning (IL), which tends to overfit to expert demonstrations and struggles to generalize to structurally diverse or unseen instances. In this work, we propose Tree-Gate Proximal Policy Optimization (TGPPO), a novel framework that employs Proximal Policy Optimization (PPO), a Reinforcement Learning (RL) algorithm, to train a branching policy aimed at improving generalization across heterogeneous MILP instances. Our approach builds on a parameterized state space representation that dynamically captures the evolving context of the search tree. Empirical evaluations show that TGPPO often outperforms existing learning-based policies in terms of reducing the number of nodes explored and improving p-Primal-Dual Integrals (PDI), particularly in out-of-distribution instances. These results highlight the potential of RL to develop robust and adaptable branching strategies for MILP solvers.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12985v1" target="_blank"><h2>Angular Gradient Sign Method: Uncovering Vulnerabilities in Hyperbolic Networks <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Minsoo Jo, Dongyoon Yang, Taesup Kim<br><strong><u>Categories:</u></strong> cs.LG, cs.CV<br><strong><u>Comments:</u></strong> Accepted by AAAI 2026<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Adversarial examples in neural networks have been extensively studied in Euclidean geometry, but recent advances in \textit{hyperbolic networks} call for a reevaluation of attack strategies in non-Euclidean geometries. Existing methods such as FGSM and PGD apply perturbations without regard to the underlying hyperbolic structure, potentially leading to inefficient or geometrically inconsistent attacks. In this work, we propose a novel adversarial attack that explicitly leverages the geometric properties of hyperbolic space. Specifically, we compute the gradient of the loss function in the tangent space of hyperbolic space, decompose it into a radial (depth) component and an angular (semantic) component, and apply perturbation derived solely from the angular direction. Our method generates adversarial examples by focusing perturbations in semantically sensitive directions encoded in angular movement within the hyperbolic geometry. Empirical results on image classification, cross-modal retrieval tasks and network architectures demonstrate that our attack achieves higher fooling rates than conventional adversarial attacks, while producing high-impact perturbations with deeper insights into vulnerabilities of hyperbolic embeddings. This work highlights the importance of geometry-aware adversarial strategies in curved representation spaces and provides a principled framework for attacking hierarchical embeddings.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12976v1" target="_blank"><h2>MCAQ-YOLO: Morphological Complexity-Aware Quantization for Efficient Object Detection with Curriculum Learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yoonjae Seo, Ermal Elbasani, Jaehong Lee<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> 9 pages, 2 figures, 7 tables. Preprint<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Most neural network quantization methods apply uniform bit precision across spatial regions, ignoring the heterogeneous structural and textural complexity of visual data. This paper introduces MCAQ-YOLO, a morphological complexity-aware quantization framework for object detection. The framework employs five morphological metrics - fractal dimension, texture entropy, gradient variance, edge density, and contour complexity - to characterize local visual morphology and guide spatially adaptive bit allocation. By correlating these metrics with quantization sensitivity, MCAQ-YOLO dynamically adjusts bit precision according to spatial complexity. In addition, a curriculum-based quantization-aware training scheme progressively increases quantization difficulty to stabilize optimization and accelerate convergence. Experimental results demonstrate a strong correlation between morphological complexity and quantization sensitivity and show that MCAQ-YOLO achieves superior detection accuracy and convergence efficiency compared with uniform quantization. On a safety equipment dataset, MCAQ-YOLO attains 85.6 percent mAP@0.5 with an average of 4.2 bits and a 7.6x compression ratio, yielding 3.5 percentage points higher mAP than uniform 4-bit quantization while introducing only 1.8 ms of additional runtime overhead per image. Cross-dataset validation on COCO and Pascal VOC further confirms consistent performance gains, indicating that morphology-driven spatial quantization can enhance efficiency and robustness for computationally constrained, safety-critical visual recognition tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12971v1" target="_blank"><h2>Esim: EVM Bytecode Similarity Detection Based on Stable-Semantic Graph <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zhuo Chen, Gaoqiang Ji, Yiling He, Lei Wu, Yajin Zhou<br><strong><u>Categories:</u></strong> cs.CR, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Decentralized finance (DeFi) is experiencing rapid expansion. However, prevalent code reuse and limited open-source contributions have introduced significant challenges to the blockchain ecosystem, including plagiarism and the propagation of vulnerable code. Consequently, an effective and accurate similarity detection method for EVM bytecode is urgently needed to identify similar contracts. Traditional binary similarity detection methods are typically based on instruction stream or control flow graph (CFG), which have limitations on EVM bytecode due to specific features like low-level EVM bytecode and heavily-reused basic blocks. Moreover, the highly-diverse Solidity Compiler (Solc) versions further complicate accurate similarity detection.
  Motivated by these challenges, we propose a novel EVM bytecode representation called Stable-Semantic Graph (SSG), which captures relationships between 'stable instructions' (special instructions identified by our study). Moreover, we implement a prototype, Esim, which embeds SSG into matrices for similarity detection using a heterogeneous graph neural network. Esim demonstrates high accuracy in SSG construction, achieving F1-scores of 100% for control flow and 95.16% for data flow, and its similarity detection performance reaches 96.3% AUC, surpassing traditional approaches. Our large-scale study, analyzing 2,675,573 smart contracts on six EVM-compatible chains over a one-year period, also demonstrates that Esim outperforms the SOTA tool Etherscan in vulnerability search.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12964v1" target="_blank"><h2>CalibrateMix: Guided-Mixup Calibration of Image Semi-Supervised Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Mehrab Mustafy Rahman, Jayanth Mohan, Tiberiu Sosea, Cornelia Caragea<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Semi-supervised learning (SSL) has demonstrated high performance in image classification tasks by effectively utilizing both labeled and unlabeled data. However, existing SSL methods often suffer from poor calibration, with models yielding overconfident predictions that misrepresent actual prediction likelihoods. Recently, neural networks trained with {\tt mixup} that linearly interpolates random examples from the training set have shown better calibration in supervised settings. However, calibration of neural models remains under-explored in semi-supervised settings. Although effective in supervised model calibration, random mixup of pseudolabels in SSL presents challenges due to the overconfidence and unreliability of pseudolabels. In this work, we introduce CalibrateMix, a targeted mixup-based approach that aims to improve the calibration of SSL models while maintaining or even improving their classification accuracy. Our method leverages training dynamics of labeled and unlabeled samples to identify ``easy-to-learn'' and ``hard-to-learn'' samples, which in turn are utilized in a targeted mixup of easy and hard samples. Experimental results across several benchmark image datasets show that our method achieves lower expected calibration error (ECE) and superior accuracy compared to existing SSL approaches.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12955v1" target="_blank"><h2>Global Cross-Time Attention Fusion for Enhanced Solar Flare Prediction from Multivariate Time Series <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Onur Vural, Shah Muhammad Hamdi, Soukaina Filali Boubrahimi<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> This work has been accepted at the 2025 IEEE International Conference on Big Data (IEEE BigData 2025) on October 23, 2025<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Multivariate time series classification is increasingly investigated in space weather research as a means to predict intense solar flare events, which can cause widespread disruptions across modern technological systems. Magnetic field measurements of solar active regions are converted into structured multivariate time series, enabling predictive modeling across segmented observation windows. However, the inherently imbalanced nature of solar flare occurrences, where intense flares are rare compared to minor flare events, presents a significant barrier to effective learning. To address this challenge, we propose a novel Global Cross-Time Attention Fusion (GCTAF) architecture, a transformer-based model to enhance long-range temporal modeling. Unlike traditional self-attention mechanisms that rely solely on local interactions within time series, GCTAF injects a set of learnable cross-attentive global tokens that summarize salient temporal patterns across the entire sequence. These tokens are refined through cross-attention with the input sequence and fused back into the temporal representation, enabling the model to identify globally significant, non-contiguous time points that are critical for flare prediction. This mechanism functions as a dynamic attention-driven temporal summarizer that augments the model's capacity to capture discriminative flare-related dynamics. We evaluate our approach on the benchmark solar flare dataset and show that GCTAF effectively detects intense flares and improves predictive performance, demonstrating that refining transformer-based architectures presents a high-potential alternative for solar flare prediction tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12951v1" target="_blank"><h2>A FEDformer-Based Hybrid Framework for Anomaly Detection and Risk Forecasting in Financial Time Series <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Ziling Fan, Ruijia Liang, Yiwen Hu<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> anomaly detection (title, abstract), transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Financial markets are inherently volatile and prone to sudden disruptions such as market crashes, flash collapses, and liquidity crises. Accurate anomaly detection and early risk forecasting in financial time series are therefore crucial for preventing systemic instability and supporting informed investment decisions. Traditional deep learning models, such as LSTM and GRU, often fail to capture long-term dependencies and complex periodic patterns in highly nonstationary financial data. To address this limitation, this study proposes a FEDformer-Based Hybrid Framework for Anomaly Detection and Risk Forecasting in Financial Time Series, which integrates the Frequency Enhanced Decomposed Transformer (FEDformer) with a residual-based anomaly detector and a risk forecasting head. The FEDformer module models temporal dynamics in both time and frequency domains, decomposing signals into trend and seasonal components for improved interpretability. The residual-based detector identifies abnormal fluctuations by analyzing prediction errors, while the risk head predicts potential financial distress using learned latent embeddings. Experiments conducted on the S&P 500, NASDAQ Composite, and Brent Crude Oil datasets (2000-2024) demonstrate the superiority of the proposed model over benchmark methods, achieving a 15.7 percent reduction in RMSE and an 11.5 percent improvement in F1-score for anomaly detection. These results confirm the effectiveness of the model in capturing financial volatility, enabling reliable early-warning systems for market crash prediction and risk management.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12937v1" target="_blank"><h2>Yanyun-3: Enabling Cross-Platform Strategy Game Operation with Vision-Language Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Guoyan Wang, Yanyan Huang, Chunlin Chen, Lifeng Wang, Yuxiang Sun<br><strong><u>Categories:</u></strong> cs.AI, cs.CV<br><strong><u>Comments:</u></strong> 32 pages, 13 figures<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Automated operation in cross-platform strategy games demands agents with robust generalization across diverse user interfaces and dynamic battlefield conditions. While vision-language models (VLMs) have shown considerable promise in multimodal reasoning, their application to complex human-computer interaction scenarios--such as strategy gaming--remains largely unexplored. Here, we introduce Yanyun-3, a general-purpose agent framework that, for the first time, enables autonomous cross-platform operation across three heterogeneous strategy game environments. By integrating the vision-language reasoning of Qwen2.5-VL with the precise execution capabilities of UI-TARS, Yanyun-3 successfully performs core tasks including target localization, combat resource allocation, and area control. Through systematic ablation studies, we evaluate the effects of various multimodal data combinations--static images, multi-image sequences, and videos--and propose the concept of combination granularity to differentiate between intra-sample fusion and inter-sample mixing strategies. We find that a hybrid strategy, which fuses multi-image and video data while mixing in static images (MV+S), substantially outperforms full fusion: it reduces inference time by 63% and boosts the BLEU-4 score by a factor of 12 (from 4.81% to 62.41%, approximately 12.98x). Operating via a closed-loop pipeline of screen capture, model inference, and action execution, the agent demonstrates strong real-time performance and cross-platform generalization. Beyond providing an efficient solution for strategy game automation, our work establishes a general paradigm for enhancing VLM performance through structured multimodal data organization, offering new insights into the interplay between static perception and dynamic reasoning in embodied intelligence.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12934v2" target="_blank"><h2>AIF: Asynchronous Inference Framework for Cost-Effective Pre-Ranking <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zhi Kou, Xiang-Rong Sheng, Shuguang Han, Zhishan Zhao, Yueyao Cheng, Han Zhu, Jian Xu, Bo Zheng<br><strong><u>Categories:</u></strong> cs.LG, cs.IR<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> In industrial recommendation systems, pre-ranking models based on deep neural networks (DNNs) commonly adopt a sequential execution framework: feature fetching and model forward computation are triggered only after receiving candidates from the upstream retrieval stage. This design introduces inherent bottlenecks, including redundant computations of identical users/items and increased latency due to strictly sequential operations, which jointly constrain the model's capacity and system efficiency. To address these limitations, we propose the Asynchronous Inference Framework (AIF), a cost-effective computational architecture that decouples interaction-independent components, those operating within a single user or item, from real-time prediction. AIF reorganizes the model inference process by performing user-side computations in parallel with the retrieval stage and conducting item-side computations in a nearline manner. This means that interaction-independent components are calculated just once and completed before the real-time prediction phase of the pre-ranking stage. As a result, AIF enhances computational efficiency and reduces latency, freeing up resources to significantly improve the feature set and model architecture of interaction-independent components. Moreover, we delve into model design within the AIF framework, employing approximated methods for interaction-dependent components in online real-time predictions. By co-designing both the framework and the model, our solution achieves notable performance gains without significantly increasing computational and latency costs. This has enabled the successful deployment of AIF in the Taobao display advertising system.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12922v1" target="_blank"><h2>Tokenize Once, Recommend Anywhere: Unified Item Tokenization for Multi-domain LLM-based Recommendation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yu Hou, Won-Yong Shin<br><strong><u>Categories:</u></strong> cs.IR, cs.AI, cs.LG, cs.NE, cs.SI<br><strong><u>Comments:</u></strong> 20 pages, 8 figures, 9 tables; Annual AAAI Conference on Artificial Intelligence (AAAI-26) (to appear) (Please cite our conference version.)<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> latent space (abstract)<br><p><strong><u>Abstract:</u></strong> Large language model (LLM)-based recommender systems have achieved high-quality performance by bridging the discrepancy between the item space and the language space through item tokenization. However, existing item tokenization methods typically require training separate models for each item domain, limiting generalization. Moreover, the diverse distributions and semantics across item domains make it difficult to construct a unified tokenization that preserves domain-specific information. To address these challenges, we propose UniTok, a Unified item Tokenization framework that integrates our own mixture-of-experts (MoE) architecture with a series of codebooks to convert items into discrete tokens, enabling scalable tokenization while preserving semantic information across multiple item domains. Specifically, items from different domains are first projected into a unified latent space through a shared encoder. They are then routed to domain-specific experts to capture the unique semantics, while a shared expert, which is always active, encodes common knowledge transferable across domains. Additionally, to mitigate semantic imbalance across domains, we present a mutual information calibration mechanism, which guides the model towards retaining similar levels of semantic information for each domain. Comprehensive experiments on wide-ranging real-world datasets demonstrate that the proposed UniTok framework is (a) highly effective: achieving up to 51.89% improvements over strong benchmarks, (b) theoretically sound: showing the analytical validity of our architectural design and optimization; and (c) highly generalizable: demonstrating robust performance across diverse domains without requiring per-domain retraining, a capability not supported by existing baselines.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12916v1" target="_blank"><h2>Fault2Flow: An AlphaEvolve-Optimized Human-in-the-Loop Multi-Agent System for Fault-to-Workflow Automation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yafang Wang, Yangjie Tian, Xiaoyu Shen, Gaoyang Zhang, Jiaze Sun, He Zhang, Ruohua Xu, Feng Zhao<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Power grid fault diagnosis is a critical process hindered by its reliance on manual, error-prone methods. Technicians must manually extract reasoning logic from dense regulations and attempt to combine it with tacit expert knowledge, which is inefficient, error-prone, and lacks maintainability as ragulations are updated and experience evolves. While Large Language Models (LLMs) have shown promise in parsing unstructured text, no existing framework integrates these two disparate knowledge sources into a single, verified, and executable workflow. To bridge this gap, we propose Fault2Flow, an LLM-based multi-agent system. Fault2Flow systematically: (1) extracts and structures regulatory logic into PASTA-formatted fault trees; (2) integrates expert knowledge via a human-in-the-loop interface for verification; (3) optimizes the reasoning logic using a novel AlphaEvolve module; and (4) synthesizes the final, verified logic into an n8n-executable workflow. Experimental validation on transformer fault diagnosis datasets confirms 100\% topological consistency and high semantic fidelity. Fault2Flow establishes a reproducible path from fault analysis to operational automation, substantially reducing expert workload.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12908v1" target="_blank"><h2>DeepSport: A Multimodal Large Language Model for Comprehensive Sports Video Reasoning via Agentic Reinforcement Learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Junbo Zou, Haotian Xia, Zhen Ye, Shengjie Zhang, Christopher Lai, Vicente Ordonez, Weining Shen, Hanjie Chen<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Sports video understanding presents unique challenges, requiring models to perceive high-speed dynamics, comprehend complex rules, and reason over long temporal contexts. While Multimodal Large Language Models (MLLMs) have shown promise in genral domains, the current state of research in sports remains narrowly focused: existing approaches are either single-sport centric, limited to specific tasks, or rely on training-free paradigms that lack robust, learned reasoning process. To address this gap, we introduce DeepSport, the first end-to-end trained MLLM framework designed for multi-task, multi-sport video understanding. DeepSport shifts the paradigm from passive frame processing to active, iterative reasoning, empowering the model to ``think with videos'' by dynamically interrogating content via a specialized frame-extraction tool. To enable this, we propose a data distillation pipeline that synthesizes high-quality Chain-of-Thought (CoT) trajectories from 10 diverse data source, creating a unified resource of 78k training data. We then employ a two-stage training strategy, Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) with a novel gated tool-use reward, to optimize the model's reasoning process. Extensive experiments on the testing benchmark of 6.7k questions demonstrate that DeepSport achieves state-of-the-art performance, significantly outperforming baselines of both proprietary model and open-source models. Our work establishes a new foundation for domain-specific video reasoning to address the complexities of diverse sports.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13794v1" target="_blank"><h2>FusionFM: All-in-One Multi-Modal Image Fusion with Flow Matching <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Huayi Zhu, Xiu Shu, Youqiang Xiong, Qiao Liu, Rui Chen, Di Yuan, Xiaojun Chang, Zhenyu He<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multi-modal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Current multi-modal image fusion methods typically rely on task-specific models, leading to high training costs and limited scalability. While generative methods provide a unified modeling perspective, they often suffer from slow inference due to the complex sampling trajectories from noise to image. To address this, we formulate image fusion as a direct probabilistic transport from source modalities to the fused image distribution, leveraging the flow matching paradigm to improve sampling efficiency and structural consistency. To mitigate the lack of high-quality fused images for supervision, we collect fusion results from multiple state-of-the-art models as priors, and employ a task-aware selection function to select the most reliable pseudo-labels for each task. We further introduce a Fusion Refiner module that employs a divide-and-conquer strategy to systematically identify, decompose, and enhance degraded components in selected pseudo-labels. For multi-task scenarios, we integrate elastic weight consolidation and experience replay mechanisms to preserve cross-task performance and enhance continual learning ability from both parameter stability and memory retention perspectives. Our approach achieves competitive performance across diverse fusion tasks, while significantly improving sampling efficiency and maintaining a lightweight model design. The code will be available at: https://github.com/Ist-Zhy/FusionFM.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12903v1" target="_blank"><h2>Contrastive Entropy Bounds for Density and Conditional Density Decomposition <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Bo Hu, Jose C. Principe<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> This paper studies the interpretability of neural network features from a Bayesian Gaussian view, where optimizing a cost is reaching a probabilistic bound; learning a model approximates a density that makes the bound tight and the cost optimal, often with a Gaussian mixture density. The two examples are Mixture Density Networks (MDNs) using the bound for the marginal and autoencoders using the conditional bound. It is a known result, not only for autoencoders, that minimizing the error between inputs and outputs maximizes the dependence between inputs and the middle.
  We use Hilbert space and decomposition to address cases where a multiple-output network produces multiple centers defining a Gaussian mixture. Our first finding is that an autoencoder's objective is equivalent to maximizing the trace of a Gaussian operator, the sum of eigenvalues under bases orthonormal w.r.t. the data and model distributions. This suggests that, when a one-to-one correspondence as needed in autoencoders is unnecessary, we can instead maximize the nuclear norm of this operator, the sum of singular values, to maximize overall rank rather than trace. Thus the trace of a Gaussian operator can be used to train autoencoders, and its nuclear norm can be used as divergence to train MDNs.
  Our second test uses inner products and norms in a Hilbert space to define bounds and costs. Such bounds often have an extra norm compared to KL-based bounds, which increases sample diversity and prevents the trivial solution where a multiple-output network produces the same constant, at the cost of requiring a sample batch to estimate and optimize. We propose an encoder-mixture-decoder architecture whose decoder is multiple-output, producing multiple centers per sample, potentially tightening the bound. Assuming the data are small-variance Gaussian mixtures, this upper bound can be tracked and analyzed quantitatively.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12891v1" target="_blank"><h2>Near-infrared [P II] and [Fe II] line mapping of Galactic supernova remnants <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Takuma Kokusho, Yuki Katsurada, Yong-Hyun Lee, Bon-Chul Koo, Takahiro Nagayama, Hidehiro Kaneda, Koji S. Kawabata, Tatsuya Nakaoka, Ho-Gyu Lee, Rommy L. S. E. Aliste Castillo<br><strong><u>Categories:</u></strong> astro-ph.GA<br><strong><u>Comments:</u></strong> 14 pages, 8 figures, accepted for publication in PASJ<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> Phosphorus (P) is one of the key ingredients for life, yet its origins in galaxies remain poorly understood. In order to investigate the production of P by supernovae, we performed near-infrared (IR) [P II] and [Fe II] line mapping of 26 Galactic supernova remnants (SNRs) with the Infrared Survey Facility and Kanata telescopes, using the narrow-band filters tuned to these lines. By combining our data with archival [Fe II] maps from UKIRT, we detected both the [P II] and [Fe II] emissions in five SNRs, only the [Fe II] emission in 15 SNRs, and no line emissions in the remaining six. Using the observed [P II]/[Fe II] ratios and upper limits for non-detections, we derived the P/Fe abundance ratios, which vary by up to two orders of magnitude among our sample SNRs. This suggests that the production rate of P and/or the degree of dust destruction may differ from remnant to remnant, the latter being due to the fact that P is volatile while Fe is mostly locked in dust grains. We used the mid- and far-IR maps to examine the dust content for the five SNRs where both the line emissions are detected. As a result, we find that high P/Fe abundance ratios in the northern and southeastern regions of Cassiopeia A and the Crab Nebula, respectively, are not likely due to dust destruction but may reflect an asymmetric ejection of P during supernova explosions. In the Crab Nebula, it is also possible that near-IR [Ni II] emission contaminates the observed flux in the southeastern region, suggesting that the Ni/Fe abundance ratio, rather than the P/Fe abundance ratio, is relatively high in this part of the remnant.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12882v2" target="_blank"><h2>Towards High-Consistency Embodied World Model with Multi-View Trajectory Videos <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Taiyi Su, Jian Zhu, Yaxuan Li, Chong Ma, Zitai Huang, Hanli Wang, Yi Xu<br><strong><u>Categories:</u></strong> cs.RO, cs.AI<br><strong><u>Comments:</u></strong> 15 pages, 23 figures<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Embodied world models aim to predict and interact with the physical world through visual observations and actions. However, existing models struggle to accurately translate low-level actions (e.g., joint positions) into precise robotic movements in predicted frames, leading to inconsistencies with real-world physical interactions. To address these limitations, we propose MTV-World, an embodied world model that introduces Multi-view Trajectory-Video control for precise visuomotor prediction. Specifically, instead of directly using low-level actions for control, we employ trajectory videos obtained through camera intrinsic and extrinsic parameters and Cartesian-space transformation as control signals. However, projecting 3D raw actions onto 2D images inevitably causes a loss of spatial information, making a single view insufficient for accurate interaction modeling. To overcome this, we introduce a multi-view framework that compensates for spatial information loss and ensures high-consistency with physical world. MTV-World forecasts future frames based on multi-view trajectory videos as input and conditioning on an initial frame per view. Furthermore, to systematically evaluate both robotic motion precision and object interaction accuracy, we develop an auto-evaluation pipeline leveraging multimodal large models and referring video object segmentation models. To measure spatial consistency, we formulate it as an object location matching problem and adopt the Jaccard Index as the evaluation metric. Extensive experiments demonstrate that MTV-World achieves precise control execution and accurate physical interaction modeling in complex dual-arm scenarios.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12874v1" target="_blank"><h2>Classification of Hope in Textual Data using Transformer-Based Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Chukwuebuka Fortunate Ijezue, Tania-Amanda Fredrick Eneye, Maaz Amjad<br><strong><u>Categories:</u></strong> cs.CL, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> This paper presents a transformer-based approach for classifying hope expressions in text. We developed and compared three architectures (BERT, GPT-2, and DeBERTa) for both binary classification (Hope vs. Not Hope) and multiclass categorization (five hope-related categories). Our initial BERT implementation achieved 83.65% binary and 74.87% multiclass accuracy. In the extended comparison, BERT demonstrated superior performance (84.49% binary, 72.03% multiclass accuracy) while requiring significantly fewer computational resources (443s vs. 704s training time) than newer architectures. GPT-2 showed lowest overall accuracy (79.34% binary, 71.29% multiclass), while DeBERTa achieved moderate results (80.70% binary, 71.56% multiclass) but at substantially higher computational cost (947s for multiclass training). Error analysis revealed architecture-specific strengths in detecting nuanced hope expressions, with GPT-2 excelling at sarcasm detection (92.46% recall). This study provides a framework for computational analysis of hope, with applications in mental health and social media analysis, while demonstrating that architectural suitability may outweigh model size for specialized emotion detection tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12869v1" target="_blank"><h2>On the Fundamental Limits of LLMs at Scale <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Muhammad Ahmed Mohsin, Muhammad Umer, Ahsan Bilal, Zeeshan Memon, Muhammad Ibtsaam Qadir, Sagnik Bhattacharya, Hassan Rizwan, Abhiram R. Gorle, Maahe Zehra Kazmi, Ayesha Mohsin, Muhammad Usman Rafique, Zihao He, Pulkit Mehta, Muhammad Ali Jamshed, John M. Cioffi<br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.DC, cs.IT, cs.MA<br><strong><u>Comments:</u></strong> Submitted to TMLR 2025<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Large Language Models (LLMs) have benefited enormously from scaling, yet these gains are bounded by five fundamental limitations: (1) hallucination, (2) context compression, (3) reasoning degradation, (4) retrieval fragility, and (5) multimodal misalignment. While existing surveys describe these phenomena empirically, they lack a rigorous theoretical synthesis connecting them to the foundational limits of computation, information, and learning. This work closes that gap by presenting a unified, proof-informed framework that formalizes the innate theoretical ceilings of LLM scaling. First, computability and uncomputability imply an irreducible residue of error: for any computably enumerable model family, diagonalization guarantees inputs on which some model must fail, and undecidable queries (e.g., halting-style tasks) induce infinite failure sets for all computable predictors. Second, information-theoretic and statistical constraints bound attainable accuracy even on decidable tasks, finite description length enforces compression error, and long-tail factual knowledge requires prohibitive sample complexity. Third, geometric and computational effects compress long contexts far below their nominal size due to positional under-training, encoding attenuation, and softmax crowding. We further show how likelihood-based training favors pattern completion over inference, how retrieval under token limits suffers from semantic drift and coupling noise, and how multimodal scaling inherits shallow cross-modal alignment. Across sections, we pair theorems and empirical evidence to outline where scaling helps, where it saturates, and where it cannot progress, providing both theoretical foundations and practical mitigation paths like bounded-oracle retrieval, positional curricula, and sparse or hierarchical attention.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12868v1" target="_blank"><h2>Video Finetuning Improves Reasoning Between Frames <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Ruiqi Yang, Tian Yun, Zihan Wang, Ellie Pavlick<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> Accepted at CogInterp @ NeurIPS 2025<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Multimodal large language models (LLMs) have made rapid progress in visual understanding, yet their extension from images to videos often reduces to a naive concatenation of frame tokens. In this work, we investigate what video finetuning brings to multimodal LLMs. We propose Visual Chain-of-Thought (vCoT), an explicit reasoning process that generates transitional event descriptions between consecutive frames. Using vCoT, we systematically compare image-only LVLMs with their video-finetuned counterparts, both with and without access to these transitional cues. Our experiments show that vCoT significantly improves the performance of image-only models on long-form video question answering, while yielding only marginal gains for video-finetuned models. This suggests that the latter already capture frame-to-frame transitions implicitly. Moreover, we find that video models transfer this temporal reasoning ability to purely static settings, outperforming image models' baselines on relational visual reasoning tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12852v1" target="_blank"><h2>From Black-Box to White-Box: Control-Theoretic Neural Network Interpretability <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jihoon Moon<br><strong><u>Categories:</u></strong> cs.LG, cs.AI, eess.SY<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> Deep neural networks achieve state of the art performance but remain difficult to interpret mechanistically. In this work, we propose a control theoretic framework that treats a trained neural network as a nonlinear state space system and uses local linearization, controllability and observability Gramians, and Hankel singular values to analyze its internal computation. For a given input, we linearize the network around the corresponding hidden activation pattern and construct a state space model whose state consists of hidden neuron activations. The input state and state output Jacobians define local controllability and observability Gramians, from which we compute Hankel singular values and associated modes. These quantities provide a principled notion of neuron and pathway importance: controllability measures how easily each neuron can be excited by input perturbations, observability measures how strongly each neuron influences the output, and Hankel singular values rank internal modes that carry input output energy. We illustrate the framework on simple feedforward networks, including a 1 2 2 1 SwiGLU network and a 2 3 3 2 GELU network. By comparing different operating points, we show how activation saturation reduces controllability, shrinks the dominant Hankel singular value, and shifts the dominant internal mode to a different subset of neurons. The proposed method turns a neural network into a collection of local white box dynamical models and suggests which internal directions are natural candidates for pruning or constraints to improve interpretability.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12851v1" target="_blank"><h2>NeuroLex: A Lightweight Domain Language Model for EEG Report Understanding and Generation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Kang Yin, Hye-Bin Shin<br><strong><u>Categories:</u></strong> cs.CL, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Clinical electroencephalogram (EEG) reports encode domain-specific linguistic conventions that general-purpose language models (LMs) fail to capture. We introduce NeuroLex, a lightweight domain-adaptive language model trained purely on EEG report text from the Harvard Electroencephalography Database. Unlike existing biomedical LMs, NeuroLex is tailored to the linguistic and diagnostic characteristics of EEG reporting, enabling it to serve as both an independent textual model and a decoder backbone for multimodal EEG-language systems. Using span-corruption pretraining and instruction-style fine-tuning on report polishing, paragraph summarization, and terminology question answering, NeuroLex learns the syntax and reasoning patterns characteristic of EEG interpretation. Comprehensive evaluations show that it achieves lower perplexity, higher extraction and summarization accuracy, better label efficiency, and improved robustness to negation and factual hallucination compared with general models of the same scale. With an EEG-aware linguistic backbone, NeuroLex bridges biomedical text modeling and brain-computer interface applications, offering a foundation for interpretable and language-driven neural decoding.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12846v1" target="_blank"><h2>RoS-Guard: Robust and Scalable Online Change Detection with Delay-Optimal Guarantees <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zelin Zhu, Yancheng Huang, Kai Yang<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> anomaly detection (abstract)<br><p><strong><u>Abstract:</u></strong> Online change detection (OCD) aims to rapidly identify change points in streaming data and is critical in applications such as power system monitoring, wireless network sensing, and financial anomaly detection. Existing OCD methods typically assume precise system knowledge, which is unrealistic due to estimation errors and environmental variations. Moreover, existing OCD methods often struggle with efficiency in large-scale systems. To overcome these challenges, we propose RoS-Guard, a robust and optimal OCD algorithm tailored for linear systems with uncertainty. Through a tight relaxation and reformulation of the OCD optimization problem, RoS-Guard employs neural unrolling to enable efficient parallel computation via GPU acceleration. The algorithm provides theoretical guarantees on performance, including expected false alarm rate and worst-case average detection delay. Extensive experiments validate the effectiveness of RoS-Guard and demonstrate significant computational speedup in large-scale system scenarios.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12838v1" target="_blank"><h2>Connectivity-Guided Sparsification of 2-FWL GNNs: Preserving Full Expressivity with Improved Efficiency <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Rongqin Chen, Fan Mo, Pak Lon Ip, Shenghui Zhang, Dan Wu, Ye Li, Leong Hou U<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> Accepted by AAAI 2026<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Higher-order Graph Neural Networks (HOGNNs) based on the 2-FWL test achieve superior expressivity by modeling 2- and 3-node interactions, but at $\mathcal{O}(n^3)$ computational cost. However, this computational burden is typically mitigated by existing efficiency methods at the cost of reduced expressivity. We propose \textbf{Co-Sparsify}, a connectivity-aware sparsification framework that eliminates \emph{provably redundant} computations while preserving full 2-FWL expressive power. Our key insight is that 3-node interactions are expressively necessary only within \emph{biconnected components} -- maximal subgraphs where every pair of nodes lies on a cycle. Outside these components, structural relationships can be fully captured via 2-node message passing or global readout, rendering higher-order modeling unnecessary. Co-Sparsify restricts 2-node message passing to connected components and 3-node interactions to biconnected ones, removing computation without approximation or sampling. We prove that Co-Sparsified GNNs are as expressive as the 2-FWL test. Empirically, on PPGN, Co-Sparsify matches or exceeds accuracy on synthetic substructure counting tasks and achieves state-of-the-art performance on real-world benchmarks (ZINC, QM9). This study demonstrates that high expressivity and scalability are not mutually exclusive: principled, topology-guided sparsification enables powerful, efficient GNNs with theoretical guarantees.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12834v1" target="_blank"><h2>SAGA: Source Attribution of Generative AI Videos <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Rohit Kundu, Vishal Mohanty, Hao Xiong, Shan Jia, Athula Balachandran, Amit K. Roy-Chowdhury<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> The proliferation of generative AI has led to hyper-realistic synthetic videos, escalating misuse risks and outstripping binary real/fake detectors. We introduce SAGA (Source Attribution of Generative AI videos), the first comprehensive framework to address the urgent need for AI-generated video source attribution at a large scale. Unlike traditional detection, SAGA identifies the specific generative model used. It uniquely provides multi-granular attribution across five levels: authenticity, generation task (e.g., T2V/I2V), model version, development team, and the precise generator, offering far richer forensic insights. Our novel video transformer architecture, leveraging features from a robust vision foundation model, effectively captures spatio-temporal artifacts. Critically, we introduce a data-efficient pretrain-and-attribute strategy, enabling SAGA to achieve state-of-the-art attribution using only 0.5\% of source-labeled data per class, matching fully supervised performance. Furthermore, we propose Temporal Attention Signatures (T-Sigs), a novel interpretability method that visualizes learned temporal differences, offering the first explanation for why different video generators are distinguishable. Extensive experiments on public datasets, including cross-domain scenarios, demonstrate that SAGA sets a new benchmark for synthetic video provenance, providing crucial, interpretable insights for forensic and regulatory applications.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12829v1" target="_blank"><h2>An Evaluation of Representation Learning Methods in Particle Physics Foundation Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Michael Chen, Raghav Kansal, Abhijith Gandrakota, Zichun Hao, Jennifer Ngadiuba, Maria Spiropulu<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> We present a systematic evaluation of representation learning objectives for particle physics within a unified framework. Our study employs a shared transformer-based particle-cloud encoder with standardized preprocessing, matched sampling, and a consistent evaluation protocol on a jet classification dataset. We compare contrastive (supervised and self-supervised), masked particle modeling, and generative reconstruction objectives under a common training regimen. In addition, we introduce targeted supervised architectural modifications that achieve state-of-the-art performance on benchmark evaluations. This controlled comparison isolates the contributions of the learning objective, highlights their respective strengths and limitations, and provides reproducible baselines. We position this work as a reference point for the future development of foundation models in particle physics, enabling more transparent and robust progress across the community.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12823v1" target="_blank"><h2>Enhancing LLM Code Generation Capabilities through Test-Driven Development and Code Interpreter <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Sajed Jalil, Shuvo Saha, Hossain Mohammad Seym<br><strong><u>Categories:</u></strong> cs.SE, cs.LG, cs.PL<br><strong><u>Comments:</u></strong> AACL-IJCNLP 2025 Workshop BLP Shared Task 2, 6 pages, 7 figures, 3 tables<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Over the past few years, improving LLM code generation capabilities has been a key focus in NLP research. Despite Bengali having 242 million native speakers worldwide, it receives little attention when it comes to training LLMs. More recently, various fine-tuning and augmented generation techniques have been employed to significantly enhance code generation performance. However, they require considerable expertise and resources to utilize effectively as an end user. The goal of our work is to democratize access to powerful code generation tools in resource-constrained emerging markets, enabling users to leverage them in their native language.
  We introduce a novel approach that combines Test-Driven Development (TDD) and Code Interpreter (CI), utilizing open-weight models, which improves the baseline accuracy for code generation with Bengali prompts and achieves an overall accuracy of 85%. Our approach requires no finetuning and proves that even the smallest models in the same family can attain up to 98% accuracy compared to the largest models. All of our results are publicly shared in GitHub for validation and reproducibility.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12817v1" target="_blank"><h2>Assessing Automated Fact-Checking for Medical LLM Responses with Knowledge Graphs <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Shasha Zhou, Mingyu Huang, Jack Cole, Charles Britton, Ming Yin, Jan Wolber, Ke Li<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> Accepted as a conference paper at AAAI'26<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> explainability (abstract)<br><p><strong><u>Abstract:</u></strong> The recent proliferation of large language models (LLMs) holds the potential to revolutionize healthcare, with strong capabilities in diverse medical tasks. Yet, deploying LLMs in high-stakes healthcare settings requires rigorous verification and validation to understand any potential harm. This paper investigates the reliability and viability of using medical knowledge graphs (KGs) for the automated factuality evaluation of LLM-generated responses. To ground this investigation, we introduce FAITH, a framework designed to systematically probe the strengths and limitations of this KG-based approach. FAITH operates without reference answers by decomposing responses into atomic claims, linking them to a medical KG, and scoring them based on evidence paths. Experiments on diverse medical tasks with human subjective evaluations demonstrate that KG-grounded evaluation achieves considerably higher correlations with clinician judgments and can effectively distinguish LLMs with varying capabilities. It is also robust to textual variances. The inherent explainability of its scoring can further help users understand and mitigate the limitations of current LLMs. We conclude that while limitations exist, leveraging KGs is a prominent direction for automated factuality assessment in healthcare.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12810v1" target="_blank"><h2>MSRNet: A Multi-Scale Recursive Network for Camouflaged Object Detection <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Leena Alghamdi, Muhammad Usman, Hafeez Anwar, Abdul Bais, Saeed Anwar<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, eess.IV<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Camouflaged object detection is an emerging and challenging computer vision task that requires identifying and segmenting objects that blend seamlessly into their environments due to high similarity in color, texture, and size. This task is further complicated by low-light conditions, partial occlusion, small object size, intricate background patterns, and multiple objects. While many sophisticated methods have been proposed for this task, current methods still struggle to precisely detect camouflaged objects in complex scenarios, especially with small and multiple objects, indicating room for improvement. We propose a Multi-Scale Recursive Network that extracts multi-scale features via a Pyramid Vision Transformer backbone and combines them via specialized Attention-Based Scale Integration Units, enabling selective feature merging. For more precise object detection, our decoder recursively refines features by incorporating Multi-Granularity Fusion Units. A novel recursive-feedback decoding strategy is developed to enhance global context understanding, helping the model overcome the challenges in this task. By jointly leveraging multi-scale learning and recursive feature optimization, our proposed method achieves performance gains, successfully detecting small and multiple camouflaged objects. Our model achieves state-of-the-art results on two benchmark datasets for camouflaged object detection and ranks second on the remaining two. Our codes, model weights, and results are available at \href{https://github.com/linaagh98/MSRNet}{https://github.com/linaagh98/MSRNet}.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12793v1" target="_blank"><h2>Neuro-Logic Lifelong Learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Bowen He, Xiaoan Xu, Alper Kamil Bozkurt, Vahid Tarokh, Juncheng Dong<br><strong><u>Categories:</u></strong> cs.AI, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Solving Inductive Logic Programming (ILP) problems with neural networks is a key challenge in Neural-Symbolic Ar- tificial Intelligence (AI). While most research has focused on designing novel network architectures for individual prob- lems, less effort has been devoted to exploring new learning paradigms involving a sequence of problems. In this work, we investigate lifelong learning ILP, which leverages the com- positional and transferable nature of logic rules for efficient learning of new problems. We introduce a compositional framework, demonstrating how logic rules acquired from ear- lier tasks can be efficiently reused in subsequent ones, leading to improved scalability and performance. We formalize our approach and empirically evaluate it on sequences of tasks. Experimental results validate the feasibility and advantages of this paradigm, opening new directions for continual learn- ing in Neural-Symbolic AI.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12788v1" target="_blank"><h2>Physics-Constrained Adaptive Neural Networks Enable Real-Time Semiconductor Manufacturing Optimization with Minimal Training Data <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Rubén Darío Guerrero<br><strong><u>Categories:</u></strong> cs.LG, cs.AR, math.OC<br><strong><u>Comments:</u></strong> 32 pages, 21 figures, 10 tables<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> The semiconductor industry faces a computational crisis in extreme ultraviolet (EUV) lithography optimization, where traditional methods consume billions of CPU hours while failing to achieve sub-nanometer precision. We present a physics-constrained adaptive learning framework that automatically calibrates electromagnetic approximations through learnable parameters $\boldsymbolθ = \{θ_d, θ_a, θ_b, θ_p, θ_c\}$ while simultaneously minimizing Edge Placement Error (EPE) between simulated aerial images and target photomasks. The framework integrates differentiable modules for Fresnel diffraction, material absorption, optical point spread function blur, phase-shift effects, and contrast modulation with direct geometric pattern matching objectives, enabling cross-geometry generalization with minimal training data. Through physics-constrained learning on 15 representative patterns spanning current production to future research nodes, we demonstrate consistent sub-nanometer EPE performance (0.664-2.536 nm range) using only 50 training samples per pattern. Adaptive physics learning achieves an average improvement of 69.9\% over CNN baselines without physics constraints, with a significant inference speedup over rigorous electromagnetic solvers after training completion. This approach requires 90\% fewer training samples through cross-geometry generalization compared to pattern-specific CNN training approaches. This work establishes physics-constrained adaptive learning as a foundational methodology for real-time semiconductor manufacturing optimization, addressing the critical gap between academic physics-informed neural networks and industrial deployment requirements through joint physics calibration and manufacturing precision objectives.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12770v1" target="_blank"><h2>MolEdit: Knowledge Editing for Multimodal Molecule Language Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zhenyu Lei, Patrick Soga, Yaochen Zhu, Yinhan He, Yushun Dong, Jundong Li<br><strong><u>Categories:</u></strong> cs.LG, cs.CE<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Understanding and continuously refining multimodal molecular knowledge is crucial for advancing biomedicine, chemistry, and materials science. Molecule language models (MoLMs) have become powerful tools in these domains, integrating structural representations (e.g., SMILES strings, molecular graphs) with rich contextual descriptions (e.g., physicochemical properties). However, MoLMs can encode and propagate inaccuracies due to outdated web-mined training corpora or malicious manipulation, jeopardizing downstream discovery pipelines. While knowledge editing has been explored for general-domain AI, its application to MoLMs remains uncharted, presenting unique challenges due to the multifaceted and interdependent nature of molecular knowledge. In this paper, we take the first step toward MoLM editing for two critical tasks: molecule-to-caption generation and caption-to-molecule generation. To address molecule-specific challenges, we propose MolEdit, a powerful framework that enables targeted modifications while preserving unrelated molecular knowledge. MolEdit combines a Multi-Expert Knowledge Adapter that routes edits to specialized experts for different molecular facets with an Expertise-Aware Editing Switcher that activates the adapters only when input closely matches the stored edits across all expertise, minimizing interference with unrelated knowledge. To systematically evaluate editing performance, we introduce MEBench, a comprehensive benchmark assessing multiple dimensions, including Reliability (accuracy of the editing), Locality (preservation of irrelevant knowledge), and Generality (robustness to reformed queries). Across extensive experiments on two popular MoLM backbones, MolEdit delivers up to 18.8% higher Reliability and 12.0% better Locality than baselines while maintaining efficiency. The code is available at: https://github.com/LzyFischer/MolEdit.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12769v1" target="_blank"><h2>Event-CausNet: Unlocking Causal Knowledge from Text with Large Language Models for Reliable Spatio-Temporal Forecasting <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Luyao Niu, Zepu Wang, Shuyi Guan, Yang Liu, Peng Sun<br><strong><u>Categories:</u></strong> cs.AI, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> While spatio-temporal Graph Neural Networks (GNNs) excel at modeling recurring traffic patterns, their reliability plummets during non-recurring events like accidents. This failure occurs because GNNs are fundamentally correlational models, learning historical patterns that are invalidated by the new causal factors introduced during disruptions. To address this, we propose Event-CausNet, a framework that uses a Large Language Model to quantify unstructured event reports, builds a causal knowledge base by estimating average treatment effects, and injects this knowledge into a dual-stream GNN-LSTM network using a novel causal attention mechanism to adjust and enhance the forecast. Experiments on a real-world dataset demonstrate that Event-CausNet achieves robust performance, reducing prediction error (MAE) by up to 35.87%, significantly outperforming state-of-the-art baselines. Our framework bridges the gap between correlational models and causal reasoning, providing a solution that is more accurate and transferable, while also offering crucial interpretability, providing a more reliable foundation for real-world traffic management during critical disruptions.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12768v1" target="_blank"><h2>Evidence of Phase Transitions in Small Transformer-Based Language Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Noah Hong, Tao Hong<br><strong><u>Categories:</u></strong> cs.CL, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> Phase transitions have been proposed as the origin of emergent abilities in large language models (LLMs), where new capabilities appear abruptly once models surpass critical thresholds of scale. Prior work, such as that of Wei et al., demonstrated these phenomena under model and data scaling, with transitions revealed after applying a log scale to training compute. In this work, we ask three complementary questions: (1) Are phase transitions unique to large models, or can they also be observed in small transformer-based language models? (2) Can such transitions be detected directly in linear training space, rather than only after log rescaling? and (3) Can these transitions emerge at early stages of training? To investigate, we train a small GPT-style transformer on a character-level corpus and analyze the evolution of vocabulary usage throughout training. We track the average word length, the number of correct versus incorrect words, and shifts in vocabulary diversity. Building on these measures, we apply Poisson and sub-Poisson statistics to quantify how words connect and reorganize. This combined analysis reveals a distinct transition point during training. Notably, these transitions are not apparent in standard loss or validation curves, but become visible through our vocabulary- and statistics-based probes. Our findings suggest that phase-transition reorganizations are a general feature of language model training, observable even in modest models, detectable directly in linear training space, and occurring surprisingly early as coherence emerges. This perspective provides new insight into the nonlinear dynamics of language model training and underscores the importance of tailored metrics for uncovering phase transition behaviors</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12767v1" target="_blank"><h2>RoCoISLR: A Romanian Corpus for Isolated Sign Language Recognition <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Cătălin-Alexandru Rîpanu, Andrei-Theodor Hotnog, Giulia-Stefania Imbrea, Dumitru-Clementin Cercel<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> 5 pages, 3 figures, 4 tables<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Automatic sign language recognition plays a crucial role in bridging the communication gap between deaf communities and hearing individuals; however, most available datasets focus on American Sign Language. For Romanian Isolated Sign Language Recognition (RoISLR), no large-scale, standardized dataset exists, which limits research progress. In this work, we introduce a new corpus for RoISLR, named RoCoISLR, comprising over 9,000 video samples that span nearly 6,000 standardized glosses from multiple sources. We establish benchmark results by evaluating seven state-of-the-art video recognition models-I3D, SlowFast, Swin Transformer, TimeSformer, Uniformer, VideoMAE, and PoseConv3D-under consistent experimental setups, and compare their performance with that of the widely used WLASL2000 corpus. According to the results, transformer-based architectures outperform convolutional baselines; Swin Transformer achieved a Top-1 accuracy of 34.1%. Our benchmarks highlight the challenges associated with long-tail class distributions in low-resource sign languages, and RoCoISLR provides the initial foundation for systematic RoISLR research.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12764v2" target="_blank"><h2>INC: An Indirect Neural Corrector for Auto-Regressive Hybrid PDE Solvers <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Hao Wei, Aleksandra Franz, Bjoern List, Nils Thuerey<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> Accepted at NeurIPS 2025. 35 pages, 10 figures<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> When simulating partial differential equations, hybrid solvers combine coarse numerical solvers with learned correctors. They promise accelerated simulations while adhering to physical constraints. However, as shown in our theoretical framework, directly applying learned corrections to solver outputs leads to significant autoregressive errors, which originate from amplified perturbations that accumulate during long-term rollouts, especially in chaotic regimes. To overcome this, we propose the Indirect Neural Corrector ($\mathrm{INC}$), which integrates learned corrections into the governing equations rather than applying direct state updates. Our key insight is that $\mathrm{INC}$ reduces the error amplification on the order of $Δt^{-1} + L$, where $Δt$ is the timestep and $L$ the Lipschitz constant. At the same time, our framework poses no architectural requirements and integrates seamlessly with arbitrary neural networks and solvers. We test $\mathrm{INC}$ in extensive benchmarks, covering numerous differentiable solvers, neural backbones, and test cases ranging from a 1D chaotic system to 3D turbulence. $\mathrm{INC}$ improves the long-term trajectory performance ($R^2$) by up to 158.7%, stabilizes blowups under aggressive coarsening, and for complex 3D turbulence cases yields speed-ups of several orders of magnitude. $\mathrm{INC}$ thus enables stable, efficient PDE emulation with formal error reduction, paving the way for faster scientific and engineering simulations with reliable physics guarantees. Our source code is available at https://github.com/tum-pbs/INC</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12755v1" target="_blank"><h2>Prompt-Driven Domain Adaptation for End-to-End Autonomous Driving via In-Context RL <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Aleesha Khurram, Amir Moeini, Shangtong Zhang, Rohan Chandra<br><strong><u>Categories:</u></strong> cs.RO, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> domain adaptation (title, abstract)<br><p><strong><u>Abstract:</u></strong> Despite significant progress and advances in autonomous driving, many end-to-end systems still struggle with domain adaptation (DA), such as transferring a policy trained under clear weather to adverse weather conditions. Typical DA strategies in the literature include collecting additional data in the target domain or re-training the model, or both. Both these strategies quickly become impractical as we increase scale and complexity of driving. These limitations have encouraged investigation into few-shot and zero-shot prompt-driven DA at inference time involving LLMs and VLMs. These methods work by adding a few state-action trajectories during inference to the prompt (similar to in-context learning). However, there are two limitations of such an approach: $(i)$ prompt-driven DA methods are currently restricted to perception tasks such as detection and segmentation and $(ii)$ they require expert few-shot data. In this work, we present a new approach to inference-time few-shot prompt-driven DA for closed-loop autonomous driving in adverse weather condition using in-context reinforcement learning (ICRL). Similar to other prompt-driven DA methods, our approach does not require any updates to the model parameters nor does it require additional data collection in adversarial weather regime. Furthermore, our approach advances the state-of-the-art in prompt-driven DA by extending to closed driving using general trajectories observed during inference. Our experiments using the CARLA simulator show that ICRL results in safer, more efficient, and more comfortable driving policies in the target domain compared to state-of-the-art prompt-driven DA baselines.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12754v1" target="_blank"><h2>Adaptively Coordinating with Novel Partners via Learned Latent Strategies <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Benjamin Li, Shuyang Shi, Lucia Romero, Huao Li, Yaqi Xie, Woojun Kim, Stefanos Nikolaidis, Michael Lewis, Katia Sycara, Simon Stepputtis<br><strong><u>Categories:</u></strong> cs.AI, cs.LG, cs.MA<br><strong><u>Comments:</u></strong> Accepted to NeurIPS 2025<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> variational autoencoder (abstract)<br><p><strong><u>Abstract:</u></strong> Adaptation is the cornerstone of effective collaboration among heterogeneous team members. In human-agent teams, artificial agents need to adapt to their human partners in real time, as individuals often have unique preferences and policies that may change dynamically throughout interactions. This becomes particularly challenging in tasks with time pressure and complex strategic spaces, where identifying partner behaviors and selecting suitable responses is difficult. In this work, we introduce a strategy-conditioned cooperator framework that learns to represent, categorize, and adapt to a broad range of potential partner strategies in real-time. Our approach encodes strategies with a variational autoencoder to learn a latent strategy space from agent trajectory data, identifies distinct strategy types through clustering, and trains a cooperator agent conditioned on these clusters by generating partners of each strategy type. For online adaptation to novel partners, we leverage a fixed-share regret minimization algorithm that dynamically infers and adjusts the partner's strategy estimation during interaction. We evaluate our method in a modified version of the Overcooked domain, a complex collaborative cooking environment that requires effective coordination among two players with a diverse potential strategy space. Through these experiments and an online user study, we demonstrate that our proposed agent achieves state of the art performance compared to existing baselines when paired with novel human, and agent teammates.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12745v1" target="_blank"><h2>DIVIDE: A Framework for Learning from Independent Multi-Mechanism Data Using Deep Encoders and Gaussian Processes <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Vivek Chawla, Boris Slautin, Utkarsh Pratiush, Dayakar Penumadu, Sergei Kalinin<br><strong><u>Categories:</u></strong> cs.LG, cond-mat.mtrl-sci<br><strong><u>Comments:</u></strong> 33 pages, 10 main figures, 7 additional in SI<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> latent space (abstract)<br><p><strong><u>Abstract:</u></strong> Scientific datasets often arise from multiple independent mechanisms such as spatial, categorical or structural effects, whose combined influence obscures their individual contributions. We introduce DIVIDE, a framework that disentangles these influences by integrating mechanism-specific deep encoders with a structured Gaussian Process in a joint latent space. Disentanglement here refers to separating independently acting generative factors. The encoders isolate distinct mechanisms while the Gaussian Process captures their combined effect with calibrated uncertainty. The architecture supports structured priors, enabling interpretable and mechanism-aware prediction as well as efficient active learning. DIVIDE is demonstrated on synthetic datasets combining categorical image patches with nonlinear spatial fields, on FerroSIM spin lattice simulations of ferroelectric patterns, and on experimental PFM hysteresis loops from PbTiO3 films. Across benchmarks, DIVIDE separates mechanisms, reproduces additive and scaled interactions, and remains robust under noise. The framework extends naturally to multifunctional datasets where mechanical, electromagnetic or optical responses coexist.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12742v1" target="_blank"><h2>Stabilizing Self-Consuming Diffusion Models with Latent Space Filtering <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zhongteng Cai, Yaxuan Wang, Yang Liu, Xueru Zhang<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> Accepted by AAAI-26<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> latent space (title, abstract)<br><p><strong><u>Abstract:</u></strong> As synthetic data proliferates across the Internet, it is often reused to train successive generations of generative models. This creates a ``self-consuming loop" that can lead to training instability or \textit{model collapse}. Common strategies to address the issue -- such as accumulating historical training data or injecting fresh real data -- either increase computational cost or require expensive human annotation. In this paper, we empirically analyze the latent space dynamics of self-consuming diffusion models and observe that the low-dimensional structure of latent representations extracted from synthetic data degrade over generations. Based on this insight, we propose \textit{Latent Space Filtering} (LSF), a novel approach that mitigates model collapse by filtering out less realistic synthetic data from mixed datasets. Theoretically, we present a framework that connects latent space degradation to empirical observations. Experimentally, we show that LSF consistently outperforms existing baselines across multiple real-world datasets, effectively mitigating model collapse without increasing training cost or relying on human annotation.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12737v1" target="_blank"><h2>From Images to Physics: Probabilistic Inference of Galaxy Parameters and Emission Lines via VAE & Normalizing Flows <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Adiba Amira Siddiqa, Sayed Shafaat Mahmud, Rafael Martinez-Galarza<br><strong><u>Categories:</u></strong> astro-ph.GA, astro-ph.IM<br><strong><u>Comments:</u></strong> 9 pages, 5 figures, Accepted at NeurIPS 2025 Machine Learning and the Physical Sciences Workshop<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> variational autoencoder (abstract), VAE (title, abstract)<br><p><strong><u>Abstract:</u></strong> We introduce a Variational Autoencoder (VAE)--Normalizing Flow (NF) framework for rapid probabilistic inference of galaxy properties and emission line fluxes at $z \leq 0.3$ from SDSS \textit{gri} imaging and photometry. Our model probabilistically infers stellar mass, star formation rate (SFR), redshift, gas-phase metallicity, and central black hole mass for a given galaxy. The model accruacy matches current non-spectroscopic methods for stellar mass and redshift, surpasses them for SFR and metallicity, and introduces the first probabilistic central black hole mass estimates from imaging + photometry. It also delivers probabilistic estimates of H$α$, H$β$, [N~\textsc{ii}], and [O~\textsc{iii}] emission line fluxes directly from imaging, enabling SFR, metallicity, dust, and AGN/shock diagnostics without spectroscopy. This approach opens new pathways for scalable, physics-informed inference in upcoming surveys such as Roman and Rubin LSST.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14796v1" target="_blank"><h2>Opinion Mining and Analysis Using Hybrid Deep Neural Networks <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Adel Hidri, Suleiman Ali Alsaif, Muteeb Alahmari, Eman AlShehri, Minyar Sassi Hidri<br><strong><u>Categories:</u></strong> cs.CL, cs.AI<br><strong><u>Comments:</u></strong> 22 pages, 4 figures, 11 tables<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> Understanding customer attitudes has become a critical component of decision-making due to the growing influence of social media and e-commerce. Text-based opinions are the most structured, hence playing an important role in sentiment analysis. Most of the existing methods, which include lexicon-based approaches and traditional machine learning techniques, are insufficient for handling contextual nuances and scalability. While the latter has limitations in model performance and generalization, deep learning (DL) has achieved improvement, especially on semantic relationship capturing with recurrent neural networks (RNNs) and convolutional neural networks (CNNs). The aim of the study is to enhance opinion mining by introducing a hybrid deep neural network model that combines a bidirectional gated recurrent unit (BGRU) and long short-term memory (LSTM) layers to improve sentiment analysis, particularly addressing challenges such as contextual nuance, scalability, and class imbalance. To substantiate the efficacy of the proposed model, we conducted comprehensive experiments utilizing benchmark datasets, encompassing IMDB movie critiques and Amazon product evaluations. The introduced hybrid BGRULSTM (HBGRU-LSTM) architecture attained a testing accuracy of 95%, exceeding the performance of traditional DL frameworks such as LSTM (93.06%), CNN+LSTM (93.31%), and GRU+LSTM (92.20%). Moreover, our model exhibited a noteworthy enhancement in recall for negative sentiments, escalating from 86% (unbalanced dataset) to 96% (balanced dataset), thereby ensuring a more equitable and just sentiment classification. Furthermore, the model diminished misclassification loss from 20.24% for unbalanced to 13.3% for balanced dataset, signifying enhanced generalization and resilience.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12725v1" target="_blank"><h2>Convolutional Model Trees <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> William Ward Armstrong<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 9 pages. No figures. This paper gives an algorithm for creating a continuously differentiable approximation from sample data from the same type of function(in theory) using a forest of model trees (like CART trees with linear functions instead of constants)<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> convolutional (title)<br><p><strong><u>Abstract:</u></strong> A method for creating a forest of model trees to fit samples of a function defined on images is described in several steps: down-sampling the images, determining a tree's hyperplanes, applying convolutions to the hyperplanes to handle small distortions of training images, and creating forests of model trees to increase accuracy and achieve a smooth fit. A 1-to-1 correspondence among pixels of images, coefficients of hyperplanes and coefficients of leaf functions offers the possibility of dealing with larger distortions such as arbitrary rotations or changes of perspective. A theoretical method for smoothing forest outputs to produce a continuously differentiable approximation is described. Within that framework, a training procedure is proved to converge.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12723v1" target="_blank"><h2>LAYA: Layer-wise Attention Aggregation for Interpretable Depth-Aware Neural Networks <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Gennaro Vessio<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Deep neural networks typically rely on the representation produced by their final hidden layer to make predictions, implicitly assuming that this single vector fully captures the semantics encoded across all preceding transformations. However, intermediate layers contain rich and complementary information -- ranging from low-level patterns to high-level abstractions -- that is often discarded when the decision head depends solely on the last representation. This paper revisits the role of the output layer and introduces LAYA (Layer-wise Attention Aggregator), a novel output head that dynamically aggregates internal representations through attention. Instead of projecting only the deepest embedding, LAYA learns input-conditioned attention weights over layer-wise features, yielding an interpretable and architecture-agnostic mechanism for synthesizing predictions. Experiments on vision and language benchmarks show that LAYA consistently matches or improves the performance of standard output heads, with relative gains of up to about one percentage point in accuracy, while providing explicit layer-attribution scores that reveal how different abstraction levels contribute to each decision. Crucially, these interpretability signals emerge directly from the model's computation, without any external post hoc explanations. The code to reproduce LAYA is publicly available at: https://github.com/gvessio/LAYA.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13791v1" target="_blank"><h2>XAI-Driven Deep Learning for Protein Sequence Functional Group Classification <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Pratik Chakraborty, Aryan Bhargava<br><strong><u>Categories:</u></strong> q-bio.QM, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> 8 pages, 4 figures<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), explainable (abstract), neural network (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Proteins perform essential biological functions, and accurate classification of their sequences is critical for understanding structure-function relationships, enzyme mechanisms, and molecular interactions. This study presents a deep learning-based framework for functional group classification of protein sequences derived from the Protein Data Bank (PDB). Four architectures were implemented: Convolutional Neural Network (CNN), Bidirectional Long Short-Term Memory (BiLSTM), CNN-BiLSTM hybrid, and CNN with Attention. Each model was trained using k-mer integer encoding to capture both local and long-range dependencies. Among these, the CNN achieved the highest validation accuracy of 91.8%, demonstrating the effectiveness of localized motif detection. Explainable AI techniques, including Grad-CAM and Integrated Gradients, were applied to interpret model predictions and identify biologically meaningful sequence motifs. The discovered motifs, enriched in histidine, aspartate, glutamate, and lysine, represent amino acid residues commonly found in catalytic and metal-binding regions of transferase enzymes. These findings highlight that deep learning models can uncover functionally relevant biochemical signatures, bridging the gap between predictive accuracy and biological interpretability in protein sequence analysis.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12709v1" target="_blank"><h2>Adaptive Graph Rewiring to Mitigate Over-Squashing in Mesh-Based GNNs for Fluid Dynamics Simulations <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Sangwoo Seo, Hyunsung Kim, Jiwan Kim, Chanyoung Park<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> Preprint<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Mesh-based simulation using Graph Neural Networks (GNNs) has been recognized as a promising approach for modeling fluid dynamics. However, the mesh refinement techniques which allocate finer resolution to regions with steep gradients can induce the over-squashing problem in mesh-based GNNs, which prevents the capture of long-range physical interactions. Conventional graph rewiring methods attempt to alleviate this issue by adding new edges, but they typically complete all rewiring operations before applying them to the GNN. These approaches are physically unrealistic, as they assume instantaneous interactions between distant nodes and disregard the distance information between particles. To address these limitations, we propose a novel framework, called Adaptive Graph Rewiring in Mesh-Based Graph Neural Networks (AdaMeshNet), that introduces an adaptive rewiring process into the message-passing procedure to model the gradual propagation of physical interactions. Our method computes a rewiring delay score for bottleneck nodes in the mesh graph, based on the shortest-path distance and the velocity difference. Using this score, it dynamically selects the message-passing layer at which new edges are rewired, which can lead to adaptive rewiring in a mesh graph. Extensive experiments on mesh-based fluid simulations demonstrate that AdaMeshNet outperforms conventional rewiring methods, effectively modeling the sequential nature of physical interactions and enabling more accurate predictions.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13790v1" target="_blank"><h2>GeoPl@ntNet: A Platform for Exploring Essential Biodiversity Variables <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Lukas Picek, César Leblanc, Alexis Joly, Pierre Bonnet, Rémi Palard, Maximilien Servajean<br><strong><u>Categories:</u></strong> q-bio.QM, cs.AI<br><strong><u>Comments:</u></strong> 4 pages, 5 figures, and 2 tables<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> This paper describes GeoPl@ntNet, an interactive web application designed to make Essential Biodiversity Variables accessible and understandable to everyone through dynamic maps and fact sheets. Its core purpose is to allow users to explore high-resolution AI-generated maps of species distributions, habitat types, and biodiversity indicators across Europe. These maps, developed through a cascading pipeline involving convolutional neural networks and large language models, provide an intuitive yet information-rich interface to better understand biodiversity, with resolutions as precise as 50x50 meters. The website also enables exploration of specific regions, allowing users to select areas of interest on the map (e.g., urban green spaces, protected areas, or riverbanks) to view local species and their coverage. Additionally, GeoPl@ntNet generates comprehensive reports for selected regions, including insights into the number of protected species, invasive species, and endemic species.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12694v1" target="_blank"><h2>X-VMamba: Explainable Vision Mamba <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Mohamed A. Mabrok, Yalda Zafari<br><strong><u>Categories:</u></strong> cs.CV, cs.LG, math.DS<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> explainable (title), transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> State Space Models (SSMs), particularly the Mamba architecture, have recently emerged as powerful alternatives to Transformers for sequence modeling, offering linear computational complexity while achieving competitive performance. Yet, despite their effectiveness, understanding how these Vision SSMs process spatial information remains challenging due to the lack of transparent, attention-like mechanisms. To address this gap, we introduce a controllability-based interpretability framework that quantifies how different parts of the input sequence (tokens or patches) influence the internal state dynamics of SSMs. We propose two complementary formulations: a Jacobian-based method applicable to any SSM architecture that measures influence through the full chain of state propagation, and a Gramian-based approach for diagonal SSMs that achieves superior speed through closed-form analytical solutions. Both methods operate in a single forward pass with linear complexity, requiring no architectural modifications or hyperparameter tuning. We validate our framework through experiments on three diverse medical imaging modalities, demonstrating that SSMs naturally implement hierarchical feature refinement from diffuse low-level textures in early layers to focused, clinically meaningful patterns in deeper layers. Our analysis reveals domain-specific controllability signatures aligned with diagnostic criteria, progressive spatial selectivity across the network hierarchy, and the substantial influence of scanning strategies on attention patterns. Beyond medical imaging, we articulate applications spanning computer vision, natural language processing, and cross-domain tasks. Our framework establishes controllability analysis as a unified, foundational interpretability paradigm for SSMs across all domains. Code and analysis tools will be made available upon publication</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12693v1" target="_blank"><h2>HEDGE: Hallucination Estimation via Dense Geometric Entropy for VQA with Vision-Language Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Sushant Gautam, Michael A. Riegler, Pål Halvorsen<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Vision-language models (VLMs) enable open-ended visual question answering but remain prone to hallucinations. We present HEDGE, a unified framework for hallucination detection that combines controlled visual perturbations, semantic clustering, and robust uncertainty metrics. HEDGE integrates sampling, distortion synthesis, clustering (entailment- and embedding-based), and metric computation into a reproducible pipeline applicable across multimodal architectures.
  Evaluations on VQA-RAD and KvasirVQA-x1 with three representative VLMs (LLaVA-Med, Med-Gemma, Qwen2.5-VL) reveal clear architecture- and prompt-dependent trends. Hallucination detectability is highest for unified-fusion models with dense visual tokenization (Qwen2.5-VL) and lowest for architectures with restricted tokenization (Med-Gemma). Embedding-based clustering often yields stronger separation when applied directly to the generated answers, whereas NLI-based clustering remains advantageous for LLaVA-Med and for longer, sentence-level responses. Across configurations, the VASE metric consistently provides the most robust hallucination signal, especially when paired with embedding clustering and a moderate sampling budget (n ~ 10-15). Prompt design also matters: concise, label-style outputs offer clearer semantic structure than syntactically constrained one-sentence responses.
  By framing hallucination detection as a geometric robustness problem shaped jointly by sampling scale, prompt structure, model architecture, and clustering strategy, HEDGE provides a principled, compute-aware foundation for evaluating multimodal reliability. The hedge-bench PyPI library enables reproducible and extensible benchmarking, with full code and experimental resources available at https://github.com/Simula/HEDGE .</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12691v1" target="_blank"><h2>R$^{2}$Seg: Training-Free OOD Medical Tumor Segmentation via Anatomical Reasoning and Statistical Rejection <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Shuaike Shen, Ke Liu, Jiaqing Xie, Shangde Gao, Chunhua Shen, Ge Liu, Mireia Crispin-Ortuzar, Shangqi Gao<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> multi-modal (abstract)<br><p><strong><u>Abstract:</u></strong> Foundation models for medical image segmentation struggle under out-of-distribution (OOD) shifts, often producing fragmented false positives on OOD tumors. We introduce R$^{2}$Seg, a training-free framework for robust OOD tumor segmentation that operates via a two-stage Reason-and-Reject process. First, the Reason step employs an LLM-guided anatomical reasoning planner to localize organ anchors and generate multi-scale ROIs. Second, the Reject step applies two-sample statistical testing to candidates generated by a frozen foundation model (BiomedParse) within these ROIs. This statistical rejection filter retains only candidates significantly different from normal tissue, effectively suppressing false positives. Our framework requires no parameter updates, making it compatible with zero-update test-time augmentation and avoiding catastrophic forgetting. On multi-center and multi-modal tumor segmentation benchmarks, R$^{2}$Seg substantially improves Dice, specificity, and sensitivity over strong baselines and the original foundation models. Code are available at https://github.com/Eurekashen/R2Seg.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12690v1" target="_blank"><h2>Improving Direct Persian-English Speech-to-Speech Translation with Discrete Units and Synthetic Parallel Data <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Sina Rashidi, Hossein Sameti<br><strong><u>Categories:</u></strong> cs.CL, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Direct speech-to-speech translation (S2ST), in which all components are trained jointly, is an attractive alternative to cascaded systems because it offers a simpler pipeline and lower inference latency. However, direct S2ST models require large amounts of parallel speech data in the source and target languages, which are rarely available for low-resource languages such as Persian. This paper presents a direct S2ST system for translating Persian speech into English speech, as well as a pipeline for synthetic parallel Persian-English speech generation. The model comprises three components: (1) a conformer-based encoder, initialized from self-supervised pre-training, maps source speech to high-level acoustic representations; (2) a causal transformer decoder with relative position multi-head attention translates these representations into discrete target speech units; (3) a unit-based neural vocoder generates waveforms from the predicted discrete units. To mitigate the data scarcity problem, we construct a new Persian-English parallel speech corpus by translating Persian speech transcriptions into English using a large language model and then synthesizing the corresponding English speech with a state-of-the-art zero-shot text-to-speech system. The resulting corpus increases the amount of available parallel speech by roughly a factor of six. On the Persian-English portion of the CVSS corpus, the proposed model achieves improvement of 4.6 ASR BLEU with the synthetic data over direct baselines. These results indicate that combining self-supervised pre-training, discrete speech units, and synthetic parallel data is effective for improving direct S2ST in low-resource language pairs such as Persian-English</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12682v1" target="_blank"><h2>Attention-Enhanced Convolutional Autoencoder and Structured Delay Embeddings for Weather Prediction <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Amirpasha Hedayat, Karthik Duraisamy<br><strong><u>Categories:</u></strong> cs.LG, physics.ao-ph<br><strong><u>Comments:</u></strong> 13 pages, 7 figures, Preprint<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> convolutional (title, abstract), dimensionality reduction (abstract), latent space (abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Weather prediction is a quintessential problem involving the forecasting of a complex, nonlinear, and chaotic high-dimensional dynamical system. This work introduces an efficient reduced-order modeling (ROM) framework for short-range weather prediction and investigates fundamental questions in dimensionality reduction and reduced order modeling of such systems. Unlike recent AI-driven models, which require extensive computational resources, our framework prioritizes efficiency while achieving reasonable accuracy. Specifically, a ResNet-based convolutional autoencoder augmented by block attention modules is developed to reduce the dimensionality of high-dimensional weather data. Subsequently, a linear operator is learned in the time-delayed embedding of the latent space to efficiently capture the dynamics. Using the ERA5 reanalysis dataset, we demonstrate that this framework performs well in-distribution as evidenced by effectively predicting weather patterns within training data periods. We also identify important limitations in generalizing to future states, particularly in maintaining prediction accuracy beyond the training window. Our analysis reveals that weather systems exhibit strong temporal correlations that can be effectively captured through linear operations in an appropriately constructed embedding space, and that projection error rather than inference error is the main bottleneck. These findings shed light on some key challenges in reduced-order modeling of chaotic systems and point toward opportunities for hybrid approaches that combine efficient reduced-order models as baselines with more sophisticated AI architectures, particularly for applications in long-term climate modeling where computational efficiency is paramount.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12648v1" target="_blank"><h2>Scalable Hierarchical AI-Blockchain Framework for Real-Time Anomaly Detection in Large-Scale Autonomous Vehicle Networks <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Rathin Chandra Shit, Sharmila Subudhi<br><strong><u>Categories:</u></strong> cs.CR, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> Submitted to the Journal<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> anomaly detection (title, abstract), multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> The security of autonomous vehicle networks is facing major challenges, owing to the complexity of sensor integration, real-time performance demands, and distributed communication protocols that expose vast attack surfaces around both individual and network-wide safety. Existing security schemes are unable to provide sub-10 ms (milliseconds) anomaly detection and distributed coordination of large-scale networks of vehicles within an acceptable safety/privacy framework. This paper introduces a three-tier hybrid security architecture HAVEN (Hierarchical Autonomous Vehicle Enhanced Network), which decouples real-time local threat detection and distributed coordination operations. It incorporates a light ensemble anomaly detection model on the edge (first layer), Byzantine-fault-tolerant federated learning to aggregate threat intelligence at a regional scale (middle layer), and selected blockchain mechanisms (top layer) to ensure critical security coordination. Extensive experimentation is done on a real-world autonomous driving dataset. Large-scale simulations with the number of vehicles ranging between 100 and 1000 and different attack types, such as sensor spoofing, jamming, and adversarial model poisoning, are conducted to test the scalability and resiliency of HAVEN. Experimental findings show sub-10 ms detection latency with an accuracy of 94% and F1-score of 92% across multimodal sensor data, Byzantine fault tolerance validated with 20\% compromised nodes, and a reduced blockchain storage overhead, guaranteeing sufficient differential privacy. The proposed framework overcomes the important trade-off between real-time safety obligation and distributed security coordination with novel three-tiered processing. The scalable architecture of HAVEN is shown to provide great improvement in detection accuracy as well as network resilience over other methods.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13789v1" target="_blank"><h2>Uncovering and Aligning Anomalous Attention Heads to Defend Against NLP Backdoor Attacks <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Haotian Jin, Yang Li, Haihui Fan, Lin Shen, Xiangfang Li, Bo Li<br><strong><u>Categories:</u></strong> cs.CR, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Backdoor attacks pose a serious threat to the security of large language models (LLMs), causing them to exhibit anomalous behavior under specific trigger conditions. The design of backdoor triggers has evolved from fixed triggers to dynamic or implicit triggers. This increased flexibility in trigger design makes it challenging for defenders to identify their specific forms accurately. Most existing backdoor defense methods are limited to specific types of triggers or rely on an additional clean model for support. To address this issue, we propose a backdoor detection method based on attention similarity, enabling backdoor detection without prior knowledge of the trigger. Our study reveals that models subjected to backdoor attacks exhibit unusually high similarity among attention heads when exposed to triggers. Based on this observation, we propose an attention safety alignment approach combined with head-wise fine-tuning to rectify potentially contaminated attention heads, thereby effectively mitigating the impact of backdoor attacks. Extensive experimental results demonstrate that our method significantly reduces the success rate of backdoor attacks while preserving the model's performance on downstream tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12644v1" target="_blank"><h2>NFQ2.0: The CartPole Benchmark Revisited <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Sascha Lange, Roland Hafner, Martin Riedmiller<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> This article revisits the 20-year-old neural fitted Q-iteration (NFQ) algorithm on its classical CartPole benchmark. NFQ was a pioneering approach towards modern Deep Reinforcement Learning (Deep RL) in applying multi-layer neural networks to reinforcement learning for real-world control problems. We explore the algorithm's conceptual simplicity and its transition from online to batch learning, which contributed to its stability. Despite its initial success, NFQ required extensive tuning and was not easily reproducible on real-world control problems. We propose a modernized variant NFQ2.0 and apply it to the CartPole task, concentrating on a real-world system build from standard industrial components, to investigate and improve the learning process's repeatability and robustness. Through ablation studies, we highlight key design decisions and hyperparameters that enhance performance and stability of NFQ2.0 over the original variant. Finally, we demonstrate how our findings can assist practitioners in reproducing and improving results and applying deep reinforcement learning more effectively in industrial contexts.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12643v1" target="_blank"><h2>Adaptive Dual-Layer Web Application Firewall (ADL-WAF) Leveraging Machine Learning for Enhanced Anomaly and Threat Detection <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Ahmed Sameh, Sahar Selim<br><strong><u>Categories:</u></strong> cs.CR, cs.LG, cs.NI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> anomaly detection (abstract)<br><p><strong><u>Abstract:</u></strong> Web Application Firewalls are crucial for protecting web applications against a wide range of cyber threats. Traditional Web Application Firewalls often struggle to effectively distinguish between malicious and legitimate traffic, leading to limited efficacy in threat detection. To overcome these limitations, this paper proposes an Adaptive Dual-Layer WAF employing a two-layered Machine Learning model designed to enhance the accuracy of anomaly and threat detection. The first layer employs a Decision Tree (DT) algorithm to detect anomalies by identifying traffic deviations from established normal patterns. The second layer employs Support Vector Machine to classify these anomalies as either threat anomalies or benign anomalies. Our Adaptive Dual Layer WAF incorporates comprehensive data pre-processing and feature engineering techniques and has been thoroughly evaluated using five large benchmark datasets. Evaluation using these datasets shows that ADL WAF achieves a detection accuracy of 99.88% and a precision of 100%, significantly enhancing anomaly detection and reducing false positives. These findings suggest that integrating machine learning techniques into WAFs can substantially improve web application security by providing more accurate and efficient threat detection.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12631v1" target="_blank"><h2>Multivariate Diffusion Transformer with Decoupled Attention for High-Fidelity Mask-Text Collaborative Facial Generation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yushe Cao, Dianxi Shi, Xing Fu, Xuechao Zou, Haikuo Peng, Xueqi Li, Chun Yu, Junliang Xing<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract), transformer (title, abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> While significant progress has been achieved in multimodal facial generation using semantic masks and textual descriptions, conventional feature fusion approaches often fail to enable effective cross-modal interactions, thereby leading to suboptimal generation outcomes. To address this challenge, we introduce MDiTFace--a customized diffusion transformer framework that employs a unified tokenization strategy to process semantic mask and text inputs, eliminating discrepancies between heterogeneous modality representations. The framework facilitates comprehensive multimodal feature interaction through stacked, newly designed multivariate transformer blocks that process all conditions synchronously. Additionally, we design a novel decoupled attention mechanism by dissociating implicit dependencies between mask tokens and temporal embeddings. This mechanism segregates internal computations into dynamic and static pathways, enabling caching and reuse of features computed in static pathways after initial calculation, thereby reducing additional computational overhead introduced by mask condition by over 94% while maintaining performance. Extensive experiments demonstrate that MDiTFace significantly outperforms other competing methods in terms of both facial fidelity and conditional consistency.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12614v1" target="_blank"><h2>OPFormer: Object Pose Estimation leveraging foundation model with geometric encoding <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Artem Moroz, Vít Zeman, Martin Mikšík, Elizaveta Isianova, Miroslav David, Pavel Burget, Varun Burde<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.LG, cs.RO<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> We introduce a unified, end-to-end framework that seamlessly integrates object detection and pose estimation with a versatile onboarding process. Our pipeline begins with an onboarding stage that generates object representations from either traditional 3D CAD models or, in their absence, by rapidly reconstructing a high-fidelity neural representation (NeRF) from multi-view images. Given a test image, our system first employs the CNOS detector to localize target objects. For each detection, our novel pose estimation module, OPFormer, infers the precise 6D pose. The core of OPFormer is a transformer-based architecture that leverages a foundation model for robust feature extraction. It uniquely learns a comprehensive object representation by jointly encoding multiple template views and enriches these features with explicit 3D geometric priors using Normalized Object Coordinate Space (NOCS). A decoder then establishes robust 2D-3D correspondences to determine the final pose. Evaluated on the challenging BOP benchmarks, our integrated system demonstrates a strong balance between accuracy and efficiency, showcasing its practical applicability in both model-based and model-free scenarios.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12609v1" target="_blank"><h2>Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yunxin Li, Xinyu Chen, Shenyuan Jiang, Haoyuan Shi, Zhenyu Liu, Xuanyu Zhang, Nanhao Deng, Zhenran Xu, Yicheng Ma, Meishan Zhang, Baotian Hu, Min Zhang<br><strong><u>Categories:</u></strong> cs.CL, cs.AI, cs.CV<br><strong><u>Comments:</u></strong> 47 pages,10 Figures, Project Website:this https URLCodes:this https URL<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee's Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the Qwen2.5-7B dense architecture, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12601v1" target="_blank"><h2>Symmetry-Aware Graph Metanetwork Autoencoders: Model Merging through Parameter Canonicalization <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Odysseas Boufalis, Jorge Carrasco-Pollo, Joshua Rosenthal, Eduardo Terres-Caballero, Alejandro García-Castellanos<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Neural network parameterizations exhibit inherent symmetries that yield multiple equivalent minima within the loss landscape. Scale Graph Metanetworks (ScaleGMNs) explicitly leverage these symmetries by proposing an architecture equivariant to both permutation and parameter scaling transformations. Previous work by Ainsworth et al. (2023) addressed permutation symmetries through a computationally intensive combinatorial assignment problem, demonstrating that leveraging permutation symmetries alone can map networks into a shared loss basin. In this work, we extend their approach by also incorporating scaling symmetries, presenting an autoencoder framework utilizing ScaleGMNs as invariant encoders. Experimental results demonstrate that our method aligns Implicit Neural Representations (INRs) and Convolutional Neural Networks (CNNs) under both permutation and scaling symmetries without explicitly solving the assignment problem. This approach ensures that similar networks naturally converge within the same basin, facilitating model merging, i.e., smooth linear interpolation while avoiding regions of high loss. The code is publicly available on our GitHub repository.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12592v1" target="_blank"><h2>Knowledge is Overrated: A zero-knowledge machine learning and cryptographic hashing-based framework for verifiable, low latency inference at the LHC <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Pratik Jawahar, Caterina Doglioni, Maurizio Pierini<br><strong><u>Categories:</u></strong> hep-ex, cs.AI, stat.ML<br><strong><u>Comments:</u></strong> ML4PS NeurIPS 2025<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> anomaly detection (abstract)<br><p><strong><u>Abstract:</u></strong> Low latency event-selection (trigger) algorithms are essential components of Large Hadron Collider (LHC) operation. Modern machine learning (ML) models have shown great offline performance as classifiers and could improve trigger performance, thereby improving downstream physics analyses. However, inference on such large models does not satisfy the $40\text{MHz}$ online latency constraint at the LHC. In this work, we propose \texttt{PHAZE}, a novel framework built on cryptographic techniques like hashing and zero-knowledge machine learning (zkML) to achieve low latency inference, via a certifiable, early-exit mechanism from an arbitrarily large baseline model. We lay the foundations for such a framework to achieve nanosecond-order latency and discuss its inherent advantages, such as built-in anomaly detection, within the scope of LHC triggers, as well as its potential to enable a dynamic low-level trigger in the future.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12590v2" target="_blank"><h2>Fine-Grained Representation for Lane Topology Reasoning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Guoqing Xu, Yiheng Li, Yang Yang<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> Accepted by AAAI 2026<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Precise modeling of lane topology is essential for autonomous driving, as it directly impacts navigation and control decisions. Existing methods typically represent each lane with a single query and infer topological connectivity based on the similarity between lane queries. However, this kind of design struggles to accurately model complex lane structures, leading to unreliable topology prediction. In this view, we propose a Fine-Grained lane topology reasoning framework (TopoFG). It divides the procedure from bird's-eye-view (BEV) features to topology prediction via fine-grained queries into three phases, i.e., Hierarchical Prior Extractor (HPE), Region-Focused Decoder (RFD), and Robust Boundary-Point Topology Reasoning (RBTR). Specifically, HPE extracts global spatial priors from the BEV mask and local sequential priors from in-lane keypoint sequences to guide subsequent fine-grained query modeling. RFD constructs fine-grained queries by integrating the spatial and sequential priors. It then samples reference points in RoI regions of the mask and applies cross-attention with BEV features to refine the query representations of each lane. RBTR models lane connectivity based on boundary-point query features and further employs a topological denoising strategy to reduce matching ambiguity. By integrating spatial and sequential priors into fine-grained queries and applying a denoising strategy to boundary-point topology reasoning, our method precisely models complex lane structures and delivers trustworthy topology predictions. Extensive experiments on the OpenLane-V2 benchmark demonstrate that TopoFG achieves new state-of-the-art performance, with an OLS of 48.0 on subsetA and 45.4 on subsetB.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12581v1" target="_blank"><h2>LMM-IR: Large-Scale Netlist-Aware Multimodal Framework for Static IR-Drop Prediction <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Kai Ma, Zhen Wang, Hongquan He, Qi Xu, Tinghuan Chen, Hao Geng<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> latent space (abstract), multimodal (title, abstract), transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Static IR drop analysis is a fundamental and critical task in the field of chip design. Nevertheless, this process can be quite time-consuming, potentially requiring several hours. Moreover, addressing IR drop violations frequently demands iterative analysis, thereby causing the computational burden. Therefore, fast and accurate IR drop prediction is vital for reducing the overall time invested in chip design. In this paper, we firstly propose a novel multimodal approach that efficiently processes SPICE files through large-scale netlist transformer (LNT). Our key innovation is representing and processing netlist topology as 3D point cloud representations, enabling efficient handling of netlist with up to hundreds of thousands to millions nodes. All types of data, including netlist files and image data, are encoded into latent space as features and fed into the model for static voltage drop prediction. This enables the integration of data from multiple modalities for complementary predictions. Experimental results demonstrate that our proposed algorithm can achieve the best F1 score and the lowest MAE among the winning teams of the ICCAD 2023 contest and the state-of-the-art algorithms.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12573v1" target="_blank"><h2>Mitigating Length Bias in RLHF through a Causal Lens <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Hyeonji Kim, Sujeong Oh, Sanghack Lee<br><strong><u>Categories:</u></strong> cs.CL, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> data augmentation (abstract)<br><p><strong><u>Abstract:</u></strong> Reinforcement learning from human feedback (RLHF) is widely used to align large language models (LLMs) with human preferences. However, RLHF-trained reward models often exhibit length bias -- a systematic tendency to favor longer responses by conflating verbosity with quality. We propose a causal framework for analyzing and mitigating length bias in RLHF reward modeling. Central to our approach is a counterfactual data augmentation method that generates response pairs designed to isolate content quality from verbosity. These counterfactual examples are then used to train the reward model, enabling it to assess responses based on content quality independently of verbosity. Specifically, we construct (1) length-divergent pairs with similar content and (2) content-divergent pairs of similar length. Empirical evaluations show that our method reduces length bias in reward assignment and leads to more concise, content-focused outputs from the policy model. These findings demonstrate that the proposed approach effectively reduces length bias and improves the robustness and content sensitivity of reward modeling in RLHF pipelines.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13784v1" target="_blank"><h2>Temporal Object-Aware Vision Transformer for Few-Shot Video Object Detection <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yogesh Kumar, Anand Mishra<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> Accepted at AAAI 2026 Main Track<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> transformer (title)<br><p><strong><u>Abstract:</u></strong> Few-shot Video Object Detection (FSVOD) addresses the challenge of detecting novel objects in videos with limited labeled examples, overcoming the constraints of traditional detection methods that require extensive training data. This task presents key challenges, including maintaining temporal consistency across frames affected by occlusion and appearance variations, and achieving novel object generalization without relying on complex region proposals, which are often computationally expensive and require task-specific training. Our novel object-aware temporal modeling approach addresses these challenges by incorporating a filtering mechanism that selectively propagates high-confidence object features across frames. This enables efficient feature progression, reduces noise accumulation, and enhances detection accuracy in a few-shot setting. By utilizing few-shot trained detection and classification heads with focused feature propagation, we achieve robust temporal consistency without depending on explicit object tube proposals. Our approach achieves performance gains, with AP improvements of 3.7% (FSVOD-500), 5.3% (FSYTV-40), 4.3% (VidOR), and 4.5 (VidVRD) in the 5-shot setting. Further results demonstrate improvements in 1-shot, 3-shot, and 10-shot configurations. We make the code public at: https://github.com/yogesh-iitj/fs-video-vit</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12529v1" target="_blank"><h2>Accepted with Minor Revisions: Value of AI-Assisted Scientific Writing <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Sanchaita Hazra, Doeun Lee, Bodhisattwa Prasad Majumder, Sachin Kumar<br><strong><u>Categories:</u></strong> cs.HC, cs.AI, cs.CL<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Large Language Models have seen expanding application across domains, yet their effectiveness as assistive tools for scientific writing -- an endeavor requiring precision, multimodal synthesis, and domain expertise -- remains insufficiently understood. We examine the potential of LLMs to support domain experts in scientific writing, with a focus on abstract composition. We design an incentivized randomized controlled trial with a hypothetical conference setup where participants with relevant expertise are split into an author and reviewer pool. Inspired by methods in behavioral science, our novel incentive structure encourages authors to edit the provided abstracts to an acceptable quality for a peer-reviewed submission. Our 2x2 between-subject design expands into two dimensions: the implicit source of the provided abstract and the disclosure of it. We find authors make most edits when editing human-written abstracts compared to AI-generated abstracts without source attribution, often guided by higher perceived readability in AI generation. Upon disclosure of source information, the volume of edits converges in both source treatments. Reviewer decisions remain unaffected by the source of the abstract, but bear a significant correlation with the number of edits made. Careful stylistic edits, especially in the case of AI-generated abstracts, in the presence of source information, improve the chance of acceptance. We find that AI-generated abstracts hold potential to reach comparable levels of acceptability to human-written ones with minimal revision, and that perceptions of AI authorship, rather than objective quality, drive much of the observed editing behavior. Our findings reverberate the significance of source disclosure in collaborative scientific writing.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12507v1" target="_blank"><h2>Hierarchical Frequency-Decomposition Graph Neural Networks for Road Network Representation Learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jingtian Ma, Jingyuan Wang, Leong Hou U<br><strong><u>Categories:</u></strong> cs.LG, cs.GR<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract), transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Road networks are critical infrastructures underpinning intelligent transportation systems and their related applications. Effective representation learning of road networks remains challenging due to the complex interplay between spatial structures and frequency characteristics in traffic patterns. Existing graph neural networks for modeling road networks predominantly fall into two paradigms: spatial-based methods that capture local topology but tend to over-smooth representations, and spectral-based methods that analyze global frequency components but often overlook localized variations. This spatial-spectral misalignment limits their modeling capacity for road networks exhibiting both coarse global trends and fine-grained local fluctuations. To bridge this gap, we propose HiFiNet, a novel hierarchical frequency-decomposition graph neural network that unifies spatial and spectral modeling. HiFiNet constructs a multi-level hierarchy of virtual nodes to enable localized frequency analysis, and employs a decomposition-updating-reconstruction framework with a topology-aware graph transformer to separately model and fuse low- and high-frequency signals. Theoretically justified and empirically validated on multiple real-world datasets across four downstream tasks, HiFiNet demonstrates superior performance and generalization ability in capturing effective road network representations.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12502v1" target="_blank"><h2>BSO: Binary Spiking Online Optimization Algorithm <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yu Liang, Yu Yang, Wenjie Wei, Ammar Belatreche, Shuai Wang, Malu Zhang, Yang Yang<br><strong><u>Categories:</u></strong> cs.LG, cs.CV<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Binary Spiking Neural Networks (BSNNs) offer promising efficiency advantages for resource-constrained computing. However, their training algorithms often require substantial memory overhead due to latent weights storage and temporal processing requirements. To address this issue, we propose Binary Spiking Online (BSO) optimization algorithm, a novel online training algorithm that significantly reduces training memory. BSO directly updates weights through flip signals under the online training framework. These signals are triggered when the product of gradient momentum and weights exceeds a threshold, eliminating the need for latent weights during training. To enhance performance, we propose T-BSO, a temporal-aware variant that leverages the inherent temporal dynamics of BSNNs by capturing gradient information across time steps for adaptive threshold adjustment. Theoretical analysis establishes convergence guarantees for both BSO and T-BSO, with formal regret bounds characterizing their convergence rates. Extensive experiments demonstrate that both BSO and T-BSO achieve superior optimization performance compared to existing training methods for BSNNs. The codes are available at https://github.com/hamings1/BSO.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12480v1" target="_blank"><h2>MaskAnyNet: Rethinking Masked Image Regions as Valuable Information in Supervised Learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jingshan Hong, Haigen Hu, Huihuang Zhang, Qianwei Zhou, Zhao Li<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> In supervised learning, traditional image masking faces two key issues: (i) discarded pixels are underutilized, leading to a loss of valuable contextual information; (ii) masking may remove small or critical features, especially in fine-grained tasks. In contrast, masked image modeling (MIM) has demonstrated that masked regions can be reconstructed from partial input, revealing that even incomplete data can exhibit strong contextual consistency with the original image. This highlights the potential of masked regions as sources of semantic diversity. Motivated by this, we revisit the image masking approach, proposing to treat masked content as auxiliary knowledge rather than ignored. Based on this, we propose MaskAnyNet, which combines masking with a relearning mechanism to exploit both visible and masked information. It can be easily extended to any model with an additional branch to jointly learn from the recomposed masked region. This approach leverages the semantic diversity of the masked regions to enrich features and preserve fine-grained details. Experiments on CNN and Transformer backbones show consistent gains across multiple benchmarks. Further analysis confirms that the proposed method improves semantic diversity through the reuse of masked content.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12462v1" target="_blank"><h2>Redundancy-optimized Multi-head Attention Networks for Multi-View Multi-Label Feature Selection <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yuzhou Liu, Jiarui Liu, Wanfu Gao<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 9 pages, 4 figures<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Multi-view multi-label data offers richer perspectives for artificial intelligence, but simultaneously presents significant challenges for feature selection due to the inherent complexity of interrelations among features, views and labels. Attention mechanisms provide an effective way for analyzing these intricate relationships. They can compute importance weights for information by aggregating correlations between Query and Key matrices to focus on pertinent values. However, existing attention-based feature selection methods predominantly focus on intra-view relationships, neglecting the complementarity of inter-view features and the critical feature-label correlations. Moreover, they often fail to account for feature redundancy, potentially leading to suboptimal feature subsets. To overcome these limitations, we propose a novel method based on Redundancy-optimized Multi-head Attention Networks for Multi-view Multi-label Feature Selection (RMAN-MMFS). Specifically, we employ each individual attention head to model intra-view feature relationships and use the cross-attention mechanisms between different heads to capture inter-view feature complementarity. Furthermore, we design static and dynamic feature redundancy terms: the static term mitigates redundancy within each view, while the dynamic term explicitly models redundancy between unselected and selected features across the entire selection process, thereby promoting feature compactness. Comprehensive evaluations on six real-world datasets, compared against six multi-view multi-label feature selection methods, demonstrate the superior performance of the proposed method.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12460v1" target="_blank"><h2>Personality-guided Public-Private Domain Disentangled Hypergraph-Former Network for Multimodal Depression Detection <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Changzeng Fu, Shiwen Zhao, Yunze Zhang, Zhongquan Jian, Shiqi Zhao, Chaoran Liu<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> AAAI 2026 accepted<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), multimodal (title, abstract), transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Depression represents a global mental health challenge requiring efficient and reliable automated detection methods. Current Transformer- or Graph Neural Networks (GNNs)-based multimodal depression detection methods face significant challenges in modeling individual differences and cross-modal temporal dependencies across diverse behavioral contexts. Therefore, we propose P$^3$HF (Personality-guided Public-Private Domain Disentangled Hypergraph-Former Network) with three key innovations: (1) personality-guided representation learning using LLMs to transform discrete individual features into contextual descriptions for personalized encoding; (2) Hypergraph-Former architecture modeling high-order cross-modal temporal relationships; (3) event-level domain disentanglement with contrastive learning for improved generalization across behavioral contexts. Experiments on MPDD-Young dataset show P$^3$HF achieves around 10\% improvement on accuracy and weighted F1 for binary and ternary depression classification task over existing methods. Extensive ablation studies validate the independent contribution of each architectural component, confirming that personality-guided representation learning and high-order hypergraph reasoning are both essential for generating robust, individual-aware depression-related representations. The code is released at https://github.com/hacilab/P3HF.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12451v1" target="_blank"><h2>A Multicollinearity-Aware Signal-Processing Framework for Cross-$β$ Identification via X-ray Scattering of Alzheimer's Tissue <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Abdullah Al Bashit, Prakash Nepal, Lee Makowski<br><strong><u>Categories:</u></strong> eess.IV, cs.LG<br><strong><u>Comments:</u></strong> 19 pages, 4 figures, journal paper under review<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> X-ray scattering measurements of in situ human brain tissue encode structural signatures of pathological cross-$β$ inclusions, yet systematic exploitation of these data for automated detection remains challenging due to substrate contamination, strong inter-feature correlations, and limited sample sizes. This work develops a three-stage classification framework for identifying cross-$β$ structural inclusions-a hallmark of Alzheimer's disease-in X-ray scattering profiles of post-mortem human brain. Stage 1 employs a Bayes-optimal classifier to separate mica substrate from tissue regions on the basis of their distinct scattering signatures. Stage 2 introduces a multicollinearityaware, class-conditional correlation pruning scheme with formal guarantees on the induced Bayes risk and approximation error, thereby reducing redundancy while retaining class-discriminative information. Stage 3 trains a compact neural network on the pruned feature set to detect the presence or absence of cross-$β$ fibrillar ordering. The top-performing model, optimized with a composite loss combining Focal and Dice objectives, attains a test F1-score of 84.30% using 11 of 211 candidate features and 174 trainable parameters. The overall framework yields an interpretable, theory-grounded strategy for data-limited classification problems involving correlated, high-dimensional experimental measurements, exemplified here by X-ray scattering profiles of neurodegenerative tissue.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12449v1" target="_blank"><h2>MOON2.0: Dynamic Modality-balanced Multimodal Representation Learning for E-commerce Product Understanding <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zhanheng Nie, Chenghan Fu, Daoze Zhang, Junxian Wu, Wanxian Guan, Pengjie Wang, Jian Xu, Bo Zheng<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.IR, cs.LG<br><strong><u>Comments:</u></strong> 11 pages, 7 figures<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> The rapid growth of e-commerce calls for multimodal models that comprehend rich visual and textual product information. Although recent multimodal large language models (MLLMs) for product understanding exhibit strong capability in representation learning for e-commerce, they still face three challenges: (i) the modality imbalance induced by modality mixed training; (ii) underutilization of the intrinsic alignment relationships among visual and textual information within a product; and (iii) limited handling of noise in e-commerce multimodal data. To address these, we propose MOON2.0, a dynamic modality-balanced multimodal representation learning framework for e-commerce product understanding. MOON2.0 comprises: (1) a Modality-driven Mixture-of-Experts (MoE) module that adaptively processes input samples by their modality composition, enabling Multimodal Joint Learning to mitigate the modality imbalance; (2) a Dual-level Alignment method to better leverage semantic alignment properties inside individual products; and (3) an MLLM-based Image-text Co-augmentation strategy that integrates textual enrichment with visual expansion, coupled with Dynamic Sample Filtering to improve training data quality. We further introduce MBE2.0, a co-augmented multimodal representation benchmark for e-commerce representation learning and evaluation. Experiments show that MOON2.0 delivers state-of-the-art zero-shot performance on MBE2.0 and multiple public datasets. Furthermore, attention-based heatmap visualization provides qualitative evidence of improved multimodal alignment of MOON2.0.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12442v1" target="_blank"><h2>Global-Lens Transformers: Adaptive Token Mixing for Dynamic Link Prediction <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Tao Zou, Chengfeng Wu, Tianxi Liao, Junchen Ye, Bowen Du<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> Accepted by AAAI 2026<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Dynamic graph learning plays a pivotal role in modeling evolving relationships over time, especially for temporal link prediction tasks in domains such as traffic systems, social networks, and recommendation platforms. While Transformer-based models have demonstrated strong performance by capturing long-range temporal dependencies, their reliance on self-attention results in quadratic complexity with respect to sequence length, limiting scalability on high-frequency or large-scale graphs. In this work, we revisit the necessity of self-attention in dynamic graph modeling. Inspired by recent findings that attribute the success of Transformers more to their architectural design than attention itself, we propose GLFormer, a novel attention-free Transformer-style framework for dynamic graphs. GLFormer introduces an adaptive token mixer that performs context-aware local aggregation based on interaction order and time intervals. To capture long-term dependencies, we further design a hierarchical aggregation module that expands the temporal receptive field by stacking local token mixers across layers. Experiments on six widely-used dynamic graph benchmarks show that GLFormer achieves SOTA performance, which reveals that attention-free architectures can match or surpass Transformer baselines in dynamic graph settings with significantly improved efficiency.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12438v1" target="_blank"><h2>Real-Time Drivers' Drowsiness Detection and Analysis through Deep Learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> ANK Zaman, Prosenjit Chatterjee, Rajat Sharma<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.HC, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> A long road trip is fun for drivers. However, a long drive for days can be tedious for a driver to accommodate stringent deadlines to reach distant destinations. Such a scenario forces drivers to drive extra miles, utilizing extra hours daily without sufficient rest and breaks. Once a driver undergoes such a scenario, it occasionally triggers drowsiness during driving. Drowsiness in driving can be life-threatening to any individual and can affect other drivers' safety; therefore, a real-time detection system is needed. To identify fatigued facial characteristics in drivers and trigger the alarm immediately, this research develops a real-time driver drowsiness detection system utilizing deep convolutional neural networks (DCNNs) and OpenCV.Our proposed and implemented model takes real- time facial images of a driver using a live camera and utilizes a Python-based library named OpenCV to examine the facial images for facial landmarks like sufficient eye openings and yawn-like mouth movements. The DCNNs framework then gathers the data and utilizes a per-trained model to detect the drowsiness of a driver using facial landmarks. If the driver is identified as drowsy, the system issues a continuous alert in real time, embedded in the Smart Car technology.By potentially saving innocent lives on the roadways, the proposed technique offers a non-invasive, inexpensive, and cost-effective way to identify drowsiness. Our proposed and implemented DCNNs embedded drowsiness detection model successfully react with NTHU-DDD dataset and Yawn-Eye-Dataset with drowsiness detection classification accuracy of 99.6% and 97% respectively.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12434v1" target="_blank"><h2>VISAGNN: Versatile Staleness-Aware Efficient Training on Large-Scale Graphs <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Rui Xue<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Graph Neural Networks (GNNs) have shown exceptional success in graph representation learning and a wide range of real-world applications. However, scaling deeper GNNs poses challenges due to the neighbor explosion problem when training on large-scale graphs. To mitigate this, a promising class of GNN training algorithms utilizes historical embeddings to reduce computation and memory costs while preserving the expressiveness of the model. These methods leverage historical embeddings for out-of-batch nodes, effectively approximating full-batch training without losing any neighbor information-a limitation found in traditional sampling methods. However, the staleness of these historical embeddings often introduces significant bias, acting as a bottleneck that can adversely affect model performance. In this paper, we propose a novel VersatIle Staleness-Aware GNN, named VISAGNN, which dynamically and adaptively incorporates staleness criteria into the large-scale GNN training process. By embedding staleness into the message passing mechanism, loss function, and historical embeddings during training, our approach enables the model to adaptively mitigate the negative effects of stale embeddings, thereby reducing estimation errors and enhancing downstream accuracy. Comprehensive experiments demonstrate the effectiveness of our method in overcoming the staleness issue of existing historical embedding techniques, showcasing its superior performance and efficiency on large-scale benchmarks, along with significantly faster convergence.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12423v1" target="_blank"><h2>GRAPHTEXTACK: A Realistic Black-Box Node Injection Attack on LLM-Enhanced GNNs <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jiaji Ma, Puja Trivedi, Danai Koutra<br><strong><u>Categories:</u></strong> cs.CR, cs.LG<br><strong><u>Comments:</u></strong> AAAI 2026<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), multi-modal (abstract)<br><p><strong><u>Abstract:</u></strong> Text-attributed graphs (TAGs), which combine structural and textual node information, are ubiquitous across many domains. Recent work integrates Large Language Models (LLMs) with Graph Neural Networks (GNNs) to jointly model semantics and structure, resulting in more general and expressive models that achieve state-of-the-art performance on TAG benchmarks. However, this integration introduces dual vulnerabilities: GNNs are sensitive to structural perturbations, while LLM-derived features are vulnerable to prompt injection and adversarial phrasing. While existing adversarial attacks largely perturb structure or text independently, we find that uni-modal attacks cause only modest degradation in LLM-enhanced GNNs. Moreover, many existing attacks assume unrealistic capabilities, such as white-box access or direct modification of graph data. To address these gaps, we propose GRAPHTEXTACK, the first black-box, multi-modal{, poisoning} node injection attack for LLM-enhanced GNNs. GRAPHTEXTACK injects nodes with carefully crafted structure and semantics to degrade model performance, operating under a realistic threat model without relying on model internals or surrogate models. To navigate the combinatorial, non-differentiable search space of connectivity and feature assignments, GRAPHTEXTACK introduces a novel evolutionary optimization framework with a multi-objective fitness function that balances local prediction disruption and global graph influence. Extensive experiments on five datasets and two state-of-the-art LLM-enhanced GNN models show that GRAPHTEXTACK significantly outperforms 12 strong baselines.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13780v1" target="_blank"><h2>Self-Attention as Distributional Projection: A Unified Interpretation of Transformer Architecture <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Nihal Mehta<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 17 pages, 0 figures. This work provides a mathematical interpretation of self-attention mechanisms in Transformers through distributional semantics principles<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> This paper presents a mathematical interpretation of self-attention by connecting it to distributional semantics principles. We show that self-attention emerges from projecting corpus-level co-occurrence statistics into sequence context. Starting from the co-occurrence matrix underlying GloVe embeddings, we demonstrate how the projection naturally captures contextual influence, with the query-key-value mechanism arising as the natural asymmetric extension for modeling directional relationships. Positional encodings and multi-head attention then follow as structured refinements of this same projection principle. Our analysis demonstrates that the Transformer architecture's particular algebraic form follows from these projection principles rather than being an arbitrary design choice.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12421v1" target="_blank"><h2>From Black Box to Bijection: Interpreting Machine Learning to Build a Zeta Map Algorithm <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Xiaoyu Huang, Blake Jackson, Kyu-Hwan Lee<br><strong><u>Categories:</u></strong> math.CO, cs.LG<br><strong><u>Comments:</u></strong> Extended abstract submitted to the 38th FPSAC (2026, Seattle). 12 pages, 1 figure<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> There is a large class of problems in algebraic combinatorics which can be distilled into the same challenge: construct an explicit combinatorial bijection. Traditionally, researchers have solved challenges like these by visually inspecting the data for patterns, formulating conjectures, and then proving them. But what is to be done if patterns fail to emerge until the data grows beyond human scale? In this paper, we propose a new workflow for discovering combinatorial bijections via machine learning. As a proof of concept, we train a transformer on paired Dyck paths and use its learned attention patterns to derive a new algorithmic description of the zeta map, which we call the \textit{Scaffolding Map}.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12404v1" target="_blank"><h2>SynthGuard: An Open Platform for Detecting AI-Generated Multimedia with Multimodal LLMs <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Shail Desai, Aditya Pawar, Li Lin, Xin Wang, Shu Hu<br><strong><u>Categories:</u></strong> cs.MM, cs.AI, cs.SD<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> explainable (abstract), multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Artificial Intelligence (AI) has made it possible for anyone to create images, audio, and video with unprecedented ease, enriching education, communication, and creative expression. At the same time, the rapid rise of AI-generated media has introduced serious risks, including misinformation, identity misuse, and the erosion of public trust as synthetic content becomes increasingly indistinguishable from real media. Although deepfake detection has advanced, many existing tools remain closed-source, limited in modality, or lacking transparency and educational value, making it difficult for users to understand how detection decisions are made. To address these gaps, we introduce SynthGuard, an open, user-friendly platform for detecting and analyzing AI-generated multimedia using both traditional detectors and multimodal large language models (MLLMs). SynthGuard provides explainable inference, unified image and audio support, and an interactive interface designed to make forensic analysis accessible to researchers, educators, and the public. The SynthGuard platform is available at: https://in-engr-nova.it.purdue.edu/</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12400v1" target="_blank"><h2>MSLoRA: Multi-Scale Low-Rank Adaptation via Attention Reweighting <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Xu Yang, Gady Agam<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract), transformer (abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> We introduce MSLoRA, a backbone-agnostic, parameter-efficient adapter that reweights feature responses rather
  than re-tuning the underlying backbone. Existing low-rank adaptation methods are mostly confined to vision
  transformers (ViTs) and struggle to generalize across architectures. MSLoRA unifies adaptation for both convolutional neural networks (CNNs) and
  ViTs by combining a low-rank linear projection with a multi-scale nonlinear transformation that jointly
  modulates spatial and channel attention. The two components are fused through pointwise multiplication and
  a residual connection, yielding a lightweight module that shifts feature attention while keeping pretrained
  weights frozen.
  Extensive experiments demonstrate that MSLoRA consistently improves transfer performance on classification,
  detection, and segmentation tasks with roughly less than 5\% of backbone parameters.
  The design further enables stable optimization, fast convergence, and strong cross-architecture
  generalization. By reweighting rather than re-tuning, MSLoRA provides a simple and universal approach
  for efficient adaptation of frozen vision backbones.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12398v1" target="_blank"><h2>On the Dimension-Free Approximation of Deep Neural Networks for Symmetric Korobov Functions <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yulong Lu, Tong Mao, Jinchao Xu, Yahong Yang<br><strong><u>Categories:</u></strong> cs.LG, math.NA<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> Deep neural networks have been widely used as universal approximators for functions with inherent physical structures, including permutation symmetry. In this paper, we construct symmetric deep neural networks to approximate symmetric Korobov functions and prove that both the convergence rate and the constant prefactor scale at most polynomially with respect to the ambient dimension. This represents a substantial improvement over prior approximation guarantees that suffer from the curse of dimensionality. Building on these approximation bounds, we further derive a generalization-error rate for learning symmetric Korobov functions whose leading factors likewise avoid the curse of dimensionality.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12394v1" target="_blank"><h2>Multi-Domain EEG Representation Learning with Orthogonal Mapping and Attention-based Fusion for Cognitive Load Classification <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Prithila Angkan, Amin Jalali, Paul Hungler, Ali Etemad<br><strong><u>Categories:</u></strong> cs.HC, cs.LG<br><strong><u>Comments:</u></strong> This work has been submitted to the Transactions on Human Machine Systems for possible publication<br><strong><u>Published:</u></strong> 2025-11-16<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> We propose a new representation learning solution for the classification of cognitive load based on Electroencephalogram (EEG). Our method integrates both time and frequency domains by first passing the raw EEG signals through the convolutional encoder to obtain the time domain representations. Next, we measure the Power Spectral Density (PSD) for all five EEG frequency bands and generate the channel power values as 2D images referred to as multi-spectral topography maps. These multi-spectral topography maps are then fed to a separate encoder to obtain the representations in frequency domain. Our solution employs a multi-domain attention module that maps these domain-specific embeddings onto a shared embedding space to emphasize more on important inter-domain relationships to enhance the representations for cognitive load classification. Additionally, we incorporate an orthogonal projection constraint during the training of our method to effectively increase the inter-class distances while improving intra-class clustering. This enhancement allows efficient discrimination between different cognitive states and aids in better grouping of similar states within the feature space. We validate the effectiveness of our model through extensive experiments on two public EEG datasets, CL-Drive and CLARE for cognitive load classification. Our results demonstrate the superiority of our multi-domain approach over the traditional single-domain techniques. Moreover, we conduct ablation and sensitivity analyses to assess the impact of various components of our method. Finally, robustness experiments on different amounts of added noise demonstrate the stability of our method compared to other state-of-the-art solutions.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12389v1" target="_blank"><h2>Calibrated Decomposition of Aleatoric and Epistemic Uncertainty in Deep Features for Inference-Time Adaptation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Divake Kumar, Patrick Poggi, Sina Tayebati, Devashri Naik, Nilesh Ahuja, Amit Ranjan Trivedi<br><strong><u>Categories:</u></strong> cs.CV, stat.ML<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> Most estimators collapse all uncertainty modes into a single confidence score, preventing reliable reasoning about when to allocate more compute or adjust inference. We introduce Uncertainty-Guided Inference-Time Selection, a lightweight inference time framework that disentangles aleatoric (data-driven) and epistemic (model-driven) uncertainty directly in deep feature space. Aleatoric uncertainty is estimated using a regularized global density model, while epistemic uncertainty is formed from three complementary components that capture local support deficiency, manifold spectral collapse, and cross-layer feature inconsistency. These components are empirically orthogonal and require no sampling, no ensembling, and no additional forward passes. We integrate the decomposed uncertainty into a distribution free conformal calibration procedure that yields significantly tighter prediction intervals at matched coverage. Using these components for uncertainty guided adaptive model selection reduces compute by approximately 60 percent on MOT17 with negligible accuracy loss, enabling practical self regulating visual inference. Additionally, our ablation results show that the proposed orthogonal uncertainty decomposition consistently yields higher computational savings across all MOT17 sequences, improving margins by 13.6 percentage points over the total-uncertainty baseline.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12388v1" target="_blank"><h2>CEDL: Centre-Enhanced Discriminative Learning for Anomaly Detection <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zahra Zamanzadeh Darban, Qizhou Wang, Charu C. Aggarwal, Geoffrey I. Webb, Ehsan Abbasnejad, Mahsa Salehi<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 20 pages, 2 figures, 3 tables<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> anomaly detection (title, abstract)<br><p><strong><u>Abstract:</u></strong> Supervised anomaly detection methods perform well in identifying known anomalies that are well represented in the training set. However, they often struggle to generalise beyond the training distribution due to decision boundaries that lack a clear definition of normality. Existing approaches typically address this by regularising the representation space during training, leading to separate optimisation in latent and label spaces. The learned normality is therefore not directly utilised at inference, and their anomaly scores often fall within arbitrary ranges that require explicit mapping or calibration for probabilistic interpretation. To achieve unified learning of geometric normality and label discrimination, we propose Centre-Enhanced Discriminative Learning (CEDL), a novel supervised anomaly detection framework that embeds geometric normality directly into the discriminative objective. CEDL reparameterises the conventional sigmoid-derived prediction logit through a centre-based radial distance function, unifying geometric and discriminative learning in a single end-to-end formulation. This design enables interpretable, geometry-aware anomaly scoring without post-hoc thresholding or reference calibration. Extensive experiments on tabular, time-series, and image data demonstrate that CEDL achieves competitive and balanced performance across diverse real-world anomaly detection tasks, validating its effectiveness and broad applicability.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12382v1" target="_blank"><h2>AGGRNet: Selective Feature Extraction and Aggregation for Enhanced Medical Image Classification <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Ansh Makwe, Akansh Agrawal, Prateek Jain, Akshan Agrawal, Priyanka Bagade<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Medical image analysis for complex tasks such as severity grading and disease subtype classification poses significant challenges due to intricate and similar visual patterns among classes, scarcity of labeled data, and variability in expert interpretations. Despite the usefulness of existing attention-based models in capturing complex visual patterns for medical image classification, underlying architectures often face challenges in effectively distinguishing subtle classes since they struggle to capture inter-class similarity and intra-class variability, resulting in incorrect diagnosis. To address this, we propose AGGRNet framework to extract informative and non-informative features to effectively understand fine-grained visual patterns and improve classification for complex medical image analysis tasks. Experimental results show that our model achieves state-of-the-art performance on various medical imaging datasets, with the best improvement up to 5% over SOTA models on the Kvasir dataset.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12381v1" target="_blank"><h2>Don't Think of the White Bear: Ironic Negation in Transformer Models Under Cognitive Load <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Logan Mann, Nayan Saxena, Sarah Tandon, Chenhao Sun, Savar Toteja, Kevin Zhu<br><strong><u>Categories:</u></strong> cs.CL, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> transformer (title), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Negation instructions such as 'do not mention $X$' can paradoxically increase the accessibility of $X$ in human thought, a phenomenon known as ironic rebound. Large language models (LLMs) face the same challenge: suppressing a concept requires internally activating it, which may prime rebound instead of avoidance. We investigated this tension with two experiments. \textbf{(1) Load \& content}: after a negation instruction, we vary distractor text (semantic, syntactic, repetition) and measure rebound strength. \textbf{(2) Polarity separation}: We test whether models distinguish neutral from negative framings of the same concept and whether this separation predicts rebound persistence. Results show that rebound consistently arises immediately after negation and intensifies with longer or semantic distractors, while repetition supports suppression. Stronger polarity separation correlates with more persistent rebound. Together, these findings, complemented by a circuit tracing analysis that identifies sparse middle-layer attention heads amplifying forbidden tokens while early layers suppress, link cognitive predictions of ironic rebound with mechanistic insights into long-context interference. To support future work, we release ReboundBench, a dataset of $5,000$ systematically varied negation prompts designed to probe rebound in LLMs.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12362v1" target="_blank"><h2>Characterization of type Ibn SNe <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> D. Farias, C. Gall, V. A. Villar, K. Auchettl, K. M. de Soto, A. Gagliano, W. B. Hoogendam, G. Narayan, A. Sedgewick, S. K. Yadavalli, Y. Zenati, C. R. Angus, K. W. Davis, J. Hjorth, W. V. Jacobson-Galán, D. O. Jones, C. D. Kilpatrick, M. J. Bustamante Rosell, D. A. Coulter, G. Dimitriadis, R. J. Foley, A. Gangopadhyay, H. Gao, M. E. Huber, L. Izzo, J. L. Johnson, A. L. Piro, A. Rest, C. Rojas-Bravo, M. R. Siebert, K. Taggart, S. Tinyanont<br><strong><u>Categories:</u></strong> astro-ph.HE, astro-ph.SR<br><strong><u>Comments:</u></strong> 25 pages, 13 figures, 7 tables<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> Type Ibn supernovae (SNe) are characterized by narrow helium (He I) lines from photons produced by the unshocked circumstellar material (CSM). About 80 SNe Ibn have been discovered to date, and only a handful have extensive observational records. Thus, many open questions regarding the progenitor system and the origin of the CSM remain. Here we investigate potential correlations between the spectral features of the prominent He I $λ$5876 line and the optical and X-ray light curve properties of SNe Ibn. We compile the largest sample of 61 SNe Ibn to date, of which 24 SNe have photometric and spectroscopic data from the Young Supernova Experiment and 37 SNe have archival data sets. We fit 24 SNe Ibn with sufficient photometric coverage ($B$ to $z$ bands) using semi-analytical models from MOSFiT. We demonstrate that the light curves of SNe Ibn are more diverse than previous analyses suggest, with absolute $r$-band peak magnitudes of $-19.4\pm0.6$~mag and rise (from $-10$ days to peak) and decay-rates (from peak to +10 days) of $-0.08\pm0.06$ and $0.08\pm0.03$ mag/day, respectively. We find that the majority of SNe Ibn in the sub-sample are consistent with a low-energy explosion ($<10^{51}$ erg) of a star with a compact envelope surrounded by $\sim$0.1 M$_{\odot}$ of helium-rich CSM. The inferred ejecta masses are small ($\sim 1$ M$_{\odot}$) and expand with a velocity of $\sim$5000 km/s. Our spectroscopic analysis shows that the mean velocity of the narrow component of the He I lines, associated to the CSM, peaks at $\sim1100$ km/s. The mean CSM and ejecta masses inferred for a sub-sample of SNe Ibn indicate that their progenitors are not massive ($\sim10$ M$_{\odot}$), single stars at the moment of explosion, but are likely binary systems. This agrees with the detection of potential companion stars of SNe Ibn progenitors, and the inferred CSM properties from stellar evolution models.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12351v1" target="_blank"><h2>Dynamic Reward Scaling for Multivariate Time Series Anomaly Detection: A VAE-Enhanced Reinforcement Learning Approach <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Bahareh Golchin, Banafsheh Rekabdar<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> variational autoencoder (abstract), VAE (title, abstract), anomaly detection (title, abstract)<br><p><strong><u>Abstract:</u></strong> Detecting anomalies in multivariate time series is essential for monitoring complex industrial systems, where high dimensionality, limited labeled data, and subtle dependencies between sensors cause significant challenges. This paper presents a deep reinforcement learning framework that combines a Variational Autoencoder (VAE), an LSTM-based Deep Q-Network (DQN), dynamic reward shaping, and an active learning module to address these issues in a unified learning framework. The main contribution is the implementation of Dynamic Reward Scaling for Multivariate Time Series Anomaly Detection (DRSMT), which demonstrates how each component enhances the detection process. The VAE captures compact latent representations and reduces noise. The DQN enables adaptive, sequential anomaly classification, and the dynamic reward shaping balances exploration and exploitation during training by adjusting the importance of reconstruction and classification signals. In addition, active learning identifies the most uncertain samples for labeling, reducing the need for extensive manual supervision. Experiments on two multivariate benchmarks, namely Server Machine Dataset (SMD) and Water Distribution Testbed (WADI), show that the proposed method outperforms existing baselines in F1-score and AU-PR. These results highlight the effectiveness of combining generative modeling, reinforcement learning, and selective supervision for accurate and scalable anomaly detection in real-world multivariate systems.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12346v1" target="_blank"><h2>CLAReSNet: When Convolution Meets Latent Attention for Hyperspectral Image Classification <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Asmit Bandyopadhyay, Anindita Das Bhattacharjee, Rakesh Das<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), transformer (abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Hyperspectral image (HSI) classification faces critical challenges, including high spectral dimensionality, complex spectral-spatial correlations, and limited training samples with severe class imbalance. While CNNs excel at local feature extraction and transformers capture long-range dependencies, their isolated application yields suboptimal results due to quadratic complexity and insufficient inductive biases. We propose CLAReSNet (Convolutional Latent Attention Residual Spectral Network), a hybrid architecture that integrates multi-scale convolutional extraction with transformer-style attention via an adaptive latent bottleneck. The model employs a multi-scale convolutional stem with deep residual blocks and an enhanced Convolutional Block Attention Module for hierarchical spatial features, followed by spectral encoder layers combining bidirectional RNNs (LSTM/GRU) with Multi-Scale Spectral Latent Attention (MSLA). MSLA reduces complexity from $\mathcal{O}(T^2D)$ to $\mathcal{O}(T\log(T)D)$ by adaptive latent token allocation (8-64 tokens) that scales logarithmically with the sequence length. Hierarchical cross-attention fusion dynamically aggregates multi-level representations for robust classification. Experiments conducted on the Indian Pines and Salinas datasets show state-of-the-art performance, achieving overall accuracies of 99.71% and 99.96%, significantly surpassing HybridSN, SSRN, and SpectralFormer. The learned embeddings exhibit superior inter-class separability and compact intra-class clustering, validating CLAReSNet's effectiveness under limited samples and severe class imbalance.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12321v1" target="_blank"><h2>Learning Time in Static Classifiers <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Xi Ding, Lei Wang, Piotr Koniusz, Yongsheng Gao<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> Accepted at the Fortieth AAAI Conference on Artificial Intelligence (AAAI 2026)<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> anomaly detection (abstract)<br><p><strong><u>Abstract:</u></strong> Real-world visual data rarely presents as isolated, static instances. Instead, it often evolves gradually over time through variations in pose, lighting, object state, or scene context. However, conventional classifiers are typically trained under the assumption of temporal independence, limiting their ability to capture such dynamics. We propose a simple yet effective framework that equips standard feedforward classifiers with temporal reasoning, all without modifying model architectures or introducing recurrent modules. At the heart of our approach is a novel Support-Exemplar-Query (SEQ) learning paradigm, which structures training data into temporally coherent trajectories. These trajectories enable the model to learn class-specific temporal prototypes and align prediction sequences via a differentiable soft-DTW loss. A multi-term objective further promotes semantic consistency and temporal smoothness. By interpreting input sequences as evolving feature trajectories, our method introduces a strong temporal inductive bias through loss design alone. This proves highly effective in both static and temporal tasks: it enhances performance on fine-grained and ultra-fine-grained image classification, and delivers precise, temporally consistent predictions in video anomaly detection. Despite its simplicity, our approach bridges static and temporal learning in a modular and data-efficient manner, requiring only a simple classifier on top of pre-extracted features.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12316v1" target="_blank"><h2>BlinDNO: A Distributional Neural Operator for Dynamical System Reconstruction from Time-Label-Free data <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zhijun Zeng, Junqing Chen, Zuoqiang Shi<br><strong><u>Categories:</u></strong> cs.LG, cs.CE, math.DS<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> We study an inverse problem for stochastic and quantum dynamical systems in a time-label-free setting, where only unordered density snapshots sampled at unknown times drawn from an observation-time distribution are available. These observations induce a distribution over state densities, from which we seek to recover the parameters of the underlying evolution operator. We formulate this as learning a distribution-to-function neural operator and propose BlinDNO, a permutation-invariant architecture that integrates a multiscale U-Net encoder with an attention-based mixer. Numerical experiments on a wide range of stochastic and quantum systems, including a 3D protein-folding mechanism reconstruction problem in a cryo-EM setting, demonstrate that BlinDNO reliably recovers governing parameters and consistently outperforms existing neural inverse operator baselines.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12305v1" target="_blank"><h2>MMSense: Adapting Vision-based Foundation Model for Multi-task Multi-modal Wireless Sensing <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zhizhen Li, Xuanhao Luo, Xueren Ge, Longyu Zhou, Xingqin Lin, Yuchen Liu<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> multi-modal (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Large AI models have been widely adopted in wireless communications for channel modeling, beamforming, and resource optimization. However, most existing efforts remain limited to single-modality inputs and channel-specific objec- tives, overlooking the broader potential of large foundation models for unified wireless sensing. To bridge this gap, we propose MMSense, a multi-modal, multi-task foundation model that jointly addresses channel-centric, environment-aware, and human-centered sensing. Our framework integrates image, radar, LiDAR, and textual data by transforming them into vision- compatible representations, enabling effective cross-modal align- ment within a unified feature space. A modality gating mecha- nism adaptively fuses these representations, while a vision-based large language model backbone enables unified feature align- ment and instruction-driven task adaptation. Furthermore, task- specific sequential attention and uncertainty-based loss weighting mechanisms enhance cross-task generalization. Experiments on real wireless scenario datasets show that our approach outper- forms both task-specific and large-model baselines, confirming its strong generalization across heterogeneous sensing tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12301v1" target="_blank"><h2>Rethinking Bias in Generative Data Augmentation for Medical AI: a Frequency Recalibration Method <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Chi Liu, Jincheng Liu, Congcong Zhu, Minghao Wang, Sheng Shen, Jia Gu, Tianqing Zhu, Wanlei Zhou<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> Accepted for AAAI 2026 (Main Track Poster)<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> data augmentation (title, abstract)<br><p><strong><u>Abstract:</u></strong> Developing Medical AI relies on large datasets and easily suffers from data scarcity. Generative data augmentation (GDA) using AI generative models offers a solution to synthesize realistic medical images. However, the bias in GDA is often underestimated in medical domains, with concerns about the risk of introducing detrimental features generated by AI and harming downstream tasks. This paper identifies the frequency misalignment between real and synthesized images as one of the key factors underlying unreliable GDA and proposes the Frequency Recalibration (FreRec) method to reduce the frequency distributional discrepancy and thus improve GDA. FreRec involves (1) Statistical High-frequency Replacement (SHR) to roughly align high-frequency components and (2) Reconstructive High-frequency Mapping (RHM) to enhance image quality and reconstruct high-frequency details. Extensive experiments were conducted in various medical datasets, including brain MRIs, chest X-rays, and fundus images. The results show that FreRec significantly improves downstream medical image classification performance compared to uncalibrated AI-synthesized samples. FreRec is a standalone post-processing step that is compatible with any generative model and can integrate seamlessly with common medical GDA pipelines.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12280v1" target="_blank"><h2>D$^{3}$ToM: Decider-Guided Dynamic Token Merging for Accelerating Diffusion MLLMs <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Shuochen Chang, Xiaofeng Zhang, Qingyang Liu, Li Niu<br><strong><u>Categories:</u></strong> cs.CV, cs.CL, cs.LG<br><strong><u>Comments:</u></strong> Accepted by AAAI Conference on Artificial Intelligence (AAAI) 2026. Code available atthis https URL<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract), transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Diffusion-based multimodal large language models (Diffusion MLLMs) have recently demonstrated impressive non-autoregressive generative capabilities across vision-and-language tasks. However, Diffusion MLLMs exhibit substantially slower inference than autoregressive models: Each denoising step employs full bidirectional self-attention over the entire sequence, resulting in cubic decoding complexity that becomes computationally impractical with thousands of visual tokens. To address this challenge, we propose D$^{3}$ToM, a Decider-guided dynamic token merging method that dynamically merges redundant visual tokens at different denoising steps to accelerate inference in Diffusion MLLMs. At each denoising step, D$^{3}$ToM uses decider tokens-the tokens generated in the previous denoising step-to build an importance map over all visual tokens. Then it maintains a proportion of the most salient tokens and merges the remainder through similarity-based aggregation. This plug-and-play module integrates into a single transformer layer, physically shortening the visual token sequence for all subsequent layers without altering model parameters. Moreover, D$^{3}$ToM employs a merge ratio that dynamically varies with each denoising step, aligns with the native decoding process of Diffusion MLLMs, achieving superior performance under equivalent computational budgets. Extensive experiments show that D$^{3}$ToM accelerates inference while preserving competitive performance. The code is released at https://github.com/bcmi/D3ToM-Diffusion-MLLM.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12265v1" target="_blank"><h2>Calibrated Adversarial Sampling: Multi-Armed Bandit-Guided Generalization Against Unforeseen Attacks <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Rui Wang, Zeming Wei, Xiyue Zhang, Meng Sun<br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.CR, cs.CV, math.OC<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Deep Neural Networks (DNNs) are known to be vulnerable to various adversarial perturbations. To address the safety concerns arising from these vulnerabilities, adversarial training (AT) has emerged as one of the most effective paradigms for enhancing the robustness of DNNs. However, existing AT frameworks primarily focus on a single or a limited set of attack types, leaving DNNs still exposed to attack types that may be encountered in practice but not addressed during training. In this paper, we propose an efficient fine-tuning method called Calibrated Adversarial Sampling (CAS) to address these issues. From the optimization perspective within the multi-armed bandit framework, it dynamically designs rewards and balances exploration and exploitation by considering the dynamic and interdependent characteristics of multiple robustness dimensions. Experiments on benchmark datasets show that CAS achieves superior overall robustness while maintaining high clean accuracy, providing a new paradigm for robust generalization of DNNs.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12263v1" target="_blank"><h2>CrossVid: A Comprehensive Benchmark for Evaluating Cross-Video Reasoning in Multimodal Large Language Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jingyao Li, Jingyun Wang, Molin Tan, Haochen Wang, Cilin Yan, Likun Shi, Jiayin Cai, Xiaolong Jiang, Yao Hu<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> 30 pages, 28 figures<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Cross-Video Reasoning (CVR) presents a significant challenge in video understanding, which requires simultaneous understanding of multiple videos to aggregate and compare information across groups of videos. Most existing video understanding benchmarks focus on single-video analysis, failing to assess the ability of multimodal large language models (MLLMs) to simultaneously reason over various videos. Recent benchmarks evaluate MLLMs' capabilities on multi-view videos that capture different perspectives of the same scene. However, their limited tasks hinder a thorough assessment of MLLMs in diverse real-world CVR scenarios. To this end, we introduce CrossVid, the first benchmark designed to comprehensively evaluate MLLMs' spatial-temporal reasoning ability in cross-video contexts. Firstly, CrossVid encompasses a wide spectrum of hierarchical tasks, comprising four high-level dimensions and ten specific tasks, thereby closely reflecting the complex and varied nature of real-world video understanding. Secondly, CrossVid provides 5,331 videos, along with 9,015 challenging question-answering pairs, spanning single-choice, multiple-choice, and open-ended question formats. Through extensive experiments on various open-source and closed-source MLLMs, we observe that Gemini-2.5-Pro performs best on CrossVid, achieving an average accuracy of 50.4%. Notably, our in-depth case study demonstrates that most current MLLMs struggle with CVR tasks, primarily due to their inability to integrate or compare evidence distributed across multiple videos for reasoning. These insights highlight the potential of CrossVid to guide future advancements in enhancing MLLMs' CVR capabilities.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12262v1" target="_blank"><h2>Implications of Mini-EUSO measurements for a space-based observation of UHECRs <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Mario Bertaina, Matteo Battisti, JEM-EUSO collaboration<br><strong><u>Categories:</u></strong> astro-ph.IM, hep-ex<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Mini--EUSO (Multiwavelength Imaging New Instrument for the Extreme Universe Space Observatory, known as \emph{UV atmosphere} in the Russian Space Program) is the first mission of the JEM-EUSO program on board the International Space Station. It was launched in August 2019 and it is operating since October 2019 being located in the Russian section (Zvezda module) of the station and viewing our planet from a nadir-facing UV-transparent window. The instrument is based on the concept of the original JEM-EUSO mission and consists of an optical system employing two Fresnel lenses of 25 cm each and a focal surface composed of 36 Multi-Anode Photomultiplier tubes, 64 channels each, for a total of 2304 channels with single photon counting sensitivity and an overall field of view of 44$^\circ \times $44$^\circ$. Mini-EUSO can map the night-time Earth in the near UV range (predominantly between 290 nm and 430 nm), with a spatial resolution of about 6~km and different temporal resolutions of 2.5~$μ$s, 320~$μ$s and 41 ms. Mini-EUSO observations are extremely important to better assess the potential of a space-based detector in studying Ultra-High Energy Cosmic Rays (UHECRs) such as K-EUSO and POEMMA. In this contribution we focus the attention on the results of the UV measurements and we place them in the context of UHECR observations from space, namely the estimation of exposure for the planned M-EUSO (Multi-messenger Extreme Universe Space Observatory) mission.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12261v1" target="_blank"><h2>Cross-view Joint Learning for Mixed-Missing Multi-view Unsupervised Feature Selection <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zongxin Shen, Yanyong Huang, Dongjie Wang, Jinyuan Chang, Fengmao Lv, Tianrui Li, Xiaoyi Jiang<br><strong><u>Categories:</u></strong> cs.LG, stat.ML<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Incomplete multi-view unsupervised feature selection (IMUFS), which aims to identify representative features from unlabeled multi-view data containing missing values, has received growing attention in recent years. Despite their promising performance, existing methods face three key challenges: 1) by focusing solely on the view-missing problem, they are not well-suited to the more prevalent mixed-missing scenario in practice, where some samples lack entire views or only partial features within views; 2) insufficient utilization of consistency and diversity across views limits the effectiveness of feature selection; and 3) the lack of theoretical analysis makes it unclear how feature selection and data imputation interact during the joint learning process. Being aware of these, we propose CLIM-FS, a novel IMUFS method designed to address the mixed-missing problem. Specifically, we integrate the imputation of both missing views and variables into a feature selection model based on nonnegative orthogonal matrix factorization, enabling the joint learning of feature selection and adaptive data imputation. Furthermore, we fully leverage consensus cluster structure and cross-view local geometrical structure to enhance the synergistic learning process. We also provide a theoretical analysis to clarify the underlying collaborative mechanism of CLIM-FS. Experimental results on eight real-world multi-view datasets demonstrate that CLIM-FS outperforms state-of-the-art methods.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12257v1" target="_blank"><h2>Bregman geometry-aware split Gibbs sampling for Bayesian Poisson inverse problems <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Elhadji Cisse Faye, Mame Diarra Fall, Nicolas Dobigeon, Eric Barat<br><strong><u>Categories:</u></strong> stat.CO, cs.CV, eess.IV, stat.ML<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> data augmentation (abstract)<br><p><strong><u>Abstract:</u></strong> This paper proposes a novel Bayesian framework for solving Poisson inverse problems by devising a Monte Carlo sampling algorithm which accounts for the underlying non-Euclidean geometry. To address the challenges posed by the Poisson likelihood -- such as non-Lipschitz gradients and positivity constraints -- we derive a Bayesian model which leverages exact and asymptotically exact data augmentations. In particular, the augmented model incorporates two sets of splitting variables both derived through a Bregman divergence based on the Burg entropy. Interestingly the resulting augmented posterior distribution is characterized by conditional distributions which benefit from natural conjugacy properties and preserve the intrinsic geometry of the latent and splitting variables. This allows for efficient sampling via Gibbs steps, which can be performed explicitly for all conditionals, except the one incorporating the regularization potential. For this latter, we resort to a Hessian Riemannian Langevin Monte Carlo (HRLMC) algorithm which is well suited to handle priors with explicit or easily computable score functions. By operating on a mirror manifold, this Langevin step ensures that the sampling satisfies the positivity constraints and more accurately reflects the underlying problem structure. Performance results obtained on denoising, deblurring, and positron emission tomography (PET) experiments demonstrate that the method achieves competitive performance in terms of reconstruction quality compared to optimization- and sampling-based approaches.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12248v1" target="_blank"><h2>Deep Unfolded BM3D: Unrolling Non-local Collaborative Filtering into a Trainable Neural Network <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Kerem Basim, Mehmet Ozan Unal, Metin Ertas, Isa Yildirim<br><strong><u>Categories:</u></strong> eess.IV, cs.AI, cs.CV<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> neural network (title)<br><p><strong><u>Abstract:</u></strong> Block-Matching and 3D Filtering (BM3D) exploits non-local self-similarity priors for denoising but relies on fixed parameters. Deep models such as U-Net are more flexible but often lack interpretability and fail to generalize across noise regimes. In this study, we propose Deep Unfolded BM3D (DU-BM3D), a hybrid framework that unrolls BM3D into a trainable architecture by replacing its fixed collaborative filtering with a learnable U-Net denoiser. This preserves BM3D's non-local structural prior while enabling end-to-end optimization. We evaluate DU-BM3D on low-dose CT (LDCT) denoising and show that it outperforms classic BM3D and standalone U-Net across simulated LDCT at different noise levels, yielding higher PSNR and SSIM, especially in high-noise conditions.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12234v1" target="_blank"><h2>A Review of Statistical and Machine Learning Approaches for Coral Bleaching Assessment <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Soham Sarkar, Arnab Hazra<br><strong><u>Categories:</u></strong> stat.AP, stat.ML<br><strong><u>Comments:</u></strong> 40 pages, 3 figures, 3 tables<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> Coral bleaching is a major concern for marine ecosystems; more than half of the world's coral reefs have either bleached or died over the past three decades. Increasing sea surface temperatures, along with various spatiotemporal environmental factors, are considered the primary reasons behind coral bleaching. The statistical and machine learning communities have focused on multiple aspects of the environment in detail. However, the literature on various stochastic modeling approaches for assessing coral bleaching is extremely scarce. Data-driven strategies are crucial for effective reef management, and this review article provides an overview of existing statistical and machine learning methods for assessing coral bleaching. Statistical frameworks, including simple regression models, generalized linear models, generalized additive models, Bayesian regression models, spatiotemporal models, and resilience indicators, such as Fisher's Information and Variance Index, are commonly used to explore how different environmental stressors influence coral bleaching. On the other hand, machine learning methods, including random forests, decision trees, support vector machines, and spatial operators, are more popular for detecting nonlinear relationships, analyzing high-dimensional data, and allowing integration of heterogeneous data from diverse sources. In addition to summarizing these models, we also discuss potential data-driven future research directions, with a focus on constructing statistical and machine learning models in specific contexts related to coral bleaching.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12214v1" target="_blank"><h2>ViTE: Virtual Graph Trajectory Expert Router for Pedestrian Trajectory Prediction <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Ruochen Li, Zhanxing Zhu, Tanqiu Qiao, Hubert P. H. Shum<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Pedestrian trajectory prediction is critical for ensuring safety in autonomous driving, surveillance systems, and urban planning applications. While early approaches primarily focus on one-hop pairwise relationships, recent studies attempt to capture high-order interactions by stacking multiple Graph Neural Network (GNN) layers. However, these approaches face a fundamental trade-off: insufficient layers may lead to under-reaching problems that limit the model's receptive field, while excessive depth can result in prohibitive computational costs. We argue that an effective model should be capable of adaptively modeling both explicit one-hop interactions and implicit high-order dependencies, rather than relying solely on architectural depth. To this end, we propose ViTE (Virtual graph Trajectory Expert router), a novel framework for pedestrian trajectory prediction. ViTE consists of two key modules: a Virtual Graph that introduces dynamic virtual nodes to model long-range and high-order interactions without deep GNN stacks, and an Expert Router that adaptively selects interaction experts based on social context using a Mixture-of-Experts design. This combination enables flexible and scalable reasoning across varying interaction patterns. Experiments on three benchmarks (ETH/UCY, NBA, and SDD) demonstrate that our method consistently achieves state-of-the-art performance, validating both its effectiveness and practical efficiency.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12213v1" target="_blank"><h2>MME-RAG: Multi-Manager-Expert Retrieval-Augmented Generation for Fine-Grained Entity Recognition in Task-Oriented Dialogues <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Liang Xue, Haoyu Liu, Yajun Tian, Xinyu Zhong, Yang Liu<br><strong><u>Categories:</u></strong> cs.CL, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> domain adaptation (abstract)<br><p><strong><u>Abstract:</u></strong> Fine-grained entity recognition is crucial for reasoning and decision-making in task-oriented dialogues, yet current large language models (LLMs) continue to face challenges in domain adaptation and retrieval controllability. We introduce MME-RAG, a Multi-Manager-Expert Retrieval-Augmented Generation framework that decomposes entity recognition into two coordinated stages: type-level judgment by lightweight managers and span-level extraction by specialized experts. Each expert is supported by a KeyInfo retriever that injects semantically aligned, few-shot exemplars during inference, enabling precise and domain-adaptive extraction without additional training. Experiments on CrossNER, MIT-Movie, MIT-Restaurant, and our newly constructed multi-domain customer-service dataset demonstrate that MME-RAG performs better than recent baselines in most domains. Ablation studies further show that both the hierarchical decomposition and KeyInfo-guided retrieval are key drivers of robustness and cross-domain generalization, establishing MME-RAG as a scalable and interpretable solution for adaptive dialogue understanding.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12199v2" target="_blank"><h2>MPD-SGR: Robust Spiking Neural Networks with Membrane Potential Distribution-Driven Surrogate Gradient Regularization <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Runhao Jiang, Chengzhi Jiang, Rui Yan, Huajin Tang<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> Accepted by AAAI 2026<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> The surrogate gradient (SG) method has shown significant promise in enhancing the performance of deep spiking neural networks (SNNs), but it also introduces vulnerabilities to adversarial attacks. Although spike coding strategies and neural dynamics parameters have been extensively studied for their impact on robustness, the critical role of gradient magnitude, which reflects the model's sensitivity to input perturbations, remains underexplored. In SNNs, the gradient magnitude is primarily determined by the interaction between the membrane potential distribution (MPD) and the SG function. In this study, we investigate the relationship between the MPD and SG and their implications for improving the robustness of SNNs. Our theoretical analysis reveals that reducing the proportion of membrane potentials lying within the gradient-available range of the SG function effectively mitigates the sensitivity of SNNs to input perturbations. Building upon this insight, we propose a novel MPD-driven surrogate gradient regularization (MPD-SGR) method, which enhances robustness by explicitly regularizing the MPD based on its interaction with the SG function. Extensive experiments across multiple image classification benchmarks and diverse network architectures confirm that the MPD-SGR method significantly enhances the resilience of SNNs to adversarial perturbations and exhibits strong generalizability across diverse network configurations, SG functions, and spike encoding schemes.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12191v1" target="_blank"><h2>Evaluation of Multi- and Single-objective Learning Algorithms for Imbalanced Data <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Szymon Wojciechowski, Michał Woźniak<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Many machine learning tasks aim to find models that work well not for a single, but for a group of criteria, often opposing ones. One such example is imbalanced data classification, where, on the one hand, we want to achieve the best possible classification quality for data from the minority class without degrading the classification quality of the majority class. One solution is to propose an aggregate learning criterion and reduce the multi-objective learning task to a single-criteria optimization problem. Unfortunately, such an approach is characterized by ambiguity of interpretation since the value of the aggregated criterion does not indicate the value of the component criteria. Hence, there are more and more proposals for algorithms based on multi-objective optimization (MOO), which can simultaneously optimize multiple criteria. However, such an approach results in a set of multiple non-dominated solutions (Pareto front). The selection of a single solution from the Pareto front is a challenge itself, and much attention is paid to the issue of how to select it considering user preferences, as well as how to compare solutions returned by different MOO algorithms among themselves. Thus, a significant gap has been identified in the classifier evaluation methodology, i.e., how to reliably compare methods returning single solutions with algorithms returning solutions in the form of Pareto fronts.
  To fill the aforementioned gap, this article proposes a new, reliable way of evaluating algorithms based on multi-objective algorithms with methods that return single solutions while pointing out solutions from a Pareto front tailored to the user's preferences. This work focuses only on algorithm comparison, not their learning. The algorithms selected for this study are illustrative to help understand the proposed approach.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12181v1" target="_blank"><h2>MixAR: Mixture Autoregressive Image Generation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jinyuan Hu, Jiayou Zhang, Shaobo Cui, Kun Zhang, Guangyi Chen<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> latent space (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Autoregressive (AR) approaches, which represent images as sequences of discrete tokens from a finite codebook, have achieved remarkable success in image generation. However, the quantization process and the limited codebook size inevitably discard fine-grained information, placing bottlenecks on fidelity. Motivated by this limitation, recent studies have explored autoregressive modeling in continuous latent spaces, which offers higher generation quality. Yet, unlike discrete tokens constrained by a fixed codebook, continuous representations lie in a vast and unstructured space, posing significant challenges for efficient autoregressive modeling. To address these challenges, we introduce MixAR, a novel framework that leverages mixture training paradigms to inject discrete tokens as prior guidance for continuous AR modeling. MixAR is a factorized formulation that leverages discrete tokens as prior guidance for continuous autoregressive prediction. We investigate several discrete-continuous mixture strategies, including self-attention (DC-SA), cross-attention (DC-CA), and a simple approach (DC-Mix) that replaces homogeneous mask tokens with informative discrete counterparts. Moreover, to bridge the gap between ground-truth training tokens and inference tokens produced by the pre-trained AR model, we propose Training-Inference Mixture (TI-Mix) to achieve consistent training and generation distributions. In our experiments, we demonstrate a favorable balance of the DC-Mix strategy between computational efficiency and generation fidelity, and consistent improvement of TI-Mix.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12180v1" target="_blank"><h2>Understanding InfoNCE: Transition Probability Matrix Induced Feature Clustering <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Ge Cheng, Shuo Wang, Yun Zhang<br><strong><u>Categories:</u></strong> cs.LG, stat.ML<br><strong><u>Comments:</u></strong> 31 pages, 8 figures<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> data augmentation (abstract)<br><p><strong><u>Abstract:</u></strong> Contrastive learning has emerged as a cornerstone of unsupervised representation learning across vision, language, and graph domains, with InfoNCE as its dominant objective. Despite its empirical success, the theoretical underpinnings of InfoNCE remain limited. In this work, we introduce an explicit feature space to model augmented views of samples and a transition probability matrix to capture data augmentation dynamics. We demonstrate that InfoNCE optimizes the probability of two views sharing the same source toward a constant target defined by this matrix, naturally inducing feature clustering in the representation space. Leveraging this insight, we propose Scaled Convergence InfoNCE (SC-InfoNCE), a novel loss function that introduces a tunable convergence target to flexibly control feature similarity alignment. By scaling the target matrix, SC-InfoNCE enables flexible control over feature similarity alignment, allowing the training objective to better match the statistical properties of downstream data. Experiments on benchmark datasets, including image, graph, and text tasks, show that SC-InfoNCE consistently achieves strong and reliable performance across diverse domains.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12174v1" target="_blank"><h2>TSGDiff: Rethinking Synthetic Time Series Generation from a Pure Graph Perspective <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Lifeng Shen, Xuyang Li, Lele Long<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> Accepted by AAAI 2026<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> latent space (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Diffusion models have shown great promise in data generation, yet generating time series data remains challenging due to the need to capture complex temporal dependencies and structural patterns. In this paper, we present \textit{TSGDiff}, a novel framework that rethinks time series generation from a graph-based perspective. Specifically, we represent time series as dynamic graphs, where edges are constructed based on Fourier spectrum characteristics and temporal dependencies. A graph neural network-based encoder-decoder architecture is employed to construct a latent space, enabling the diffusion process to model the structural representation distribution of time series effectively. Furthermore, we propose the Topological Structure Fidelity (Topo-FID) score, a graph-aware metric for assessing the structural similarity of time series graph representations. Topo-FID integrates two sub-metrics: Graph Edit Similarity, which quantifies differences in adjacency matrices, and Structural Entropy Similarity, which evaluates the entropy of node degree distributions. This comprehensive metric provides a more accurate assessment of structural fidelity in generated time series. Experiments on real-world datasets demonstrate that \textit{TSGDiff} generates high-quality synthetic time series data generation, faithfully preserving temporal dependencies and structural integrity, thereby advancing the field of synthetic time series generation.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12170v1" target="_blank"><h2>Rethinking Multimodal Point Cloud Completion: A Completion-by-Correction Perspective <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Wang Luo, Di Wu, Hengyuan Na, Yinlin Zhu, Miao Hu, Guocong Quan<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> Accepted by AAAI 2026<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Point cloud completion aims to reconstruct complete 3D shapes from partial observations, which is a challenging problem due to severe occlusions and missing geometry. Despite recent advances in multimodal techniques that leverage complementary RGB images to compensate for missing geometry, most methods still follow a Completion-by-Inpainting paradigm, synthesizing missing structures from fused latent features. We empirically show that this paradigm often results in structural inconsistencies and topological artifacts due to limited geometric and semantic constraints. To address this, we rethink the task and propose a more robust paradigm, termed Completion-by-Correction, which begins with a topologically complete shape prior generated by a pretrained image-to-3D model and performs feature-space correction to align it with the partial observation. This paradigm shifts completion from unconstrained synthesis to guided refinement, enabling structurally consistent and observation-aligned reconstruction. Building upon this paradigm, we introduce PGNet, a multi-stage framework that conducts dual-feature encoding to ground the generative prior, synthesizes a coarse yet structurally aligned scaffold, and progressively refines geometric details via hierarchical correction. Experiments on the ShapeNetViPC dataset demonstrate the superiority of PGNet over state-of-the-art baselines in terms of average Chamfer Distance (-23.5%) and F-score (+7.1%).</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12167v1" target="_blank"><h2>Rapid Machine Learning-Driven Detection of Pesticides and Dyes Using Raman Spectroscopy <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Quach Thi Thai Binh, Thuan Phuoc, Xuan Hai, Thang Bach Phan, Vu Thi Hanh Thu, Nguyen Tuan Hung<br><strong><u>Categories:</u></strong> cond-mat.mtrl-sci, cs.LG<br><strong><u>Comments:</u></strong> 25 pages, 9 figures<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> dimensionality reduction (abstract)<br><p><strong><u>Abstract:</u></strong> The extensive use of pesticides and synthetic dyes poses critical threats to food safety, human health, and environmental sustainability, necessitating rapid and reliable detection methods. Raman spectroscopy offers molecularly specific fingerprints but suffers from spectral noise, fluorescence background, and band overlap, limiting its real-world applicability. Here, we propose a deep learning framework based on ResNet-18 feature extraction, combined with advanced classifiers, including XGBoost, SVM, and their hybrid integration, to detect pesticides and dyes from Raman spectroscopy, called MLRaman. The MLRaman with the CNN-XGBoost model achieved a predictive accuracy of 97.4% and a perfect AUC of 1.0, while it with the CNN-SVM model provided competitive results with robust class-wise discrimination. Dimensionality reduction analyses (PCA, t-SNE, UMAP) confirmed the separability of Raman embeddings across 10 analytes, including 7 pesticides and 3 dyes. Finally, we developed a user-friendly Streamlit application for real-time prediction, which successfully identified unseen Raman spectra from our independent experiments and also literature sources, underscoring strong generalization capacity. This study establishes a scalable, practical MLRaman model for multi-residue contaminant monitoring, with significant potential for deployment in food safety and environmental surveillance.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12158v1" target="_blank"><h2>Data-Efficient Self-Supervised Algorithms for Fine-Grained Birdsong Analysis <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Houtan Ghaffari, Lukas Rauch, Paul Devos<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), data augmentation (abstract)<br><p><strong><u>Abstract:</u></strong> Many bioacoustics, neuroscience, and linguistics research utilize birdsongs as proxy models to acquire knowledge in diverse areas. Developing models generally requires precisely annotated data at the level of syllables. Hence, automated and data-efficient methods that reduce annotation costs are in demand. This work presents a lightweight, yet performant neural network architecture for birdsong annotation called Residual-MLP-RNN. Then, it presents a robust three-stage training pipeline for developing reliable deep birdsong syllable detectors with minimal expert labor. The first stage is self-supervised learning from unlabeled data. Two of the most successful pretraining paradigms are explored, namely, masked prediction and online clustering. The second stage is supervised training with effective data augmentations to create a robust model for frame-level syllable detection. The third stage is semi-supervised post-training, which leverages the unlabeled data again. However, unlike the initial phase, this time it is aligned with the downstream task. The performance of this data-efficient approach is demonstrated for the complex song of the Canary in extreme label-scarcity scenarios. Canary has one of the most difficult songs to annotate, which implicitly validates the method for other birds. Finally, the potential of self-supervised embeddings is assessed for linear probing and unsupervised birdsong analysis.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12154v1" target="_blank"><h2>Open Banking Foundational Model: Learning Language Representations from Few Financial Transactions <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Gustavo Polleti, Marlesson Santana, Eduardo Fontes<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> We introduced a multimodal foundational model for financial transactions that integrates both structured attributes and unstructured textual descriptions into a unified representation. By adapting masked language modeling to transaction sequences, we demonstrated that our approach not only outperforms classical feature engineering and discrete event sequence methods but is also particularly effective in data-scarce Open Banking scenarios. To our knowledge, this is the first large-scale study across thousands of financial institutions in North America, providing evidence that multimodal representations can generalize across geographies and institutions. These results highlight the potential of self-supervised models to advance financial applications ranging from fraud prevention and credit risk to customer insights</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12147v1" target="_blank"><h2>Finding Time Series Anomalies using Granular-ball Vector Data Description <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Lifeng Shen, Liang Peng, Ruiwen Liu, Shuyin Xia, Yi Liu<br><strong><u>Categories:</u></strong> cs.LG, stat.ML<br><strong><u>Comments:</u></strong> Accepted by AAAI 2026<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> anomaly detection (abstract), latent space (abstract)<br><p><strong><u>Abstract:</u></strong> Modeling normal behavior in dynamic, nonlinear time series data is challenging for effective anomaly detection. Traditional methods, such as nearest neighbor and clustering approaches, often depend on rigid assumptions, such as a predefined number of reliable neighbors or clusters, which frequently break down in complex temporal scenarios. To address these limitations, we introduce the Granular-ball One-Class Network (GBOC), a novel approach based on a data-adaptive representation called Granular-ball Vector Data Description (GVDD). GVDD partitions the latent space into compact, high-density regions represented by granular-balls, which are generated through a density-guided hierarchical splitting process and refined by removing noisy structures. Each granular-ball serves as a prototype for local normal behavior, naturally positioning itself between individual instances and clusters while preserving the local topological structure of the sample set. During training, GBOC improves the compactness of representations by aligning samples with their nearest granular-ball centers. During inference, anomaly scores are computed based on the distance to the nearest granular-ball. By focusing on dense, high-quality regions and significantly reducing the number of prototypes, GBOC delivers both robustness and efficiency in anomaly detection. Extensive experiments validate the effectiveness and superiority of the proposed method, highlighting its ability to handle the challenges of time series anomaly detection.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12139v1" target="_blank"><h2>Fusion-ResNet: A Lightweight multi-label NILM Model Using PCA-ICA Feature Fusion <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Sahar Moghimian Hoosh, Ilia Kamyshev, Henni Ouerdane<br><strong><u>Categories:</u></strong> cs.LG, eess.SY<br><strong><u>Comments:</u></strong> Extended version of the conference paper "Enhancing Non-Intrusive Load Monitoring with Features Extracted by Independent Component Analysis" --arXiv:2501.16817. Instead of solely using ICA or PCA for feature extraction, we propose the fusion of ICA and PCA, which outperforms other baseline models. This extended version is meant for journal publication<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Non-intrusive load monitoring (NILM) is an advanced load monitoring technique that uses data-driven algorithms to disaggregate the total power consumption of a household into the consumption of individual appliances. However, real-world NILM deployment still faces major challenges, including overfitting, low model generalization, and disaggregating a large number of appliances operating at the same time. To address these challenges, this work proposes an end-to-end framework for the NILM classification task, which consists of high-frequency labeled data, a feature extraction method, and a lightweight neural network. Within this framework, we introduce a novel feature extraction method that fuses Independent Component Analysis (ICA) and Principal Component Analysis (PCA) features. Moreover, we propose a lightweight architecture for multi-label NILM classification (Fusion-ResNet). The proposed feature-based model achieves a higher $F1$ score on average and across different appliances compared to state-of-the-art NILM classifiers while minimizing the training and inference time. Finally, we assessed the performance of our model against baselines with a varying number of simultaneously active devices. Results demonstrate that Fusion-ResNet is relatively robust to stress conditions with up to 15 concurrently active appliances.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12132v1" target="_blank"><h2>FairGSE: Fairness-Aware Graph Neural Network without High False Positive Rates <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zhenqiang Ye, Jinjie Lu, Tianlong Gu, Fengrui Hao, Xuemin Wang<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> AAAI 2026<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> Graph neural networks (GNNs) have emerged as the mainstream paradigm for graph representation learning due to their effective message aggregation. However, this advantage also amplifies biases inherent in graph topology, raising fairness concerns. Existing fairness-aware GNNs provide satisfactory performance on fairness metrics such as Statistical Parity and Equal Opportunity while maintaining acceptable accuracy trade-offs. Unfortunately, we observe that this pursuit of fairness metrics neglects the GNN's ability to predict negative labels, which renders their predictions with extremely high False Positive Rates (FPR), resulting in negative effects in high-risk scenarios. To this end, we advocate that classification performance should be carefully calibrated while improving fairness, rather than simply constraining accuracy loss. Furthermore, we propose Fair GNN via Structural Entropy (\textbf{FairGSE}), a novel framework that maximizes two-dimensional structural entropy (2D-SE) to improve fairness without neglecting false positives. Experiments on several real-world datasets show FairGSE reduces FPR by 39\% vs. state-of-the-art fairness-aware GNNs, with comparable fairness improvement.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12122v1" target="_blank"><h2>Dynamic Anomaly Identification in Accounting Transactions via Multi-Head Self-Attention Networks <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yi Wang, Ruoyi Fang, Anzhuo Xie, Hanrui Feng, Jianlin Lai<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> anomaly detection (abstract), transformer (abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> This study addresses the problem of dynamic anomaly detection in accounting transactions and proposes a real-time detection method based on a Transformer to tackle the challenges of hidden abnormal behaviors and high timeliness requirements in complex trading environments. The approach first models accounting transaction data by representing multi-dimensional records as time-series matrices and uses embedding layers and positional encoding to achieve low-dimensional mapping of inputs. A sequence modeling structure with multi-head self-attention is then constructed to capture global dependencies and aggregate features from multiple perspectives, thereby enhancing the ability to detect abnormal patterns. The network further integrates feed-forward layers and regularization strategies to achieve deep feature representation and accurate anomaly probability estimation. To validate the effectiveness of the method, extensive experiments were conducted on a public dataset, including comparative analysis, hyperparameter sensitivity tests, environmental sensitivity tests, and data sensitivity tests. Results show that the proposed method outperforms baseline models in AUC, F1-Score, Precision, and Recall, and maintains stable performance under different environmental conditions and data perturbations. These findings confirm the applicability and advantages of the Transformer-based framework for dynamic anomaly detection in accounting transactions and provide methodological support for intelligent financial risk control and auditing.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12121v3" target="_blank"><h2>To Align or Not to Align: Strategic Multimodal Representation Alignment for Optimal Performance <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Wanlong Fang, Tianle Zhang, Alvin Chan<br><strong><u>Categories:</u></strong> cs.LG, cs.MM<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Multimodal learning often relies on aligning representations across modalities to enable effective information integration, an approach traditionally assumed to be universally beneficial. However, prior research has primarily taken an observational approach, examining naturally occurring alignment in multimodal data and exploring its correlation with model performance, without systematically studying the direct effects of explicitly enforced alignment between representations of different modalities. In this work, we investigate how explicit alignment influences both model performance and representation alignment under different modality-specific information structures. Specifically, we introduce a controllable contrastive learning module that enables precise manipulation of alignment strength during training, allowing us to explore when explicit alignment improves or hinders performance. Our results on synthetic and real datasets under different data characteristics show that the impact of explicit alignment on the performance of unimodal models is related to the characteristics of the data: the optimal level of alignment depends on the amount of redundancy between the different modalities. We identify an optimal alignment strength that balances modality-specific signals and shared redundancy in the mixed information distributions. This work provides practical guidance on when and how explicit alignment should be applied to achieve optimal unimodal encoder performance.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12101v1" target="_blank"><h2>Decoupled Action Head: Confining Task Knowledge to Conditioning Layers <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jian Zhou, Sihao Lin, Shuai Fu, Qi WU<br><strong><u>Categories:</u></strong> cs.RO, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Behavior Cloning (BC) is a data-driven supervised learning approach that has gained increasing attention with the success of scaling laws in language and vision domains. Among its implementations in robotic manipulation, Diffusion Policy (DP), with its two variants DP-CNN (DP-C) and DP-Transformer (DP-T), is one of the most effective and widely adopted models, demonstrating the advantages of predicting continuous action sequences. However, both DP and other BC methods remain constrained by the scarcity of paired training data, and the internal mechanisms underlying DP's effectiveness remain insufficiently understood, leading to limited generalization and a lack of principled design in model development. In this work, we propose a decoupled training recipe that leverages nearly cost-free kinematics-generated trajectories as observation-free data to pretrain a general action head (action generator). The pretrained action head is then frozen and adapted to novel tasks through feature modulation. Our experiments demonstrate the feasibility of this approach in both in-distribution and out-of-distribution scenarios. As an additional benefit, decoupling improves training efficiency; for instance, DP-C achieves up to a 41% speedup. Furthermore, the confinement of task-specific knowledge to the conditioning components under decoupling, combined with the near-identical performance of DP-C in both normal and decoupled training, indicates that the action generation backbone plays a limited role in robotic manipulation. Motivated by this observation, we introduce DP-MLP, which replaces the 244M-parameter U-Net backbone of DP-C with only 4M parameters of simple MLP blocks, achieving a 83.9% faster training speed under normal training and 89.1% under decoupling.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12092v1" target="_blank"><h2>SenseRay-3D: Generalizable and Physics-Informed Framework for End-to-End Indoor Propagation Modeling <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yu Zheng, Kezhi Wang, Wenji Xi, Gang Yu, Jiming Chen, Jie Zhang<br><strong><u>Categories:</u></strong> cs.LG, cs.NI<br><strong><u>Comments:</u></strong> Submitted for possible journal publications<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Modeling indoor radio propagation is crucial for wireless network planning and optimization. However, existing approaches often rely on labor-intensive manual modeling of geometry and material properties, resulting in limited scalability and efficiency. To overcome these challenges, this paper presents SenseRay-3D, a generalizable and physics-informed end-to-end framework that predicts three-dimensional (3D) path-loss heatmaps directly from RGB-D scans, thereby eliminating the need for explicit geometry reconstruction or material annotation. The proposed framework builds a sensing-driven voxelized scene representation that jointly encodes occupancy, electromagnetic material characteristics, and transmitter-receiver geometry, which is processed by a SwinUNETR-based neural network to infer environmental path-loss relative to free-space path-loss. A comprehensive synthetic indoor propagation dataset is further developed to validate the framework and to serve as a standardized benchmark for future research. Experimental results show that SenseRay-3D achieves a mean absolute error of 4.27 dB on unseen environments and supports real-time inference at 217 ms per sample, demonstrating its scalability, efficiency, and physical consistency. SenseRay-3D paves a new path for sense-driven, generalizable, and physics-consistent modeling of indoor propagation, marking a major leap beyond our pioneering EM DeepRay framework.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12085v1" target="_blank"><h2>Explainable Transformer-Based Email Phishing Classification with Adversarial Robustness <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Sajad U P<br><strong><u>Categories:</u></strong> cs.CR, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> explainable (title, abstract), transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> Phishing and related cyber threats are becoming more varied and technologically advanced. Among these, email-based phishing remains the most dominant and persistent threat. These attacks exploit human vulnerabilities to disseminate malware or gain unauthorized access to sensitive information. Deep learning (DL) models, particularly transformer-based models, have significantly enhanced phishing mitigation through their contextual understanding of language. However, some recent threats, specifically Artificial Intelligence (AI)-generated phishing attacks, are reducing the overall system resilience of phishing detectors. In response, adversarial training has shown promise against AI-generated phishing threats. This study presents a hybrid approach that uses DistilBERT, a smaller, faster, and lighter version of the BERT transformer model for email classification. Robustness against text-based adversarial perturbations is reinforced using Fast Gradient Method (FGM) adversarial training. Furthermore, the framework integrates the LIME Explainable AI (XAI) technique to enhance the transparency of the DistilBERT architecture. The framework also uses the Flan-T5-small language model from Hugging Face to generate plain-language security narrative explanations for end-users. This combined approach ensures precise phishing classification while providing easily understandable justifications for the model's decisions.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14792v1" target="_blank"><h2>Application of Graph Based Vision Transformers Architectures for Accurate Temperature Prediction in Fiber Specklegram Sensors <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Abhishek Sebastian<br><strong><u>Categories:</u></strong> eess.IV, cs.AI, cs.CV<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> explainable (abstract), transformer (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Fiber Specklegram Sensors (FSS) are highly effective for environmental monitoring, particularly for detecting temperature variations. However, the nonlinear nature of specklegram data presents significant challenges for accurate temperature prediction. This study investigates the use of transformer-based architectures, including Vision Transformers (ViTs), Swin Transformers, and emerging models such as Learnable Importance Non-Symmetric Attention Vision Transformers (LINA-ViT) and Multi-Adaptive Proximity Vision Graph Attention Transformers (MAP-ViGAT), to predict temperature from specklegram data over a range of 0 to 120 Celsius. The results show that ViTs achieved a Mean Absolute Error (MAE) of 1.15, outperforming traditional models such as CNNs. GAT-ViT and MAP-ViGAT variants also demonstrated competitive accuracy, highlighting the importance of adaptive attention mechanisms and graph-based structures in capturing complex modal interactions and phase shifts in specklegram data. Additionally, this study incorporates Explainable AI (XAI) techniques, including attention maps and saliency maps, to provide insights into the decision-making processes of the transformer models, improving interpretability and transparency. These findings establish transformer architectures as strong benchmarks for optical fiber-based temperature sensing and offer promising directions for industrial monitoring and structural health assessment applications.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12081v1" target="_blank"><h2>From Scaling to Structured Expressivity: Rethinking Transformers for CTR Prediction <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Bencheng Yan, Yuejie Lei, Zhiyuan Zeng, Di Wang, Kaiyi Lin, Pengjie Wang, Jian Xu, Bo Zheng<br><strong><u>Categories:</u></strong> cs.IR, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Despite massive investments in scale, deep models for click-through rate (CTR) prediction often exhibit rapidly diminishing returns - a stark contrast to the smooth, predictable gains seen in large language models. We identify the root cause as a structural misalignment: Transformers assume sequential compositionality, while CTR data demand combinatorial reasoning over high-cardinality semantic fields. Unstructured attention spreads capacity indiscriminately, amplifying noise under extreme sparsity and breaking scalable learning. To restore alignment, we introduce the Field-Aware Transformer (FAT), which embeds field-based interaction priors into attention through decomposed content alignment and cross-field modulation. This design ensures model complexity scales with the number of fields F, not the total vocabulary size n >> F, leading to tighter generalization and, critically, observed power-law scaling in AUC as model width increases. We present the first formal scaling law for CTR models, grounded in Rademacher complexity, that explains and predicts this behavior. On large-scale benchmarks, FAT improves AUC by up to +0.51% over state-of-the-art methods. Deployed online, it delivers +2.33% CTR and +0.66% RPM. Our work establishes that effective scaling in recommendation arises not from size, but from structured expressivity-architectural coherence with data semantics.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12075v1" target="_blank"><h2>Treatment Stitching with Schrödinger Bridge for Enhancing Offline Reinforcement Learning in Adaptive Treatment Strategies <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Dong-Hee Shin, Deok-Joong Lee, Young-Han Son, Tae-Eui Kam<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 19 pages, 5 figures, AAAI conference<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> data augmentation (abstract)<br><p><strong><u>Abstract:</u></strong> Adaptive treatment strategies (ATS) are sequential decision-making processes that enable personalized care by dynamically adjusting treatment decisions in response to evolving patient symptoms. While reinforcement learning (RL) offers a promising approach for optimizing ATS, its conventional online trial-and-error learning mechanism is not permissible in clinical settings due to risks of harm to patients. Offline RL tackles this limitation by learning policies exclusively from historical treatment data, but its performance is often constrained by data scarcity-a pervasive challenge in clinical domains. To overcome this, we propose Treatment Stitching (TreatStitch), a novel data augmentation framework that generates clinically valid treatment trajectories by intelligently stitching segments from existing treatment data. Specifically, TreatStitch identifies similar intermediate patient states across different trajectories and stitches their respective segments. Even when intermediate states are too dissimilar to stitch directly, TreatStitch leverages the Schrödinger bridge method to generate smooth and energy-efficient bridging trajectories that connect dissimilar states. By augmenting these synthetic trajectories into the original dataset, offline RL can learn from a more diverse dataset, thereby improving its ability to optimize ATS. Extensive experiments across multiple treatment datasets demonstrate the effectiveness of TreatStitch in enhancing offline RL performance. Furthermore, we provide a theoretical justification showing that TreatStitch maintains clinical validity by avoiding out-of-distribution transitions.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12073v1" target="_blank"><h2>Informed Bootstrap Augmentation Improves EEG Decoding <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Woojae Jeong, Wenhui Cui, Kleanthis Avramidis, Takfarinas Medani, Shrikanth Narayanan, Richard Leahy<br><strong><u>Categories:</u></strong> eess.SP, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> data augmentation (abstract)<br><p><strong><u>Abstract:</u></strong> Electroencephalography (EEG) offers detailed access to neural dynamics but remains constrained by noise and trial-by-trial variability, limiting decoding performance in data-restricted or complex paradigms. Data augmentation is often employed to enhance feature representations, yet conventional uniform averaging overlooks differences in trial informativeness and can degrade representational quality. We introduce a weighted bootstrapping approach that prioritizes more reliable trials to generate higher-quality augmented samples. In a Sentence Evaluation paradigm, weights were computed from relative ERP differences and applied during probabilistic sampling and averaging. Across conditions, weighted bootstrapping improved decoding accuracy relative to unweighted (from 68.35% to 71.25% at best), demonstrating that emphasizing reliable trials strengthens representational quality. The results demonstrate that reliability-based augmentation yields more robust and discriminative EEG representations. The code is publicly available at https://github.com/lyricists/NeuroBootstrap.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12072v1" target="_blank"><h2>ProAV-DiT: A Projected Latent Diffusion Transformer for Efficient Synchronized Audio-Video Generation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jiahui Sun, Weining Wang, Mingzhen Sun, Yirong Yang, Xinxin Zhu, Jing Liu<br><strong><u>Categories:</u></strong> cs.MM, cs.AI, cs.SD<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> latent space (abstract), multimodal (abstract), transformer (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Sounding Video Generation (SVG) remains a challenging task due to the inherent structural misalignment between audio and video, as well as the high computational cost of multimodal data processing. In this paper, we introduce ProAV-DiT, a Projected Latent Diffusion Transformer designed for efficient and synchronized audio-video generation. To address structural inconsistencies, we preprocess raw audio into video-like representations, aligning both the temporal and spatial dimensions between audio and video. At its core, ProAV-DiT adopts a Multi-scale Dual-stream Spatio-Temporal Autoencoder (MDSA), which projects both modalities into a unified latent space using orthogonal decomposition, enabling fine-grained spatiotemporal modeling and semantic alignment. To further enhance temporal coherence and modality-specific fusion, we introduce a multi-scale attention mechanism, which consists of multi-scale temporal self-attention and group cross-modal attention. Furthermore, we stack the 2D latents from MDSA into a unified 3D latent space, which is processed by a spatio-temporal diffusion Transformer. This design efficiently models spatiotemporal dependencies, enabling the generation of high-fidelity synchronized audio-video content while reducing computational overhead. Extensive experiments conducted on standard benchmarks demonstrate that ProAV-DiT outperforms existing methods in both generation quality and computational efficiency.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12061v1" target="_blank"><h2>MovSemCL: Movement-Semantics Contrastive Learning for Trajectory Similarity <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zhichen Lai, Hua Lu, Huan Li, Jialiang Li, Christian S. Jensen<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.DB<br><strong><u>Comments:</u></strong> 8 pages, 6 figures; accepted by AAAI 2026 as an Oral paper<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> anomaly detection (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Trajectory similarity computation is fundamental functionality that is used for, e.g., clustering, prediction, and anomaly detection. However, existing learning-based methods exhibit three key limitations: (1) insufficient modeling of trajectory semantics and hierarchy, lacking both movement dynamics extraction and multi-scale structural representation; (2) high computational costs due to point-wise encoding; and (3) use of physically implausible augmentations that distort trajectory semantics. To address these issues, we propose MovSemCL, a movement-semantics contrastive learning framework for trajectory similarity computation. MovSemCL first transforms raw GPS trajectories into movement-semantics features and then segments them into patches. Next, MovSemCL employs intra- and inter-patch attentions to encode local as well as global trajectory patterns, enabling efficient hierarchical representation and reducing computational costs. Moreover, MovSemCL includes a curvature-guided augmentation strategy that preserves informative segments (e.g., turns and intersections) and masks redundant ones, generating physically plausible augmented views. Experiments on real-world datasets show that MovSemCL is capable of outperforming state-of-the-art methods, achieving mean ranks close to the ideal value of 1 at similarity search tasks and improvements by up to 20.3% at heuristic approximation, while reducing inference latency by up to 43.4%.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12056v1" target="_blank"><h2>PipeDiT: Accelerating Diffusion Transformers in Video Generation with Task Pipelining and Model Decoupling <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Sijie Wang, Qiang Wang, Shaohuai Shi<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.DC<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> variational autoencoder (abstract), VAE (abstract), transformer (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Video generation has been advancing rapidly, and diffusion transformer (DiT) based models have demonstrated remark- able capabilities. However, their practical deployment is of- ten hindered by slow inference speeds and high memory con- sumption. In this paper, we propose a novel pipelining frame- work named PipeDiT to accelerate video generation, which is equipped with three main innovations. First, we design a pipelining algorithm (PipeSP) for sequence parallelism (SP) to enable the computation of latent generation and commu- nication among multiple GPUs to be pipelined, thus reduc- ing inference latency. Second, we propose DeDiVAE to de- couple the diffusion module and the variational autoencoder (VAE) module into two GPU groups, whose executions can also be pipelined to reduce memory consumption and infer- ence latency. Third, to better utilize the GPU resources in the VAE group, we propose an attention co-processing (Aco) method to further reduce the overall video generation latency. We integrate our PipeDiT into both OpenSoraPlan and Hun- yuanVideo, two state-of-the-art open-source video generation frameworks, and conduct extensive experiments on two 8- GPU systems. Experimental results show that, under many common resolution and timestep configurations, our PipeDiT achieves 1.06x to 4.02x speedups over OpenSoraPlan and HunyuanVideo.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13771v1" target="_blank"><h2>ExplainableGuard: Interpretable Adversarial Defense for Large Language Models Using Chain-of-Thought Reasoning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Shaowei Guan, Yu Zhai, Zhengyu Zhang, Yanze Wang, Hin Chi Kwok<br><strong><u>Categories:</u></strong> cs.CR, cs.AI<br><strong><u>Comments:</u></strong> 9 pages, 2 figures<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> explainable (title, abstract)<br><p><strong><u>Abstract:</u></strong> Large Language Models (LLMs) are increasingly vulnerable to adversarial attacks that can subtly manipulate their outputs. While various defense mechanisms have been proposed, many operate as black boxes, lacking transparency in their decision-making. This paper introduces ExplainableGuard, an interpretable adversarial defense framework leveraging the chain-of-thought (CoT) reasoning capabilities of DeepSeek-Reasoner. Our approach not only detects and neutralizes adversarial perturbations in text but also provides step-by-step explanations for each defense action. We demonstrate how tailored CoT prompts guide the LLM to perform a multi-faceted analysis (character, word, structural, and semantic) and generate a purified output along with a human-readable justification. Preliminary results on the GLUE Benchmark and IMDB Movie Reviews dataset show promising defense efficacy. Additionally, a human evaluation study reveals that ExplainableGuard's explanations outperform ablated variants in clarity, specificity, and actionability, with a 72.5% deployability-trust rating, underscoring its potential for more trustworthy LLM deployments.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12047v1" target="_blank"><h2>DCMM-Transformer: Degree-Corrected Mixed-Membership Attention for Medical Imaging <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Huimin Cheng, Xiaowei Yu, Shushan Wu, Luyang Fang, Chao Cao, Jing Zhang, Tianming Liu, Dajiang Zhu, Wenxuan Zhong, Ping Ma<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Medical images exhibit latent anatomical groupings, such as organs, tissues, and pathological regions, that standard Vision Transformers (ViTs) fail to exploit. While recent work like SBM-Transformer attempts to incorporate such structures through stochastic binary masking, they suffer from non-differentiability, training instability, and the inability to model complex community structure. We present DCMM-Transformer, a novel ViT architecture for medical image analysis that incorporates a Degree-Corrected Mixed-Membership (DCMM) model as an additive bias in self-attention. Unlike prior approaches that rely on multiplicative masking and binary sampling, our method introduces community structure and degree heterogeneity in a fully differentiable and interpretable manner. Comprehensive experiments across diverse medical imaging datasets, including brain, chest, breast, and ocular modalities, demonstrate the superior performance and generalizability of the proposed approach. Furthermore, the learned group structure and structured attention modulation substantially enhance interpretability by yielding attention maps that are anatomically meaningful and semantically coherent.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12046v1" target="_blank"><h2>BackWeak: Backdooring Knowledge Distillation Simply with Weak Triggers and Fine-tuning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Shanmin Wang, Dongdong Zhao<br><strong><u>Categories:</u></strong> cs.CR, cs.AI, cs.CV, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Knowledge Distillation (KD) is essential for compressing large models, yet relying on pre-trained "teacher" models downloaded from third-party repositories introduces serious security risks -- most notably backdoor attacks. Existing KD backdoor methods are typically complex and computationally intensive: they employ surrogate student models and simulated distillation to guarantee transferability, and they construct triggers in a way similar to universal adversarial perturbations (UAPs), which being not stealthy in magnitude, inherently exhibit strong adversarial behavior. This work questions whether such complexity is necessary and constructs stealthy "weak" triggers -- imperceptible perturbations that have negligible adversarial effect. We propose BackWeak, a simple, surrogate-free attack paradigm. BackWeak shows that a powerful backdoor can be implanted by simply fine-tuning a benign teacher with a weak trigger using a very small learning rate. We demonstrate that this delicate fine-tuning is sufficient to embed a backdoor that reliably transfers to diverse student architectures during a victim's standard distillation process, yielding high attack success rates. Extensive empirical evaluations on multiple datasets, model architectures, and KD methods show that BackWeak is efficient, simpler, and often more stealthy than previous elaborate approaches. This work calls on researchers studying KD backdoor attacks to pay particular attention to the trigger's stealthiness and its potential adversarial characteristics.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12041v2" target="_blank"><h2>Mesh-based Super-resolution of Detonation Flows with Multiscale Graph Transformers <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Shivam Barwey, Pinaki Pal<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> Super-resolution flow reconstruction using state-of-the-art data-driven techniques is valuable for a variety of applications, such as subgrid/subfilter closure modeling, accelerating spatiotemporal forecasting, data compression, and serving as an upscaling tool for sparse experimental measurements. In the present work, a first-of-its-kind multiscale graph transformer approach is developed for mesh-based super-resolution (SR-GT) of reacting flows. The novel data-driven modeling paradigm leverages a graph-based flow-field representation compatible with complex geometries and non-uniform/unstructured grids. Further, the transformer backbone captures long-range dependencies between different parts of the low-resolution flow-field, identifies important features, and then generates the super-resolved flow-field that preserves those features at a higher resolution. The performance of SR-GT is demonstrated in the context of spectral-element-discretized meshes for a challenging test problem of 2D detonation propagation within a premixed hydrogen-air mixture exhibiting highly complex multiscale reacting flow behavior. The SR-GT framework utilizes a unique element-local (+ neighborhood) graph representation for the coarse input, which is then tokenized before being processed by the transformer component to produce the fine output. It is demonstrated that SR-GT provides high super-resolution accuracy for reacting flow-field features and superior performance compared to traditional interpolation-based SR schemes.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12034v1" target="_blank"><h2>Calibrated Multimodal Representation Learning with Missing Modalities <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Xiaohao Liu, Xiaobo Xia, Jiaheng Wei, Shuo Yang, Xiu Su, See-Kiong Ng, Tat-Seng Chua<br><strong><u>Categories:</u></strong> cs.CV, cs.LG, cs.MM<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> latent space (abstract), multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Multimodal representation learning harmonizes distinct modalities by aligning them into a unified latent space. Recent research generalizes traditional cross-modal alignment to produce enhanced multimodal synergy but requires all modalities to be present for a common instance, making it challenging to utilize prevalent datasets with missing modalities. We provide theoretical insights into this issue from an anchor shift perspective. Observed modalities are aligned with a local anchor that deviates from the optimal one when all modalities are present, resulting in an inevitable shift. To address this, we propose CalMRL for multimodal representation learning to calibrate incomplete alignments caused by missing modalities. Specifically, CalMRL leverages the priors and the inherent connections among modalities to model the imputation for the missing ones at the representation level. To resolve the optimization dilemma, we employ a bi-step learning method with the closed-form solution of the posterior distribution of shared latents. We validate its mitigation of anchor shift and convergence with theoretical guidance. By equipping the calibrated alignment with the existing advanced method, we offer new flexibility to absorb data with missing modalities, which is originally unattainable. Extensive experiments and comprehensive analyses demonstrate the superiority of CalMRL. Our code, model checkpoints, and evaluation raw data will be publicly available.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12027v1" target="_blank"><h2>GCAgent: Long-Video Understanding via Schematic and Narrative Episodic Memory <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jeong Hun Yeo, Sangyun Chung, Sungjune Park, Dae Hoe Kim, Jinyoung Moon, Yong Man Ro<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Long-video understanding remains a significant challenge for Multimodal Large Language Models (MLLMs) due to inherent token limitations and the complexity of capturing long-term temporal dependencies. Existing methods often fail to capture the global context and complex event relationships necessary for deep video reasoning. To address this, we introduce GCAgent, a novel Global-Context-Aware Agent framework that achieves comprehensive long-video understanding. Our core innovation is the Schematic and Narrative Episodic Memory. This memory structurally models events and their causal and temporal relations into a concise, organized context, fundamentally resolving the long-term dependency problem. Operating in a multi-stage Perception-Action-Reflection cycle, our GCAgent utilizes a Memory Manager to retrieve relevant episodic context for robust, context-aware inference. Extensive experiments confirm that GCAgent significantly enhances long-video understanding, achieving up to 23.5\% accuracy improvement on the Video-MME Long split over a strong MLLM baseline. Furthermore, our framework establishes state-of-the-art performance among comparable 7B-scale MLLMs, achieving 73.4\% accuracy on the Long split and the highest overall average (71.9\%) on the Video-MME benchmark, validating our agent-based reasoning paradigm and structured memory for cognitively-inspired long-video understanding.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12017v1" target="_blank"><h2>Gravitational wave standard sirens from GWTC-3 combined with DESI DR2 and DESY5: A late-universe probe of the Hubble constant and dark energy <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Ji-Yu Song, Guo-Hong Du, Tian-Nuo Li, Ling-Feng Wang, Jing-Zhao Qi, Jing-Fei Zhang, Xin Zhang<br><strong><u>Categories:</u></strong> astro-ph.CO, gr-qc, hep-ph, hep-th<br><strong><u>Comments:</u></strong> 10 pages, 3 figures<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> Recently, the combination of the Dark Energy Spectroscopic Instrument (DESI) Data Release 2 (DR2) baryon acoustic oscillation (BAO) data and the Planck cosmic microwave background (CMB) measurements has shown a $\sim$3$σ$ preference for a dynamical dark energy model with a phantom-crossing behavior. However, such a phantom-crossing dark energy evolution further exacerbates the already severe Hubble tension in the $Λ$CDM model. Moreover, there exists a $\sim2σ$ tension between the DESI DR2 BAO and CMB datasets. Therefore, it is essential to measure the Hubble constant and dark-energy equation-of-state (EoS) parameters using only late-universe observations. In this work, we investigate a novel late-universe data combination: gravitational-wave (GW) standard sirens, BAO, and Type Ia supernovae (SNe Ia). This combination provides a fully distance-ladder- and CMB-independent determination of the Hubble constant and the dark-energy EoS. Using 47 GW standard sirens from the third Gravitational-Wave Transient Catalog, the DESI DR2 BAO data, and DESY5 SNe Ia data, in the $w_0w_a$CDM model, we obtain $H_0=74.8^{+6.3}_{-8.9}$ km s$^{-1}$ Mpc$^{-1}$, $Ω_{\rm m}=0.320^{+0.015}_{-0.012}$, $w_0=-0.775^{+0.072}_{-0.074}$, and $w_a=-0.80\pm0.47$, indicating a mild phantom-crossing behavior within the $1σ$ credible interval with an $H_0$ value consistent with the distance ladder measurements. Our analysis demonstrates the power of GW standard sirens in breaking parameter degeneracies, and this novel data combination provides joint constraints on the Hubble constant and the dark-energy EoS parameters.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12008v1" target="_blank"><h2>Adaptive Diagnostic Reasoning Framework for Pathology with Multimodal Large Language Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yunqi Hong, Johnson Kao, Liam Edwards, Nein-Tzu Liu, Chung-Yen Huang, Alex Oliveira-Kowaleski, Cho-Jui Hsieh, Neil Y. C. Lin<br><strong><u>Categories:</u></strong> cs.AI, cs.CV, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> AI tools in pathology have improved screening throughput, standardized quantification, and revealed prognostic patterns that inform treatment. However, adoption remains limited because most systems still lack the human-readable reasoning needed to audit decisions and prevent errors. We present RECAP-PATH, an interpretable framework that establishes a self-learning paradigm, shifting off-the-shelf multimodal large language models from passive pattern recognition to evidence-linked diagnostic reasoning. At its core is a two-phase learning process that autonomously derives diagnostic criteria: diversification expands pathology-style explanations, while optimization refines them for accuracy. This self-learning approach requires only small labeled sets and no white-box access or weight updates to generate cancer diagnoses. Evaluated on breast and prostate datasets, RECAP-PATH produced rationales aligned with expert assessment and delivered substantial gains in diagnostic accuracy over baselines. By uniting visual understanding with reasoning, RECAP-PATH provides clinically trustworthy AI and demonstrates a generalizable path toward evidence-linked interpretation.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12006v1" target="_blank"><h2>Uncertainty-Guided Selective Adaptation Enables Cross-Platform Predictive Fluorescence Microscopy <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Kai-Wen K. Yang, Andrew Bai, Alexandra Bermudez, Yunqi Hong, Zoe Latham, Iris Sloan, Michael Liu, Vishrut Goyal, Cho-Jui Hsieh, Neil Y. C. Lin<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), domain adaptation (abstract)<br><p><strong><u>Abstract:</u></strong> Deep learning is transforming microscopy, yet models often fail when applied to images from new instruments or acquisition settings. Conventional adversarial domain adaptation (ADDA) retrains entire networks, often disrupting learned semantic representations. Here, we overturn this paradigm by showing that adapting only the earliest convolutional layers, while freezing deeper layers, yields reliable transfer. Building on this principle, we introduce Subnetwork Image Translation ADDA with automatic depth selection (SIT-ADDA-Auto), a self-configuring framework that integrates shallow-layer adversarial alignment with predictive uncertainty to automatically select adaptation depth without target labels. We demonstrate robustness via multi-metric evaluation, blinded expert assessment, and uncertainty-depth ablations. Across exposure and illumination shifts, cross-instrument transfer, and multiple stains, SIT-ADDA improves reconstruction and downstream segmentation over full-encoder adaptation and non-adversarial baselines, with reduced drift of semantic features. Our results provide a design rule for label-free adaptation in microscopy and a recipe for field settings; the code is publicly available.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12003v1" target="_blank"><h2>Look As You Think: Unifying Reasoning and Visual Evidence Attribution for Verifiable Document RAG via Reinforcement Learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Shuochen Liu, Pengfei Luo, Chao Zhang, Yuhao Chen, Haotian Zhang, Qi Liu, Xin Kou, Tong Xu, Enhong Chen<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> Poster of AAAI'2026<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Aiming to identify precise evidence sources from visual documents, visual evidence attribution for visual document retrieval-augmented generation (VD-RAG) ensures reliable and verifiable predictions from vision-language models (VLMs) in multimodal question answering. Most existing methods adopt end-to-end training to facilitate intuitive answer verification. However, they lack fine-grained supervision and progressive traceability throughout the reasoning process. In this paper, we introduce the Chain-of-Evidence (CoE) paradigm for VD-RAG. CoE unifies Chain-of-Thought (CoT) reasoning and visual evidence attribution by grounding reference elements in reasoning steps to specific regions with bounding boxes and page indexes. To enable VLMs to generate such evidence-grounded reasoning, we propose Look As You Think (LAT), a reinforcement learning framework that trains models to produce verifiable reasoning paths with consistent attribution. During training, LAT evaluates the attribution consistency of each evidence region and provides rewards only when the CoE trajectory yields correct answers, encouraging process-level self-verification. Experiments on vanilla Qwen2.5-VL-7B-Instruct with Paper- and Wiki-VISA benchmarks show that LAT consistently improves the vanilla model in both single- and multi-image settings, yielding average gains of 8.23% in soft exact match (EM) and 47.0% in IoU@0.5. Meanwhile, LAT not only outperforms the supervised fine-tuning baseline, which is trained to directly produce answers with attribution, but also exhibits stronger generalization across domains.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11993v1" target="_blank"><h2>Dynamic Parameter Optimization for Highly Transferable Transformation-Based Attacks <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jiaming Liang, Chi-Man Pun<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-15<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Despite their wide application, the vulnerabilities of deep neural networks raise societal concerns. Among them, transformation-based attacks have demonstrated notable success in transfer attacks. However, existing attacks suffer from blind spots in parameter optimization, limiting their full potential. Specifically, (1) prior work generally considers low-iteration settings, yet attacks perform quite differently at higher iterations, so characterizing overall performance based only on low-iteration results is misleading. (2) Existing attacks use uniform parameters for different surrogate models, iterations, and tasks, which greatly impairs transferability. (3) Traditional transformation parameter optimization relies on grid search. For n parameters with m steps each, the complexity is O(mn). Large computational overhead limits further optimization of parameters. To address these limitations, we conduct an empirical study with various transformations as baselines, revealing three dynamic patterns of transferability with respect to parameter strength. We further propose a novel Concentric Decay Model (CDM) to effectively explain these patterns. Building on these insights, we propose an efficient Dynamic Parameter Optimization (DPO) based on the rise-then-fall pattern, reducing the complexity to O(nlogm). Comprehensive experiments on existing transformation-based attacks across different surrogate models, iterations, and tasks demonstrate that our DPO can significantly improve transferability.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11951v1" target="_blank"><h2>Temporal Micro-Doppler Spectrogram-based ViT Multiclass Target Classification <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Nghia Thinh Nguyen, Tri Nhu Do<br><strong><u>Categories:</u></strong> eess.SP, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> explainable (abstract), transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> In this paper, we propose a new Temporal MDS-Vision Transformer (T-MDS-ViT) for multiclass target classification using millimeter-wave FMCW radar micro-Doppler spectrograms. Specifically, we design a transformer-based architecture that processes stacked range-velocity-angle (RVA) spatiotemporal tensors via patch embeddings and cross-axis attention mechanisms to explicitly model the sequential nature of MDS data across multiple frames. The T-MDS-ViT exploits mobility-aware constraints in its attention layer correspondences to maintain separability under target overlaps and partial occlusions. Next, we apply an explainable mechanism to examine how the attention layers focus on characteristic high-energy regions of the MDS representations and their effect on class-specific kinematic features. We also demonstrate that our proposed framework is superior to existing CNN-based methods in terms of classification accuracy while achieving better data efficiency and real-time deployability.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11945v1" target="_blank"><h2>Augmenting The Weather: A Hybrid Counterfactual-SMOTE Algorithm for Improving Crop Growth Prediction When Climate Changes <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Mohammed Temraz, Mark T Keane<br><strong><u>Categories:</u></strong> cs.AI, cs.LG<br><strong><u>Comments:</u></strong> 31 pages, 8 figures<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> explainable (abstract), data augmentation (abstract)<br><p><strong><u>Abstract:</u></strong> In recent years, humanity has begun to experience the catastrophic effects of climate change as economic sectors (such as agriculture) struggle with unpredictable and extreme weather events. Artificial Intelligence (AI) should help us handle these climate challenges but its most promising solutions are not good at dealing with climate-disrupted data; specifically, machine learning methods that work from historical data-distributions, are not good at handling out-of-distribution, outlier events. In this paper, we propose a novel data augmentation method, that treats the predictive problems around climate change as being, in part, due to class-imbalance issues; that is, prediction from historical datasets is difficult because, by definition, they lack sufficient minority-class instances of "climate outlier events". This novel data augmentation method -- called Counterfactual-Based SMOTE (CFA-SMOTE) -- combines an instance-based counterfactual method from Explainable AI (XAI) with the well-known class-imbalance method, SMOTE. CFA-SMOTE creates synthetic data-points representing outlier, climate-events that augment the dataset to improve predictive performance. We report comparative experiments using this CFA-SMOTE method, comparing it to benchmark counterfactual and class-imbalance methods under different conditions (i.e., class-imbalance ratios). The focal climate-change domain used relies on predicting grass growth on Irish dairy farms, during Europe-wide drought and forage crisis of 2018.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11940v1" target="_blank"><h2>Learning the relative composition of EEG signals using pairwise relative shift pretraining <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Christopher Sandino, Sayeri Lala, Geeling Chau, Melika Ayoughi, Behrooz Mahasseni, Ellen Zippi, Ali Moin, Erdrin Azemi, Hanlin Goh<br><strong><u>Categories:</u></strong> cs.LG, eess.SP<br><strong><u>Comments:</u></strong> Foundation Models for the Brain and Body NeurIPS 2025 Workshop<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> transfer learning (abstract), transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Self-supervised learning (SSL) offers a promising approach for learning electroencephalography (EEG) representations from unlabeled data, reducing the need for expensive annotations for clinical applications like sleep staging and seizure detection. While current EEG SSL methods predominantly use masked reconstruction strategies like masked autoencoders (MAE) that capture local temporal patterns, position prediction pretraining remains underexplored despite its potential to learn long-range dependencies in neural signals. We introduce PAirwise Relative Shift or PARS pretraining, a novel pretext task that predicts relative temporal shifts between randomly sampled EEG window pairs. Unlike reconstruction-based methods that focus on local pattern recovery, PARS encourages encoders to capture relative temporal composition and long-range dependencies inherent in neural signals. Through comprehensive evaluation on various EEG decoding tasks, we demonstrate that PARS-pretrained transformers consistently outperform existing pretraining strategies in label-efficient and transfer learning settings, establishing a new paradigm for self-supervised EEG representation learning.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11935v1" target="_blank"><h2>SurvBench: A Standardised Preprocessing Pipeline for Multi-Modal Electronic Health Record Survival Analysis <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Munib Mesinovic, Tingting Zhu<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> multi-modal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Electronic health record (EHR) data present tremendous opportunities for advancing survival analysis through deep learning, yet reproducibility remains severely constrained by inconsistent preprocessing methodologies. We present SurvBench, a comprehensive, open-source preprocessing pipeline that transforms raw PhysioNet datasets into standardised, model-ready tensors for multi-modal survival analysis. SurvBench provides data loaders for three major critical care databases, MIMIC-IV, eICU, and MC-MED, supporting diverse modalities including time-series vitals, static demographics, ICD diagnosis codes, and radiology reports. The pipeline implements rigorous data quality controls, patient-level splitting to prevent data leakage, explicit missingness tracking, and standardised temporal aggregation. SurvBench handles both single-risk (e.g., in-hospital mortality) and competing-risks scenarios (e.g., multiple discharge outcomes). The outputs are compatible with pycox library packages and implementations of standard statistical and deep learning models. By providing reproducible, configuration-driven preprocessing with comprehensive documentation, SurvBench addresses the "preprocessing gap" that has hindered fair comparison of deep learning survival models, enabling researchers to focus on methodological innovation rather than data engineering.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11934v1" target="_blank"><h2>A Systematic Analysis of Out-of-Distribution Detection Under Representation and Training Paradigm Shifts <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> C. César Claros Olivares, Austin J. Brockmeier<br><strong><u>Categories:</u></strong> cs.LG, cs.CV<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> We present a systematic comparison of out-of-distribution (OOD) detection methods across CLIP-stratified regimes using AURC and AUGRC as primary metrics. Experiments cover two representation paradigms: CNNs trained from scratch and a fine-tuned Vision Transformer (ViT), evaluated on CIFAR-10/100, SuperCIFAR-100, and TinyImageNet. Using a multiple-comparison-controlled, rank-based pipeline (Friedman test with Conover-Holm post-hoc) and Bron-Kerbosch cliques, we find that the learned feature space largely determines OOD efficacy. For both CNNs and ViTs, probabilistic scores (e.g., MSR, GEN) dominate misclassification (ID) detection. Under stronger shifts, geometry-aware scores (e.g., NNGuide, fDBD, CTM) prevail on CNNs, whereas on ViTs GradNorm and KPCA Reconstruction Error remain consistently competitive. We further show a class-count-dependent trade-off for Monte-Carlo Dropout (MCD) and that a simple PCA projection improves several detectors. These results support a representation-centric view of OOD detection and provide statistically grounded guidance for method selection under distribution shift.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11930v1" target="_blank"><h2>Enhancing XR Auditory Realism via Multimodal Scene-Aware Acoustic Rendering <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Tianyu Xu, Jihan Li, Penghe Zu, Pranav Sahay, Maruchi Kim, Jack Obeng-Marnu, Farley Miller, Xun Qian, Katrina Passarella, Mahitha Rachumalla, Rajeev Nongpiur, D. Shin<br><strong><u>Categories:</u></strong> cs.HC, cs.CV, cs.LG, cs.SD<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> In Extended Reality (XR), rendering sound that accurately simulates real-world acoustics is pivotal in creating lifelike and believable virtual experiences. However, existing XR spatial audio rendering methods often struggle with real-time adaptation to diverse physical scenes, causing a sensory mismatch between visual and auditory cues that disrupts user immersion. To address this, we introduce SAMOSA, a novel on-device system that renders spatially accurate sound by dynamically adapting to its physical environment. SAMOSA leverages a synergistic multimodal scene representation by fusing real-time estimations of room geometry, surface materials, and semantic-driven acoustic context. This rich representation then enables efficient acoustic calibration via scene priors, allowing the system to synthesize a highly realistic Room Impulse Response (RIR). We validate our system through technical evaluation using acoustic metrics for RIR synthesis across various room configurations and sound types, alongside an expert evaluation (N=12). Evaluation results demonstrate SAMOSA's feasibility and efficacy in enhancing XR auditory realism.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11928v1" target="_blank"><h2>Beyond the Laplacian: Interpolated Spectral Augmentation for Graph Neural Networks <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Ziyao Cui, Edric Tam<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> Graph neural networks (GNNs) are fundamental tools in graph machine learning. The performance of GNNs relies crucially on the availability of informative node features, which can be limited or absent in real-life datasets and applications. A natural remedy is to augment the node features with embeddings computed from eigenvectors of the graph Laplacian matrix. While it is natural to default to Laplacian spectral embeddings, which capture meaningful graph connectivity information, we ask whether spectral embeddings from alternative graph matrices can also provide useful representations for learning. We introduce Interpolated Laplacian Embeddings (ILEs), which are derived from a simple yet expressive family of graph matrices. Using tools from spectral graph theory, we offer a straightforward interpretation of the structural information that ILEs capture. We demonstrate through simulations and experiments on real-world datasets that feature augmentation via ILEs can improve performance across commonly used GNN architectures. Our work offers a straightforward and practical approach that broadens the practitioner's spectral augmentation toolkit when node features are limited.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11920v1" target="_blank"><h2>PAH Marks the Spot: Digging for Buried Clusters in Nearby Star-forming Galaxies <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Gabrielle B. Graham, Daniel A. Dale, Chase L. Smith, Elisabeth Brann, Kaycee D. Conder, Samuel Crowe, Sumitra Dhileepkumar, Nicole A. Imming, Emilio Mendez, Zachary Pleska, Kelsey Sako, Amirnezam Amiri, Ashley T. Barnes, Médéric Boquien, Rupali Chandar, Ryan Chown, Oleg Y. Gnedin, Kathryn Grasha, Stephen Hannon, Hamid Hassani, Rémy Indebetouw, Hwihyun Kim, Jaeyeon Kim, Hannah Koziol, Kirsten L. Larson, Janice C. Lee, Adam K. Leroy, Elias K. Oakes, M. Jimena Rodríguez, Erik Rosolowsky, Karin Sandstrom, Eva Schinnerer, Jessica Sutter, David A. Thilker, Leonardo Ubeda, Bradley C. Whitmore, Tony D. Weinbeck, Thomas G. Williams, Aida Wofford, J. Eduardo Méndez-Delgado, Qiushi Chris Tian, the PHANGS Collaboration<br><strong><u>Categories:</u></strong> astro-ph.GA<br><strong><u>Comments:</u></strong> 27 pages, 14 figures. To be published in The Astronomical Journal<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract), transfer learning (abstract)<br><p><strong><u>Abstract:</u></strong> The joint capabilities of the Hubble Space Telescope (HST) and JWST allow for an unparalleled look at the early lives of star clusters at near- and mid-infrared wavelengths. We present here a multiband analysis of embedded young stellar clusters in 11 nearby, star-forming galaxies, using the PHANGS-JWST and PHANGS-HST datasets. We use the Zooniverse citizen science platform to conduct an initial by-eye search for embedded clusters in near-UV/optical/near-infrared images that trace stellar continuum emission, the Paschen$α$ and H$α$ recombination lines, and the 3.3 $μ$m polycyclic aromatic hydrocarbon feature and its underlying continuum. With this approach, we identify 292 embedded cluster candidates for which we characterize their ages, masses, and levels of line-of-sight extinction by comparing the photometric data to predictions from stellar population models. The embedded cluster candidates have a median age of 4.5 Myr and an average line-of-sight extinction $\left< A_V \right> = 6.0$ mag. We determine lower limits on source stellar masses, resulting in a median stellar mass of $10^3$ $M_{\odot}$. We use this sample of embedded cluster candidates to train multiple convolutional neural network models to carry out deep transfer learning-based searches for embedded clusters. With the aim of optimizing models for future catalog production, we compare results for four variations of training data using two neural networks. Confusion matrices for all eight model configurations, as well as inter-model identification trends, are presented. With refinement of the training sample, we determine that optimized models could serve as a pathway for future embedded cluster identification beyond our 11 galaxy sample.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11918v1" target="_blank"><h2>Batch Matrix-form Equations and Implementation of Multilayer Perceptrons <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Wieger Wesselink, Bram Grooten, Huub van de Wetering, Qiao Xiao, Decebal Constantin Mocanu<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 32 pages; submitted to JMLR<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Multilayer perceptrons (MLPs) remain fundamental to modern deep learning, yet their algorithmic details are rarely presented in complete, explicit \emph{batch matrix-form}. Rather, most references express gradients per sample or rely on automatic differentiation. Although automatic differentiation can achieve equally high computational efficiency, the usage of batch matrix-form makes the computational structure explicit, which is essential for transparent, systematic analysis, and optimization in settings such as sparse neural networks. This paper fills that gap by providing a mathematically rigorous and implementation-ready specification of MLPs in batch matrix-form. We derive forward and backward equations for all standard and advanced layers, including batch normalization and softmax, and validate all equations using the symbolic mathematics library SymPy. From these specifications, we construct uniform reference implementations in NumPy, PyTorch, JAX, TensorFlow, and a high-performance C++ backend optimized for sparse operations. Our main contributions are: (1) a complete derivation of batch matrix-form backpropagation for MLPs, (2) symbolic validation of all gradient equations, (3) uniform Python and C++ reference implementations grounded in a small set of matrix primitives, and (4) demonstration of how explicit formulations enable efficient sparse computation. Together, these results establish a validated, extensible foundation for understanding, teaching, and researching neural network algorithms.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11912v1" target="_blank"><h2>A Systematic Study of Model Extraction Attacks on Graph Foundation Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Haoyan Xu, Ruizhi Qian, Jiate Li, Yushun Dong, Minghao Lin, Hanson Yan, Zhengtao Yao, Qinghua Liu, Junhao Dong, Ruopeng Huang, Yue Zhao, Mengyuan Li<br><strong><u>Categories:</u></strong> cs.LG, cs.CR<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> anomaly detection (abstract), neural network (abstract), multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Graph machine learning has advanced rapidly in tasks such as link prediction, anomaly detection, and node classification. As models scale up, pretrained graph models have become valuable intellectual assets because they encode extensive computation and domain expertise. Building on these advances, Graph Foundation Models (GFMs) mark a major step forward by jointly pretraining graph and text encoders on massive and diverse data. This unifies structural and semantic understanding, enables zero-shot inference, and supports applications such as fraud detection and biomedical analysis. However, the high pretraining cost and broad cross-domain knowledge in GFMs also make them attractive targets for model extraction attacks (MEAs). Prior work has focused only on small graph neural networks trained on a single graph, leaving the security implications for large-scale and multimodal GFMs largely unexplored. This paper presents the first systematic study of MEAs against GFMs. We formalize a black-box threat model and define six practical attack scenarios covering domain-level and graph-specific extraction goals, architectural mismatch, limited query budgets, partial node access, and training data discrepancies. To instantiate these attacks, we introduce a lightweight extraction method that trains an attacker encoder using supervised regression of graph embeddings. Even without contrastive pretraining data, this method learns an encoder that stays aligned with the victim text encoder and preserves its zero-shot inference ability on unseen graphs. Experiments on seven datasets show that the attacker can approximate the victim model using only a tiny fraction of its original training cost, with almost no loss in accuracy. These findings reveal that GFMs greatly expand the MEA surface and highlight the need for deployment-aware security defenses in large-scale graph learning systems.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11908v1" target="_blank"><h2>PI-NAIM: Path-Integrated Neural Adaptive Imputation Model <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Afifa Khaled, Ebrahim Hamid Sumiea<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), multimodal (abstract), multi-modal (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Medical imaging and multi-modal clinical settings often face the challange of missing modality in their diagnostic pipelines. Existing imputation methods either lack representational capacity or are computationally expensive. We propose PI-NAIM, a novel dual-path architecture that dynamically routes samples to optimized imputation approaches based on missingness complexity. Our framework integrates: (1) intelligent path routing that directs low missingness samples to efficient statistical imputation (MICE) and complex patterns to powerful neural networks (GAIN with temporal analysis); (2) cross-path attention fusion that leverages missingness-aware embeddings to intelligently combine both branches; and (3) end-to-end joint optimization of imputation accuracy and downstream task performance. Extensive experiments on MIMIC-III and multimodal benchmarks demonstrate state-of-the-art performance, achieving RMSE of 0.108 (vs. baselines' 0.119-0.152) and substantial gains in downstream tasks with an AUROC of 0.812 for mortality prediction. PI-NAIM's modular design enables seamless integration into vision pipelines handling incomplete sensor measurements, missing modalities, or corrupted inputs, providing a unified solution for real-world scenario. The code is publicly available at https://github.com/AfifaKhaled/PI-NAIM-Path-Integrated-Neural-Adaptive-Imputation-Model</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13769v1" target="_blank"><h2>Compiling to linear neurons <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Joey Velez-Ginorio, Nada Amin, Konrad Kording, Steve Zdancewic<br><strong><u>Categories:</u></strong> cs.LG, cs.PL<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> We don't program neural networks directly. Instead, we rely on an indirect style where learning algorithms, like gradient descent, determine a neural network's function by learning from data. This indirect style is often a virtue; it empowers us to solve problems that were previously impossible. But it lacks discrete structure. We can't compile most algorithms into a neural network -- even if these algorithms could help the network learn. This limitation occurs because discrete algorithms are not obviously differentiable, making them incompatible with the gradient-based learning algorithms that determine a neural network's function. To address this, we introduce $\textsf{Cajal}$: a typed, higher-order and linear programming language intended to be a minimal vehicle for exploring a direct style of programming neural networks. We prove $\textsf{Cajal}$ programs compile to linear neurons, allowing discrete algorithms to be expressed in a differentiable form compatible with gradient-based learning. With our implementation of $\textsf{Cajal}$, we conduct several experiments where we link these linear neurons against other neural networks to determine part of their function prior to learning. Linking with these neurons allows networks to learn faster, with greater data-efficiency, and in a way that's easier to debug. A key lesson is that linear programming languages provide a path towards directly programming neural networks, enabling a rich interplay between learning and the discrete structures of ordinary programming.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11899v1" target="_blank"><h2>End to End AI System for Surgical Gesture Sequence Recognition and Clinical Outcome Prediction <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Xi Li, Nicholas Matsumoto, Ujjwal Pasupulety, Atharva Deo, Cherine Yang, Jay Moran, Miguel E. Hernandez, Peter Wager, Jasmine Lin, Jeanine Kim, Alvin C. Goh, Christian Wagner, Geoffrey A. Sonn, Andrew J. Hung<br><strong><u>Categories:</u></strong> cs.AI, cs.CV<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Fine-grained analysis of intraoperative behavior and its impact on patient outcomes remain a longstanding challenge. We present Frame-to-Outcome (F2O), an end-to-end system that translates tissue dissection videos into gesture sequences and uncovers patterns associated with postoperative outcomes. Leveraging transformer-based spatial and temporal modeling and frame-wise classification, F2O robustly detects consecutive short (~2 seconds) gestures in the nerve-sparing step of robot-assisted radical prostatectomy (AUC: 0.80 frame-level; 0.81 video-level). F2O-derived features (gesture frequency, duration, and transitions) predicted postoperative outcomes with accuracy comparable to human annotations (0.79 vs. 0.75; overlapping 95% CI). Across 25 shared features, effect size directions were concordant with small differences (~ 0.07), and strong correlation (r = 0.96, p < 1e-14). F2O also captured key patterns linked to erectile function recovery, including prolonged tissue peeling and reduced energy use. By enabling automatic interpretable assessment, F2O establishes a foundation for data-driven surgical feedback and prospective clinical decision support.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11894v1" target="_blank"><h2>Chain-of-Generation: Progressive Latent Diffusion for Text-Guided Molecular Design <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Lingxiao Li, Haobo Zhang, Bin Chen, Jiayu Zhou<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 22 pages, 7 figures, 10 tables<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> latent space (abstract)<br><p><strong><u>Abstract:</u></strong> Text-conditioned molecular generation aims to translate natural-language descriptions into chemical structures, enabling scientists to specify functional groups, scaffolds, and physicochemical constraints without handcrafted rules. Diffusion-based models, particularly latent diffusion models (LDMs), have recently shown promise by performing stochastic search in a continuous latent space that compactly captures molecular semantics. Yet existing methods rely on one-shot conditioning, where the entire prompt is encoded once and applied throughout diffusion, making it hard to satisfy all the requirements in the prompt. We discuss three outstanding challenges of one-shot conditioning generation, including the poor interpretability of the generated components, the failure to generate all substructures, and the overambition in considering all requirements simultaneously. We then propose three principles to address those challenges, motivated by which we propose Chain-of-Generation (CoG), a training-free multi-stage latent diffusion framework. CoG decomposes each prompt into curriculum-ordered semantic segments and progressively incorporates them as intermediate goals, guiding the denoising trajectory toward molecules that satisfy increasingly rich linguistic constraints. To reinforce semantic guidance, we further introduce a post-alignment learning phase that strengthens the correspondence between textual and molecular latent spaces. Extensive experiments on benchmark and real-world tasks demonstrate that CoG yields higher semantic alignment, diversity, and controllability than one-shot baselines, producing molecules that more faithfully reflect complex, compositional prompts while offering transparent insight into the generation process.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11880v1" target="_blank"><h2>Transformers vs. Recurrent Models for Estimating Forest Gross Primary Production <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> David Montero, Miguel D. Mahecha, Francesco Martinuzzi, César Aybar, Anne Klosterhalfen, Alexander Knohl, Jesús Anaya, Clemens Mosig, Sebastian Wieneke<br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.CV<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), multimodal (abstract), transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> Monitoring the spatiotemporal dynamics of forest CO$_2$ uptake (Gross Primary Production, GPP), remains a central challenge in terrestrial ecosystem research. While Eddy Covariance (EC) towers provide high-frequency estimates, their limited spatial coverage constrains large-scale assessments. Remote sensing offers a scalable alternative, yet most approaches rely on single-sensor spectral indices and statistical models that are often unable to capture the complex temporal dynamics of GPP. Recent advances in deep learning (DL) and data fusion offer new opportunities to better represent the temporal dynamics of vegetation processes, but comparative evaluations of state-of-the-art DL models for multimodal GPP prediction remain scarce. Here, we explore the performance of two representative models for predicting GPP: 1) GPT-2, a transformer architecture, and 2) Long Short-Term Memory (LSTM), a recurrent neural network, using multivariate inputs. Overall, both achieve similar accuracy. But, while LSTM performs better overall, GPT-2 excels during extreme events. Analysis of temporal context length further reveals that LSTM attains similar accuracy using substantially shorter input windows than GPT-2, highlighting an accuracy-efficiency trade-off between the two architectures. Feature importance analysis reveals radiation as the dominant predictor, followed by Sentinel-2, MODIS land surface temperature, and Sentinel-1 contributions. Our results demonstrate how model architecture, context length, and multimodal inputs jointly determine performance in GPP prediction, guiding future developments of DL frameworks for monitoring terrestrial carbon dynamics.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11847v1" target="_blank"><h2>A Multimodal Manufacturing Safety Chatbot: Knowledge Base Design, Benchmark Development, and Evaluation of Multiple RAG Approaches <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Ryan Singh, Austin Hamilton, Amanda White, Michael Wise, Ibrahim Yousif, Arthur Carvalho, Zhe Shan, Reza Abrisham Baf, Mohammad Mayyas, Lora A. Cavuoto, Fadel M. Megahed<br><strong><u>Categories:</u></strong> cs.IR, cs.AI, cs.CY<br><strong><u>Comments:</u></strong> 25 pages, 5 figures<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Ensuring worker safety remains a critical challenge in modern manufacturing environments. Industry 5.0 reorients the prevailing manufacturing paradigm toward more human-centric operations. Using a design science research methodology, we identify three essential requirements for next-generation safety training systems: high accuracy, low latency, and low cost. We introduce a multimodal chatbot powered by large language models that meets these design requirements. The chatbot uses retrieval-augmented generation to ground its responses in curated regulatory and technical documentation. To evaluate our solution, we developed a domain-specific benchmark of expert-validated question and answer pairs for three representative machines: a Bridgeport manual mill, a Haas TL-1 CNC lathe, and a Universal Robots UR5e collaborative robot. We tested 24 RAG configurations using a full-factorial design and assessed them with automated evaluations of correctness, latency, and cost. Our top 2 configurations were then evaluated by ten industry experts and academic researchers. Our results show that retrieval strategy and model configuration have a significant impact on performance. The top configuration (selected for chatbot deployment) achieved an accuracy of 86.66%, an average latency of 10.04 seconds, and an average cost of $0.005 per query. Overall, our work provides three contributions: an open-source, domain-grounded safety training chatbot; a validated benchmark for evaluating AI-assisted safety instruction; and a systematic methodology for designing and assessing AI-enabled instructional and immersive safety training systems for Industry 5.0 environments.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11845v1" target="_blank"><h2>Autonomous Underwater Cognitive System for Adaptive Navigation: A SLAM-Integrated Cognitive Architecture <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> K. A. I. N Jayarathne, R. M. N. M. Rathnayaka, D. P. S. S. Peiris<br><strong><u>Categories:</u></strong> cs.RO, cs.AI, cs.AR<br><strong><u>Comments:</u></strong> 6 pages, 2 figures<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Deep-sea exploration poses significant challenges, including disorientation, communication loss, and navigational failures in dynamic underwater environments. This paper presents an Autonomous Underwater Cognitive System (AUCS) that integrates Simultaneous Localization and Mapping (SLAM) with a Soar-based cognitive architecture to enable adaptive navigation in complex oceanic conditions. The system fuses multi-sensor data from SONAR, LiDAR, IMU, and DVL with cognitive reasoning modules for perception, attention, planning, and learning. Unlike conventional SLAM systems, AUCS incorporates semantic understanding, adaptive sensor management, and memory-based learning to differentiate between dynamic and static objects, reducing false loop closures and enhancing long-term map consistency. The proposed architecture demonstrates a complete perception-cognition-action-learning loop, allowing autonomous underwater vehicles to sense, reason, and adapt intelligently. This work lays a foundation for next-generation cognitive submersible systems, improving safety, reliability, and autonomy in deep-sea exploration.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11837v1" target="_blank"><h2>MP-GFormer: A 3D-Geometry-Aware Dynamic Graph Transformer Approach for Machining Process Planning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Fatemeh Elhambakhsh, Gaurav Ameta, Aditi Roy, Hyunwoong Ko<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Machining process planning (MP) is inherently complex due to structural and geometrical dependencies among part features and machining operations. A key challenge lies in capturing dynamic interdependencies that evolve with distinct part geometries as operations are performed. Machine learning has been applied to address challenges in MP, such as operation selection and machining sequence prediction. Dynamic graph learning (DGL) has been widely used to model dynamic systems, thanks to its ability to integrate spatio-temporal relationships. However, in MP, while existing DGL approaches can capture these dependencies, they fail to incorporate three-dimensional (3D) geometric information of parts and thus lack domain awareness in predicting machining operation sequences. To address this limitation, we propose MP-GFormer, a 3D-geometry-aware dynamic graph transformer that integrates evolving 3D geometric representations into DGL through an attention mechanism to predict machining operation sequences. Our approach leverages StereoLithography surface meshes representing the 3D geometry of a part after each machining operation, with the boundary representation method used for the initial 3D designs. We evaluate MP-GFormer on a synthesized dataset and demonstrate that the method achieves improvements of 24\% and 36\% in accuracy for main and sub-operation predictions, respectively, compared to state-of-the-art approaches.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11834v1" target="_blank"><h2>Volatility in Certainty (VC): A Metric for Detecting Adversarial Perturbations During Inference in Neural Network Classifiers <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Vahid Hemmati, Ahmad Mohammadi, Abdul-Rauf Nuhu, Reza Ahmari, Parham Kebria, Abdollah Homaifar<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> Adversarial robustness remains a critical challenge in deploying neural network classifiers, particularly in real-time systems where ground-truth labels are unavailable during inference. This paper investigates \textit{Volatility in Certainty} (VC), a recently proposed, label-free metric that quantifies irregularities in model confidence by measuring the dispersion of sorted softmax outputs. Specifically, VC is defined as the average squared log-ratio of adjacent certainty values, capturing local fluctuations in model output smoothness. We evaluate VC as a proxy for classification accuracy and as an indicator of adversarial drift. Experiments are conducted on artificial neural networks (ANNs) and convolutional neural networks (CNNs) trained on MNIST, as well as a regularized VGG-like model trained on CIFAR-10. Adversarial examples are generated using the Fast Gradient Sign Method (FGSM) across varying perturbation magnitudes. In addition, mixed test sets are created by gradually introducing adversarial contamination to assess VC's sensitivity under incremental distribution shifts. Our results reveal a strong negative correlation between classification accuracy and log(VC) (correlation rho < -0.90 in most cases), suggesting that VC effectively reflects performance degradation without requiring labeled data. These findings position VC as a scalable, architecture-agnostic, and real-time performance metric suitable for early-warning systems in safety-critical applications.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11830v1" target="_blank"><h2>A Computational Method for Solving the Stochastic Joint Replenishment Problem in High Dimensions <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Barış Ata, Wouter van Eekelen, Yuan Zhong<br><strong><u>Categories:</u></strong> math.OC, cs.LG<br><strong><u>Comments:</u></strong> 52 pages, 3 figures<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> We consider a discrete-time formulation for a class of high-dimensional stochastic joint replenishment problems. First, we approximate the problem by a continuous-time impulse control problem. Exploiting connections among the impulse control problem, backward stochastic differential equations (BSDEs) with jumps, and the stochastic target problem, we develop a novel, simulation-based computational method that relies on deep neural networks to solve the impulse control problem. Based on that solution, we propose an implementable inventory control policy for the original (discrete-time) stochastic joint replenishment problem, and test it against the best available benchmarks in a series of test problems. For the problems studied thus far, our method matches or beats the best benchmark we could find, and it is computationally feasible up to at least 50 dimensions -- that is, 50 stock-keeping units (SKUs).</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11825v1" target="_blank"><h2>Real-Time Speech Enhancement via a Hybrid ViT: A Dual-Input Acoustic-Image Feature Fusion <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Behnaz Bahmei, Siamak Arzanpour, Elina Birmingham<br><strong><u>Categories:</u></strong> cs.SD, cs.AI, eess.AS<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Speech quality and intelligibility are significantly degraded in noisy environments. This paper presents a novel transformer-based learning framework to address the single-channel noise suppression problem for real-time applications. Although existing deep learning networks have shown remarkable improvements in handling stationary noise, their performance often diminishes in real-world environments characterized by non-stationary noise (e.g., dog barking, baby crying). The proposed dual-input acoustic-image feature fusion using a hybrid ViT framework effectively models both temporal and spectral dependencies in noisy signals. Designed for real-world audio environments, the proposed framework is computationally lightweight and suitable for implementation on embedded devices. To evaluate its effectiveness, four standard and commonly used quality measurements, namely PESQ, STOI, Seg SNR, and LLR, are utilized. Experimental results obtained using the Librispeech dataset as the clean speech source and the UrbanSound8K and Google Audioset datasets as the noise sources, demonstrate that the proposed method significantly improves noise reduction, speech intelligibility, and perceptual quality compared to the noisy input signal, achieving performance close to the clean reference.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11810v1" target="_blank"><h2>On the Notion that Language Models Reason <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Bertram Højer<br><strong><u>Categories:</u></strong> cs.CL, cs.AI<br><strong><u>Comments:</u></strong> Accepted at the 1st Workshop on Epistemic Intelligence in Machine Learning, EurIPS 2025<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Language models (LMs) are said to be exhibiting reasoning, but what does this entail? We assess definitions of reasoning and how key papers in the field of natural language processing (NLP) use the notion and argue that the definitions provided are not consistent with how LMs are trained, process information, and generate new tokens. To illustrate this incommensurability we assume the view that transformer-based LMs implement an \textit{implicit} finite-order Markov kernel mapping contexts to conditional token distributions. In this view, reasoning-like outputs correspond to statistical regularities and approximate statistical invariances in the learned kernel rather than the implementation of explicit logical mechanisms. This view is illustrative of the claim that LMs are "statistical pattern matchers"" and not genuine reasoners and provides a perspective that clarifies why reasoning-like outputs arise in LMs without any guarantees of logical consistency. This distinction is fundamental to how epistemic uncertainty is evaluated in LMs. We invite a discussion on the importance of how the computational processes of the systems we build and analyze in NLP research are described.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11796v1" target="_blank"><h2>Hydrodynamic instabilities in long-term three-dimensional simulations of neutrino-driven supernovae of 13 red supergiant progenitors <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Beatrice Giudici, Michael Gabler, Hans-Thomas Janka<br><strong><u>Categories:</u></strong> astro-ph.HE, astro-ph.SR<br><strong><u>Comments:</u></strong> 29 pages, 20 figures, 5 tables. Submitted to MNRAS<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> VAE (title, abstract)<br><p><strong><u>Abstract:</u></strong> We present long-term three-dimensional (3D) simulations of Type-IIP supernovae (SNe) for 13 non-rotating, single-star, red-supergiant (RSG) progenitors with zero-age-main-sequence masses between 12.5 M$_{\odot}$ and 27.3 M$_{\odot}$. The explosions were modelled with a parametric treatment of neutrino heating to obtain defined energies, ${}^{56}$Ni yields, and neutron-star properties in agreement with previous results. Our 3D SN models were evolved from core bounce until 10 days to study how the large-scale mixing of chemical elements depends on the progenitor structure. Rayleigh-Taylor instabilities (RTIs), which grow at the (C+O)/He and He/H interfaces and interact with the reverse shock forming after the SN shock has passed the He/H interface, play a crucial role in the outward mixing of ${}^{56}$Ni into the hydrogen envelope. We find most extreme ${}^{56}$Ni mixing and the highest maximum ${}^{56}$Ni velocities in lower-mass (LM) explosions despite lower explosion energies, and the weakest ${}^{56}$Ni mixing in the 3D explosions of the most massive RSGs. The efficiency of radial ${}^{56}$Ni mixing anti-correlates linearly with the helium-core mass and correlates positively with the magnitude of a local maximum of $ρr^3$ in the helium shell. This maximum causes shock deceleration and therefore facilitates high growth factors of RTI at the (C+O)/He interface in the LM explosions. Therefore fast-moving ${}^{56}$Ni created by the asymmetric neutrino-heating mechanism is carried into the ubiquitous RT-unstable region near the He/H interface and ultimately far into the envelopes of the exploding RSGs. Our correlations may aid improving mixing prescriptions in 1D SN models and deducing progenitor structures from observed SN properties.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11795v1" target="_blank"><h2>Non-Parametric Reconstruction of the Hubble Parameter from the Fourth Gravitational Wave Transient Catalog and DESI Baryonic Acoustic Oscillations <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Grégoire Pierra, Alberto Colombo, Simone Mastrogiovanni<br><strong><u>Categories:</u></strong> astro-ph.CO, gr-qc<br><strong><u>Comments:</u></strong> 15 pages, 5 figures<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> The release of the fourth Gravitational Wave Transient Catalog (GWTC-4.0) by the LIGO-Virgo-KAGRA collaboration includes more than 200 compact binary coalescence (CBC) candidates that can be used to probe the cosmic expansion. The population of merging binary black holes has been used so far to provide a constraint on the Hubble constant and dark matter fraction under the hypothesis of a flat-$Λ$-Cold-Dark-Matter Universe.
  In this work, we provide the first non-parametric constrain on the Hubble parameter from 137 dark sirens reported in GWTC-4.0. We employ the relation between detector and source frame masses for detected GW signals, to obtain a statistical redshift evaluation for the population of binary black holes (BBHs). We model the Hubble parameter as a non-parametric autoregressive process in terms of the scale factor, using splines. In addition, we introduce two novel features: the use of \textit{anchor} points for $H(z)$ derived from an external probe - here, Baryon Acoustic Oscillations (BAOs) - and a constraining power coefficient that quantifies where the inference is most data-driven by GW detections.
  We highlight three key findings: (i) using GWs alone, the Hubble parameter determination is the most GW-data-driven around redshift $z = 0.44$, yielding to $H(0.44) = 92.3_{-36.6}^{+29.9}\rm\, km s^{-1} Mpc^{-1}$. Its value at $z = 0$, the Hubble constant, is therefore less constrained by the GW data. (ii) The Hubble parameter inferred from analyses assuming a flat-$Λ$CDM cosmological model is strongly affected by the cosmological model assumption. (iii) Introducing an anchor point for $H(z)$ enhances the inferred constraints and provides a clear visualization of the redshift range where GWs contribute most to the constraining power.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11571v1" target="_blank"><h2>Optimizing Mixture of Block Attention <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Guangxuan Xiao, Junxian Guo, Kasra Mazaheri, Song Han<br><strong><u>Categories:</u></strong> cs.LG, cs.CL<br><strong><u>Comments:</u></strong> The first two authors contributed equally to this work<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Mixture of Block Attention (MoBA) (Lu et al., 2025) is a promising building block for efficiently processing long contexts in LLMs by enabling queries to sparsely attend to a small subset of key-value blocks, drastically reducing computational cost. However, the design principles governing MoBA's performance are poorly understood, and it lacks an efficient GPU implementation, hindering its practical adoption. In this paper, we first develop a statistical model to analyze MoBA's underlying mechanics. Our model reveals that performance critically depends on the router's ability to accurately distinguish relevant from irrelevant blocks based on query-key affinities. We derive a signal-to-noise ratio that formally connects architectural parameters to this retrieval accuracy. Guided by our analysis, we identify two key pathways for improvement: using smaller block sizes and applying a short convolution on keys to cluster relevant signals, which enhances routing accuracy. While theoretically better, small block sizes are inefficient on GPUs. To bridge this gap, we introduce FlashMoBA, a hardware-aware CUDA kernel that enables efficient MoBA execution even with the small block sizes our theory recommends. We validate our insights by training LLMs from scratch, showing that our improved MoBA models match the performance of dense attention baselines. FlashMoBA achieves up to 14.7x speedup over FlashAttention-2 for small blocks, making our theoretically-grounded improvements practical. Code is available at: https://github.com/mit-han-lab/flash-moba.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11558v1" target="_blank"><h2>Human-AI collaborative autonomous synthesis with pulsed laser deposition for remote epitaxy <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Asraful Haque, Daniel T. Yimam, Jawad Chowdhury, Ralph Bulanadi, Ivan Vlassiouk, John Lasseter, Sujoy Ghosh, Christopher M. Rouleau, Kai Xiao, Yongtao Liu, Eva Zarkadoula, Rama K. Vasudevan, Sumner B. Harris<br><strong><u>Categories:</u></strong> cond-mat.mtrl-sci, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> Autonomous laboratories typically rely on data-driven decision-making, occasionally with human-in-the-loop oversight to inject domain expertise. Fully leveraging AI agents, however, requires tightly coupled, collaborative workflows spanning hypothesis generation, experimental planning, execution, and interpretation. To address this, we develop and deploy a human-AI collaborative (HAIC) workflow that integrates large language models for hypothesis generation and analysis, with collaborative policy updates driving autonomous pulsed laser deposition (PLD) experiments for remote epitaxy of BaTiO$_3$/graphene. HAIC accelerated the hypothesis formation and experimental design and efficiently mapped the growth space to graphene-damage. In situ Raman spectroscopy reveals that chemistry drives degradation while the highest energy plume components seed defects, identifying a low-O$_2$ pressure low-temperature synthesis window that preserves graphene but is incompatible with optimal BaTiO$_3$ growth. Thus, we show a two-step Ar/O$_2$ deposition is required to exfoliate ferroelectric BaTiO$_3$ while maintaining a monolayer graphene interlayer. HAIC stages human insight with AI reasoning between autonomous batches to drive rapid scientific progress, providing an evolution to many existing human-in-the-loop autonomous workflows.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11553v1" target="_blank"><h2>Multistability of Self-Attention Dynamics in Transformers <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Claudio Altafini<br><strong><u>Categories:</u></strong> cs.LG, eess.SY, math.DS<br><strong><u>Comments:</u></strong> 8 pages, 3 figures<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> In machine learning, a self-attention dynamics is a continuous-time multiagent-like model of the attention mechanisms of transformers. In this paper we show that such dynamics is related to a multiagent version of the Oja flow, a dynamical system that computes the principal eigenvector of a matrix corresponding for transformers to the value matrix. We classify the equilibria of the ``single-head'' self-attention system into four classes: consensus, bipartite consensus, clustering and polygonal equilibria. Multiple asymptotically stable equilibria from the first three classes often coexist in the self-attention dynamics. Interestingly, equilibria from the first two classes are always aligned with the eigenvectors of the value matrix, often but not exclusively with the principal eigenvector.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11541v1" target="_blank"><h2>An optical--mid-infrared color evolution tool for nova identification using WISE data <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Joseph Onuegbu, Dafne Guetta, Yael Hillman, Volker Perdelwitz, Massimo Della Valle<br><strong><u>Categories:</u></strong> astro-ph.SR, astro-ph.HE, astro-ph.IM<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> We present a novel approach for characterizing nova candidates by exploiting the infrared capabilities of the Wide-field Infrared Survey Explorer (WISE) catalog. We developed a pipeline to identify novae based on well-defined infrared criteria, and leveraging this pipeline, we successfully identified 41 optically confirmed novae in the WISE catalog. In particular, we focus on the color difference between the optical V band and the WISE 3.4 microns W1 band as a diagnostic. We compared their infrared light curves with their optical counterparts. We identified a strong correlation from which we proposed a color difference model that can be used for further identification and characterization of novae. Our analysis validates the mass-loss timescale theory, which predicts that systems with lower accretion rates accumulate larger envelopes and produce more massive ejecta. We also confirm models' prediction that the early color evolution of novae is governed by ejecta expansion and cooling. From our sample statistics, we infer a Galactic nova rate of approximately 40 to 50 novae per year, consistent with modern and infrared-corrected estimates. The resultant model from this work paves the way for future large-scale investigations of nova candidates.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11788v1" target="_blank"><h2>MALBO: Optimizing LLM-Based Multi-Agent Teams via Multi-Objective Bayesian Optimization <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Antonio Sabbatella<br><strong><u>Categories:</u></strong> cs.MA, cs.AI<br><strong><u>Comments:</u></strong> Master's Thesis, University of Milano-Bicocca, 2025<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> The optimal assignment of Large Language Models (LLMs) to specialized roles in multi-agent systems is a significant challenge, defined by a vast combinatorial search space, expensive black-box evaluations, and an inherent trade-off between performance and cost. Current optimization methods focus on single-agent settings and lack a principled framework for this multi-agent, multi-objective problem.
  This thesis introduces MALBO (Multi-Agent LLM Bayesian Optimization), a systematic framework designed to automate the efficient composition of LLM-based agent teams. We formalize the assignment challenge as a multi-objective optimization problem, aiming to identify the Pareto front of configurations between task accuracy and inference cost. The methodology employs multi-objective Bayesian Optimization (MOBO) with independent Gaussian Process surrogate models. By searching over a continuous feature-space representation of the LLMs, this approach performs a sample-efficient exploration guided by the expected hypervolume improvement.
  The primary contribution is a principled and automated methodology that yields a Pareto front of optimal team configurations. Our results demonstrate that the Bayesian optimization phase, compared to an initial random search, maintained a comparable average performance while reducing the average configuration cost by over 45%. Furthermore, MALBO identified specialized, heterogeneous teams that achieve cost reductions of up to 65.8% compared to homogeneous baselines, all while maintaining maximum performance. The framework thus provides a data-driven tool for deploying cost-effective and highly specialized multi-agent AI systems.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11522v3" target="_blank"><h2>CVChess: A Deep Learning Framework for Converting Chessboard Images to Forsyth-Edwards Notation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Luthira Abeykoon, Ved Patel, Gawthaman Senthilvelan, Darshan Kasundra<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Chess has experienced a large increase in viewership since the pandemic, driven largely by the accessibility of online learning platforms. However, no equivalent assistance exists for physical chess games, creating a divide between analog and digital chess experiences. This paper presents CVChess, a deep learning framework for converting chessboard images to Forsyth-Edwards Notation (FEN), which is later input into online chess engines to provide you with the best next move. Our approach employs a convolutional neural network (CNN) with residual layers to perform piece recognition from smartphone camera images. The system processes RGB images of a physical chess board through a multistep process: image preprocessing using the Hough Line Transform for edge detection, projective transform to achieve a top-down board alignment, segmentation into 64 individual squares, and piece classification into 13 classes (6 unique white pieces, 6 unique black pieces and an empty square) using the residual CNN. Residual connections help retain low-level visual features while enabling deeper feature extraction, improving accuracy and stability during training. We train and evaluate our model using the Chess Recognition Dataset (ChessReD), containing 10,800 annotated smartphone images captured under diverse lighting conditions and angles. The resulting classifications are encoded as an FEN string, which can be fed into a chess engine to generate the most optimal move</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11502v1" target="_blank"><h2>PAS : Prelim Attention Score for Detecting Object Hallucinations in Large Vision--Language Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Nhat Hoang-Xuan, Minh Vu, My T. Thai, Manish Bhattarai<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Large vision-language models (LVLMs) are powerful, yet they remain unreliable due to object hallucinations. In this work, we show that in many hallucinatory predictions the LVLM effectively ignores the image and instead relies on previously generated output (prelim) tokens to infer new objects. We quantify this behavior via the mutual information between the image and the predicted object conditioned on the prelim, demonstrating that weak image dependence strongly correlates with hallucination. Building on this finding, we introduce the Prelim Attention Score (PAS), a lightweight, training-free signal computed from attention weights over prelim tokens. PAS requires no additional forward passes and can be computed on the fly during inference. Exploiting this previously overlooked signal, PAS achieves state-of-the-art object-hallucination detection across multiple models and datasets, enabling real-time filtering and intervention.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11490v1" target="_blank"><h2>Intrinsic Dimension Estimation for Radio Galaxy Zoo using Diffusion Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Joan Font-Quer Roset, Devina Mohan, Anna Scaife<br><strong><u>Categories:</u></strong> cs.LG, astro-ph.IM, cs.AI<br><strong><u>Comments:</u></strong> 9 pages, 5 figures, 2 tables, submitted to NeurIPS 2025 ML4PS Workshop<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> In this work, we estimate the intrinsic dimension (iD) of the Radio Galaxy Zoo (RGZ) dataset using a score-based diffusion model. We examine how the iD estimates vary as a function of Bayesian neural network (BNN) energy scores, which measure how similar the radio sources are to the MiraBest subset of the RGZ dataset. We find that out-of-distribution sources exhibit higher iD values, and that the overall iD for RGZ exceeds those typically reported for natural image datasets. Furthermore, we analyse how iD varies across Fanaroff-Riley (FR) morphological classes and as a function of the signal-to-noise ratio (SNR). While no relationship is found between FR I and FR II classes, a weak trend toward higher SNR at lower iD. Future work using the RGZ dataset could make use of the relationship between iD and energy scores to quantitatively study and improve the representations learned by various self-supervised learning algorithms.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11483v1" target="_blank"><h2>ImAgent: A Unified Multimodal Agent Framework for Test-Time Scalable Image Generation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Kaishen Wang, Ruibo Chen, Tong Zheng, Heng Huang<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> 12 pages, 5 tables, 6 figures<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Recent text-to-image (T2I) models have made remarkable progress in generating visually realistic and semantically coherent images. However, they still suffer from randomness and inconsistency with the given prompts, particularly when textual descriptions are vague or underspecified. Existing approaches, such as prompt rewriting, best-of-N sampling, and self-refinement, can mitigate these issues but usually require additional modules and operate independently, hindering test-time scaling efficiency and increasing computational overhead. In this paper, we introduce ImAgent, a training-free unified multimodal agent that integrates reasoning, generation, and self-evaluation within a single framework for efficient test-time scaling. Guided by a policy controller, multiple generation actions dynamically interact and self-organize to enhance image fidelity and semantic alignment without relying on external models. Extensive experiments on image generation and editing tasks demonstrate that ImAgent consistently improves over the backbone and even surpasses other strong baselines where the backbone model fails, highlighting the potential of unified multimodal agents for adaptive and efficient image generation under test-time scaling.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11480v1" target="_blank"><h2>Inferring response times of perceptual decisions with Poisson variational autoencoders <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Hayden R. Johnson, Anastasia N. Krouglova, Hadi Vafaii, Jacob L. Yates, Pedro J. Gonçalves<br><strong><u>Categories:</u></strong> q-bio.NC, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> To appear at the NeurIPS 2025 Workshop on Data on the Mind and Brain<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> variational autoencoder (title, abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Many properties of perceptual decision making are well-modeled by deep neural networks. However, such architectures typically treat decisions as instantaneous readouts, overlooking the temporal dynamics of the decision process. We present an image-computable model of perceptual decision making in which choices and response times arise from efficient sensory encoding and Bayesian decoding of neural spiking activity. We use a Poisson variational autoencoder to learn unsupervised representations of visual stimuli in a population of rate-coded neurons, modeled as independent homogeneous Poisson processes. A task-optimized decoder then continually infers an approximate posterior over actions conditioned on incoming spiking activity. Combining these components with an entropy-based stopping rule yields a principled and image-computable model of perceptual decisions capable of generating trial-by-trial patterns of choices and response times. Applied to MNIST digit classification, the model reproduces key empirical signatures of perceptual decision making, including stochastic variability, right-skewed response time distributions, logarithmic scaling of response times with the number of alternatives (Hick's law), and speed-accuracy trade-offs.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11466v1" target="_blank"><h2>Non-Euclidean SGD for Structured Optimization: Unified Analysis and Improved Rates <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Dmitry Kovalev, Ekaterina Borodich<br><strong><u>Categories:</u></strong> math.OC, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Recently, several instances of non-Euclidean SGD, including SignSGD, Lion, and Muon, have attracted significant interest from the optimization community due to their practical success in training deep neural networks. Consequently, a number of works have attempted to explain this success by developing theoretical convergence analyses. Unfortunately, these results cannot properly justify the superior performance of these methods, as they could not beat the convergence rate of vanilla Euclidean SGD. We resolve this important open problem by developing a new unified convergence analysis under the structured smoothness and gradient noise assumption. In particular, our results indicate that non-Euclidean SGD (i) can exploit the sparsity or low-rank structure of the upper bounds on the Hessian and gradient noise, (ii) can provably benefit from popular algorithmic tools such as extrapolation or momentum variance reduction, and (iii) can match the state-of-the-art convergence rates of adaptive and more complex optimization algorithms such as AdaGrad and Shampoo.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11462v1" target="_blank"><h2>MoCap2Radar: A Spatiotemporal Transformer for Synthesizing Micro-Doppler Radar Signatures from Motion Capture <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Kevin Chen, Kenneth W. Parker, Anish Arora<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> We present a pure machine learning process for synthesizing radar spectrograms from Motion-Capture (MoCap) data. We formulate MoCap-to-spectrogram translation as a windowed sequence-to-sequence task using a transformer-based model that jointly captures spatial relations among MoCap markers and temporal dynamics across frames. Real-world experiments show that the proposed approach produces visually and quantitatively plausible doppler radar spectrograms and achieves good generalizability. Ablation experiments show that the learned model includes both the ability to convert multi-part motion into doppler signatures and an understanding of the spatial relations between different parts of the human body.
  The result is an interesting example of using transformers for time-series signal processing. It is especially applicable to edge computing and Internet of Things (IoT) radars. It also suggests the ability to augment scarce radar datasets using more abundant MoCap data for training higher-level applications. Finally, it requires far less computation than physics-based methods for generating radar data.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11452v1" target="_blank"><h2>Synergy vs. Noise: Performance-Guided Multimodal Fusion For Biochemical Recurrence-Free Survival in Prostate Cancer <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Seth Alain Chang, Muhammad Mueez Amjad, Noorul Wahab, Ethar Alzaid, Nasir Rajpoot, Adam Shephard<br><strong><u>Categories:</u></strong> q-bio.QM, cs.CV, cs.LG, eess.IV<br><strong><u>Comments:</u></strong> 5 pages, 1 figure, 4 tables<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Multimodal deep learning (MDL) has emerged as a transformative approach in computational pathology. By integrating complementary information from multiple data sources, MDL models have demonstrated superior predictive performance across diverse clinical tasks compared to unimodal models. However, the assumption that combining modalities inherently improves performance remains largely unexamined. We hypothesise that multimodal gains depend critically on the predictive quality of individual modalities, and that integrating weak modalities may introduce noise rather than complementary information. We test this hypothesis on a prostate cancer dataset with histopathology, radiology, and clinical data to predict time-to-biochemical recurrence. Our results confirm that combining high-performing modalities yield superior performance compared to unimodal approaches. However, integrating a poor-performing modality with other higher-performing modalities degrades predictive accuracy. These findings demonstrate that multimodal benefit requires selective, performance-guided integration rather than indiscriminate modality combination, with implications for MDL design across computational pathology and medical imaging.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11446v1" target="_blank"><h2>DiffPro: Joint Timestep and Layer-Wise Precision Optimization for Efficient Diffusion Inference <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Farhana Amin, Sabiha Afroz, Kanchon Gharami, Mona Moghadampanah, Dimitrios S. Nikolopoulos<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Diffusion models produce high quality images but inference is costly due to many denoising steps and heavy matrix operations. We present DiffPro, a post-training, hardware-faithful framework that works with the exact integer kernels used in deployment and jointly tunes timesteps and per-layer precision in Diffusion Transformers (DiTs) to reduce latency and memory without any training. DiffPro combines three parts: a manifold-aware sensitivity metric to allocate weight bits, dynamic activation quantization to stabilize activations across timesteps, and a budgeted timestep selector guided by teacher-student drift. In experiments DiffPro achieves up to 6.25x model compression, fifty percent fewer timesteps, and 2.8x faster inference with Delta FID <= 10 on standard benchmarks, demonstrating practical efficiency gains. DiffPro unifies step reduction and precision planning into a single budgeted deployable plan for real-time energy-aware diffusion inference.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11439v1" target="_blank"><h2>Retrofit: Continual Learning with Bounded Forgetting for Security Applications <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yiling He, Junchi Lei, Hongyu She, Shuo Shao, Xinran Zheng, Yiping Liu, Zhan Qin, Lorenzo Cavallaro<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> transfer learning (abstract)<br><p><strong><u>Abstract:</u></strong> Modern security analytics are increasingly powered by deep learning models, but their performance often degrades as threat landscapes evolve and data representations shift. While continual learning (CL) offers a promising paradigm to maintain model effectiveness, many approaches rely on full retraining or data replay, which are infeasible in data-sensitive environments. Moreover, existing methods remain inadequate for security-critical scenarios, facing two coupled challenges in knowledge transfer: preserving prior knowledge without old data and integrating new knowledge with minimal interference.
  We propose RETROFIT, a data retrospective-free continual learning method that achieves bounded forgetting for effective knowledge transfer. Our key idea is to consolidate previously trained and newly fine-tuned models, serving as teachers of old and new knowledge, through parameter-level merging that eliminates the need for historical data. To mitigate interference, we apply low-rank and sparse updates that confine parameter changes to independent subspaces, while a knowledge arbitration dynamically balances the teacher contributions guided by model confidence. Our evaluation on two representative applications demonstrates that RETROFIT consistently mitigates forgetting while maintaining adaptability. In malware detection under temporal drift, it substantially improves the retention score, from 20.2% to 38.6% over CL baselines, and exceeds the oracle upper bound on new data. In binary summarization across decompilation levels, where analyzing stripped binaries is especially challenging, RETROFIT achieves around twice the BLEU score of transfer learning used in prior work and surpasses all baselines in cross-representation generalization.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11435v1" target="_blank"><h2>The Persistence of Cultural Memory: Investigating Multimodal Iconicity in Diffusion Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Maria-Teresa De Rosa Palmini, Eva Cetinic<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Our work addresses the ambiguity between generalization and memorization in text-to-image diffusion models, focusing on a specific case we term multimodal iconicity. This refers to instances where images and texts evoke culturally shared associations, such as when a title recalls a familiar artwork or film scene. While prior research on memorization and unlearning emphasizes forgetting, we examine what is remembered and how, focusing on the balance between recognizing cultural references and reproducing them. We introduce an evaluation framework that separates recognition, whether a model identifies a reference, from realization, how it depicts it through replication or reinterpretation, quantified through measures capturing both dimensions. By evaluating five diffusion models across 767 Wikidata-derived cultural references spanning static and dynamic imagery, we show that our framework distinguishes replication from transformation more effectively than existing similarity-based methods. To assess linguistic sensitivity, we conduct prompt perturbation experiments using synonym substitutions and literal image descriptions, finding that models often reproduce iconic visual structures even when textual cues are altered. Finally, our analysis shows that cultural alignment correlates not only with training data frequency, but also textual uniqueness, reference popularity, and creation date. Our work reveals that the value of diffusion models lies not only in what they reproduce but in how they transform and recontextualize cultural knowledge, advancing evaluation beyond simple text-image matching toward richer contextual understanding.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11423v1" target="_blank"><h2>CURENet: Combining Unified Representations for Efficient Chronic Disease Prediction <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Cong-Tinh Dao, Nguyen Minh Thao Phan, Jun-En Ding, Chenwei Wu, David Restrepo, Dongsheng Luo, Fanyi Zhao, Chun-Chieh Liao, Wen-Chih Peng, Chi-Te Wang, Pei-Fu Chen, Ling Chen, Xinglong Ju, Feng Liu, Fang-Ming Hung<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract), transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Electronic health records (EHRs) are designed to synthesize diverse data types, including unstructured clinical notes, structured lab tests, and time-series visit data. Physicians draw on these multimodal and temporal sources of EHR data to form a comprehensive view of a patient's health, which is crucial for informed therapeutic decision-making. Yet, most predictive models fail to fully capture the interactions, redundancies, and temporal patterns across multiple data modalities, often focusing on a single data type or overlooking these complexities. In this paper, we present CURENet, a multimodal model (Combining Unified Representations for Efficient chronic disease prediction) that integrates unstructured clinical notes, lab tests, and patients' time-series data by utilizing large language models (LLMs) for clinical text processing and textual lab tests, as well as transformer encoders for longitudinal sequential visits. CURENet has been capable of capturing the intricate interaction between different forms of clinical data and creating a more reliable predictive model for chronic illnesses. We evaluated CURENet using the public MIMIC-III and private FEMH datasets, where it achieved over 94\% accuracy in predicting the top 10 chronic conditions in a multi-label framework. Our findings highlight the potential of multimodal EHR integration to enhance clinical decision-making and improve patient outcomes.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11421v1" target="_blank"><h2>BOFA: Bridge-Layer Orthogonal Low-Rank Fusion for CLIP-Based Class-Incremental Learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Lan Li, Tao Hu, Da-Wei Zhou, Han-Jia Ye, De-Chuan Zhan<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> Accepted by AAAI 2026<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> multi-modal (abstract)<br><p><strong><u>Abstract:</u></strong> Class-Incremental Learning (CIL) aims to continually learn new categories without forgetting previously acquired knowledge. Vision-language models such as CLIP offer strong transferable representations via multi-modal supervision, making them promising for CIL. However, applying CLIP to CIL poses two major challenges: (1) adapting to downstream tasks often requires additional learnable modules, increasing model complexity and susceptibility to forgetting; and (2) while multi-modal representations offer complementary strengths, existing methods have yet to fully realize their potential in effectively integrating visual and textual modalities. To address these issues, we propose BOFA (Bridge-layer Orthogonal Fusion for Adaptation), a novel framework for CIL. BOFA confines all model adaptation exclusively to CLIP's existing cross-modal bridge-layer, thereby adding no extra parameters or inference cost. To prevent forgetting within this layer, it leverages Orthogonal Low-Rank Fusion, a mechanism that constrains parameter updates to a low-rank ``safe subspace" mathematically constructed to be orthogonal to past task features. This ensures stable knowledge accumulation without data replay. Furthermore, BOFA employs a cross-modal hybrid prototype that synergizes stable textual prototypes with visual counterparts derived from our stably adapted bridge-layer, enhancing classification performance. Extensive experiments on standard benchmarks show that BOFA achieves superior accuracy and efficiency compared to existing methods.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11418v1" target="_blank"><h2>Low-Bit, High-Fidelity: Optimal Transport Quantization for Flow Matching <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Dara Varam, Diaa A. Abuhani, Imran Zualkernan, Raghad AlDamani, Lujain Khalil<br><strong><u>Categories:</u></strong> cs.LG, cs.CV<br><strong><u>Comments:</u></strong> 12 pages, 8 figures<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> latent space (abstract)<br><p><strong><u>Abstract:</u></strong> Flow Matching (FM) generative models offer efficient simulation-free training and deterministic sampling, but their practical deployment is challenged by high-precision parameter requirements. We adapt optimal transport (OT)-based post-training quantization to FM models, minimizing the 2-Wasserstein distance between quantized and original weights, and systematically compare its effectiveness against uniform, piecewise, and logarithmic quantization schemes. Our theoretical analysis provides upper bounds on generative degradation under quantization, and empirical results across five benchmark datasets of varying complexity show that OT-based quantization preserves both visual generation quality and latent space stability down to 2-3 bits per parameter, where alternative methods fail. This establishes OT-based quantization as a principled, effective approach to compress FM generative models for edge and embedded AI applications.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11402v1" target="_blank"><h2>Multi-Phase Spacecraft Trajectory Optimization via Transformer-Based Reinforcement Learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Amit Jain, Victor Rodriguez-Fernandez, Richard Linares<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> Autonomous spacecraft control for mission phases such as launch, ascent, stage separation, and orbit insertion remains a critical challenge due to the need for adaptive policies that generalize across dynamically distinct regimes. While reinforcement learning (RL) has shown promise in individual astrodynamics tasks, existing approaches often require separate policies for distinct mission phases, limiting adaptability and increasing operational complexity. This work introduces a transformer-based RL framework that unifies multi-phase trajectory optimization through a single policy architecture, leveraging the transformer's inherent capacity to model extended temporal contexts. Building on proximal policy optimization (PPO), our framework replaces conventional recurrent networks with a transformer encoder-decoder structure, enabling the agent to maintain coherent memory across mission phases spanning seconds to minutes during critical operations. By integrating a Gated Transformer-XL (GTrXL) architecture, the framework eliminates manual phase transitions while maintaining stability in control decisions. We validate our approach progressively: first demonstrating near-optimal performance on single-phase benchmarks (double integrator and Van der Pol oscillator), then extending to multiphase waypoint navigation variants, and finally tackling a complex multiphase rocket ascent problem that includes atmospheric flight, stage separation, and vacuum operations. Results demonstrate that the transformer-based framework not only matches analytical solutions in simple cases but also effectively learns coherent control policies across dynamically distinct regimes, establishing a foundation for scalable autonomous mission planning that reduces reliance on phase-specific controllers while maintaining compatibility with safety-critical verification protocols.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11380v1" target="_blank"><h2>When Genes Speak: A Semantic-Guided Framework for Spatially Resolved Transcriptomics Data Clustering <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jiangkai Long, Yanran Zhu, Chang Tang, Kun Sun, Yuanyuan Liu, Xuesong Yan<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> AAAI'2026 poster paper. 12 pages, 8 figures<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Spatial transcriptomics enables gene expression profiling with spatial context, offering unprecedented insights into the tissue microenvironment. However, most computational models treat genes as isolated numerical features, ignoring the rich biological semantics encoded in their symbols. This prevents a truly deep understanding of critical biological characteristics. To overcome this limitation, we present SemST, a semantic-guided deep learning framework for spatial transcriptomics data clustering. SemST leverages Large Language Models (LLMs) to enable genes to "speak" through their symbolic meanings, transforming gene sets within each tissue spot into biologically informed embeddings. These embeddings are then fused with the spatial neighborhood relationships captured by Graph Neural Networks (GNNs), achieving a coherent integration of biological function and spatial structure. We further introduce the Fine-grained Semantic Modulation (FSM) module to optimally exploit these biological priors. The FSM module learns spot-specific affine transformations that empower the semantic embeddings to perform an element-wise calibration of the spatial features, thus dynamically injecting high-order biological knowledge into the spatial context. Extensive experiments on public spatial transcriptomics datasets show that SemST achieves state-of-the-art clustering performance. Crucially, the FSM module exhibits plug-and-play versatility, consistently improving the performance when integrated into other baseline methods.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11361v1" target="_blank"><h2>Toward Multi-Fidelity Machine Learning Force Field for Cathode Materials <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Guangyi Dong, Zhihui Wang<br><strong><u>Categories:</u></strong> cs.LG, cond-mat.mtrl-sci<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Machine learning force fields (MLFFs), which employ neural networks to map atomic structures to system energies, effectively combine the high accuracy of first-principles calculation with the computational efficiency of empirical force fields. They are widely used in computational materials simulations. However, the development and application of MLFFs for lithium-ion battery cathode materials remain relatively limited. This is primarily due to the complex electronic structure characteristics of cathode materials and the resulting scarcity of high-quality computational datasets available for force field training. In this work, we develop a multi-fidelity machine learning force field framework to enhance the data efficiency of computational results, which can simultaneously utilize both low-fidelity non-magnetic and high-fidelity magnetic computational datasets of cathode materials for training. Tests conducted on the lithium manganese iron phosphate (LMFP) cathode material system demonstrate the effectiveness of this multi-fidelity approach. This work helps to achieve high-accuracy MLFF training for cathode materials at a lower training dataset cost, and offers new perspectives for applying MLFFs to computational simulations of cathode materials.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11357v1" target="_blank"><h2>KarmaTS: A Universal Simulation Platform for Multivariate Time Series with Functional Causal Dynamics <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Haixin Li, Yanke Li, Diego Paez-Granados<br><strong><u>Categories:</u></strong> cs.AI, cs.HC<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> We introduce KarmaTS, an interactive framework for constructing lag-indexed, executable spatiotemporal causal graphical models for multivariate time series (MTS) simulation. Motivated by the challenge of access-restricted physiological data, KarmaTS generates synthetic MTS with known causal dynamics and augments real-world datasets with expert knowledge. The system constructs a discrete-time structural causal process (DSCP) by combining expert knowledge and algorithmic proposals in a mixed-initiative, human-in-the-loop workflow. The resulting DSCP supports simulation and causal interventions, including those under user-specified distribution shifts. KarmaTS handles mixed variable types, contemporaneous and lagged edges, and modular edge functionals ranging from parameterizable templates to neural network models. Together, these features enable flexible validation and benchmarking of causal discovery algorithms through expert-informed simulation.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11323v1" target="_blank"><h2>RLSLM: A Hybrid Reinforcement Learning Framework Aligning Rule-Based Social Locomotion Model with Human Social Norms <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yitian Kou, Yihe Gu, Chen Zhou, DanDan Zhu, Shuguang Kuai<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> AAAI 2026<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> Navigating human-populated environments without causing discomfort is a critical capability for socially-aware agents. While rule-based approaches offer interpretability through predefined psychological principles, they often lack generalizability and flexibility. Conversely, data-driven methods can learn complex behaviors from large-scale datasets, but are typically inefficient, opaque, and difficult to align with human intuitions. To bridge this gap, we propose RLSLM, a hybrid Reinforcement Learning framework that integrates a rule-based Social Locomotion Model, grounded in empirical behavioral experiments, into the reward function of a reinforcement learning framework. The social locomotion model generates an orientation-sensitive social comfort field that quantifies human comfort across space, enabling socially aligned navigation policies with minimal training. RLSLM then jointly optimizes mechanical energy and social comfort, allowing agents to avoid intrusions into personal or group space. A human-agent interaction experiment using an immersive VR-based setup demonstrates that RLSLM outperforms state-of-the-art rule-based models in user experience. Ablation and sensitivity analyses further show the model's significantly improved interpretability over conventional data-driven methods. This work presents a scalable, human-centered methodology that effectively integrates cognitive science and machine learning for real-world social navigation.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11320v1" target="_blank"><h2>StochEP: Stochastic Equilibrium Propagation for Spiking Convergent Recurrent Neural Networks <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jiaqi Lin, Yi Jiang, Abhronil Sengupta<br><strong><u>Categories:</u></strong> cs.ET, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> Spiking Neural Networks (SNNs) promise energy-efficient, sparse, biologically inspired computation. Training them with Backpropagation Through Time (BPTT) and surrogate gradients achieves strong performance but remains biologically implausible. Equilibrium Propagation (EP) provides a more local and biologically grounded alternative. However, existing EP frameworks, primarily based on deterministic neurons, either require complex mechanisms to handle discontinuities in spiking dynamics or fail to scale beyond simple visual tasks. Inspired by the stochastic nature of biological spiking mechanism and recent hardware trends, we propose a stochastic EP framework that integrates probabilistic spiking neurons into the EP paradigm. This formulation smoothens the optimization landscape, stabilizes training, and enables scalable learning in deep convolutional spiking convergent recurrent neural networks (CRNNs). We provide theoretical guarantees showing that the proposed stochastic EP dynamics approximate deterministic EP under mean-field theory, thereby inheriting its underlying theoretical guarantees. The proposed framework narrows the gap to both BPTT-trained SNNs and EP-trained non-spiking CRNNs in vision benchmarks while preserving locality, highlighting stochastic EP as a promising direction for neuromorphic and on-chip learning.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11311v1" target="_blank"><h2>Large-scale modality-invariant foundation models for brain MRI analysis: Application to lesion segmentation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Petros Koutsouvelis, Matej Gazda, Leroy Volmer, Sina Amirrajab, Kamil Barbierik, Branislav Setlak, Jakub Gazda, Peter Drotar<br><strong><u>Categories:</u></strong> eess.IV, cs.AI, cs.CV, cs.LG<br><strong><u>Comments:</u></strong> Submitted to IEEE ISBI 2026<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> multi-modal (abstract)<br><p><strong><u>Abstract:</u></strong> The field of computer vision is undergoing a paradigm shift toward large-scale foundation model pre-training via self-supervised learning (SSL). Leveraging large volumes of unlabeled brain MRI data, such models can learn anatomical priors that improve few-shot performance in diverse neuroimaging tasks. However, most SSL frameworks are tailored to natural images, and their adaptation to capture multi-modal MRI information remains underexplored. This work proposes a modality-invariant representation learning setup and evaluates its effectiveness in stroke and epilepsy lesion segmentation, following large-scale pre-training. Experimental results suggest that despite successful cross-modality alignment, lesion segmentation primarily benefits from preserving fine-grained modality-specific features. Model checkpoints and code are made publicly available.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11305v2" target="_blank"><h2>MOON Embedding: Multimodal Representation Learning for E-commerce Search Advertising <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Chenghan Fu, Daoze Zhang, Yukang Lin, Zhanheng Nie, Xiang Zhang, Jianyu Liu, Yueran Liu, Wanxian Guan, Pengjie Wang, Jian Xu, Bo Zheng<br><strong><u>Categories:</u></strong> cs.IR, cs.AI, cs.CV, cs.LG<br><strong><u>Comments:</u></strong> 31 pages, 12 figures<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> We introduce MOON, our comprehensive set of sustainable iterative practices for multimodal representation learning for e-commerce applications. MOON has already been fully deployed across all stages of Taobao search advertising system, including retrieval, relevance, ranking, and so on. The performance gains are particularly significant on click-through rate (CTR) prediction task, which achieves an overall +20.00% online CTR improvement. Over the past three years, this project has delivered the largest improvement on CTR prediction task and undergone five full-scale iterations. Throughout the exploration and iteration of our MOON, we have accumulated valuable insights and practical experience that we believe will benefit the research community. MOON contains a three-stage training paradigm of "Pretraining, Post-training, and Application", allowing effective integration of multimodal representations with downstream tasks. Notably, to bridge the misalignment between the objectives of multimodal representation learning and downstream training, we define the exchange rate to quantify how effectively improvements in an intermediate metric can translate into downstream gains. Through this analysis, we identify the image-based search recall as a critical intermediate metric guiding the optimization of multimodal models. Over three years and five iterations, MOON has evolved along four critical dimensions: data processing, training strategy, model architecture, and downstream application. The lessons and insights gained through the iterative improvements will also be shared. As part of our exploration into scaling effects in the e-commerce field, we further conduct a systematic study of the scaling laws governing multimodal representation learning, examining multiple factors such as the number of training tokens, negative samples, and the length of user behavior sequences.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11299v1" target="_blank"><h2>AUVIC: Adversarial Unlearning of Visual Concepts for Multi-modal Large Language Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Haokun Chen, Jianing Li, Yao Zhang, Jinhe Bi, Yan Xia, Jindong Gu, Volker Tresp<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> AAAI 2026. Code:this https URL<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract), multi-modal (title)<br><p><strong><u>Abstract:</u></strong> Multimodal Large Language Models (MLLMs) achieve impressive performance once optimized on massive datasets. Such datasets often contain sensitive or copyrighted content, raising significant data privacy concerns. Regulatory frameworks mandating the 'right to be forgotten' drive the need for machine unlearning. This technique allows for the removal of target data without resource-consuming retraining. However, while well-studied for text, visual concept unlearning in MLLMs remains underexplored. A primary challenge is precisely removing a target visual concept without disrupting model performance on related entities. To address this, we introduce AUVIC, a novel visual concept unlearning framework for MLLMs. AUVIC applies adversarial perturbations to enable precise forgetting. This approach effectively isolates the target concept while avoiding unintended effects on similar entities. To evaluate our method, we construct VCUBench. It is the first benchmark designed to assess visual concept unlearning in group contexts. Experimental results demonstrate that AUVIC achieves state-of-the-art target forgetting rates while incurs minimal performance degradation on non-target concepts.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11286v1" target="_blank"><h2>D-GAP: Improving Out-of-Domain Robustness via Dataset-Agnostic and Gradient-Guided Augmentation in Amplitude and Pixel Spaces <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Ruoqi Wang, Haitao Wang, Shaojie Guo, Qiong Luo<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Out-of-domain (OOD) robustness is challenging to achieve in real-world computer vision applications, where shifts in image background, style, and acquisition instruments always degrade model performance. Generic augmentations show inconsistent gains under such shifts, whereas dataset-specific augmentations require expert knowledge and prior analysis. Moreover, prior studies show that neural networks adapt poorly to domain shifts because they exhibit a learning bias to domain-specific frequency components. Perturbing frequency values can mitigate such bias but overlooks pixel-level details, leading to suboptimal performance. To address these problems, we propose D-GAP (Dataset-agnostic and Gradient-guided augmentation in Amplitude and Pixel spaces), improving OOD robustness by introducing targeted augmentation in both the amplitude space (frequency space) and pixel space. Unlike conventional handcrafted augmentations, D-GAP computes sensitivity maps in the frequency space from task gradients, which reflect how strongly the model responds to different frequency components, and uses the maps to adaptively interpolate amplitudes between source and target samples. This way, D-GAP reduces the learning bias in frequency space, while a complementary pixel-space blending procedure restores fine spatial details. Extensive experiments on four real-world datasets and three domain-adaptation benchmarks show that D-GAP consistently outperforms both generic and dataset-specific augmentations, improving average OOD performance by +5.3% on real-world datasets and +1.8% on benchmark datasets.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11257v1" target="_blank"><h2>AIonopedia: an LLM agent orchestrating multimodal learning for ionic liquid discovery <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yuqi Yin, Yibo Fu, Siyuan Wang, Peng Sun, Hongyu Wang, Xiaohui Wang, Lei Zheng, Zhiyong Li, Zhirong Liu, Jianji Wang, Zhaoxi Sun<br><strong><u>Categories:</u></strong> cs.AI, cs.CE, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> The discovery of novel Ionic Liquids (ILs) is hindered by critical challenges in property prediction, including limited data, poor model accuracy, and fragmented workflows. Leveraging the power of Large Language Models (LLMs), we introduce AIonopedia, to the best of our knowledge, the first LLM agent for IL discovery. Powered by an LLM-augmented multimodal domain foundation model for ILs, AIonopedia enables accurate property predictions and incorporates a hierarchical search architecture for molecular screening and design. Trained and evaluated on a newly curated and comprehensive IL dataset, our model delivers superior performance. Complementing these results, evaluations on literature-reported systems indicate that the agent can perform effective IL modification. Moving beyond offline tests, the practical efficacy was further confirmed through real-world wet-lab validation, in which the agent demonstrated exceptional generalization capabilities on challenging out-of-distribution tasks, underscoring its ability to accelerate real-world IL discovery.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11245v1" target="_blank"><h2>Heterogeneous Attributed Graph Learning via Neighborhood-Aware Star Kernels <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Hong Huang, Chengyu Yao, Haiming Chen, Hang Gao<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Attributed graphs, typically characterized by irregular topologies and a mix of numerical and categorical attributes, are ubiquitous in diverse domains such as social networks, bioinformatics, and cheminformatics. While graph kernels provide a principled framework for measuring graph similarity, existing kernel methods often struggle to simultaneously capture heterogeneous attribute semantics and neighborhood information in attributed graphs. In this work, we propose the Neighborhood-Aware Star Kernel (NASK), a novel graph kernel designed for attributed graph learning. NASK leverages an exponential transformation of the Gower similarity coefficient to jointly model numerical and categorical features efficiently, and employs star substructures enhanced by Weisfeiler-Lehman iterations to integrate multi-scale neighborhood structural information. We theoretically prove that NASK is positive definite, ensuring compatibility with kernel-based learning frameworks such as SVMs. Extensive experiments are conducted on eleven attributed and four large-scale real-world graph benchmarks. The results demonstrate that NASK consistently achieves superior performance over sixteen state-of-the-art baselines, including nine graph kernels and seven Graph Neural Networks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11244v1" target="_blank"><h2>Toward Gaze Target Detection of Young Autistic Children <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Shijian Deng, Erin E. Kosloski, Siva Sai Nagender Vasireddy, Jia Li, Randi Sierra Sherwood, Feroz Mohamed Hatha, Siddhi Patel, Pamela R Rollins, Yapeng Tian<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> AAAI 2026 Artificial Intelligence for Social Impact Track<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> The automatic detection of gaze targets in autistic children through artificial intelligence can be impactful, especially for those who lack access to a sufficient number of professionals to improve their quality of life. This paper introduces a new, real-world AI application for gaze target detection in autistic children, which predicts a child's point of gaze from an activity image. This task is foundational for building automated systems that can measure joint attention-a core challenge in Autism Spectrum Disorder (ASD). To facilitate the study of this challenging application, we collected the first-ever Autism Gaze Target (AGT) dataset. We further propose a novel Socially Aware Coarse-to-Fine (SACF) gaze detection framework that explicitly leverages the social context of a scene to overcome the class imbalance common in autism datasets-a consequence of autistic children's tendency to show reduced gaze to faces. It utilizes a two-pathway architecture with expert models specialized in social and non-social gaze, guided by a context-awareness gate module. The results of our comprehensive experiments demonstrate that our framework achieves new state-of-the-art performance for gaze target detection in this population, significantly outperforming existing methods, especially on the critical minority class of face-directed gaze.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11235v1" target="_blank"><h2>Neural Network-Powered Finger-Drawn Biometric Authentication <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Maan Al Balkhi, Kordian Gontarska, Marko Harasic, Adrian Paschke<br><strong><u>Categories:</u></strong> cs.LG, cs.CR<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), anomaly detection (abstract), neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> This paper investigates neural network-based biometric authentication using finger-drawn digits on touchscreen devices. We evaluated CNN and autoencoder architectures for user authentication through simple digit patterns (0-9) traced with finger input. Twenty participants contributed 2,000 finger-drawn digits each on personal touchscreen devices. We compared two CNN architectures: a modified Inception-V1 network and a lightweight shallow CNN for mobile environments. Additionally, we examined Convolutional and Fully Connected autoencoders for anomaly detection. Both CNN architectures achieved ~89% authentication accuracy, with the shallow CNN requiring fewer parameters. Autoencoder approaches achieved ~75% accuracy. The results demonstrate that finger-drawn symbol authentication provides a viable, secure, and user-friendly biometric solution for touchscreen devices. This approach can be integrated with existing pattern-based authentication methods to create multi-layered security systems for mobile applications.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11231v1" target="_blank"><h2>3D Gaussian and Diffusion-Based Gaze Redirection <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Abiram Panchalingam, Indu Bodala, Stuart Middleton<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> High-fidelity gaze redirection is critical for generating augmented data to improve the generalization of gaze estimators. 3D Gaussian Splatting (3DGS) models like GazeGaussian represent the state-of-the-art but can struggle with rendering subtle, continuous gaze shifts. In this paper, we propose DiT-Gaze, a framework that enhances 3D gaze redirection models using a novel combination of Diffusion Transformer (DiT), weak supervision across gaze angles, and an orthogonality constraint loss. DiT allows higher-fidelity image synthesis, while our weak supervision strategy using synthetically generated intermediate gaze angles provides a smooth manifold of gaze directions during training. The orthogonality constraint loss mathematically enforces the disentanglement of internal representations for gaze, head pose, and expression. Comprehensive experiments show that DiT-Gaze sets a new state-of-the-art in both perceptual quality and redirection accuracy, reducing the state-of-the-art gaze error by 4.1% to 6.353 degrees, providing a superior method for creating synthetic training data. Our code and models will be made available for the research community to benchmark against.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11221v1" target="_blank"><h2>Sparse Methods for Vector Embeddings of TPC Data <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Tyler Wheeler, Michelle P. Kuchera, Raghuram Ramanujan, Ryan Krupp, Chris Wrede, Saiprasad Ravishankar, Connor L. Cross, Hoi Yan Ian Heung, Andrew J. Jones, Benjamin Votaw<br><strong><u>Categories:</u></strong> cs.LG, nucl-ex<br><strong><u>Comments:</u></strong> NeurIPS Machine Learning and the Physical Sciences Workshop 2025<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract)<br><p><strong><u>Abstract:</u></strong> Time Projection Chambers (TPCs) are versatile detectors that reconstruct charged-particle tracks in an ionizing medium, enabling sensitive measurements across a wide range of nuclear physics experiments. We explore sparse convolutional networks for representation learning on TPC data, finding that a sparse ResNet architecture, even with randomly set weights, provides useful structured vector embeddings of events. Pre-training this architecture on a simple physics-motivated binary classification task further improves the embedding quality. Using data from the GAseous Detector with GErmanium Tagging (GADGET) II TPC, a detector optimized for measuring low-energy $β$-delayed particle decays, we represent raw pad-level signals as sparse tensors, train Minkowski Engine ResNet models, and probe the resulting event-level embeddings which reveal rich event structure. As a cross-detector test, we embed data from the Active-Target TPC (AT-TPC) -- a detector designed for nuclear reaction studies in inverse kinematics -- using the same encoder. We find that even an untrained sparse ResNet model provides useful embeddings of AT-TPC data, and we observe improvements when the model is trained on GADGET data. Together, these results highlight the potential of sparse convolutional techniques as a general tool for representation learning in diverse TPC experiments.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11219v1" target="_blank"><h2>Dark Matter Capture in a Core-Collapse Supernova Revives Dark Photons <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Aritra Gupta, Manibrata Sen<br><strong><u>Categories:</u></strong> hep-ph, astro-ph.CO<br><strong><u>Comments:</u></strong> 15 pages, 7 figures, comments welcome<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> Core-collapse supernovae serve as powerful probes of light, weakly coupled particles, such as dark photons. The conventional SN1987A cooling bound constrains the dark photon mass-mixing parameter space by requiring that the luminosity from the proto-neutron star core not exceed the observed neutrino emission. In this work, we revisit these limits by including the effect of dark matter (DM) captured inside the progenitor star before collapse. The trapped DM acts as an additional scattering target for dark photons, modifying their free-streaming length and, consequently, the supernova cooling rate. We perform a self-consistent analysis for both annihilating and asymmetric DM scenarios, incorporating light-mediator effects in the capture rate calculation. For annihilating DM, the equilibrium density remains too small to affect the bounds significantly. In contrast, asymmetric DM can accumulate to large densities, leading to the formation of a "dark photosphere" that suppresses the dark-photon luminosity and reopens previously excluded regions of parameter space. Our results emphasise the importance of accounting for astrophysical DM populations when deriving stellar-cooling constraints on dark sectors.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11182v1" target="_blank"><h2>Multi-agent Undercover Gaming: Hallucination Removal via Counterfactual Test for Multimodal Reasoning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Dayong Liang, Xiao-Yong Wei, Changmeng Zheng<br><strong><u>Categories:</u></strong> cs.AI, cs.CL, cs.MA, cs.MM<br><strong><u>Comments:</u></strong> Accepted by AAAI 2026<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Hallucination continues to pose a major obstacle in the reasoning capabilities of large language models (LLMs). Although the Multi-Agent Debate (MAD) paradigm offers a promising solution by promoting consensus among multiple agents to enhance reliability, it relies on the unrealistic assumption that all debaters are rational and reflective, which is a condition that may not hold when agents themselves are prone to hallucinations. To address this gap, we introduce the Multi-agent Undercover Gaming (MUG) protocol, inspired by social deduction games like "Who is Undercover?". MUG reframes MAD as a process of detecting "undercover" agents (those suffering from hallucinations) by employing multimodal counterfactual tests. Specifically, we modify reference images to introduce counterfactual evidence and observe whether agents can accurately identify these changes, providing ground-truth for identifying hallucinating agents and enabling robust, crowd-powered multimodal reasoning. MUG advances MAD protocols along three key dimensions: (1) enabling factual verification beyond statistical consensus through counterfactual testing; (2) introducing cross-evidence reasoning via dynamically modified evidence sources instead of relying on static inputs; and (3) fostering active reasoning, where agents engage in probing discussions rather than passively answering questions. Collectively, these innovations offer a more reliable and effective framework for multimodal reasoning in LLMs. The source code can be accessed at https://github.com/YongLD/MUG.git.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11181v1" target="_blank"><h2>Dynamic Deep Graph Learning for Incomplete Multi-View Clustering with Masked Graph Reconstruction Loss <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zhenghao Zhang, Jun Xie, Xingchen Chen, Tao Yu, Hongzhu Yi, Kaixin Xu, Yuanxiang Wang, Tianyu Zong, Xinming Wang, Jiahuan Chen, Guoqing Chao, Feng Chen, Zhepeng Wang, Jungang Xu<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> The prevalence of real-world multi-view data makes incomplete multi-view clustering (IMVC) a crucial research. The rapid development of Graph Neural Networks (GNNs) has established them as one of the mainstream approaches for multi-view clustering. Despite significant progress in GNNs-based IMVC, some challenges remain: (1) Most methods rely on the K-Nearest Neighbors (KNN) algorithm to construct static graphs from raw data, which introduces noise and diminishes the robustness of the graph topology. (2) Existing methods typically utilize the Mean Squared Error (MSE) loss between the reconstructed graph and the sparse adjacency graph directly as the graph reconstruction loss, leading to substantial gradient noise during optimization. To address these issues, we propose a novel \textbf{D}ynamic Deep \textbf{G}raph Learning for \textbf{I}ncomplete \textbf{M}ulti-\textbf{V}iew \textbf{C}lustering with \textbf{M}asked Graph Reconstruction Loss (DGIMVCM). Firstly, we construct a missing-robust global graph from the raw data. A graph convolutional embedding layer is then designed to extract primary features and refined dynamic view-specific graph structures, leveraging the global graph for imputation of missing views. This process is complemented by graph structure contrastive learning, which identifies consistency among view-specific graph structures. Secondly, a graph self-attention encoder is introduced to extract high-level representations based on the imputed primary features and view-specific graphs, and is optimized with a masked graph reconstruction loss to mitigate gradient noise during optimization. Finally, a clustering module is constructed and optimized through a pseudo-label self-supervised training mechanism. Extensive experiments on multiple datasets validate the effectiveness and superiority of DGIMVCM.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11163v1" target="_blank"><h2>Training Neural Networks at Any Scale <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Thomas Pethick, Kimon Antonakopoulos, Antonio Silveti-Falls, Leena Chennuru Vankadara, Volkan Cevher<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> This article reviews modern optimization methods for training neural networks with an emphasis on efficiency and scale. We present state-of-the-art optimization algorithms under a unified algorithmic template that highlights the importance of adapting to the structures in the problem. We then cover how to make these algorithms agnostic to the scale of the problem. Our exposition is intended as an introduction for both practitioners and researchers who wish to be involved in these exciting new developments.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11161v1" target="_blank"><h2>Drift Estimation for Diffusion Processes Using Neural Networks Based on Discretely Observed Independent Paths <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yuzhen Zhao, Yating Liu, Marc Hoffmann<br><strong><u>Categories:</u></strong> stat.ML, cs.LG, math.ST<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> This paper addresses the nonparametric estimation of the drift function over a compact domain for a time-homogeneous diffusion process, based on high-frequency discrete observations from $N$ independent trajectories. We propose a neural network-based estimator and derive a non-asymptotic convergence rate, decomposed into a training error, an approximation error, and a diffusion-related term scaling as ${\log N}/{N}$. For compositional drift functions, we establish an explicit rate. In the numerical experiments, we consider a drift function with local fluctuations generated by a double-layer compositional structure featuring local oscillations, and show that the empirical convergence rate becomes independent of the input dimension $d$. Compared to the $B$-spline method, the neural network estimator achieves better convergence rates and more effectively captures local features, particularly in higher-dimensional settings.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11152v1" target="_blank"><h2>Deep Learning for Short-Term Precipitation Prediction in Four Major Indian Cities: A ConvLSTM Approach with Explainable AI <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Tanmay Ghosh, Shaurabh Anand, Rakesh Gomaji Nannewar, Nithin Nagaraj<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), explainable (title, abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Deep learning models for precipitation forecasting often function as black boxes, limiting their adoption in real-world weather prediction. To enhance transparency while maintaining accuracy, we developed an interpretable deep learning framework for short-term precipitation prediction in four major Indian cities: Bengaluru, Mumbai, Delhi, and Kolkata, spanning diverse climate zones. We implemented a hybrid Time-Distributed CNN-ConvLSTM (Convolutional Neural Network-Long Short-Term Memory) architecture, trained on multi-decadal ERA5 reanalysis data. The architecture was optimized for each city with a different number of convolutional filters: Bengaluru (32), Mumbai and Delhi (64), and Kolkata (128). The models achieved root mean square error (RMSE) values of 0.21 mm/day (Bengaluru), 0.52 mm/day (Mumbai), 0.48 mm/day (Delhi), and 1.80 mm/day (Kolkata). Through interpretability analysis using permutation importance, Gradient-weighted Class Activation Mapping (Grad-CAM), temporal occlusion, and counterfactual perturbation, we identified distinct patterns in the model's behavior. The model relied on city-specific variables, with prediction horizons ranging from one day for Bengaluru to five days for Kolkata. This study demonstrates how explainable AI (xAI) can provide accurate forecasts and transparent insights into precipitation patterns in diverse urban environments.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11147v1" target="_blank"><h2>Galactic foreground residual biases in CMB lensing convergence reconstruction and delensing of B-mode maps <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Kishan Deka, Pawel Bielewicz<br><strong><u>Categories:</u></strong> astro-ph.CO<br><strong><u>Comments:</u></strong> 15 pages, 15 figures, 3 tables, submitted for publication to A&A journal<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Diffused contamination from Galactic foreground emission is one of the main concern for reconstruction of the Cosmic Microwave Background (CMB) lensing potential for next-generation of CMB polarisation experiments. Using realistic simulations we investigate the impact of Galactic foreground residuals from multi-frequency foreground cleaning method on the lensing reconstruction, delensing CMB B-mode maps and constraints of the tensor-to-scalar ratio for CMB-S4-like experiment. We pay special attention to studies of the errors coming from small angular scale non-Gaussianity of the foreground residuals. We show that component separation is essential for the lensing reconstruction reducing Galactic emission contribution to the lensing reconstruction errors by one order of magnitude. The residual foreground contribution is dominated by terms coming from Gaussian components of the residual maps. Errors coming from non-Gaussian components are around three orders of magnitude smaller than the Gaussian one even for recent and the most complex models of the Galactic emission considered in this work. Although the bias in the reconstruction errors due to Gaussian component of the residuals is small, it is comparable to the cosmic variance limit for the lensing power spectrum. For this reason we correct for this bias in delensing of B-mode maps and constraining the tensor-to-scalar ratio. We show also that for the delensed B-mode maps with a simple quadratic estimator, residuals of the Galactic emission after component separation, errors are two orders of magnitude smaller than uncertainties from leftover of the lensing signal. However, for high-sensitivity CMB experiments and more efficient delensing algorithms that remove up to 90% of the lensing signal, the foreground residuals will become one of the main sources of errors.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11143v1" target="_blank"><h2>Anomaly Detection in High-Dimensional Bank Account Balances via Robust Methods <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Federico Maddanu, Tommaso Proietti, Riccardo Crupi<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> anomaly detection (title)<br><p><strong><u>Abstract:</u></strong> Detecting point anomalies in bank account balances is essential for financial institutions, as it enables the identification of potential fraud, operational issues, or other irregularities. Robust statistics is useful for flagging outliers and for providing estimates of the data distribution parameters that are not affected by contaminated observations. However, such a strategy is often less efficient and computationally expensive under high dimensional setting. In this paper, we propose and evaluate empirically several robust approaches that may be computationally efficient in medium and high dimensional datasets, with high breakdown points and low computational time. Our application deals with around 2.6 million daily records of anonymous users' bank account balances.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11141v1" target="_blank"><h2>PRSM: A Measure to Evaluate CLIP's Robustness Against Paraphrases <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Udo Schlegel, Franziska Weeber, Jian Lan, Thomas Seidl<br><strong><u>Categories:</u></strong> cs.CL, cs.CY, cs.LG<br><strong><u>Comments:</u></strong> 8 pages, accpeted as short paper at MMM 2026<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Contrastive Language-Image Pre-training (CLIP) is a widely used multimodal model that aligns text and image representations through large-scale training. While it performs strongly on zero-shot and few-shot tasks, its robustness to linguistic variation, particularly paraphrasing, remains underexplored. Paraphrase robustness is essential for reliable deployment, especially in socially sensitive contexts where inconsistent representations can amplify demographic biases. In this paper, we introduce the Paraphrase Ranking Stability Metric (PRSM), a novel measure for quantifying CLIP's sensitivity to paraphrased queries. Using the Social Counterfactuals dataset, a benchmark designed to reveal social and demographic biases, we empirically assess CLIP's stability under paraphrastic variation, examine the interaction between paraphrase robustness and gender, and discuss implications for fairness and equitable deployment of multimodal systems. Our analysis reveals that robustness varies across paraphrasing strategies, with subtle yet consistent differences observed between male- and female-associated queries.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11137v1" target="_blank"><h2>One-Shot Transfer Learning for Nonlinear PDEs with Perturbative PINNs <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Samuel Auroy, Pavlos Protopapas<br><strong><u>Categories:</u></strong> math.NA, cs.LG<br><strong><u>Comments:</u></strong> Accepted at Machine Learning and the Physical Sciences Workshop, NeurIPS 2025<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), transfer learning (title, abstract)<br><p><strong><u>Abstract:</u></strong> We propose a framework for solving nonlinear partial differential equations (PDEs) by combining perturbation theory with one-shot transfer learning in Physics-Informed Neural Networks (PINNs). Nonlinear PDEs with polynomial terms are decomposed into a sequence of linear subproblems, which are efficiently solved using a Multi-Head PINN. Once the latent representation of the linear operator is learned, solutions to new PDE instances with varying perturbations, forcing terms, or boundary/initial conditions can be obtained in closed form without retraining.
  We validate the method on KPP-Fisher and wave equations, achieving errors on the order of 1e-3 while adapting to new problem instances in under 0.2 seconds; comparable accuracy to classical solvers but with faster transfer. Sensitivity analyses show predictable error growth with epsilon and polynomial degree, clarifying the method's effective regime.
  Our contributions are: (i) extending one-shot transfer learning from nonlinear ODEs to PDEs, (ii) deriving a closed-form solution for adapting to new PDE instances, and (iii) demonstrating accuracy and efficiency on canonical nonlinear PDEs. We conclude by outlining extensions to derivative-dependent nonlinearities and higher-dimensional PDEs.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11134v1" target="_blank"><h2>GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jingxuan Wei, Caijun Jia, Xi Bai, Xinglong Xu, Siyuan Li, Linzhuang Sun, Bihui Yu, Conghui He, Lijun Wu, Cheng Tan<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> 35 pages, 22 figures<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> The advent of Unified Multimodal Models (UMMs) signals a paradigm shift in artificial intelligence, moving from passive perception to active, cross-modal generation. Despite their unprecedented ability to synthesize information, a critical gap persists in evaluation: existing benchmarks primarily assess discriminative understanding or unconstrained image generation separately, failing to measure the integrated cognitive process of generative reasoning. To bridge this gap, we propose that geometric construction provides an ideal testbed as it inherently demands a fusion of language comprehension and precise visual generation. We introduce GGBench, a benchmark designed specifically to evaluate geometric generative reasoning. It provides a comprehensive framework for systematically diagnosing a model's ability to not only understand and reason but to actively construct a solution, thereby setting a more rigorous standard for the next generation of intelligent systems. Project website: https://opendatalab-raiser.github.io/GGBench/.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11124v1" target="_blank"><h2>AV-Dialog: Spoken Dialogue Models with Audio-Visual Input <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Tuochao Chen, Bandhav Veluri, Hongyu Gong, Shyamnath Gollakota<br><strong><u>Categories:</u></strong> cs.CL, cs.AI, cs.CV, cs.MM, cs.SD<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Dialogue models falter in noisy, multi-speaker environments, often producing irrelevant responses and awkward turn-taking. We present AV-Dialog, the first multimodal dialog framework that uses both audio and visual cues to track the target speaker, predict turn-taking, and generate coherent responses. By combining acoustic tokenization with multi-task, multi-stage training on monadic, synthetic, and real audio-visual dialogue datasets, AV-Dialog achieves robust streaming transcription, semantically grounded turn-boundary detection and accurate responses, resulting in a natural conversational flow. Experiments show that AV-Dialog outperforms audio-only models under interference, reducing transcription errors, improving turn-taking prediction, and enhancing human-rated dialogue quality. These results highlight the power of seeing as well as hearing for speaker-aware interaction, paving the way for {spoken} dialogue agents that perform {robustly} in real-world, noisy environments.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11111v1" target="_blank"><h2>SMART: A Surrogate Model for Predicting Application Runtime in Dragonfly Systems <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Xin Wang, Pietro Lodi Rizzini, Sourav Medya, Zhiling Lan<br><strong><u>Categories:</u></strong> cs.LG, cs.DC<br><strong><u>Comments:</u></strong> Accepted at AAAI 2026<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> The Dragonfly network, with its high-radix and low-diameter structure, is a leading interconnect in high-performance computing. A major challenge is workload interference on shared network links. Parallel discrete event simulation (PDES) is commonly used to analyze workload interference. However, high-fidelity PDES is computationally expensive, making it impractical for large-scale or real-time scenarios. Hybrid simulation that incorporates data-driven surrogate models offers a promising alternative, especially for forecasting application runtime, a task complicated by the dynamic behavior of network traffic. We present \ourmodel, a surrogate model that combines graph neural networks (GNNs) and large language models (LLMs) to capture both spatial and temporal patterns from port level router data. \ourmodel outperforms existing statistical and machine learning baselines, enabling accurate runtime prediction and supporting efficient hybrid simulation of Dragonfly networks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11081v1" target="_blank"><h2>Echoless Label-Based Pre-computation for Memory-Efficient Heterogeneous Graph Learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jun Hu, Shangheng Chen, Yufei He, Yuan Li, Bryan Hooi, Bingsheng He<br><strong><u>Categories:</u></strong> cs.LG, cs.SI<br><strong><u>Comments:</u></strong> Accepted by AAAI 2026<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Heterogeneous Graph Neural Networks (HGNNs) are widely used for deep learning on heterogeneous graphs. Typical end-to-end HGNNs require repetitive message passing during training, limiting efficiency for large-scale real-world graphs. Pre-computation-based HGNNs address this by performing message passing only once during preprocessing, collecting neighbor information into regular-shaped tensors, which enables efficient mini-batch training. Label-based pre-computation methods collect neighbors' label information but suffer from training label leakage, where a node's own label information propagates back to itself during multi-hop message passing - the echo effect. Existing mitigation strategies are memory-inefficient on large graphs or suffer from compatibility issues with advanced message passing methods. We propose Echoless Label-based Pre-computation (Echoless-LP), which eliminates training label leakage with Partition-Focused Echoless Propagation (PFEP). PFEP partitions target nodes and performs echoless propagation, where nodes in each partition collect label information only from neighbors in other partitions, avoiding echo while remaining memory-efficient and compatible with any message passing method. We also introduce an Asymmetric Partitioning Scheme (APS) and a PostAdjust mechanism to address information loss from partitioning and distributional shifts across partitions. Experiments on public datasets demonstrate that Echoless-LP achieves superior performance and maintains memory efficiency compared to baselines.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11079v2" target="_blank"><h2>ARCTraj: A Dataset and Benchmark of Human Reasoning Trajectories for Abstract Problem Solving <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Sejin Kim, Hayan Choi, Seokki Lee, Sundong Kim<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> explainability (abstract), transformer (abstract)<br><p><strong><u>Abstract:</u></strong> We present ARCTraj, a dataset and methodological framework for modeling human reasoning through complex visual tasks in the Abstraction and Reasoning Corpus (ARC). While ARC has inspired extensive research on abstract reasoning, most existing approaches rely on static input--output supervision, which limits insight into how reasoning unfolds over time. ARCTraj addresses this gap by recording temporally ordered, object-level actions that capture how humans iteratively transform inputs into outputs, revealing intermediate reasoning steps that conventional datasets overlook. Collected via the O2ARC web interface, it contains around 10,000 trajectories annotated with task identifiers, timestamps, and success labels across 400 training tasks from the ARC-AGI-1 benchmark. It further defines a unified reasoning pipeline encompassing data collection, action abstraction, Markov decision process (MDP) formulation, and downstream learning, enabling integration with reinforcement learning, generative modeling, and sequence modeling methods such as PPO, World Models, GFlowNets, Diffusion agents, and Decision Transformers. Analyses of spatial selection, color attribution, and strategic convergence highlight the structure and diversity of human reasoning. Together, these contributions position ARCTraj as a structured and interpretable foundation for studying human-like reasoning, advancing explainability, alignment, and generalizable intelligence.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11066v1" target="_blank"><h2>S2D-ALIGN: Shallow-to-Deep Auxiliary Learning for Anatomically-Grounded Radiology Report Generation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jiechao Gao, Chang Liu, Yuangang Li<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.CL<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract), multi-modal (abstract)<br><p><strong><u>Abstract:</u></strong> Radiology Report Generation (RRG) aims to automatically generate diagnostic reports from radiology images. To achieve this, existing methods have leveraged the powerful cross-modal generation capabilities of Multimodal Large Language Models (MLLMs), primarily focusing on optimizing cross-modal alignment between radiographs and reports through Supervised Fine-Tuning (SFT). However, by only performing instance-level alignment with the image-text pairs, the standard SFT paradigm fails to establish anatomically-grounded alignment, where the templated nature of reports often leads to sub-optimal generation quality. To address this, we propose \textsc{S2D-Align}, a novel SFT paradigm that establishes anatomically-grounded alignment by leveraging auxiliary signals of varying granularities. \textsc{S2D-Align} implements a shallow-to-deep strategy, progressively enriching the alignment process: it begins with the coarse radiograph-report pairing, then introduces reference reports for instance-level guidance, and ultimately utilizes key phrases to ground the generation in specific anatomical details. To bridge the different alignment stages, we introduce a memory-based adapter that empowers feature sharing, thereby integrating coarse and fine-grained guidance. For evaluation, we conduct experiments on the public \textsc{MIMIC-CXR} and \textsc{IU X-Ray} benchmarks, where \textsc{S2D-Align} achieves state-of-the-art performance compared to existing methods. Ablation studies validate the effectiveness of our multi-stage, auxiliary-guided approach, highlighting a promising direction for enhancing grounding capabilities in complex, multi-modal generation tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11065v1" target="_blank"><h2>From Retinal Pixels to Patients: Evolution of Deep Learning Research in Diabetic Retinopathy Screening <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Muskaan Chopra, Lorenz Sparrenberg, Armin Berger, Sarthak Khanna, Jan H. Terheyden, Rafet Sifa<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> Accepted in IEEE BigData 2025<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Diabetic Retinopathy (DR) remains a leading cause of preventable blindness, with early detection critical for reducing vision loss worldwide. Over the past decade, deep learning has transformed DR screening, progressing from early convolutional neural networks trained on private datasets to advanced pipelines addressing class imbalance, label scarcity, domain shift, and interpretability. This survey provides the first systematic synthesis of DR research spanning 2016-2025, consolidating results from 50+ studies and over 20 datasets. We critically examine methodological advances, including self- and semi-supervised learning, domain generalization, federated training, and hybrid neuro-symbolic models, alongside evaluation protocols, reporting standards, and reproducibility challenges. Benchmark tables contextualize performance across datasets, while discussion highlights open gaps in multi-center validation and clinical trust. By linking technical progress with translational barriers, this work outlines a practical agenda for reproducible, privacy-preserving, and clinically deployable DR AI. Beyond DR, many of the surveyed innovations extend broadly to medical imaging at scale.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11062v1" target="_blank"><h2>LiteAttention: A Temporal Sparse Attention for Diffusion Transformers <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Dor Shmilovich, Tony Wu, Aviad Dahan, Yuval Domb<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Diffusion Transformers, particularly for video generation, achieve remarkable quality but suffer from quadratic attention complexity, leading to prohibitive latency. Existing acceleration methods face a fundamental trade-off: dynamically estimating sparse attention patterns at each denoising step incurs high computational overhead and estimation errors, while static sparsity patterns remain fixed and often suboptimal throughout denoising. We identify a key structural property of diffusion attention, namely, its sparsity patterns exhibit strong temporal coherence across denoising steps. Tiles deemed non-essential at step $t$ typically remain so at step $t+δ$. Leveraging this observation, we introduce LiteAttention, a method that exploits temporal coherence to enable evolutionary computation skips across the denoising sequence. By marking non-essential tiles early and propagating skip decisions forward, LiteAttention eliminates redundant attention computations without repeated profiling overheads, combining the adaptivity of dynamic methods with the efficiency of static ones. We implement a highly optimized LiteAttention kernel on top of FlashAttention and demonstrate substantial speedups on production video diffusion models, with no degradation in quality. The code and implementation details will be publicly released.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11048v1" target="_blank"><h2>PINGS-X: Physics-Informed Normalized Gaussian Splatting with Axes Alignment for Efficient Super-Resolution of 4D Flow MRI <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Sun Jo, Seok Young Hong, JinHyun Kim, Seungmin Kang, Ahjin Choi, Don-Gwan An, Simon Song, Je Hyeong Hong<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> Accepted at AAAI 2026. Supplementary material included after references. 27 pages, 21 figures, 11 tables<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> 4D flow magnetic resonance imaging (MRI) is a reliable, non-invasive approach for estimating blood flow velocities, vital for cardiovascular diagnostics. Unlike conventional MRI focused on anatomical structures, 4D flow MRI requires high spatiotemporal resolution for early detection of critical conditions such as stenosis or aneurysms. However, achieving such resolution typically results in prolonged scan times, creating a trade-off between acquisition speed and prediction accuracy. Recent studies have leveraged physics-informed neural networks (PINNs) for super-resolution of MRI data, but their practical applicability is limited as the prohibitively slow training process must be performed for each patient. To overcome this limitation, we propose PINGS-X, a novel framework modeling high-resolution flow velocities using axes-aligned spatiotemporal Gaussian representations. Inspired by the effectiveness of 3D Gaussian splatting (3DGS) in novel view synthesis, PINGS-X extends this concept through several non-trivial novel innovations: (i) normalized Gaussian splatting with a formal convergence guarantee, (ii) axes-aligned Gaussians that simplify training for high-dimensional data while preserving accuracy and the convergence guarantee, and (iii) a Gaussian merging procedure to prevent degenerate solutions and boost computational efficiency. Experimental results on computational fluid dynamics (CFD) and real 4D flow MRI datasets demonstrate that PINGS-X substantially reduces training time while achieving superior super-resolution accuracy. Our code and datasets are available at https://github.com/SpatialAILab/PINGS-X.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11046v1" target="_blank"><h2>Enhancing Graph Representations with Neighborhood-Contextualized Message-Passing <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Brian Godwin Lim<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Graph neural networks (GNNs) have become an indispensable tool for analyzing relational data. In the literature, classical GNNs may be classified into three variants: convolutional, attentional, and message-passing. While the standard message-passing variant is highly expressive, its typical pair-wise messages nevertheless only consider the features of the center node and each neighboring node individually. This design fails to incorporate the rich contextual information contained within the broader local neighborhood, potentially hindering its ability to learn complex relationships within the entire set of neighboring nodes. To address this limitation, this work first formalizes the concept of neighborhood-contextualization, rooted in a key property of the attentional variant. This then serves as the foundation for generalizing the message-passing variant to the proposed neighborhood-contextualized message-passing (NCMP) framework. To demonstrate its utility, a simple, practical, and efficient method to parametrize and operationalize NCMP is presented, leading to the development of the proposed Soft-Isomorphic Neighborhood-Contextualized Graph Convolution Network (SINC-GCN). A preliminary analysis on a synthetic binary node classification problem then underscores both the expressivity and efficiency of the proposed GNN architecture. Overall, the paper lays the foundation for the novel NCMP framework as a practical path toward further enhancing the graph representational power of classical GNNs.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11038v1" target="_blank"><h2>SemanticNN: Compressive and Error-Resilient Semantic Offloading for Extremely Weak Devices <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jiaming Huang, Yi Gao, Fuchang Pan, Renjie Li, Wei Dong<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.DC<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> With the rapid growth of the Internet of Things (IoT), integrating artificial intelligence (AI) on extremely weak embedded devices has garnered significant attention, enabling improved real-time performance and enhanced data privacy. However, the resource limitations of such devices and unreliable network conditions necessitate error-resilient device-edge collaboration systems. Traditional approaches focus on bit-level transmission correctness, which can be inefficient under dynamic channel conditions. In contrast, we propose SemanticNN, a semantic codec that tolerates bit-level errors in pursuit of semantic-level correctness, enabling compressive and resilient collaborative inference offloading under strict computational and communication constraints. It incorporates a Bit Error Rate (BER)-aware decoder that adapts to dynamic channel conditions and a Soft Quantization (SQ)-based encoder to learn compact representations. Building on this architecture, we introduce Feature-augmentation Learning, a novel training strategy that enhances offloading efficiency. To address encoder-decoder capability mismatches from asymmetric resources, we propose XAI-based Asymmetry Compensation to enhance decoding semantic fidelity. We conduct extensive experiments on STM32 using three models and six datasets across image classification and object detection tasks. Experimental results demonstrate that, under varying transmission error rates, SemanticNN significantly reduces feature transmission volume by 56.82-344.83x while maintaining superior inference accuracy.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11034v1" target="_blank"><h2>CrossMed: A Multimodal Cross-Task Benchmark for Compositional Generalization in Medical Imaging <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Pooja Singh, Siddhant Ujjain, Tapan Kumar Gandhi, Sandeep Kumar<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Recent advances in multimodal large language models have enabled unified processing of visual and textual inputs, offering promising applications in general-purpose medical AI. However, their ability to generalize compositionally across unseen combinations of imaging modality, anatomy, and task type remains underexplored. We introduce CrossMed, a benchmark designed to evaluate compositional generalization (CG) in medical multimodal LLMs using a structured Modality-Anatomy-Task (MAT) schema. CrossMed reformulates four public datasets, CheXpert (X-ray classification), SIIM-ACR (X-ray segmentation), BraTS 2020 (MRI classification and segmentation), and MosMedData (CT classification) into a unified visual question answering (VQA) format, resulting in 20,200 multiple-choice QA instances. We evaluate two open-source multimodal LLMs, LLaVA-Vicuna-7B and Qwen2-VL-7B, on both Related and Unrelated MAT splits, as well as a zero-overlap setting where test triplets share no Modality, Anatomy, or Task with the training data. Models trained on Related splits achieve 83.2 percent classification accuracy and 0.75 segmentation cIoU, while performance drops significantly under Unrelated and zero-overlap conditions, demonstrating the benchmark difficulty. We also show cross-task transfer, where segmentation performance improves by 7 percent cIoU even when trained using classification-only data. Traditional models (ResNet-50 and U-Net) show modest gains, confirming the broad utility of the MAT framework, while multimodal LLMs uniquely excel at compositional generalization. CrossMed provides a rigorous testbed for evaluating zero-shot, cross-task, and modality-agnostic generalization in medical vision-language models.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13758v1" target="_blank"><h2>ChemFixer: Correcting Invalid Molecules to Unlock Previously Unseen Chemical Space <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jun-Hyoung Park, Ho-Jun Song, Seong-Whan Lee<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> This is the author's preprint version of the article accepted to IEEE JBHI. Final published version:this https URL. High-quality PDF (publisher version):this https URL. Note: Some figures may appear distorted due to arXiv's TeXLive rendering<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Deep learning-based molecular generation models have shown great potential in efficiently exploring vast chemical spaces by generating potential drug candidates with desired properties. However, these models often produce chemically invalid molecules, which limits the usable scope of the learned chemical space and poses significant challenges for practical applications. To address this issue, we propose ChemFixer, a framework designed to correct invalid molecules into valid ones. ChemFixer is built on a transformer architecture, pre-trained using masking techniques, and fine-tuned on a large-scale dataset of valid/invalid molecular pairs that we constructed. Through comprehensive evaluations across diverse generative models, ChemFixer improved molecular validity while effectively preserving the chemical and biological distributional properties of the original outputs. This indicates that ChemFixer can recover molecules that could not be previously generated, thereby expanding the diversity of potential drug candidates. Furthermore, ChemFixer was effectively applied to a drug-target interaction (DTI) prediction task using limited data, improving the validity of generated ligands and discovering promising ligand-protein pairs. These results suggest that ChemFixer is not only effective in data-limited scenarios, but also extensible to a wide range of downstream tasks. Taken together, ChemFixer shows promise as a practical tool for various stages of deep learning-based drug discovery, enhancing molecular validity and expanding accessible chemical space.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11025v1" target="_blank"><h2>AirCopBench: A Benchmark for Multi-drone Collaborative Embodied Perception and Reasoning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jirong Zha, Yuxuan Fan, Tianyu Zhang, Geng Chen, Yingfeng Chen, Chen Gao, Xinlei Chen<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Multimodal Large Language Models (MLLMs) have shown promise in single-agent vision tasks, yet benchmarks for evaluating multi-agent collaborative perception remain scarce. This gap is critical, as multi-drone systems provide enhanced coverage, robustness, and collaboration compared to single-sensor setups. Existing multi-image benchmarks mainly target basic perception tasks using high-quality single-agent images, thus failing to evaluate MLLMs in more complex, egocentric collaborative scenarios, especially under real-world degraded perception conditions.To address these challenges, we introduce AirCopBench, the first comprehensive benchmark designed to evaluate MLLMs in embodied aerial collaborative perception under challenging perceptual conditions. AirCopBench includes 14.6k+ questions derived from both simulator and real-world data, spanning four key task dimensions: Scene Understanding, Object Understanding, Perception Assessment, and Collaborative Decision, across 14 task types. We construct the benchmark using data from challenging degraded-perception scenarios with annotated collaborative events, generating large-scale questions through model-, rule-, and human-based methods under rigorous quality control. Evaluations on 40 MLLMs show significant performance gaps in collaborative perception tasks, with the best model trailing humans by 24.38% on average and exhibiting inconsistent results across tasks. Fine-tuning experiments further confirm the feasibility of sim-to-real transfer in aerial collaborative perception and reasoning.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11020v1" target="_blank"><h2>Data Poisoning Vulnerabilities Across Healthcare AI Architectures: A Security Threat Analysis <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Farhad Abtahi, Fernando Seoane, Iván Pau, Mario Vega-Barbas<br><strong><u>Categories:</u></strong> cs.CR, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Healthcare AI systems face major vulnerabilities to data poisoning that current defenses and regulations cannot adequately address. We analyzed eight attack scenarios in four categories: architectural attacks on convolutional neural networks, large language models, and reinforcement learning agents; infrastructure attacks exploiting federated learning and medical documentation systems; critical resource allocation attacks affecting organ transplantation and crisis triage; and supply chain attacks targeting commercial foundation models. Our findings indicate that attackers with access to only 100-500 samples can compromise healthcare AI regardless of dataset size, often achieving over 60 percent success, with detection taking an estimated 6 to 12 months or sometimes not occurring at all. The distributed nature of healthcare infrastructure creates many entry points where insiders with routine access can launch attacks with limited technical skill. Privacy laws such as HIPAA and GDPR can unintentionally shield attackers by restricting the analyses needed for detection. Supply chain weaknesses allow a single compromised vendor to poison models across 50 to 200 institutions. The Medical Scribe Sybil scenario shows how coordinated fake patient visits can poison data through legitimate clinical workflows without requiring a system breach. Current regulations lack mandatory adversarial robustness testing, and federated learning can worsen risks by obscuring attribution. We recommend multilayer defenses including required adversarial testing, ensemble-based detection, privacy-preserving security mechanisms, and international coordination on AI security standards. We also question whether opaque black-box models are suitable for high-stakes clinical decisions, suggesting a shift toward interpretable systems with verifiable safety guarantees.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11009v1" target="_blank"><h2>Unsupervised Robust Domain Adaptation: Paradigm, Theory and Algorithm <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Fuxiang Huang, Xiaowei Fu, Shiyu Ye, Lina Ma, Wen Li, Xinbo Gao, David Zhang, Lei Zhang<br><strong><u>Categories:</u></strong> cs.LG, cs.CV<br><strong><u>Comments:</u></strong> To appear in IJCV<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), domain adaptation (title, abstract)<br><p><strong><u>Abstract:</u></strong> Unsupervised domain adaptation (UDA) aims to transfer knowledge from a label-rich source domain to an unlabeled target domain by addressing domain shifts. Most UDA approaches emphasize transfer ability, but often overlook robustness against adversarial attacks. Although vanilla adversarial training (VAT) improves the robustness of deep neural networks, it has little effect on UDA. This paper focuses on answering three key questions: 1) Why does VAT, known for its defensive effectiveness, fail in the UDA paradigm? 2) What is the generalization bound theory under attacks and how does it evolve from classical UDA theory? 3) How can we implement a robustification training procedure without complex modifications? Specifically, we explore and reveal the inherent entanglement challenge in general UDA+VAT paradigm, and propose an unsupervised robust domain adaptation (URDA) paradigm. We further derive the generalization bound theory of the URDA paradigm so that it can resist adversarial noise and domain shift. To the best of our knowledge, this is the first time to establish the URDA paradigm and theory. We further introduce a simple, novel yet effective URDA algorithm called Disentangled Adversarial Robustness Training (DART), a two-step training procedure that ensures both transferability and robustness. DART first pre-trains an arbitrary UDA model, and then applies an instantaneous robustification post-training step via disentangled distillation.Experiments on four benchmark datasets with/without attacks show that DART effectively enhances robustness while maintaining domain adaptability, and validate the URDA paradigm and theory.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.11000v2" target="_blank"><h2>DialogGraph-LLM: Graph-Informed LLMs for End-to-End Audio Dialogue Intent Recognition <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> HongYu Liu, Junxin Li, Changxi Guo, Hao Chen, Yaqian Huang, Yifu Guo, Huan Yang, Lihua Cai<br><strong><u>Categories:</u></strong> cs.SD, cs.AI<br><strong><u>Comments:</u></strong> 8 pages, 2 figures. To appear in: Proceedings of the 28th European Conference on Artificial Intelligence (ECAI 2025), Frontiers in Artificial Intelligence and Applications, Vol. 413. DOI:https://doi.org/10.3233/FAIA251182<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Recognizing speaker intent in long audio dialogues among speakers has a wide range of applications, but is a non-trivial AI task due to complex inter-dependencies in speaker utterances and scarce annotated data. To address these challenges, an end-to-end framework, namely DialogGraph-LLM, is proposed in the current work. DialogGraph-LLM combines a novel Multi-Relational Dialogue Attention Network (MR-DAN) architecture with multimodal foundation models (e.g., Qwen2.5-Omni-7B) for direct acoustic-to-intent inference. An adaptive semi-supervised learning strategy is designed using LLM with a confidence-aware pseudo-label generation mechanism based on dual-threshold filtering using both global and class confidences, and an entropy-based sample selection process that prioritizes high-information unlabeled instances. Extensive evaluations on the proprietary MarketCalls corpus and the publicly available MIntRec 2.0 benchmark demonstrate DialogGraph-LLM's superiority over strong audio and text-driven baselines. The framework demonstrates strong performance and efficiency in intent recognition in real world scenario audio dialogues, proving its practical value for audio-rich domains with limited supervision. Our code is available at https://github.com/david188888/DialogGraph-LLM.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.10998v1" target="_blank"><h2>SPICE -- modelling synthetic spectra of stars with non-homogeneous surfaces <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> M. Jabłońska, T. Różański, L. Casagrande, H. Shah, P. A. Kołaczek-Szymański, M. Rychlicki, Yuan-Sen Ting<br><strong><u>Categories:</u></strong> astro-ph.SR, astro-ph.IM<br><strong><u>Comments:</u></strong> 17 pages, 24 figures<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> time-domain (abstract)<br><p><strong><u>Abstract:</u></strong> In the era of large time-domain spectro-photometric surveys, surface variations such as starspots, chemical inhomogeneities, pulsations, rotational distortions, and binary interactions can now be directly detected and modelled. Accurately interpreting these phenomena requires stellar spectral synthesis frameworks that go beyond the assumption of homogeneous surface properties. Yet most existing tools remain limited by this simplification, hindering their applicability to stars with complex surface structures. To address this need, we present SPICE (SPectral Integration Compiled Engine), an open-source Python package for generating high-resolution spectra and photometry from non-homogeneous stellar surface models. SPICE integrates angle-dependent specific intensities from each surface element, enabling forward modelling of both photometric and spectroscopic variability. Case studies demonstrate applications to spotted stars, Cepheid pulsations, and eclipsing binaries, making SPICE well-suited for analysing current and upcoming survey data. In addition, SPICE can directly import meshes from PHOEBE, enabling the modelling of complex binary configurations beyond these case studies.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.10997v1" target="_blank"><h2>PROMISE: Prompt-Attentive Hierarchical Contrastive Learning for Robust Cross-Modal Representation with Missing Modalities <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jiajun Chen, Sai Cheng, Yutao Yuan, Yirui Zhang, Haitao Yuan, Peng Peng, Yi Zhong<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> Accepted by AAAI'2026 Main Conference<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Multimodal models integrating natural language and visual information have substantially improved generalization of representation models. However, their effectiveness significantly declines in real-world situations where certain modalities are missing or unavailable. This degradation primarily stems from inconsistent representation learning between complete multimodal data and incomplete modality scenarios. Existing approaches typically address missing modalities through relatively simplistic generation methods, yet these approaches fail to adequately preserve cross-modal consistency, leading to suboptimal performance. To overcome this limitation, we propose a novel multimodal framework named PROMISE, a PROMpting-Attentive HIerarchical ContraStive LEarning approach designed explicitly for robust cross-modal representation under conditions of missing modalities. Specifically, PROMISE innovatively incorporates multimodal prompt learning into a hierarchical contrastive learning framework, equipped with a specially designed prompt-attention mechanism. This mechanism dynamically generates robust and consistent representations for scenarios where particular modalities are absent, thereby effectively bridging the representational gap between complete and incomplete data. Extensive experiments conducted on benchmark datasets, along with comprehensive ablation studies, clearly demonstrate the superior performance of PROMISE compared to current state-of-the-art multimodal methods.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13756v1" target="_blank"><h2>Multi-Horizon Time Series Forecasting of non-parametric CDFs with Deep Lattice Networks <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Niklas Erdmann, Lars Bentsen, Roy Stenbro, Heine Nygard Riise, Narada Dilp Warakagoda, Paal E. Engelstad<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Probabilistic forecasting is not only a way to add more information to a prediction of the future, but it also builds on weaknesses in point prediction. Sudden changes in a time series can still be captured by a cumulative distribution function (CDF), while a point prediction is likely to miss it entirely. The modeling of CDFs within forecasts has historically been limited to parametric approaches, but due to recent advances, this no longer has to be the case. We aim to advance the fields of probabilistic forecasting and monotonic networks by connecting them and propose an approach that permits the forecasting of implicit, complete, and nonparametric CDFs. For this purpose, we propose an adaptation to deep lattice networks (DLN) for monotonically constrained simultaneous/implicit quantile regression in time series forecasting. Quantile regression usually produces quantile crossovers, which need to be prevented to achieve a legitimate CDF. By leveraging long short term memory units (LSTM) as the embedding layer, and spreading quantile inputs to all sub-lattices of a DLN with an extended output size, we can produce a multi-horizon forecast of an implicit CDF due to the monotonic constraintability of DLNs that prevent quantile crossovers. We compare and evaluate our approach's performance to relevant state of the art within the context of a highly relevant application of time series forecasting: Day-ahead, hourly forecasts of solar irradiance observations. Our experiments show that the adaptation of a DLN performs just as well or even better than an unconstrained approach. Further comparison of the adapted DLN against a scalable monotonic neural network shows that our approach performs better. With this adaptation of DLNs, we intend to create more interest and crossover investigations in techniques of monotonic neural networks and probabilistic forecasting.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.10979v1" target="_blank"><h2>PAS: A Training-Free Stabilizer for Temporal Encoding in Video LLMs <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Bowen Sun, Yujun Cai, Ming-Hsuan Yang, Hang Wu, Yiwei Wang<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> 13 pages, 5 figures<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Video LLMs suffer from temporal inconsistency: small shifts in frame timing can flip attention and suppress relevant frames. We trace this instability to the common extension of Rotary Position Embeddings to video through multimodal RoPE. The induced inverse Fourier time kernel exhibits frame-scale ripples that multiply adjacent frames by different factors, which perturbs attention that should otherwise be governed by the raw query key inner product. We present Phase Aggregated Smoothing (PAS), a simple, training-free mechanism that applies small opposed phase offsets across heads and then aggregates their outputs. PAS preserves the per-head spectrum magnitude, while the aggregation effectively smooths the temporal kernel and reduces phase sensitivity without changing the positional encoding structure. Our analysis shows that the RoPE rotated logit can be approximated as a content dot product scaled by a time kernel; smoothing this kernel yields Lipschitz stability of attention to small temporal shifts; multi phase averaging attenuates high frequency ripples while preserving per-head spectra under Nyquist-valid sampling. Experiments on multiple video understanding benchmarks under matched token budgets show consistent improvements with negligible computational overhead. PAS provides a plug and play upgrade for robust temporal encoding in Video LLMs.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13755v1" target="_blank"><h2>Adaptive Redundancy Regulation for Balanced Multimodal Information Refinement <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zhe Yang, Wenrui Li, Hongtao Chen, Penghong Wang, Ruiqin Xiong, Xiaopeng Fan<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Multimodal learning aims to improve performance by leveraging data from multiple sources. During joint multimodal training, due to modality bias, the advantaged modality often dominates backpropagation, leading to imbalanced optimization. Existing methods still face two problems: First, the long-term dominance of the dominant modality weakens representation-output coupling in the late stages of training, resulting in the accumulation of redundant information. Second, previous methods often directly and uniformly adjust the gradients of the advantaged modality, ignoring the semantics and directionality between modalities. To address these limitations, we propose Adaptive Redundancy Regulation for Balanced Multimodal Information Refinement (RedReg), which is inspired by information bottleneck principle. Specifically, we construct a redundancy phase monitor that uses a joint criterion of effective gain growth rate and redundancy to trigger intervention only when redundancy is high. Furthermore, we design a co-information gating mechanism to estimate the contribution of the current dominant modality based on cross-modal semantics. When the task primarily relies on a single modality, the suppression term is automatically disabled to preserve modality-specific information. Finally, we project the gradient of the dominant modality onto the orthogonal complement of the joint multimodal gradient subspace and suppress the gradient according to redundancy. Experiments show that our method demonstrates superiority among current major methods in most scenarios. Ablation experiments verify the effectiveness of our method. The code is available at https://github.com/xia-zhe/RedReg.git</p><br><hr><br><a href="https://arxiv.org/pdf/2511.10936v1" target="_blank"><h2>GraphToxin: Reconstructing Full Unlearned Graphs from Graph Unlearning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Ying Song, Balaji Palanisamy<br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.CR<br><strong><u>Comments:</u></strong> Submitted to S&P 2026. Code will be available<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Graph unlearning has emerged as a promising solution for complying with "the right to be forgotten" regulations by enabling the removal of sensitive information upon request. However, this solution is not foolproof. The involvement of multiple parties creates new attack surfaces, and residual traces of deleted data can still remain in the unlearned graph neural networks. These vulnerabilities can be exploited by attackers to recover the supposedly erased samples, thereby undermining the inherent functionality of graph unlearning. In this work, we propose GraphToxin, the first graph reconstruction attack against graph unlearning. Specifically, we introduce a novel curvature matching module to provide a fine-grained guidance for full unlearned graph recovery. We demonstrate that GraphToxin can successfully subvert the regulatory guarantees expected from graph unlearning - it can recover not only a deleted individual's information and personal links but also sensitive content from their connections, thereby posing substantially more detrimental threats. Furthermore, we extend GraphToxin to multiple node removals under both white-box and black-box setting. We highlight the necessity of a worst-case analysis and propose a comprehensive evaluation framework to systematically assess the attack performance under both random and worst-case node removals. This provides a more robust and realistic measure of the vulnerability of graph unlearning methods to graph reconstruction attacks. Our extensive experiments demonstrate the effectiveness and flexibility of GraphToxin. Notably, we show that existing defense mechanisms are largely ineffective against this attack and, in some cases, can even amplify its performance. Given the severe privacy risks posed by GraphToxin, our work underscores the urgent need for the development of more effective and robust defense strategies against this attack.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.10935v1" target="_blank"><h2>CAT-Net: A Cross-Attention Tone Network for Cross-Subject EEG-EMG Fusion Tone Decoding <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yifan Zhuang, Calvin Huang, Zepeng Yu, Yongjie Zou, Jiawei Ju<br><strong><u>Categories:</u></strong> cs.SD, cs.LG, q-bio.NC<br><strong><u>Comments:</u></strong> This is the extended version with technical appendices. The version of record appears in AAAI-26. Please cite the AAAI version<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Brain-computer interface (BCI) speech decoding has emerged as a promising tool for assisting individuals with speech impairments. In this context, the integration of electroencephalography (EEG) and electromyography (EMG) signals offers strong potential for enhancing decoding performance. Mandarin tone classification presents particular challenges, as tonal variations convey distinct meanings even when phonemes remain identical. In this study, we propose a novel cross-subject multimodal BCI decoding framework that fuses EEG and EMG signals to classify four Mandarin tones under both audible and silent speech conditions. Inspired by the cooperative mechanisms of neural and muscular systems in speech production, our neural decoding architecture combines spatial-temporal feature extraction branches with a cross-attention fusion mechanism, enabling informative interaction between modalities. We further incorporate domain-adversarial training to improve cross-subject generalization. We collected 4,800 EEG trials and 4,800 EMG trials from 10 participants using only twenty EEG and five EMG channels, demonstrating the feasibility of minimal-channel decoding. Despite employing lightweight modules, our model outperforms state-of-the-art baselines across all conditions, achieving average classification accuracies of 87.83% for audible speech and 88.08% for silent speech. In cross-subject evaluations, it still maintains strong performance with accuracies of 83.27% and 85.10% for audible and silent speech, respectively. We further conduct ablation studies to validate the effectiveness of each component. Our findings suggest that tone-level decoding with minimal EEG-EMG channels is feasible and potentially generalizable across subjects, contributing to the development of practical BCI applications.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.10919v1" target="_blank"><h2>Heterogeneous Multisource Transfer Learning via Model Averaging for Positive-Unlabeled Data <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jialei Liu, Jun Liao, Kuangnan Fang<br><strong><u>Categories:</u></strong> stat.ML, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-14<br><strong><u>Matching Keywords:</u></strong> transfer learning (title, abstract)<br><p><strong><u>Abstract:</u></strong> Positive-Unlabeled (PU) learning presents unique challenges due to the lack of explicitly labeled negative samples, particularly in high-stakes domains such as fraud detection and medical diagnosis. To address data scarcity and privacy constraints, we propose a novel transfer learning with model averaging framework that integrates information from heterogeneous data sources - including fully binary labeled, semi-supervised, and PU data sets - without direct data sharing. For each source domain type, a tailored logistic regression model is conducted, and knowledge is transferred to the PU target domain through model averaging. Optimal weights for combining source models are determined via a cross-validation criterion that minimizes the Kullback-Leibler divergence. We establish theoretical guarantees for weight optimality and convergence, covering both misspecified and correctly specified target models, with further extensions to high-dimensional settings using sparsity-penalized estimators. Extensive simulations and real-world credit risk data analyses demonstrate that our method outperforms other comparative methods in terms of predictive accuracy and robustness, especially under limited labeled data and heterogeneous environments.</p><br><hr><br><hr><p><em>Summary: Showing 613 papers (586 new, 27 seen before)</em></p></body></html>