<link href='https://fonts.googleapis.com/css?family=Montserrat' rel='stylesheet'><style>     body {font-family: 'Montserrat'; background: #F3F3F3; width: 740px; margin: 0 auto; line-height: 150%; margin-top: 50px; font-size: 15px}         h1 {font-size: 70px}         a {color: #45ABC2}         em {font-size: 120%} </style><h1><center>ArXiv Alert</center></h1><font color='#DDAD5C'><em>Update: from 16 Jun 2025 to 18 Jun 2025</em></font><a href="http://arxiv.org/pdf/2506.13628v2" target="_blank"><h2>Graph-Convolutional-Beta-VAE for Synthetic Abdominal Aorta Aneurysm
  Generation</h2></a><strong><u>Authors:</u></strong>  Francesco Fabbri, Martino Andrea Scarpolini, Angelo Iollo, Francesco Viola, Francesco Tudisco</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, q-bio.TO</br><strong><u>Comments:</u></strong> Typo in the title</br><p><strong><u>Abstract:</u></strong> Synthetic data generation plays a crucial role in medical research by
mitigating privacy concerns and enabling large-scale patient data analysis.
This study presents a beta-Variational Autoencoder Graph Convolutional Neural
Network framework for generating synthetic Abdominal Aorta Aneurysms (AAA).
Using a small real-world dataset, our approach extracts key anatomical features
and captures complex statistical relationships within a compact disentangled
latent space. To address data limitations, low-impact data augmentation based
on Procrustes analysis was employed, preserving anatomical integrity. The
generation strategies, both deterministic and stochastic, manage to enhance
data diversity while ensuring realism. Compared to PCA-based approaches, our
model performs more robustly on unseen data by capturing complex, nonlinear
anatomical variations. This enables more comprehensive clinical and statistical
analyses than the original dataset alone. The resulting synthetic AAA dataset
preserves patient privacy while providing a scalable foundation for medical
research, device testing, and computational modeling.</p></br><a href="http://arxiv.org/pdf/2506.13866v1" target="_blank"><h2>Beyond diagonal approximations: improved covariance modeling for pulsar
  timing array data analysis</h2></a><strong><u>Authors:</u></strong>  Marco Crisostomi, Rutger van Haasteren, Patrick M. Meyers, Michele Vallisneri</br><strong><u>Categories:</u></strong> astro-ph.IM, astro-ph.CO, astro-ph.GA, astro-ph.HE, gr-qc</br><strong><u>Comments:</u></strong> 14 pages, 9 figures</br><p><strong><u>Abstract:</u></strong> Pulsar Timing Array (PTA) searches for nHz gravitational-wave backgrounds
(GWBs) typically model time-correlated noise by assuming a diagonal covariance
in Fourier space, neglecting inter-frequency correlations introduced by the
finite observation window. We show that this diagonal approximation can lead to
biased estimates of spectral parameters, especially for the common red process
that represents the GWB. To address these limitations, we present a method that
(i) computes the time-domain autocorrelation on a coarse grid using a fast
Fourier transform (FFT), (ii) interpolates it accurately to the unevenly
sampled observation times, and (iii) incorporates it into a low-rank likelihood
via the Sherman--Morrison--Woodbury identity. Using both analytic covariance
comparisons and end-to-end simulations inspired by the NANOGrav 15-year
dataset, we demonstrate that our method captures frequency correlations
faithfully, avoids Gibbs ringing, and recovers unbiased spectral parameters
with modest computational cost. As PTA datasets increase in sensitivity and
complexity, our approach offers a practical and scalable path to fully accurate
covariance modeling for current and future analyses.</p></br><a href="http://arxiv.org/pdf/2506.14390v1" target="_blank"><h2>Enclosing Prototypical Variational Autoencoder for Explainable
  Out-of-Distribution Detection</h2></a><strong><u>Authors:</u></strong>  Conrad Orglmeister, Erik Bochinski, Volker Eiselein, Elvira Fleig</br><strong><u>Categories:</u></strong> cs.LG, cs.CV</br><strong><u>Comments:</u></strong> This preprint has not undergone peer review or any post-submission improvements or corrections. The Version of Record of this contribution is published in Computer Safety, Reliability and Security - SAFECOMP 2024 Workshops - DECSoS, SASSUR, TOASTS, and WAISE, and is available online atthis https URL</br><p><strong><u>Abstract:</u></strong> Understanding the decision-making and trusting the reliability of Deep
Machine Learning Models is crucial for adopting such methods to safety-relevant
applications. We extend self-explainable Prototypical Variational models with
autoencoder-based out-of-distribution (OOD) detection: A Variational
Autoencoder is applied to learn a meaningful latent space which can be used for
distance-based classification, likelihood estimation for OOD detection, and
reconstruction. The In-Distribution (ID) region is defined by a Gaussian
mixture distribution with learned prototypes representing the center of each
mode. Furthermore, a novel restriction loss is introduced that promotes a
compact ID region in the latent space without collapsing it into single points.
The reconstructive capabilities of the Autoencoder ensure the explainability of
the prototypes and the ID region of the classifier, further aiding the
discrimination of OOD samples. Extensive evaluations on common OOD detection
benchmarks as well as a large-scale dataset from a real-world railway
application demonstrate the usefulness of the approach, outperforming previous
methods.</p></br><a href="http://arxiv.org/pdf/2506.13955v1" target="_blank"><h2>Bridging Unsupervised and Semi-Supervised Anomaly Detection: A
  Theoretically-Grounded and Practical Framework with Synthetic Anomalies</h2></a><strong><u>Authors:</u></strong>  Matthew Lau, Tian-Yi Zhou, Xiangchi Yuan, Jizhou Chen, Wenke Lee, Xiaoming Huo</br><strong><u>Categories:</u></strong> stat.ML, cs.CR, cs.LG, stat.AP</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Anomaly detection (AD) is a critical task across domains such as
cybersecurity and healthcare. In the unsupervised setting, an effective and
theoretically-grounded principle is to train classifiers to distinguish normal
data from (synthetic) anomalies. We extend this principle to semi-supervised
AD, where training data also include a limited labeled subset of anomalies
possibly present in test time. We propose a theoretically-grounded and
empirically effective framework for semi-supervised AD that combines known and
synthetic anomalies during training. To analyze semi-supervised AD, we
introduce the first mathematical formulation of semi-supervised AD, which
generalizes unsupervised AD. Here, we show that synthetic anomalies enable (i)
better anomaly modeling in low-density regions and (ii) optimal convergence
guarantees for neural network classifiers -- the first theoretical result for
semi-supervised AD. We empirically validate our framework on five diverse
benchmarks, observing consistent performance gains. These improvements also
extend beyond our theoretical framework to other classification-based AD
methods, validating the generalizability of the synthetic anomaly principle in
AD.</p></br><a href="http://arxiv.org/pdf/2506.13060v1" target="_blank"><h2>Rethinking Explainability in the Era of Multimodal AI</h2></a><strong><u>Authors:</u></strong>  Chirag Agarwal</br><strong><u>Categories:</u></strong> cs.AI, cs.LG</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> While multimodal AI systems (models jointly trained on heterogeneous data
types such as text, time series, graphs, and images) have become ubiquitous and
achieved remarkable performance across high-stakes applications, transparent
and accurate explanation algorithms are crucial for their safe deployment and
ensure user trust. However, most existing explainability techniques remain
unimodal, generating modality-specific feature attributions, concepts, or
circuit traces in isolation and thus failing to capture cross-modal
interactions. This paper argues that such unimodal explanations systematically
misrepresent and fail to capture the cross-modal influence that drives
multimodal model decisions, and the community should stop relying on them for
interpreting multimodal models. To support our position, we outline key
principles for multimodal explanations grounded in modality: Granger-style
modality influence (controlled ablations to quantify how removing one modality
changes the explanation for another), Synergistic faithfulness (explanations
capture the model's predictive power when modalities are combined), and Unified
stability (explanations remain consistent under small, cross-modal
perturbations). This targeted shift to multimodal explanations will help the
community uncover hidden shortcuts, mitigate modality bias, improve model
reliability, and enhance safety in high-stakes settings where incomplete
explanations can have serious consequences.</p></br><a href="http://arxiv.org/pdf/2506.14368v1" target="_blank"><h2>VLBA astrometry of PSRs B0329+54 and B1133+16: Improved pulsar distances
  and comparison of global ionospheric models</h2></a><strong><u>Authors:</u></strong>  Ashish Kumar, Adam T. Deller, Pankaj Jain, Javier Moldón</br><strong><u>Categories:</u></strong> astro-ph.HE, astro-ph.GA, astro-ph.IM, astro-ph.SR</br><strong><u>Comments:</u></strong> Accepted for publication in PASA</br><p><strong><u>Abstract:</u></strong> Very long baseline interferometry (VLBI) astrometry is used to determine the
three-dimensional position and proper motion of astronomical objects. A typical
VLBI astrometric campaign generally includes around ten observations, making it
challenging to characterise systematic uncertainties. Our study on two bright
pulsars, B0329+54 and B1133+16, involves analysis of broadband Very Long
Baseline Array (VLBA) data over $\sim30$ epochs (spanning approximately $3.5\,
{\rm years}$). This extended dataset has significantly improved the precision
of the astrometric estimates of these pulsars. Our broadband study suggests
that, as expected, the primary contribution to systematic uncertainties in
L-band VLBI astrometry originates from the ionosphere. We have also assessed
the effectiveness of the modified TEC (total electron content) mapping
function, which converts vertical TEC to slant TEC, in correcting ionospheric
dispersive delays using global TEC maps. The parallax and proper motion
obtained from the multiple data sets, calibrated using the traditional and the
modified TEC mapping functions, are consistent. However, the reduced chi-square
values from least-squares fitting and precision of the fitted astrometric
parameters show no significant improvement, and hence, the effectiveness of the
new TEC mapping function on astrometry is unclear. For B0329+54, the refined
parallax estimate is $0.611^{+0.013}_{-0.013}\, {\rm mas}$, with best-fit
proper motion of $\mu_{\alpha} = 16.960^{+0.011}_{-0.010}\, {\rm mas\,
yr^{-1}}$ in R.A. and and $\mu_{\delta} = -10.382^{+0.022}_{-0.022}\, {\rm
mas\, yr^{-1}}$ in Dec. For B1133+16, the new estimated parallax is
$2.705^{+0.009}_{-0.009}\, {\rm mas}$, with proper motions of $\mu_{\alpha} =
-73.777^{+0.008}_{-0.008}\, {\rm mas\, yr^{-1}}$ and $\mu_{\delta} =
366.573^{+0.019}_{-0.019}\, {\rm mas\, yr^{-1}}$.</p></br><a href="http://arxiv.org/pdf/2506.14329v1" target="_blank"><h2>Adjustment for Confounding using Pre-Trained Representations</h2></a><strong><u>Authors:</u></strong>  Rickmer Schulte, David Rügamer, Thomas Nagler</br><strong><u>Categories:</u></strong> stat.ML, cs.AI, cs.LG, stat.CO, stat.ME</br><strong><u>Comments:</u></strong> Accepted at ICML 2025</br><p><strong><u>Abstract:</u></strong> There is growing interest in extending average treatment effect (ATE)
estimation to incorporate non-tabular data, such as images and text, which may
act as sources of confounding. Neglecting these effects risks biased results
and flawed scientific conclusions. However, incorporating non-tabular data
necessitates sophisticated feature extractors, often in combination with ideas
of transfer learning. In this work, we investigate how latent features from
pre-trained neural networks can be leveraged to adjust for sources of
confounding. We formalize conditions under which these latent features enable
valid adjustment and statistical inference in ATE estimation, demonstrating
results along the example of double machine learning. We discuss critical
challenges inherent to latent feature learning and downstream parameter
estimation arising from the high dimensionality and non-identifiability of
representations. Common structural assumptions for obtaining fast convergence
rates with additive or sparse linear models are shown to be unrealistic for
latent features. We argue, however, that neural networks are largely
insensitive to these issues. In particular, we show that neural networks can
achieve fast convergence rates by adapting to intrinsic notions of sparsity and
dimension of the learning problem.</p></br><a href="http://arxiv.org/pdf/2506.13907v1" target="_blank"><h2>Extreme AGN feedback in the fossil galaxy group SDSSTG 4436</h2></a><strong><u>Authors:</u></strong>  D. Eckert, F. Gastaldello, L. Lovisari, S. McGee, T. Pasini, M. Brienza, K. Kolokythas, E. O'Sullivan, A. Simionescu, M. Sun, M. Ayromlou, M. A. Bourne, Y. Chen, W. Cui, S. Ettori, A. Finoguenov, G. Gozaliasl, R. Kale, F. Mernier, B. D. Oppenheimer, G. Schellenberger, R. Seppi, E. Tempel</br><strong><u>Categories:</u></strong> astro-ph.GA, astro-ph.CO, astro-ph.HE</br><strong><u>Comments:</u></strong> 15 pages, 10 figures, re-submitted to A&A after minor revision</br><p><strong><u>Abstract:</u></strong> Supermassive black hole feedback is the currently favoured mechanism to
regulate the star formation rate of galaxies and prevent the formation of
ultra-massive galaxies ($M_\star>10^{12}M_\odot$). However, the mechanism
through which the outflowing energy is transferred to the surrounding medium
strongly varies from one galaxy evolution model to another, such that a unified
model for AGN feedback does not currently exist. The hot atmospheres of galaxy
groups are highly sensitive laboratories of the feedback process, as the
injected black hole energy is comparable to the binding energy of halo gas
particles. Here we report multi-wavelength observations of the fossil galaxy
group SDSSTG 4436. The hot atmosphere of this system exhibits a highly relaxed
morphology centred on the giant elliptical galaxy NGC~3298. The X-ray emission
from the system features a compact core ($<$10 kpc) and a steep increase in the
entropy and cooling time of the gas, with the cooling time reaching the age of
the Universe $\sim15$ kpc from the centre of the galaxy. The observed entropy
profile implies a total injected energy of $\sim1.5\times10^{61}$ ergs, which
given the high level of relaxation could not have been injected by a recent
merging event. Star formation in the central galaxy NGC~3298 is strongly
quenched and its stellar population is very old ($\sim$10.6 Gyr). The currently
detected radio jets have low power and are confined within the central compact
core. All the available evidence implies that this system was affected by giant
AGN outbursts which excessively heated the neighbouring gas and prevented the
formation of a self-regulated feedback cycle. Our findings imply that AGN
outbursts can be energetic enough to unbind gas particles and lead to the
disruption of cool cores.</p></br><a href="http://arxiv.org/pdf/2506.14113v1" target="_blank"><h2>SKOLR: Structured Koopman Operator Linear RNN for Time-Series
  Forecasting</h2></a><strong><u>Authors:</u></strong>  Yitian Zhang, Liheng Ma, Antonios Valkanas, Boris N. Oreshkin, Mark Coates</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Koopman operator theory provides a framework for nonlinear dynamical system
analysis and time-series forecasting by mapping dynamics to a space of
real-valued measurement functions, enabling a linear operator representation.
Despite the advantage of linearity, the operator is generally
infinite-dimensional. Therefore, the objective is to learn measurement
functions that yield a tractable finite-dimensional Koopman operator
approximation. In this work, we establish a connection between Koopman operator
approximation and linear Recurrent Neural Networks (RNNs), which have recently
demonstrated remarkable success in sequence modeling. We show that by
considering an extended state consisting of lagged observations, we can
establish an equivalence between a structured Koopman operator and linear RNN
updates. Building on this connection, we present SKOLR, which integrates a
learnable spectral decomposition of the input signal with a multilayer
perceptron (MLP) as the measurement functions and implements a structured
Koopman operator via a highly parallel linear RNN stack. Numerical experiments
on various forecasting benchmarks and dynamical systems show that this
streamlined, Koopman-theory-based design delivers exceptional performance.</p></br><a href="http://arxiv.org/pdf/2506.13926v1" target="_blank"><h2>Search for Astrophysical Transients on Limiting Time Scales and Their
  Classification Based on INTEGRAL Data</h2></a><strong><u>Authors:</u></strong>  G. Yu. Mozgunov, A. S. Pozanenko, P. Yu. Minaev, I. V. Chelovekov, S. A. Grebenev, A. G. Demin, A. V. Ridnaya, D. S. Svinkin, Yu. R. Temiraev, D. D. Frederiks</br><strong><u>Categories:</u></strong> astro-ph.HE, astro-ph.IM</br><strong><u>Comments:</u></strong> 22 pages, 23 figures, 5 tables</br><p><strong><u>Abstract:</u></strong> We have searched for ultra-long (> 100 s) gamma-ray transients in the data
from the anticoincidence shield (ACS) of the SPI gamma-ray spectrometer onboard
the INTEGRAL orbital observatory and classified them by machine learning
methods. We have found about 4364 candidates for such events in the SPI-ACS
data by the `blind' threshold search method. We have developed an algorithm for
automatic processing of their light curves that distinguishes a candidate for
transients on various time scales and allows its duration and fluence to be
determined. The algorithm has been applied to calculate (and compare) the
fluxes in the light curves recorded by various INTEGRAL detectors: IREM,
SPI-ACS, SPI, ISGRI, and PICsIT. These fluxes have been used to train the
classifier based on gradient boosting. Subsequently, we have performed a
cluster analysis of the candidates found by the dimensionality reduction and
clustering methods. In conclusion we have compared the remaining candidates
with the data from the Konus-WIND gamma-ray detectors. Thus, we have confirmed
16 candidates for astrophysical transients, including four candidates for
ultra-long gamma-ray bursts from the events detected by the SPI-ACS detector.
Out of the probable events, but unconfirmed by other experiments, up to 270
events can be classified as real gamma-ray bursts.</p></br><a href="http://arxiv.org/pdf/2506.14202v1" target="_blank"><h2>DiffusionBlocks: Blockwise Training for Generative Models via
  Score-Based Diffusion</h2></a><strong><u>Authors:</u></strong>  Makoto Shing, Takuya Akiba</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML</br><strong><u>Comments:</u></strong> To appear at TTODLer-FM Workshop of the 42nd International Conference on Machine Learning</br><p><strong><u>Abstract:</u></strong> Training large neural networks with end-to-end backpropagation creates
significant memory bottlenecks, limiting accessibility to state-of-the-art AI
research. We propose $\textit{DiffusionBlocks}$, a novel training framework
that interprets neural network blocks as performing denoising operations in a
continuous-time diffusion process. By partitioning the network into
independently trainable blocks and optimizing noise level assignments based on
equal cumulative probability mass, our approach achieves significant memory
efficiency while maintaining competitive performance compared to traditional
backpropagation in generative tasks. Experiments on image generation and
language modeling tasks demonstrate memory reduction proportional to the number
of blocks while achieving superior performance. DiffusionBlocks provides a
promising pathway for democratizing access to large-scale neural network
training with limited computational resources.</p></br><a href="http://arxiv.org/pdf/2506.13717v1" target="_blank"><h2>Contrastive Self-Supervised Learning As Neural Manifold Packing</h2></a><strong><u>Authors:</u></strong>  Guanming Zhang, David J. Heeger, Stefano Martiniani</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, q-bio.NC, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Contrastive self-supervised learning based on point-wise comparisons has been
widely studied for vision tasks. In the visual cortex of the brain, neuronal
responses to distinct stimulus classes are organized into geometric structures
known as neural manifolds. Accurate classification of stimuli can be achieved
by effectively separating these manifolds, akin to solving a packing problem.
We introduce Contrastive Learning As Manifold Packing (CLAMP), a
self-supervised framework that recasts representation learning as a manifold
packing problem. CLAMP introduces a loss function inspired by the potential
energy of short-range repulsive particle systems, such as those encountered in
the physics of simple liquids and jammed packings. In this framework, each
class consists of sub-manifolds embedding multiple augmented views of a single
image. The sizes and positions of the sub-manifolds are dynamically optimized
by following the gradient of a packing loss. This approach yields interpretable
dynamics in the embedding space that parallel jamming physics, and introduces
geometrically meaningful hyperparameters within the loss function. Under the
standard linear evaluation protocol, which freezes the backbone and trains only
a linear classifier, CLAMP achieves competitive performance with
state-of-the-art self-supervised models. Furthermore, our analysis reveals that
neural manifolds corresponding to different categories emerge naturally and are
effectively separated in the learned representation space, highlighting the
potential of CLAMP to bridge insights from physics, neural science, and machine
learning.</p></br><a href="http://arxiv.org/pdf/2506.14002v1" target="_blank"><h2>Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse
  Autoencoders</h2></a><strong><u>Authors:</u></strong>  Siyu Chen, Heejune Sheen, Xuyuan Xiong, Tianhao Wang, Zhuoran Yang</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.IT, math.IT, stat.ML, I.2.6</br><strong><u>Comments:</u></strong> 136 pages, 21 figures</br><p><strong><u>Abstract:</u></strong> We study the challenge of achieving theoretically grounded feature recovery
using Sparse Autoencoders (SAEs) for the interpretation of Large Language
Models. Existing SAE training algorithms often lack rigorous mathematical
guarantees and suffer from practical limitations such as hyperparameter
sensitivity and instability. To address these issues, we first propose a novel
statistical framework for the feature recovery problem, which includes a new
notion of feature identifiability by modeling polysemantic features as sparse
mixtures of underlying monosemantic concepts. Building on this framework, we
introduce a new SAE training algorithm based on ``bias adaptation'', a
technique that adaptively adjusts neural network bias parameters to ensure
appropriate activation sparsity. We theoretically \highlight{prove that this
algorithm correctly recovers all monosemantic features} when input data is
sampled from our proposed statistical model. Furthermore, we develop an
improved empirical variant, Group Bias Adaptation (GBA), and
\highlight{demonstrate its superior performance against benchmark methods when
applied to LLMs with up to 1.5 billion parameters}. This work represents a
foundational step in demystifying SAE training by providing the first SAE
algorithm with theoretical recovery guarantees, thereby advancing the
development of more transparent and trustworthy AI systems through enhanced
mechanistic interpretability.</p></br><a href="http://arxiv.org/pdf/2506.13318v1" target="_blank"><h2>Vine Copulas as Differentiable Computational Graphs</h2></a><strong><u>Authors:</u></strong>  Tuoyuan Cheng, Thibault Vatter, Thomas Nagler, Kan Chen</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Vine copulas are sophisticated models for multivariate distributions and are
increasingly used in machine learning. To facilitate their integration into
modern ML pipelines, we introduce the vine computational graph, a DAG that
abstracts the multilevel vine structure and associated computations. On this
foundation, we devise new algorithms for conditional sampling, efficient
sampling-order scheduling, and constructing vine structures for customized
conditioning variables. We implement these ideas in torchvinecopulib, a
GPU-accelerated Python library built upon PyTorch, delivering improved
scalability for fitting, sampling, and density evaluation. Our experiments
illustrate how gradient flowing through the vine can improve Vine Copula
Autoencoders and that incorporating vines for uncertainty quantification in
deep learning can outperform MC-dropout, deep ensembles, and Bayesian Neural
Networks in sharpness, calibration, and runtime. By recasting vine copula
models as computational graphs, our work connects classical dependence modeling
with modern deep-learning toolchains and facilitates the integration of
state-of-the-art copula methods in modern machine learning pipelines.</p></br><a href="http://arxiv.org/pdf/2506.13277v2" target="_blank"><h2>SeqPE: Transformer with Sequential Position Encoding</h2></a><strong><u>Authors:</u></strong>  Huayang Li, Yahui Liu, Hongyu Sun, Deng Cai, Leyang Cui, Wei Bi, Peilin Zhao, Taro Watanabe</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.CL, cs.CV</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Since self-attention layers in Transformers are permutation invariant by
design, positional encodings must be explicitly incorporated to enable spatial
understanding. However, fixed-size lookup tables used in traditional learnable
position embeddings (PEs) limit extrapolation capabilities beyond pre-trained
sequence lengths. Expert-designed methods such as ALiBi and RoPE, mitigate this
limitation but demand extensive modifications for adapting to new modalities,
underscoring fundamental challenges in adaptability and scalability. In this
work, we present SeqPE, a unified and fully learnable position encoding
framework that represents each $n$-dimensional position index as a symbolic
sequence and employs a lightweight sequential position encoder to learn their
embeddings in an end-to-end manner. To regularize SeqPE's embedding space, we
introduce two complementary objectives: a contrastive objective that aligns
embedding distances with a predefined position-distance function, and a
knowledge distillation loss that anchors out-of-distribution position
embeddings to in-distribution teacher representations, further enhancing
extrapolation performance. Experiments across language modeling, long-context
question answering, and 2D image classification demonstrate that SeqPE not only
surpasses strong baselines in perplexity, exact match (EM), and
accuracy--particularly under context length extrapolation--but also enables
seamless generalization to multi-dimensional inputs without requiring manual
architectural redesign. We release our code, data, and checkpoints at
https://github.com/ghrua/seqpe.</p></br><a href="http://arxiv.org/pdf/2506.14438v1" target="_blank"><h2>sHGCN: Simplified hyperbolic graph convolutional neural networks</h2></a><strong><u>Authors:</u></strong>  Pol Arévalo, Alexis Molina, Álvaro Ciudad</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Hyperbolic geometry has emerged as a powerful tool for modeling complex,
structured data, particularly where hierarchical or tree-like relationships are
present. By enabling embeddings with lower distortion, hyperbolic neural
networks offer promising alternatives to Euclidean-based models for capturing
intricate data structures. Despite these advantages, they often face
performance challenges, particularly in computational efficiency and tasks
requiring high precision. In this work, we address these limitations by
simplifying key operations within hyperbolic neural networks, achieving notable
improvements in both runtime and performance. Our findings demonstrate that
streamlined hyperbolic operations can lead to substantial gains in
computational speed and predictive accuracy, making hyperbolic neural networks
a more viable choice for a broader range of applications.</p></br><a href="http://arxiv.org/pdf/2506.14280v1" target="_blank"><h2>Improving LoRA with Variational Learning</h2></a><strong><u>Authors:</u></strong>  Bai Cong, Nico Daheim, Yuesong Shen, Rio Yokota, Mohammad Emtiyaz Khan, Thomas Möllenhoff</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.CL, stat.ML</br><strong><u>Comments:</u></strong> 16 pages, 4 figures</br><p><strong><u>Abstract:</u></strong> Bayesian methods have recently been used to improve LoRA finetuning and,
although they improve calibration, their effect on other metrics (such as
accuracy) is marginal and can sometimes even be detrimental. Moreover, Bayesian
methods also increase computational overheads and require additional tricks for
them to work well. Here, we fix these issues by using a recently proposed
variational algorithm called IVON. We show that IVON is easy to implement and
has similar costs to AdamW, and yet it can also drastically improve many
metrics by using a simple posterior pruning technique. We present extensive
results on billion-scale LLMs (Llama and Qwen series) going way beyond the
scale of existing applications of IVON. For example, we finetune a Llama-3.2-3B
model on a set of commonsense reasoning tasks and improve accuracy over AdamW
by 1.3% and reduce ECE by 5.4%, outperforming AdamW and other recent Bayesian
methods like Laplace-LoRA and BLoB. Overall, our results show that variational
learning with IVON can effectively improve LoRA finetuning.</p></br><a href="http://arxiv.org/pdf/2506.14262v1" target="_blank"><h2>Knowledge Adaptation as Posterior Correction</h2></a><strong><u>Authors:</u></strong>  Mohammad Emtiyaz Khan</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Adaptation is the holy grail of intelligence, but even the best AI models
(like GPT) lack the adaptivity of toddlers. So the question remains: how can
machines adapt quickly? Despite a lot of progress on model adaptation to
facilitate continual and federated learning, as well as model merging, editing,
unlearning, etc., little is known about the mechanisms by which machines can
naturally learn to adapt in a similar way as humans and animals. Here, we show
that all such adaptation methods can be seen as different ways of `correcting'
the approximate posteriors. More accurate posteriors lead to smaller
corrections, which in turn imply quicker adaptation. The result is obtained by
using a dual-perspective of the Bayesian Learning Rule of Khan and Rue (2023)
where interference created during adaptation is characterized by the
natural-gradient mismatch over the past data. We present many examples to
demonstrate the use of posterior-correction as a natural mechanism for the
machines to learn to adapt quickly.</p></br><a href="http://arxiv.org/pdf/2506.14020v1" target="_blank"><h2>Bures-Wasserstein Flow Matching for Graph Generation</h2></a><strong><u>Authors:</u></strong>  Keyue Jiang, Jiahao Cui, Xiaowen Dong, Laura Toni</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Graph generation has emerged as a critical task in fields ranging from
molecule design to drug discovery. Contemporary approaches, notably diffusion
and flow-based models, have achieved solid graph generative performance through
constructing a probability path that interpolates between a reference
distribution and the data distribution. However, these methods typically model
the evolution of individual nodes and edges independently and use linear
interpolations to build the path assuming that the data lie in Euclidean space.
We show that this is suboptimal given the intrinsic non-Euclidean structure and
interconnected patterns of graphs, and it poses risks to the sampling
convergence. To build a better probability path, we model the joint evolution
of the nodes and edges by representing graphs as connected systems
parameterized by Markov random fields (MRF). We then leverage the optimal
transport displacement between MRF objects to design the probability path for
graph generation. Based on this, we introduce BWFlow, a flow-matching framework
for graph generation that respects the underlying geometry of graphs and
provides smooth velocities in the probability path. The novel framework can be
adapted to both continuous and discrete flow-matching algorithms. Experimental
evaluations in plain graph generation and 2D/3D molecule generation validate
the effectiveness of BWFlow in graph generation with competitive performance,
stable training, and guaranteed sampling convergence.</p></br><a href="http://arxiv.org/pdf/2506.13244v2" target="_blank"><h2>No-Regret Learning Under Adversarial Resource Constraints: A Spending
  Plan Is All You Need!</h2></a><strong><u>Authors:</u></strong>  Francesco Emanuele Stradi, Matteo Castiglioni, Alberto Marchesi, Nicola Gatti, Christian Kroer</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> We study online decision making problems under resource constraints, where
both reward and cost functions are drawn from distributions that may change
adversarially over time. We focus on two canonical settings: $(i)$ online
resource allocation where rewards and costs are observed before action
selection, and $(ii)$ online learning with resource constraints where they are
observed after action selection, under full feedback or bandit feedback. It is
well known that achieving sublinear regret in these settings is impossible when
reward and cost distributions may change arbitrarily over time. To address this
challenge, we analyze a framework in which the learner is guided by a spending
plan--a sequence prescribing expected resource usage across rounds. We design
general (primal-)dual methods that achieve sublinear regret with respect to
baselines that follow the spending plan. Crucially, the performance of our
algorithms improves when the spending plan ensures a well-balanced distribution
of the budget across rounds. We additionally provide a robust variant of our
methods to handle worst-case scenarios where the spending plan is highly
imbalanced. To conclude, we study the regret of our algorithms when competing
against benchmarks that deviate from the prescribed spending plan.</p></br></body>