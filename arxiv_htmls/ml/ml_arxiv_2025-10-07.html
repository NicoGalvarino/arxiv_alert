<link href='https://fonts.googleapis.com/css?family=Montserrat' rel='stylesheet'>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$','$']],
            processEscapes: true
        },
        "HTML-CSS": {
            availableFonts: ["TeX"]
        }
    });
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <style>     body {font-family: 'Montserrat'; background: #F3F3F3; width: 740px; margin: 0 auto; line-height: 150%; margin-top: 50px; font-size: 15px}         h1 {font-size: 70px}         a {color: #45ABC2}         em {font-size: 120%} </style><h1><center>ArXiv Alert</center></h1><font color='#DDAD5C'><em>Update: from 03 Oct 2025 to 07 Oct 2025</em></font><a href="http://arxiv.org/pdf/2510.03859v1" target="_blank"><h2>Adaptive and Explainable AI Agents for Anomaly Detection in Critical IoT
  Infrastructure using LLM-Enhanced Contextual Reasoning</h2></a><strong><u>Authors:</u></strong>  Raghav Sharma, Manan Mehta</br><strong><u>Categories:</u></strong> cs.AI, cs.LG</br><strong><u>Comments:</u></strong> 22 pages</br><strong><u>Matching Keywords:</u></strong> anomaly detection (title), explainable (title), attention (abstract)</br><p><strong><u>Abstract:</u></strong> Ensuring that critical IoT systems function safely and smoothly depends a lot
on finding anomalies quickly. As more complex systems, like smart healthcare,
energy grids and industrial automation, appear, it is easier to see the
shortcomings of older methods of detection. Monitoring failures usually happen
in dynamic, high dimensional situations, especially when data is incomplete,
messy or always evolving. Such limits point out the requirement for adaptive,
intelligent systems that always improve and think. LLMs are now capable of
significantly changing how context is understood and semantic inference is done
across all types of data. This proposal suggests using an LLM supported
contextual reasoning method along with XAI agents to improve how anomalies are
found in significant IoT environments. To discover hidden patterns and notice
inconsistencies in data streams, it uses attention methods, avoids dealing with
details from every time step and uses memory buffers with meaning. Because no
code AI stresses transparency and interpretability, people can check and accept
the AI's decisions, helping ensure AI follows company policies. The two
architectures are put together in a test that compares the results of the
traditional model with those of the suggested LLM enhanced model. Important
measures to check are the accuracy of detection, how much inaccurate
information is included in the results, how clearly the findings can be read
and how fast the system responds under different test situations. The
metaheuristic is tested in simulations of real world smart grid and healthcare
contexts to check its adaptability and reliability. From the study, we see that
the new approach performs much better than most existing models in both
accuracy and interpretation, so it could be a good fit for future anomaly
detection tasks in IoT</p></br><a href="http://arxiv.org/pdf/2510.03962v1" target="_blank"><h2>SPEAR: Soft Prompt Enhanced Anomaly Recognition for Time Series Data</h2></a><strong><u>Authors:</u></strong>  Hanzhe Wei, Jiajun Wu, Jialin Yang, Henry Leung, Steve Drew</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> Accepted to 2025 IEEE International Conference on Autonomous and Trusted Computing (ATC 2025)</br><strong><u>Matching Keywords:</u></strong> anomaly detection (abstract)</br><p><strong><u>Abstract:</u></strong> Time series anomaly detection plays a crucial role in a wide range of fields,
such as healthcare and internet traffic monitoring. The emergence of large
language models (LLMs) offers new opportunities for detecting anomalies in the
ubiquitous time series data. Traditional approaches struggle with
variable-length time series sequences and context-based anomalies. We propose
Soft Prompt Enhanced Anomaly Recognition (SPEAR), a novel approach to leverage
LLMs for anomaly detection with soft prompts and quantization. Our methodology
involves quantizing and transforming the time series data into input embeddings
and combining them with learnable soft prompt embeddings. These combined
embeddings are then fed into a frozen LLM. The soft prompts are updated
iteratively based on a cross-entropy loss, allowing the model to adapt to time
series anomaly detection. The use of soft prompts helps adapt LLMs effectively
to time series tasks, while quantization ensures optimal handling of sequences,
as LLMs are designed to handle discrete sequences. Our experimental results
demonstrate that soft prompts effectively increase LLMs' performance in
downstream tasks regarding time series anomaly detection.</p></br><a href="http://arxiv.org/pdf/2510.03361v1" target="_blank"><h2>Provenance Networks: End-to-End Exemplar-Based Explainability</h2></a><strong><u>Authors:</u></strong>  Ali Kayyam, Anusha Madan Gopal, M. Anthony Lewis</br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.LG</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), explainability (title, abstract)</br><p><strong><u>Abstract:</u></strong> We introduce provenance networks, a novel class of neural models designed to
provide end-to-end, training-data-driven explainability. Unlike conventional
post-hoc methods, provenance networks learn to link each prediction directly to
its supporting training examples as part of the model's normal operation,
embedding interpretability into the architecture itself. Conceptually, the
model operates similarly to a learned KNN, where each output is justified by
concrete exemplars weighted by relevance in the feature space. This approach
facilitates systematic investigations of the trade-off between memorization and
generalization, enables verification of whether a given input was included in
the training set, aids in the detection of mislabeled or anomalous data points,
enhances resilience to input perturbations, and supports the identification of
similar inputs contributing to the generation of a new data point. By jointly
optimizing the primary task and the explainability objective, provenance
networks offer insights into model behavior that traditional deep networks
cannot provide. While the model introduces additional computational cost and
currently scales to moderately sized datasets, it provides a complementary
approach to existing explainability techniques. In particular, it addresses
critical challenges in modern deep learning, including model opaqueness,
hallucination, and the assignment of credit to data contributors, thereby
improving transparency, robustness, and trustworthiness in neural models.</p></br><a href="http://arxiv.org/pdf/2510.03486v1" target="_blank"><h2>Reasoning-based Anomaly Detection Framework: A Real-time, Scalable, and
  Automated Approach to Anomaly Detection Across Domains</h2></a><strong><u>Authors:</u></strong>  Anupam Panwar, Himadri Pal, Jiali Chen, Kyle Cho, Riddick Jiang, Miao Zhao, Rajiv Krishnamurthy</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> 11 pages, 7 figures</br><strong><u>Matching Keywords:</u></strong> anomaly detection (title, abstract)</br><p><strong><u>Abstract:</u></strong> Detecting anomalies in large, distributed systems presents several
challenges. The first challenge arises from the sheer volume of data that needs
to be processed. Flagging anomalies in a high-throughput environment calls for
a careful consideration of both algorithm and system design. The second
challenge comes from the heterogeneity of time-series datasets that leverage
such a system in production. In practice, anomaly detection systems are rarely
deployed for a single use case. Typically, there are several metrics to
monitor, often across several domains (e.g. engineering, business and
operations). A one-size-fits-all approach rarely works, so these systems need
to be fine-tuned for every application - this is often done manually. The third
challenge comes from the fact that determining the root-cause of anomalies in
such settings is akin to finding a needle in a haystack. Identifying (in real
time) a time-series dataset that is associated causally with the anomalous
time-series data is a very difficult problem. In this paper, we describe a
unified framework that addresses these challenges. Reasoning based Anomaly
Detection Framework (RADF) is designed to perform real time anomaly detection
on very large datasets. This framework employs a novel technique (mSelect) that
automates the process of algorithm selection and hyper-parameter tuning for
each use case. Finally, it incorporates a post-detection capability that allows
for faster triaging and root-cause determination. Our extensive experiments
demonstrate that RADF, powered by mSelect, surpasses state-of-the-art anomaly
detection models in AUC performance for 5 out of 9 public benchmarking
datasets. RADF achieved an AUC of over 0.85 for 7 out of 9 datasets, a
distinction unmatched by any other state-of-the-art model.</p></br><a href="http://arxiv.org/pdf/2510.03614v1" target="_blank"><h2>Neural Bayesian Filtering</h2></a><strong><u>Authors:</u></strong>  Christopher Solinas, Radovan Haluska, David Sychrovsky, Finbarr Timbers, Nolan Bard, Michael Buro, Martin Schmid, Nathan R. Sturtevant, Michael Bowling</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)</br><p><strong><u>Abstract:</u></strong> We present Neural Bayesian Filtering (NBF), an algorithm for maintaining
distributions over hidden states, called beliefs, in partially observable
systems. NBF is trained to find a good latent representation of the beliefs
induced by a task. It maps beliefs to fixed-length embedding vectors, which
condition generative models for sampling. During filtering, particle-style
updates compute posteriors in this embedding space using incoming observations
and the environment's dynamics. NBF combines the computational efficiency of
classical filters with the expressiveness of deep generative models - tracking
rapidly shifting, multimodal beliefs while mitigating the risk of particle
impoverishment. We validate NBF in state estimation tasks in three partially
observable environments.</p></br><a href="http://arxiv.org/pdf/2510.03535v1" target="_blank"><h2>Sequential decoder training for improved latent space dynamics
  identification</h2></a><strong><u>Authors:</u></strong>  William Anderson, Seung Whan Chung, Youngsoo Choi</br><strong><u>Categories:</u></strong> cs.LG, cs.NA, math.NA, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), latent space (title, abstract)</br><p><strong><u>Abstract:</u></strong> Accurate numerical solutions of partial differential equations are essential
in many scientific fields but often require computationally expensive solvers,
motivating reduced-order models (ROMs). Latent Space Dynamics Identification
(LaSDI) is a data-driven ROM framework that combines autoencoders with equation
discovery to learn interpretable latent dynamics. However, enforcing latent
dynamics during training can compromise reconstruction accuracy of the model
for simulation data. We introduce multi-stage LaSDI (mLaSDI), a framework that
improves reconstruction and prediction accuracy by sequentially learning
additional decoders to correct residual errors from previous stages. Applied to
the 1D-1V Vlasov equation, mLaSDI consistently outperforms standard LaSDI,
achieving lower prediction errors and reduced training time across a wide range
of architectures.</p></br><a href="http://arxiv.org/pdf/2510.03824v1" target="_blank"><h2>Proximal Diffusion Neural Sampler</h2></a><strong><u>Authors:</u></strong>  Wei Guo, Jaemoo Choi, Yuchen Zhu, Molei Tao, Yongxin Chen</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML</br><strong><u>Comments:</u></strong> 31 pages, 12 figures</br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)</br><p><strong><u>Abstract:</u></strong> The task of learning a diffusion-based neural sampler for drawing samples
from an unnormalized target distribution can be viewed as a stochastic optimal
control problem on path measures. However, the training of neural samplers can
be challenging when the target distribution is multimodal with significant
barriers separating the modes, potentially leading to mode collapse. We propose
a framework named \textbf{Proximal Diffusion Neural Sampler (PDNS)} that
addresses these challenges by tackling the stochastic optimal control problem
via proximal point method on the space of path measures. PDNS decomposes the
learning process into a series of simpler subproblems that create a path
gradually approaching the desired distribution. This staged procedure traces a
progressively refined path to the desired distribution and promotes thorough
exploration across modes. For a practical and efficient realization, we
instantiate each proximal step with a proximal weighted denoising cross-entropy
(WDCE) objective. We demonstrate the effectiveness and robustness of PDNS
through extensive experiments on both continuous and discrete sampling tasks,
including challenging scenarios in molecular dynamics and statistical physics.</p></br><a href="http://arxiv.org/pdf/2510.03571v1" target="_blank"><h2>Generalization of Graph Neural Network Models for Distribution Grid
  Fault Detection</h2></a><strong><u>Authors:</u></strong>  Burak Karabulut, Carlo Manna, Chris Develder</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.SY, eess.SY, I.2.6; I.2.7; C.2.1</br><strong><u>Comments:</u></strong> This paper has been submitted and accepted for IEEE SmartGridComm 2025</br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), convolutional (abstract), neural network (title, abstract), attention (abstract)</br><p><strong><u>Abstract:</u></strong> Fault detection in power distribution grids is critical for ensuring system
reliability and preventing costly outages. Moreover, fault detection
methodologies should remain robust to evolving grid topologies caused by
factors such as reconfigurations, equipment failures, and Distributed Energy
Resource (DER) integration. Current data-driven state-of-the-art methods use
Recurrent Neural Networks (RNNs) for temporal modeling and Graph Neural
Networks (GNNs) for spatial learning, in an RNN+GNN pipeline setting (RGNN in
short). Specifically, for power system fault diagnosis, Graph Convolutional
Networks (GCNs) have been adopted. Yet, various more advanced GNN architectures
have been proposed and adopted in domains outside of power systems. In this
paper, we set out to systematically and consistently benchmark various GNN
architectures in an RNN+GNN pipeline model. Specifically, to the best of our
knowledge, we are the first to (i) propose to use GraphSAGE and Graph Attention
(GAT, GATv2) in an RGNN for fault diagnosis, and (ii) provide a comprehensive
benchmark against earlier proposed RGNN solutions (RGCN) as well as pure RNN
models (especially Gated Recurrent Unit (GRU)), particularly (iii) exploring
their generalization potential for deployment in different settings than those
used for training them. Our experimental results on the IEEE 123-node
distribution network show that RGATv2 has superior generalization capabilities,
maintaining high performance with an F1-score reduction of $\sim$12% across
different topology settings. In contrast, pure RNN models largely fail,
experiencing an F1-score reduction of up to $\sim$60%, while other RGNN
variants also exhibit significant performance degradation, i.e., up to
$\sim$25% lower F1-scores.</p></br><a href="http://arxiv.org/pdf/2510.03911v1" target="_blank"><h2>THEMIS: Unlocking Pretrained Knowledge with Foundation Model Embeddings
  for Anomaly Detection in Time Series</h2></a><strong><u>Authors:</u></strong>  Yadav Mahesh Lorik, Kaushik Sarveswaran, Nagaraj Sundaramahalingam, Aravindakumar Venugopalan</br><strong><u>Categories:</u></strong> cs.LG</br><strong><u>Comments:</u></strong> Oral Presentation. AI4TS Workshop, IJCAI'25</br><strong><u>Matching Keywords:</u></strong> anomaly detection (title, abstract)</br><p><strong><u>Abstract:</u></strong> Time series anomaly detection forms a very crucial area in several domains
but poses substantial challenges. Due to time series data possessing
seasonality, trends, noise, and evolving patterns (concept drift), it becomes
very difficult to set a general notion of what constitutes normal behavior.
Anomalies themselves could be varied, ranging from a single outlier to
contextual or collective anomalies, and are normally very rare; hence, the
dataset is largely imbalanced. Additional layers of complexities arise due to
the problems of increased dimensionality of modern time series, real-time
detection criteria, setting up appropriate detection thresholds, and arriving
at results that are interpretable. To embrace these multifaceted challenges,
very strong, flexible, and interpretable approaches are required. This paper
presents THEMIS, a new framework for time series anomaly detection that
exploits pretrained knowledge from foundation models. THEMIS extracts
embeddings from the encoder of the Chronos time series foundation model and
applies outlier detection techniques like Local Outlier Factor and Spectral
Decomposition on the self-similarity matrix, to spot anomalies in the data. Our
experiments show that this modular method achieves SOTA results on the MSL
dataset and performs quite competitively on the SMAP and SWAT$^*$ datasets.
Notably, THEMIS exceeds models trained specifically for anomaly detection,
presenting hyperparameter robustness and interpretability by default. This
paper advocates for pretrained representations from foundation models for
performing efficient and adaptable anomaly detection for time series data.</p></br><a href="http://arxiv.org/pdf/2510.03605v1" target="_blank"><h2>Understanding the Role of Training Data in Test-Time Scaling</h2></a><strong><u>Authors:</u></strong>  Adel Javanmard, Baharan Mirzasoleiman, Vahab Mirrokni</br><strong><u>Categories:</u></strong> cs.AI, cs.LG, stat.ML</br><strong><u>Comments:</u></strong> 24 pages, 4 figures</br><strong><u>Matching Keywords:</u></strong> transformer (abstract)</br><p><strong><u>Abstract:</u></strong> Test-time scaling improves the reasoning capabilities of large language
models (LLMs) by allocating extra compute to generate longer Chains-of-Thoughts
(CoTs). This enables models to tackle more complex problem by breaking them
down into additional steps, backtracking, and correcting mistakes. Despite its
strong performance--demonstrated by OpenAI's o1 and DeepSeek R1, the conditions
in the training data under which long CoTs emerge, and when such long CoTs
improve the performance, remain unclear. In this paper, we study the
performance of test-time scaling for transformers trained on an in-context
weight prediction task for linear regression. Our analysis provides a
theoretical explanation for several intriguing observations: First, at any
fixed test error, increasing test-time compute allows us to reduce the number
of in-context examples (context length) in training prompts. Second, if the
skills required to solve a downstream task are not sufficiently present in the
training data, increasing test-time compute can harm performance. Finally, we
characterize task hardness via the smallest eigenvalue of its feature
covariance matrix and show that training on a diverse, relevant, and hard set
of tasks results in best performance for test-time scaling. We confirm our
findings with experiments on large, nonlinear transformer architectures.</p></br><a href="http://arxiv.org/pdf/2510.04602v1" target="_blank"><h2>Computing Wasserstein Barycenters through Gradient Flows</h2></a><strong><u>Authors:</u></strong>  Eduardo Fernandes Montesuma, Yassir Bendou, Mike Gartrell</br><strong><u>Categories:</u></strong> stat.ML, cs.AI, cs.LG</br><strong><u>Comments:</u></strong> 4 Figures, 3 Tables, under review</br><strong><u>Matching Keywords:</u></strong> domain adaptation (abstract)</br><p><strong><u>Abstract:</u></strong> Wasserstein barycenters provide a powerful tool for aggregating probability
measures, while leveraging the geometry of their ambient space. Existing
discrete methods suffer from poor scalability, as they require access to the
complete set of samples from input measures. We address this issue by recasting
the original barycenter problem as a gradient flow in the Wasserstein space.
Our approach offers two advantages. First, we achieve scalability by sampling
mini-batches from the input measures. Second, we incorporate functionals over
probability measures, which regularize the barycenter problem through internal,
potential, and interaction energies. We present two algorithms for empirical
and Gaussian mixture measures, providing convergence guarantees under the
Polyak-{\L}ojasiewicz inequality. Experimental validation on toy datasets and
domain adaptation benchmarks show that our methods outperform previous discrete
and neural net-based methods for computing Wasserstein barycenters.</p></br><a href="http://arxiv.org/pdf/2510.03929v1" target="_blank"><h2>Self-Speculative Masked Diffusions</h2></a><strong><u>Authors:</u></strong>  Andrew Campbell, Valentin De Bortoli, Jiaxin Shi, Arnaud Doucet</br><strong><u>Categories:</u></strong> stat.ML, cs.LG</br><strong><u>Comments:</u></strong> 32 pages, 7 figures, 3 tables</br><strong><u>Matching Keywords:</u></strong> neural network (abstract), transformer (abstract), attention (abstract)</br><p><strong><u>Abstract:</u></strong> We present self-speculative masked diffusions, a new class of masked
diffusion generative models for discrete data that require significantly fewer
function evaluations to generate samples. Standard masked diffusion models
predict factorized logits over currently masked positions. A number of masked
positions are then sampled, however, the factorization approximation means that
sampling too many positions in one go leads to poor sample quality. As a
result, many simulation steps and therefore neural network function evaluations
are required to generate high-quality data. We reduce the computational burden
by generating non-factorized predictions over masked positions. This is
achieved by modifying the final transformer attention mask from non-causal to
causal, enabling draft token generation and parallel validation via a novel,
model-integrated speculative sampling mechanism. This results in a
non-factorized predictive distribution over masked positions in a single
forward pass. We apply our method to GPT2 scale text modelling and protein
sequences generation, finding that we can achieve a ~2x reduction in the
required number of network forward passes relative to standard masked diffusion
models.</p></br><a href="http://arxiv.org/pdf/2510.03569v1" target="_blank"><h2>Longitudinal Flow Matching for Trajectory Modeling</h2></a><strong><u>Authors:</u></strong>  Mohammad Mohaiminul Islam, Thijs P. Kuipers, Sharvaree Vadgama, Coen de Vente, Afsana Khan, Clara I. Sánchez, Erik J. Bekkers</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.CV, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), sequential data (abstract)</br><p><strong><u>Abstract:</u></strong> Generative models for sequential data often struggle with sparsely sampled
and high-dimensional trajectories, typically reducing the learning of dynamics
to pairwise transitions. We propose \textit{Interpolative Multi-Marginal Flow
Matching} (IMMFM), a framework that learns continuous stochastic dynamics
jointly consistent with multiple observed time points. IMMFM employs a
piecewise-quadratic interpolation path as a smooth target for flow matching and
jointly optimizes drift and a data-driven diffusion coefficient, supported by a
theoretical condition for stable learning. This design captures intrinsic
stochasticity, handles irregular sparse sampling, and yields subject-specific
trajectories. Experiments on synthetic benchmarks and real-world longitudinal
neuroimaging datasets show that IMMFM outperforms existing methods in both
forecasting accuracy and further downstream tasks.</p></br><a href="http://arxiv.org/pdf/2510.03878v1" target="_blank"><h2>Multi-Modal Oral Cancer Detection Using Weighted Ensemble Convolutional
  Neural Networks</h2></a><strong><u>Authors:</u></strong>  Ajo Babu George, Sreehari J R Ajo Babu George, Sreehari J R Ajo Babu George, Sreehari J R</br><strong><u>Categories:</u></strong> cs.CV, cs.AI</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> convolutional (title, abstract), neural network (title, abstract), multimodal (abstract), multi-modal (title), transfer learning (abstract)</br><p><strong><u>Abstract:</u></strong> Aims Late diagnosis of Oral Squamous Cell Carcinoma (OSCC) contributes
significantly to its high global mortality rate, with over 50\% of cases
detected at advanced stages and a 5-year survival rate below 50\% according to
WHO statistics. This study aims to improve early detection of OSCC by
developing a multimodal deep learning framework that integrates clinical,
radiological, and histopathological images using a weighted ensemble of
DenseNet-121 convolutional neural networks (CNNs). Material and Methods A
retrospective study was conducted using publicly available datasets
representing three distinct medical imaging modalities. Each modality-specific
dataset was used to train a DenseNet-121 CNN via transfer learning.
Augmentation and modality-specific preprocessing were applied to increase
robustness. Predictions were fused using a validation-weighted ensemble
strategy. Evaluation was performed using accuracy, precision, recall, F1-score.
Results High validation accuracy was achieved for radiological (100\%) and
histopathological (95.12\%) modalities, with clinical images performing lower
(63.10\%) due to visual heterogeneity. The ensemble model demonstrated improved
diagnostic robustness with an overall accuracy of 84.58\% on a multimodal
validation dataset of 55 samples. Conclusion The multimodal ensemble framework
bridges gaps in the current diagnostic workflow by offering a non-invasive,
AI-assisted triage tool that enhances early identification of high-risk
lesions. It supports clinicians in decision-making, aligning with global
oncology guidelines to reduce diagnostic delays and improve patient outcomes.</p></br><a href="http://arxiv.org/pdf/2510.04957v1" target="_blank"><h2>Galaxy Model Subtraction with a Convolutional Denoising Autoencoder</h2></a><strong><u>Authors:</u></strong>  Rongrong Liu, Eric W. Peng, Kaixiang Wang, Laura Ferrarese, Patrick Côté</br><strong><u>Categories:</u></strong> astro-ph.IM, astro-ph.GA</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> convolutional (title, abstract)</br><p><strong><u>Abstract:</u></strong> Galaxy model subtraction removes the smooth light of nearby galaxies so that
fainter sources (e.g., stars, star clusters, background galaxies) can be
identified and measured. Traditional approaches (isophotal or parametric
fitting) are semi-automated and can be challenging for large data sets. We
build a convolutional denoising autoencoder (DAE) for galaxy model subtraction:
images are compressed to a latent representation and reconstructed to yield the
smooth galaxy, suppressing other objects. The DAE is trained on
GALFIT-generated model galaxies injected into real sky backgrounds and tested
on real images from the Next Generation Virgo Cluster Survey (NGVS). To
quantify performance, we conduct an injection-recovery experiment on residual
images by adding mock globular clusters (GCs) with known fluxes and positions.
Our tests confirm a higher recovery rate of mock GCs near galaxy centers for
complex morphologies, while matching ellipse fitting for smooth ellipticals.
Overall, the DAE achieves subtraction equivalent to isophotal ellipse fitting
for regular ellipticals and superior results for galaxies with high
ellipticities or spiral features. Photometry of small-scale sources on DAE
residuals is consistent with that on ellipse-subtracted residuals. Once
trained, the DAE processes an image cutout in $\lesssim 0.1$ s, enabling fast,
fully automatic analysis of large data sets. We make our code available for
download and use.</p></br><a href="http://arxiv.org/pdf/2510.04090v1" target="_blank"><h2>Using predefined vector systems as latent space configuration for neural
  network supervised training on data with arbitrarily large number of classes</h2></a><strong><u>Authors:</u></strong>  Nikita Gabdullin</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.CV</br><strong><u>Comments:</u></strong> 28 pages, 12 figures, 10 tables, 12 equations, 1 algorithm</br><strong><u>Matching Keywords:</u></strong> latent space (title, abstract), neural network (abstract), transformer (abstract)</br><p><strong><u>Abstract:</u></strong> Supervised learning (SL) methods are indispensable for neural network (NN)
training used to perform classification tasks. While resulting in very high
accuracy, SL training often requires making NN parameter number dependent on
the number of classes, limiting their applicability when the number of classes
is extremely large or unknown in advance. In this paper we propose a
methodology that allows one to train the same NN architecture regardless of the
number of classes. This is achieved by using predefined vector systems as the
target latent space configuration (LSC) during NN training. We discuss the
desired properties of target configurations and choose randomly perturbed
vectors of An root system for our experiments. These vectors are used to
successfully train encoders and visual transformers (ViT) on Cinic-10 and
ImageNet-1K in low- and high-dimensional cases by matching NN predictions with
the predefined vectors. Finally, ViT is trained on a dataset with 1.28 million
classes illustrating the applicability of the method to training on datasets
with extremely large number of classes. In addition, potential applications of
LSC in lifelong learning and NN distillation are discussed illustrating
versatility of the proposed methodology.</p></br><a href="http://arxiv.org/pdf/2510.04347v1" target="_blank"><h2>Unmasking Backdoors: An Explainable Defense via Gradient-Attention
  Anomaly Scoring for Pre-trained Language Models</h2></a><strong><u>Authors:</u></strong>  Anindya Sundar Das, Kangjie Chen, Monowar Bhuyan</br><strong><u>Categories:</u></strong> cs.CL, cs.LG</br><strong><u>Comments:</u></strong> 15 pages total (9 pages main text + 4 pages appendix + references), 12 figures, preprint version. The final version may differ</br><strong><u>Matching Keywords:</u></strong> explainable (title), attention (title, abstract)</br><p><strong><u>Abstract:</u></strong> Pre-trained language models have achieved remarkable success across a wide
range of natural language processing (NLP) tasks, particularly when fine-tuned
on large, domain-relevant datasets. However, they remain vulnerable to backdoor
attacks, where adversaries embed malicious behaviors using trigger patterns in
the training data. These triggers remain dormant during normal usage, but, when
activated, can cause targeted misclassifications. In this work, we investigate
the internal behavior of backdoored pre-trained encoder-based language models,
focusing on the consistent shift in attention and gradient attribution when
processing poisoned inputs; where the trigger token dominates both attention
and gradient signals, overriding the surrounding context. We propose an
inference-time defense that constructs anomaly scores by combining token-level
attention and gradient information. Extensive experiments on text
classification tasks across diverse backdoor attack scenarios demonstrate that
our method significantly reduces attack success rates compared to existing
baselines. Furthermore, we provide an interpretability-driven analysis of the
scoring mechanism, shedding light on trigger localization and the robustness of
the proposed defense.</p></br><a href="http://arxiv.org/pdf/2510.04217v1" target="_blank"><h2>MLLMEraser: Achieving Test-Time Unlearning in Multimodal Large Language
  Models through Activation Steering</h2></a><strong><u>Authors:</u></strong>  Chenlu Ding, Jiancan Wu, Leheng Sheng, Fan Zhang, Yancheng Yuan, Xiang Wang, Xiangnan He</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)</br><p><strong><u>Abstract:</u></strong> Multimodal large language models (MLLMs) have demonstrated remarkable
capabilities across vision-language tasks, yet their large-scale deployment
raises pressing concerns about memorized private data, outdated knowledge, and
harmful content. Existing unlearning approaches for MLLMs typically adapt
training-based strategies such as gradient ascent or preference optimization,
but these methods are computationally expensive, irreversible, and often
distort retained knowledge. In this work, we propose MLLMEraser, an
input-aware, training-free framework for test-time unlearning. Our approach
leverages activation steering to enable dynamic knowledge erasure without
parameter updates. Specifically, we construct a multimodal erasure direction by
contrasting adversarially perturbed, knowledge-recall image-text pairs with
knowledge-erasure counterparts, capturing both textual and visual
discrepancies. To prevent unnecessary interference, we further design an
input-aware steering mechanism that adaptively determines when and how the
erasure direction should be applied, preserving utility on retained knowledge
while enforcing forgetting on designated content. Experiments on LLaVA-1.5 and
Qwen-2.5-VL demonstrate that MLLMEraser consistently outperforms
state-of-the-art MLLM unlearning baselines, achieving stronger forgetting
performance with lower computational cost and minimal utility degradation.</p></br><a href="http://arxiv.org/pdf/2510.03923v1" target="_blank"><h2>On the Convergence and Size Transferability of Continuous-depth Graph
  Neural Networks</h2></a><strong><u>Authors:</u></strong>  Mingsong Yan, Charles Kulick, Sui Tang</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)</br><p><strong><u>Abstract:</u></strong> Continuous-depth graph neural networks, also known as Graph Neural
Differential Equations (GNDEs), combine the structural inductive bias of Graph
Neural Networks (GNNs) with the continuous-depth architecture of Neural ODEs,
offering a scalable and principled framework for modeling dynamics on graphs.
In this paper, we present a rigorous convergence analysis of GNDEs with
time-varying parameters in the infinite-node limit, providing theoretical
insights into their size transferability. To this end, we introduce Graphon
Neural Differential Equations (Graphon-NDEs) as the infinite-node limit of
GNDEs and establish their well-posedness. Leveraging tools from graphon theory
and dynamical systems, we prove the trajectory-wise convergence of GNDE
solutions to Graphon-NDE solutions. Moreover, we derive explicit convergence
rates under two deterministic graph sampling regimes: (1) weighted graphs
sampled from smooth graphons, and (2) unweighted graphs sampled from
$\{0,1\}$-valued (discontinuous) graphons. We further establish size
transferability bounds, providing theoretical justification for the practical
strategy of transferring GNDE models trained on moderate-sized graphs to
larger, structurally similar graphs without retraining. Numerical experiments
using synthetic and real data support our theoretical findings.</p></br><a href="http://arxiv.org/pdf/2510.03989v1" target="_blank"><h2>A Mathematical Explanation of Transformers for Large Language Models and
  GPTs</h2></a><strong><u>Authors:</u></strong>  Xue-Cheng Tai, Hao Liu, Lingfeng Li, Raymond H. Chan</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.NA, math.NA</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> neural network (abstract), transformer (title, abstract), attention (abstract)</br><p><strong><u>Abstract:</u></strong> The Transformer architecture has revolutionized the field of sequence
modeling and underpins the recent breakthroughs in large language models
(LLMs). However, a comprehensive mathematical theory that explains its
structure and operations remains elusive. In this work, we propose a novel
continuous framework that rigorously interprets the Transformer as a
discretization of a structured integro-differential equation. Within this
formulation, the self-attention mechanism emerges naturally as a non-local
integral operator, and layer normalization is characterized as a projection to
a time-dependent constraint. This operator-theoretic and variational
perspective offers a unified and interpretable foundation for understanding the
architecture's core components, including attention, feedforward layers, and
normalization. Our approach extends beyond previous theoretical analyses by
embedding the entire Transformer operation in continuous domains for both token
indices and feature dimensions. This leads to a principled and flexible
framework that not only deepens theoretical insight but also offers new
directions for architecture design, analysis, and control-based
interpretations. This new interpretation provides a step toward bridging the
gap between deep learning architectures and continuous mathematical modeling,
and contributes a foundational perspective to the ongoing development of
interpretable and theoretically grounded neural network models.</p></br><a href="http://arxiv.org/pdf/2510.03727v1" target="_blank"><h2>Bridging the Gap Between Multimodal Foundation Models and World Models</h2></a><strong><u>Authors:</u></strong>  Xuehai He</br><strong><u>Categories:</u></strong> cs.AI, cs.CL, cs.CV, cs.LG</br><strong><u>Comments:</u></strong> PhD thesis</br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)</br><p><strong><u>Abstract:</u></strong> Humans understand the world through the integration of multiple sensory
modalities, enabling them to perceive, reason about, and imagine dynamic
physical processes. Inspired by this capability, multimodal foundation models
(MFMs) have emerged as powerful tools for multimodal understanding and
generation. However, today's MFMs fall short of serving as effective world
models. They lack the essential ability such as perform counterfactual
reasoning, simulate dynamics, understand the spatiotemporal information,
control generated visual outcomes, and perform multifaceted reasoning. We
investigates what it takes to bridge the gap between multimodal foundation
models and world models. We begin by improving the reasoning capabilities of
MFMs through discriminative tasks and equipping MFMs with structured reasoning
skills, such as causal inference, counterfactual thinking, and spatiotemporal
reasoning, enabling them to go beyond surface correlations and understand
deeper relationships within visual and textual data. Next, we explore
generative capabilities of multimodal foundation models across both image and
video modalities, introducing new frameworks for structured and controllable
generation. Our approaches incorporate scene graphs, multimodal conditioning,
and multimodal alignment strategies to guide the generation process, ensuring
consistency with high-level semantics and fine-grained user intent. We further
extend these techniques to controllable 4D generation, enabling interactive,
editable, and morphable object synthesis over time and space.</p></br><a href="http://arxiv.org/pdf/2510.05102v1" target="_blank"><h2>TopInG: Topologically Interpretable Graph Learning via Persistent
  Rationale Filtration</h2></a><strong><u>Authors:</u></strong>  Cheng Xin, Fan Xu, Xin Ding, Jie Gao, Jiaxin Ding</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.CG, math.AT, stat.ML, 55N31, 68T05, 62R40, 05C, 68R05, I.2.6; G.2.2; I.5.1</br><strong><u>Comments:</u></strong> submitted to ICML 2025</br><strong><u>Matching Keywords:</u></strong> neural network (abstract)</br><p><strong><u>Abstract:</u></strong> Graph Neural Networks (GNNs) have shown remarkable success across various
scientific fields, yet their adoption in critical decision-making is often
hindered by a lack of interpretability. Recently, intrinsically interpretable
GNNs have been studied to provide insights into model predictions by
identifying rationale substructures in graphs. However, existing methods face
challenges when the underlying rationale subgraphs are complex and varied. In
this work, we propose TopInG: Topologically Interpretable Graph Learning, a
novel topological framework that leverages persistent homology to identify
persistent rationale subgraphs. TopInG employs a rationale filtration learning
approach to model an autoregressive generation process of rationale subgraphs,
and introduces a self-adjusted topological constraint, termed topological
discrepancy, to enforce a persistent topological distinction between rationale
subgraphs and irrelevant counterparts. We provide theoretical guarantees that
our loss function is uniquely optimized by the ground truth under specific
conditions. Extensive experiments demonstrate TopInG's effectiveness in
tackling key challenges, such as handling variform rationale subgraphs,
balancing predictive performance with interpretability, and mitigating spurious
correlations. Results show that our approach improves upon state-of-the-art
methods on both predictive accuracy and interpretation quality.</p></br><a href="http://arxiv.org/pdf/2510.04573v1" target="_blank"><h2>LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning</h2></a><strong><u>Authors:</u></strong>  Haoqiang Kang, Yizhe Zhang, Nikki Lijing Kuang, Nicklas Majamaki, Navdeep Jaitly, Yi-An Ma, Lianhui Qin</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.CL</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> variational autoencoder (abstract), VAE (abstract), attention (abstract)</br><p><strong><u>Abstract:</u></strong> Large Language Models (LLMs) demonstrate their reasoning ability through
chain-of-thought (CoT) generation. However, LLM's autoregressive decoding may
limit the ability to revisit and refine earlier tokens in a holistic manner,
which can also lead to inefficient exploration for diverse solutions. In this
paper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoning
framework that unifies the expressiveness of continuous latent representation
with the iterative refinement capabilities of latent diffusion models for an
existing LLM. We first construct a structured latent reasoning space using a
Variational Autoencoder (VAE) that encodes text reasoning steps into blocks of
thought tokens, preserving semantic information and interpretability while
offering compact but expressive representations. Subsequently, we utilize a
latent diffusion model that learns to denoise a block of latent thought tokens
with a blockwise bidirectional attention mask, enabling longer horizon and
iterative refinement with adaptive test-time compute. This design allows
efficient parallel generation of diverse reasoning trajectories, allowing the
model to plan and revise the reasoning process holistically. We conduct
evaluations on a suite of mathematical reasoning and planning benchmarks.
Empirical results show that LaDiR consistently improves accuracy, diversity,
and interpretability over existing autoregressive, diffusion-based, and latent
reasoning methods, revealing a new paradigm for text reasoning with latent
diffusion.</p></br><a href="http://arxiv.org/pdf/2510.04710v1" target="_blank"><h2>ViTs: Teaching Machines to See Time Series Anomalies Like Human Experts</h2></a><strong><u>Authors:</u></strong>  Zexin Wang, Changhua Pei, Yang Liu, Hengyue Jiang, Quan Zhou, Haotian Si, Hang Cui, Jianhui Li, Gaogang Xie, Jingjing Li, Dan Pei</br><strong><u>Categories:</u></strong> cs.LG</br><strong><u>Comments:</u></strong> 13 pages</br><strong><u>Matching Keywords:</u></strong> anomaly detection (abstract)</br><p><strong><u>Abstract:</u></strong> Web service administrators must ensure the stability of multiple systems by
promptly detecting anomalies in Key Performance Indicators (KPIs). Achieving
the goal of "train once, infer across scenarios" remains a fundamental
challenge for time series anomaly detection models. Beyond improving zero-shot
generalization, such models must also flexibly handle sequences of varying
lengths during inference, ranging from one hour to one week, without
retraining. Conventional approaches rely on sliding-window encoding and
self-supervised learning, which restrict inference to fixed-length inputs.
Large Language Models (LLMs) have demonstrated remarkable zero-shot
capabilities across general domains. However, when applied to time series data,
they face inherent limitations due to context length. To address this issue, we
propose ViTs, a Vision-Language Model (VLM)-based framework that converts time
series curves into visual representations. By rescaling time series images,
temporal dependencies are preserved while maintaining a consistent input size,
thereby enabling efficient processing of arbitrarily long sequences without
context constraints. Training VLMs for this purpose introduces unique
challenges, primarily due to the scarcity of aligned time series image-text
data. To overcome this, we employ an evolutionary algorithm to automatically
generate thousands of high-quality image-text pairs and design a three-stage
training pipeline consisting of: (1) time series knowledge injection, (2)
anomaly detection enhancement, and (3) anomaly reasoning refinement. Extensive
experiments demonstrate that ViTs substantially enhance the ability of VLMs to
understand and detect anomalies in time series data. All datasets and code will
be publicly released at: https://anonymous.4open.science/r/ViTs-C484/.</p></br><a href="http://arxiv.org/pdf/2510.04375v1" target="_blank"><h2>Adaptive Weighted Loss for Sequential Recommendations on Sparse Domains</h2></a><strong><u>Authors:</u></strong>  Akshay Mittal, Vinay Venkatesh, Krishna Kandi, Shalini Sudarshan</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)</br><p><strong><u>Abstract:</u></strong> The effectiveness of single-model sequential recommendation architectures,
while scalable, is often limited when catering to "power users" in sparse or
niche domains. Our previous research, PinnerFormerLite, addressed this by using
a fixed weighted loss to prioritize specific domains. However, this approach
can be sub-optimal, as a single, uniform weight may not be sufficient for
domains with very few interactions, where the training signal is easily diluted
by the vast, generic dataset.
  This paper proposes a novel, data-driven approach: a Dynamic Weighted Loss
function with comprehensive theoretical foundations and extensive empirical
validation. We introduce an adaptive algorithm that adjusts the loss weight for
each domain based on its sparsity in the training data, assigning a higher
weight to sparser domains and a lower weight to denser ones. This ensures that
even rare user interests contribute a meaningful gradient signal, preventing
them from being overshadowed.
  We provide rigorous theoretical analysis including convergence proofs,
complexity analysis, and bounds analysis to establish the stability and
efficiency of our approach. Our comprehensive empirical validation across four
diverse datasets (MovieLens, Amazon Electronics, Yelp Business, LastFM Music)
with state-of-the-art baselines (SIGMA, CALRec, SparseEnNet) demonstrates that
this dynamic weighting system significantly outperforms all comparison methods,
particularly for sparse domains, achieving substantial lifts in key metrics
like Recall at 10 and NDCG at 10 while maintaining performance on denser
domains and introducing minimal computational overhead.</p></br><a href="http://arxiv.org/pdf/2510.04983v1" target="_blank"><h2>AWARE, Beyond Sentence Boundaries: A Contextual Transformer Framework
  for Identifying Cultural Capital in STEM Narratives</h2></a><strong><u>Authors:</u></strong>  Khalid Mehtab Khan, Anagha Kulkarni</br><strong><u>Categories:</u></strong> cs.CL, cs.AI, cs.CY, cs.LG</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract)</br><p><strong><u>Abstract:</u></strong> Identifying cultural capital (CC) themes in student reflections can offer
valuable insights that help foster equitable learning environments in
classrooms. However, themes such as aspirational goals or family support are
often woven into narratives, rather than appearing as direct keywords. This
makes them difficult to detect for standard NLP models that process sentences
in isolation. The core challenge stems from a lack of awareness, as standard
models are pre-trained on general corpora, leaving them blind to the
domain-specific language and narrative context inherent to the data. To address
this, we introduce AWARE, a framework that systematically attempts to improve a
transformer model's awareness for this nuanced task. AWARE has three core
components: 1) Domain Awareness, adapting the model's vocabulary to the
linguistic style of student reflections; 2) Context Awareness, generating
sentence embeddings that are aware of the full essay context; and 3) Class
Overlap Awareness, employing a multi-label strategy to recognize the
coexistence of themes in a single sentence. Our results show that by making the
model explicitly aware of the properties of the input, AWARE outperforms a
strong baseline by 2.1 percentage points in Macro-F1 and shows considerable
improvements across all themes. This work provides a robust and generalizable
methodology for any text classification task in which meaning depends on the
context of the narrative.</p></br></body>