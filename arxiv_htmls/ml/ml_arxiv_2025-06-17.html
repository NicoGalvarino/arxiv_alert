<link href='https://fonts.googleapis.com/css?family=Montserrat' rel='stylesheet'><style>     body {font-family: 'Montserrat'; background: #F3F3F3; width: 740px; margin: 0 auto; line-height: 150%; margin-top: 50px; font-size: 15px}         h1 {font-size: 70px}         a {color: #45ABC2}         em {font-size: 120%} </style><h1><center>ArXiv Alert</center></h1><font color='#DDAD5C'><em>Update: from 13 Jun 2025 to 17 Jun 2025</em></font><a href="http://arxiv.org/pdf/2506.13628v1" target="_blank"><h2>Graph-Convolution-Beta-VAE for Synthetic Abdominal Aorta Aneurysm
  Generation</h2></a><strong><u>Authors:</u></strong>  Francesco Fabbri, Martino Andrea Scarpolini, Angelo Iollo, Francesco Viola, Francesco Tudisco</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, q-bio.TO</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Synthetic data generation plays a crucial role in medical research by
mitigating privacy concerns and enabling large-scale patient data analysis.
This study presents a beta-Variational Autoencoder Graph Convolutional Neural
Network framework for generating synthetic Abdominal Aorta Aneurysms (AAA).
Using a small real-world dataset, our approach extracts key anatomical features
and captures complex statistical relationships within a compact disentangled
latent space. To address data limitations, low-impact data augmentation based
on Procrustes analysis was employed, preserving anatomical integrity. The
generation strategies, both deterministic and stochastic, manage to enhance
data diversity while ensuring realism. Compared to PCA-based approaches, our
model performs more robustly on unseen data by capturing complex, nonlinear
anatomical variations. This enables more comprehensive clinical and statistical
analyses than the original dataset alone. The resulting synthetic AAA dataset
preserves patient privacy while providing a scalable foundation for medical
research, device testing, and computational modeling.</p></br><a href="http://arxiv.org/pdf/2506.12389v1" target="_blank"><h2>Revisiting Clustering of Neural Bandits: Selective Reinitialization for
  Mitigating Loss of Plasticity</h2></a><strong><u>Authors:</u></strong>  Zhiyuan Su, Sunhao Dai, Xiao Zhang</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML, I.2.m</br><strong><u>Comments:</u></strong> Accepted by KDD 2025</br><p><strong><u>Abstract:</u></strong> Clustering of Bandits (CB) methods enhance sequential decision-making by
grouping bandits into clusters based on similarity and incorporating
cluster-level contextual information, demonstrating effectiveness and
adaptability in applications like personalized streaming recommendations.
However, when extending CB algorithms to their neural version (commonly
referred to as Clustering of Neural Bandits, or CNB), they suffer from loss of
plasticity, where neural network parameters become rigid and less adaptable
over time, limiting their ability to adapt to non-stationary environments
(e.g., dynamic user preferences in recommendation). To address this challenge,
we propose Selective Reinitialization (SeRe), a novel bandit learning framework
that dynamically preserves the adaptability of CNB algorithms in evolving
environments. SeRe leverages a contribution utility metric to identify and
selectively reset underutilized units, mitigating loss of plasticity while
maintaining stable knowledge retention. Furthermore, when combining SeRe with
CNB algorithms, the adaptive change detection mechanism adjusts the
reinitialization frequency according to the degree of non-stationarity,
ensuring effective adaptation without unnecessary resets. Theoretically, we
prove that SeRe enables sublinear cumulative regret in piecewise-stationary
environments, outperforming traditional CNB approaches in long-term
performances. Extensive experiments on six real-world recommendation datasets
demonstrate that SeRe-enhanced CNB algorithms can effectively mitigate the loss
of plasticity with lower regrets, improving adaptability and robustness in
dynamic settings.</p></br><a href="http://arxiv.org/pdf/2506.13060v1" target="_blank"><h2>Rethinking Explainability in the Era of Multimodal AI</h2></a><strong><u>Authors:</u></strong>  Chirag Agarwal</br><strong><u>Categories:</u></strong> cs.AI, cs.LG</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> While multimodal AI systems (models jointly trained on heterogeneous data
types such as text, time series, graphs, and images) have become ubiquitous and
achieved remarkable performance across high-stakes applications, transparent
and accurate explanation algorithms are crucial for their safe deployment and
ensure user trust. However, most existing explainability techniques remain
unimodal, generating modality-specific feature attributions, concepts, or
circuit traces in isolation and thus failing to capture cross-modal
interactions. This paper argues that such unimodal explanations systematically
misrepresent and fail to capture the cross-modal influence that drives
multimodal model decisions, and the community should stop relying on them for
interpreting multimodal models. To support our position, we outline key
principles for multimodal explanations grounded in modality: Granger-style
modality influence (controlled ablations to quantify how removing one modality
changes the explanation for another), Synergistic faithfulness (explanations
capture the model's predictive power when modalities are combined), and Unified
stability (explanations remain consistent under small, cross-modal
perturbations). This targeted shift to multimodal explanations will help the
community uncover hidden shortcuts, mitigate modality bias, improve model
reliability, and enhance safety in high-stakes settings where incomplete
explanations can have serious consequences.</p></br><a href="http://arxiv.org/pdf/2506.12230v1" target="_blank"><h2>Statistical Machine Learning for Astronomy -- A Textbook</h2></a><strong><u>Authors:</u></strong>  Yuan-Sen Ting</br><strong><u>Categories:</u></strong> astro-ph.IM, cs.LG, stat.AP, stat.ML</br><strong><u>Comments:</u></strong> 677 pages, 152 figures. Code and tutorials available atthis https URL</br><p><strong><u>Abstract:</u></strong> This textbook provides a systematic treatment of statistical machine learning
for astronomical research through the lens of Bayesian inference, developing a
unified framework that reveals connections between modern data analysis
techniques and traditional statistical methods. We show how these techniques
emerge from familiar statistical foundations. The consistently Bayesian
perspective prioritizes uncertainty quantification and statistical rigor
essential for scientific inference in astronomy. The textbook progresses from
probability theory and Bayesian inference through supervised learning including
linear regression with measurement uncertainties, logistic regression, and
classification. Unsupervised learning topics cover Principal Component Analysis
and clustering methods. We then introduce computational techniques through
sampling and Markov Chain Monte Carlo, followed by Gaussian Processes as
probabilistic nonparametric methods and neural networks within the broader
statistical context. Our theory-focused pedagogical approach derives each
method from first principles with complete mathematical development,
emphasizing statistical insight and complementing with astronomical
applications. We prioritize understanding why algorithms work, when they are
appropriate, and how they connect to broader statistical principles. The
treatment builds toward modern techniques including neural networks through a
solid foundation in classical methods and their theoretical underpinnings. This
foundation enables thoughtful application of these methods to astronomical
research, ensuring proper consideration of assumptions, limitations, and
uncertainty propagation essential for advancing astronomical knowledge in the
era of large astronomical surveys.</p></br><a href="http://arxiv.org/pdf/2506.12809v1" target="_blank"><h2>A Review of the Long Horizon Forecasting Problem in Time Series Analysis</h2></a><strong><u>Authors:</u></strong>  Hans Krupakar, Kandappan V A</br><strong><u>Categories:</u></strong> cs.LG, cs.ET, cs.PF, stat.ML</br><strong><u>Comments:</u></strong> Submitted to International Journal of Forecasting</br><p><strong><u>Abstract:</u></strong> The long horizon forecasting (LHF) problem has come up in the time series
literature for over the last 35 years or so. This review covers aspects of LHF
in this period and how deep learning has incorporated variants of trend,
seasonality, fourier and wavelet transforms, misspecification bias reduction
and bandpass filters while contributing using convolutions, residual
connections, sparsity reduction, strided convolutions, attention masks, SSMs,
normalization methods, low-rank approximations and gating mechanisms. We
highlight time series decomposition techniques, input data preprocessing and
dataset windowing schemes that improve performance. Multi-layer perceptron
models, recurrent neural network hybrids, self-attention models that improve
and/or address the performances of the LHF problem are described, with an
emphasis on the feature space construction. Ablation studies are conducted over
the ETTm2 dataset in the multivariate and univariate high useful load (HUFL)
forecasting contexts, evaluated over the last 4 months of the dataset. The
heatmaps of MSE averages per time step over test set series in the horizon show
that there is a steady increase in the error proportionate to its length except
with xLSTM and Triformer models and motivate LHF as an error propagation
problem. The trained models are available here: https://bit.ly/LHFModelZoo</p></br><a href="http://arxiv.org/pdf/2506.12468v1" target="_blank"><h2>Delving into Instance-Dependent Label Noise in Graph Data: A
  Comprehensive Study and Benchmark</h2></a><strong><u>Authors:</u></strong>  Suyeon Kim, SeongKu Kang, Dongwoo Kim, Jungseul Ok, Hwanjo Yu</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML</br><strong><u>Comments:</u></strong> 17 pages</br><p><strong><u>Abstract:</u></strong> Graph Neural Networks (GNNs) have achieved state-of-the-art performance in
node classification tasks but struggle with label noise in real-world data.
Existing studies on graph learning with label noise commonly rely on
class-dependent label noise, overlooking the complexities of instance-dependent
noise and falling short of capturing real-world corruption patterns. We
introduce BeGIN (Benchmarking for Graphs with Instance-dependent Noise), a new
benchmark that provides realistic graph datasets with various noise types and
comprehensively evaluates noise-handling strategies across GNN architectures,
noisy label detection, and noise-robust learning. To simulate
instance-dependent corruptions, BeGIN introduces algorithmic methods and
LLM-based simulations. Our experiments reveal the challenges of
instance-dependent noise, particularly LLM-based corruption, and underscore the
importance of node-specific parameterization to enhance GNN robustness. By
comprehensively evaluating noise-handling strategies, BeGIN provides insights
into their effectiveness, efficiency, and key performance factors. We expect
that BeGIN will serve as a valuable resource for advancing research on label
noise in graphs and fostering the development of robust GNN training methods.
The code is available at https://github.com/kimsu55/BeGIN.</p></br><a href="http://arxiv.org/pdf/2506.12227v1" target="_blank"><h2>Uncovering Bias Paths with LLM-guided Causal Discovery: An Active
  Learning and Dynamic Scoring Approach</h2></a><strong><u>Authors:</u></strong>  Khadija Zanna, Akane Sano</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML, F.2.2, I.2.7</br><strong><u>Comments:</u></strong> Submitted to AIES Conference</br><p><strong><u>Abstract:</u></strong> Causal discovery (CD) plays a pivotal role in understanding the mechanisms
underlying complex systems. While recent algorithms can detect spurious
associations and latent confounding, many struggle to recover fairness-relevant
pathways in realistic, noisy settings. Large Language Models (LLMs), with their
access to broad semantic knowledge, offer a promising complement to statistical
CD approaches, particularly in domains where metadata provides meaningful
relational cues. Ensuring fairness in machine learning requires understanding
how sensitive attributes causally influence outcomes, yet CD methods often
introduce spurious or biased pathways. We propose a hybrid LLM-based framework
for CD that extends a breadth-first search (BFS) strategy with active learning
and dynamic scoring. Variable pairs are prioritized for LLM-based querying
using a composite score based on mutual information, partial correlation, and
LLM confidence, improving discovery efficiency and robustness.
  To evaluate fairness sensitivity, we construct a semi-synthetic benchmark
from the UCI Adult dataset, embedding a domain-informed causal graph with
injected noise, label corruption, and latent confounding. We assess how well CD
methods recover both global structure and fairness-critical paths.
  Our results show that LLM-guided methods, including the proposed method,
demonstrate competitive or superior performance in recovering such pathways
under noisy conditions. We highlight when dynamic scoring and active querying
are most beneficial and discuss implications for bias auditing in real-world
datasets.</p></br><a href="http://arxiv.org/pdf/2506.11869v1" target="_blank"><h2>How do Probabilistic Graphical Models and Graph Neural Networks Look at
  Network Data?</h2></a><strong><u>Authors:</u></strong>  Michela Lapenna, Caterina De Bacco</br><strong><u>Categories:</u></strong> stat.ML, cs.AI, cs.LG, math-ph, math.MP</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Graphs are a powerful data structure for representing relational data and are
widely used to describe complex real-world systems. Probabilistic Graphical
Models (PGMs) and Graph Neural Networks (GNNs) can both leverage
graph-structured data, but their inherent functioning is different. The
question is how do they compare in capturing the information contained in
networked datasets? We address this objective by solving a link prediction task
and we conduct three main experiments, on both synthetic and real networks: one
focuses on how PGMs and GNNs handle input features, while the other two
investigate their robustness to noisy features and increasing heterophily of
the graph. PGMs do not necessarily require features on nodes, while GNNs cannot
exploit the network edges alone, and the choice of input features matters. We
find that GNNs are outperformed by PGMs when input features are low-dimensional
or noisy, mimicking many real scenarios where node attributes might be scalar
or noisy. Then, we find that PGMs are more robust than GNNs when the
heterophily of the graph is increased. Finally, to assess performance beyond
prediction tasks, we also compare the two frameworks in terms of their
computational complexity and interpretability.</p></br><a href="http://arxiv.org/pdf/2506.12197v1" target="_blank"><h2>Graph Semi-Supervised Learning for Point Classification on Data
  Manifolds</h2></a><strong><u>Authors:</u></strong>  Caio F. Deberaldini Netto, Zhiyang Wang, Luana Ruiz</br><strong><u>Categories:</u></strong> cs.LG, eess.SP, stat.ML</br><strong><u>Comments:</u></strong> 26 pages</br><p><strong><u>Abstract:</u></strong> We propose a graph semi-supervised learning framework for classification
tasks on data manifolds. Motivated by the manifold hypothesis, we model data as
points sampled from a low-dimensional manifold $\mathcal{M} \subset
\mathbb{R}^F$. The manifold is approximated in an unsupervised manner using a
variational autoencoder (VAE), where the trained encoder maps data to
embeddings that represent their coordinates in $\mathbb{R}^F$. A geometric
graph is constructed with Gaussian-weighted edges inversely proportional to
distances in the embedding space, transforming the point classification problem
into a semi-supervised node classification task on the graph. This task is
solved using a graph neural network (GNN). Our main contribution is a
theoretical analysis of the statistical generalization properties of this
data-to-manifold-to-graph pipeline. We show that, under uniform sampling from
$\mathcal{M}$, the generalization gap of the semi-supervised task diminishes
with increasing graph size, up to the GNN training error. Leveraging a training
procedure which resamples a slightly larger graph at regular intervals during
training, we then show that the generalization gap can be reduced even further,
vanishing asymptotically. Finally, we validate our findings with numerical
experiments on image classification benchmarks, demonstrating the empirical
effectiveness of our approach.</p></br><a href="http://arxiv.org/pdf/2506.13717v1" target="_blank"><h2>Contrastive Self-Supervised Learning As Neural Manifold Packing</h2></a><strong><u>Authors:</u></strong>  Guanming Zhang, David J. Heeger, Stefano Martiniani</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, q-bio.NC, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Contrastive self-supervised learning based on point-wise comparisons has been
widely studied for vision tasks. In the visual cortex of the brain, neuronal
responses to distinct stimulus classes are organized into geometric structures
known as neural manifolds. Accurate classification of stimuli can be achieved
by effectively separating these manifolds, akin to solving a packing problem.
We introduce Contrastive Learning As Manifold Packing (CLAMP), a
self-supervised framework that recasts representation learning as a manifold
packing problem. CLAMP introduces a loss function inspired by the potential
energy of short-range repulsive particle systems, such as those encountered in
the physics of simple liquids and jammed packings. In this framework, each
class consists of sub-manifolds embedding multiple augmented views of a single
image. The sizes and positions of the sub-manifolds are dynamically optimized
by following the gradient of a packing loss. This approach yields interpretable
dynamics in the embedding space that parallel jamming physics, and introduces
geometrically meaningful hyperparameters within the loss function. Under the
standard linear evaluation protocol, which freezes the backbone and trains only
a linear classifier, CLAMP achieves competitive performance with
state-of-the-art self-supervised models. Furthermore, our analysis reveals that
neural manifolds corresponding to different categories emerge naturally and are
effectively separated in the learned representation space, highlighting the
potential of CLAMP to bridge insights from physics, neural science, and machine
learning.</p></br><a href="http://arxiv.org/pdf/2506.11465v1" target="_blank"><h2>RollingQ: Reviving the Cooperation Dynamics in Multimodal Transformer</h2></a><strong><u>Authors:</u></strong>  Haotian Ni, Yake Wei, Hang Liu, Gong Chen, Chong Peng, Hao Lin, Di Hu</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.CV</br><strong><u>Comments:</u></strong> Accepted by ICML 2025</br><p><strong><u>Abstract:</u></strong> Multimodal learning faces challenges in effectively fusing information from
diverse modalities, especially when modality quality varies across samples.
Dynamic fusion strategies, such as attention mechanism in Transformers, aim to
address such challenge by adaptively emphasizing modalities based on the
characteristics of input data. However, through amounts of carefully designed
experiments, we surprisingly observed that the dynamic adaptability of
widely-used self-attention models diminishes. Model tends to prefer one
modality regardless of data characteristics. This bias triggers a
self-reinforcing cycle that progressively overemphasizes the favored modality,
widening the distribution gap in attention keys across modalities and
deactivating attention mechanism's dynamic properties. To revive adaptability,
we propose a simple yet effective method Rolling Query (RollingQ), which
balances attention allocation by rotating the query to break the
self-reinforcing cycle and mitigate the key distribution gap. Extensive
experiments on various multimodal scenarios validate the effectiveness of
RollingQ and the restoration of cooperation dynamics is pivotal for enhancing
the broader capabilities of widely deployed multimodal Transformers. The source
code is available at https://github.com/GeWu-Lab/RollingQ_ICML2025.</p></br><a href="http://arxiv.org/pdf/2506.13318v1" target="_blank"><h2>Vine Copulas as Differentiable Computational Graphs</h2></a><strong><u>Authors:</u></strong>  Tuoyuan Cheng, Thibault Vatter, Thomas Nagler, Kan Chen</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Vine copulas are sophisticated models for multivariate distributions and are
increasingly used in machine learning. To facilitate their integration into
modern ML pipelines, we introduce the vine computational graph, a DAG that
abstracts the multilevel vine structure and associated computations. On this
foundation, we devise new algorithms for conditional sampling, efficient
sampling-order scheduling, and constructing vine structures for customized
conditioning variables. We implement these ideas in torchvinecopulib, a
GPU-accelerated Python library built upon PyTorch, delivering improved
scalability for fitting, sampling, and density evaluation. Our experiments
illustrate how gradient flowing through the vine can improve Vine Copula
Autoencoders and that incorporating vines for uncertainty quantification in
deep learning can outperform MC-dropout, deep ensembles, and Bayesian Neural
Networks in sharpness, calibration, and runtime. By recasting vine copula
models as computational graphs, our work connects classical dependence modeling
with modern deep-learning toolchains and facilitates the integration of
state-of-the-art copula methods in modern machine learning pipelines.</p></br><a href="http://arxiv.org/pdf/2506.13277v1" target="_blank"><h2>SeqPE: Transformer with Sequential Position Encoding</h2></a><strong><u>Authors:</u></strong>  Huyang Li, Yahui Liu, Hongyu Sun, Deng Cai, Leyang Cui, Wei Bi, Peilin Zhao, Taro Watanabe</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.CL, cs.CV</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Since self-attention layers in Transformers are permutation invariant by
design, positional encodings must be explicitly incorporated to enable spatial
understanding. However, fixed-size lookup tables used in traditional learnable
position embeddings (PEs) limit extrapolation capabilities beyond pre-trained
sequence lengths. Expert-designed methods such as ALiBi and RoPE, mitigate this
limitation but demand extensive modifications for adapting to new modalities,
underscoring fundamental challenges in adaptability and scalability. In this
work, we present SeqPE, a unified and fully learnable position encoding
framework that represents each $n$-dimensional position index as a symbolic
sequence and employs a lightweight sequential position encoder to learn their
embeddings in an end-to-end manner. To regularize SeqPE's embedding space, we
introduce two complementary objectives: a contrastive objective that aligns
embedding distances with a predefined position-distance function, and a
knowledge distillation loss that anchors out-of-distribution position
embeddings to in-distribution teacher representations, further enhancing
extrapolation performance. Experiments across language modeling, long-context
question answering, and 2D image classification demonstrate that SeqPE not only
surpasses strong baselines in perplexity, exact match (EM), and
accuracy--particularly under context length extrapolation--but also enables
seamless generalization to multi-dimensional inputs without requiring manual
architectural redesign. We release our code, data, and checkpoints at
https://github.com/ghrua/seqpe.</p></br><a href="http://arxiv.org/pdf/2506.12965v1" target="_blank"><h2>Distributional Training Data Attribution</h2></a><strong><u>Authors:</u></strong>  Bruno Mlodozeniec, Isaac Reid, Sam Power, David Krueger, Murat Erdogdu, Richard E. Turner, Roger Grosse</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Randomness is an unavoidable part of training deep learning models, yet
something that traditional training data attribution algorithms fail to
rigorously account for. They ignore the fact that, due to stochasticity in the
initialisation and batching, training on the same dataset can yield different
models. In this paper, we address this shortcoming through introducing
distributional training data attribution (d-TDA), the goal of which is to
predict how the distribution of model outputs (over training runs) depends upon
the dataset. We demonstrate the practical significance of d-TDA in experiments,
e.g. by identifying training examples that drastically change the distribution
of some target measurement without necessarily changing the mean. Intriguingly,
we also find that influence functions (IFs), a popular but poorly-understood
data attribution tool, emerge naturally from our distributional framework as
the limit to unrolled differentiation; without requiring restrictive convexity
assumptions. This provides a new mathematical motivation for their efficacy in
deep learning, and helps to characterise their limitations.</p></br><a href="http://arxiv.org/pdf/2506.12529v1" target="_blank"><h2>Similarity as Reward Alignment: Robust and Versatile Preference-based
  Reinforcement Learning</h2></a><strong><u>Authors:</u></strong>  Sara Rajaram, R. James Cotton, Fabian H. Sinz</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Preference-based Reinforcement Learning (PbRL) entails a variety of
approaches for aligning models with human intent to alleviate the burden of
reward engineering. However, most previous PbRL work has not investigated the
robustness to labeler errors, inevitable with labelers who are non-experts or
operate under time constraints. Additionally, PbRL algorithms often target very
specific settings (e.g. pairwise ranked preferences or purely offline
learning). We introduce Similarity as Reward Alignment (SARA), a simple
contrastive framework that is both resilient to noisy labels and adaptable to
diverse feedback formats and training paradigms. SARA learns a latent
representation of preferred samples and computes rewards as similarities to the
learned latent. We demonstrate strong performance compared to baselines on
continuous control offline RL benchmarks. We further demonstrate SARA's
versatility in applications such as trajectory filtering for downstream tasks,
cross-task preference transfer, and reward shaping in online learning.</p></br><a href="http://arxiv.org/pdf/2506.12459v1" target="_blank"><h2>Merlin: Multi-View Representation Learning for Robust Multivariate Time
  Series Forecasting with Unfixed Missing Rates</h2></a><strong><u>Authors:</u></strong>  Chengqing Yu, Fei Wang, Chuanguang Yang, Zezhi Shao, Tao Sun, Tangwen Qian, Wei Wei, Zhulin An, Yongjun Xu</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML</br><strong><u>Comments:</u></strong> Accepted by SIGKDD 2025 (Research Track)</br><p><strong><u>Abstract:</u></strong> Multivariate Time Series Forecasting (MTSF) involves predicting future values
of multiple interrelated time series. Recently, deep learning-based MTSF models
have gained significant attention for their promising ability to mine semantics
(global and local information) within MTS data. However, these models are
pervasively susceptible to missing values caused by malfunctioning data
collectors. These missing values not only disrupt the semantics of MTS, but
their distribution also changes over time. Nevertheless, existing models lack
robustness to such issues, leading to suboptimal forecasting performance. To
this end, in this paper, we propose Multi-View Representation Learning
(Merlin), which can help existing models achieve semantic alignment between
incomplete observations with different missing rates and complete observations
in MTS. Specifically, Merlin consists of two key modules: offline knowledge
distillation and multi-view contrastive learning. The former utilizes a teacher
model to guide a student model in mining semantics from incomplete
observations, similar to those obtainable from complete observations. The
latter improves the student model's robustness by learning from
positive/negative data pairs constructed from incomplete observations with
different missing rates, ensuring semantic alignment across different missing
rates. Therefore, Merlin is capable of effectively enhancing the robustness of
existing models against unfixed missing rates while preserving forecasting
accuracy. Experiments on four real-world datasets demonstrate the superiority
of Merlin.</p></br><a href="http://arxiv.org/pdf/2506.12721v1" target="_blank"><h2>Strategic Scaling of Test-Time Compute: A Bandit Learning Approach</h2></a><strong><u>Authors:</u></strong>  Bowen Zuo, Yinglun Zhu</br><strong><u>Categories:</u></strong> cs.AI, cs.CL, cs.LG, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Scaling test-time compute has emerged as an effective strategy for improving
the performance of large language models. However, existing methods typically
allocate compute uniformly across all queries, overlooking variation in query
difficulty. To address this inefficiency, we formulate test-time compute
allocation as a novel bandit learning problem and propose adaptive algorithms
that estimate query difficulty on the fly and allocate compute accordingly.
Compared to uniform allocation, our algorithms allocate more compute to
challenging queries while maintaining accuracy on easier ones. Among
challenging queries, our algorithms further learn to prioritize solvable
instances, effectively reducing excessive computing on unsolvable queries. We
theoretically prove that our algorithms achieve better compute efficiency than
uniform allocation and empirically validate their effectiveness on math and
code benchmarks. Specifically, our algorithms achieve up to an 11.10%
performance improvement (15.04% relative) on the MATH-500 dataset and up to a
7.41% performance improvement (14.40% relative) on LiveCodeBench.</p></br><a href="http://arxiv.org/pdf/2506.12412v1" target="_blank"><h2>Cross-Domain Conditional Diffusion Models for Time Series Imputation</h2></a><strong><u>Authors:</u></strong>  Kexin Zhang, Baoyu Jing, K. Selçuk Candan, Dawei Zhou, Qingsong Wen, Han Liu, Kaize Ding</br><strong><u>Categories:</u></strong> cs.LG, stat.ML</br><strong><u>Comments:</u></strong> Accepted by ECML-PKDD 2025</br><p><strong><u>Abstract:</u></strong> Cross-domain time series imputation is an underexplored data-centric research
task that presents significant challenges, particularly when the target domain
suffers from high missing rates and domain shifts in temporal dynamics.
Existing time series imputation approaches primarily focus on the single-domain
setting, which cannot effectively adapt to a new domain with domain shifts.
Meanwhile, conventional domain adaptation techniques struggle with data
incompleteness, as they typically assume the data from both source and target
domains are fully observed to enable adaptation. For the problem of
cross-domain time series imputation, missing values introduce high uncertainty
that hinders distribution alignment, making existing adaptation strategies
ineffective. Specifically, our proposed solution tackles this problem from
three perspectives: (i) Data: We introduce a frequency-based time series
interpolation strategy that integrates shared spectral components from both
domains while retaining domain-specific temporal structures, constructing
informative priors for imputation. (ii) Model: We design a diffusion-based
imputation model that effectively learns domain-shared representations and
captures domain-specific temporal dependencies with dedicated denoising
networks. (iii) Algorithm: We further propose a cross-domain consistency
alignment strategy that selectively regularizes output-level domain
discrepancies, enabling effective knowledge transfer while preserving
domain-specific characteristics. Extensive experiments on three real-world
datasets demonstrate the superiority of our proposed approach. Our code
implementation is available here.</p></br><a href="http://arxiv.org/pdf/2506.12542v1" target="_blank"><h2>PLD: A Choice-Theoretic List-Wise Knowledge Distillation</h2></a><strong><u>Authors:</u></strong>  Ejafa Bassam, Dawei Zhu, Kaigui Bian</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.CV, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Knowledge distillation is a model compression technique in which a compact
"student" network is trained to replicate the predictive behavior of a larger
"teacher" network. In logit-based knowledge distillation it has become the de
facto approach to augment cross-entropy with a distillation term. Typically
this term is either a KL divergence-matching marginal probabilities or a
correlation-based loss capturing intra- and inter-class relationships but in
every case it sits as an add-on to cross-entropy with its own weight that must
be carefully tuned. In this paper we adopt a choice-theoretic perspective and
recast knowledge distillation under the Plackett-Luce model by interpreting
teacher logits as "worth" scores. We introduce Plackett-Luce Distillation
(PLD), a weighted list-wise ranking loss in which the teacher model transfers
knowledge of its full ranking of classes, weighting each ranked choice by its
own confidence. PLD directly optimizes a single teacher-optimal ranking of the
true label first, followed by the remaining classes in descending teacher
confidence, yielding a convex, translation-invariant surrogate that subsumes
weighted cross-entropy. Empirically on standard image classification
benchmarks, PLD improves Top-1 accuracy by an average of +0.42% over DIST
(arXiv:2205.10536) and +1.04% over KD (arXiv:1503.02531) in homogeneous
settings and by +0.48% and +1.09% over DIST and KD, respectively, in
heterogeneous settings.</p></br><a href="http://arxiv.org/pdf/2506.12352v1" target="_blank"><h2>Efficient Network Automatic Relevance Determination</h2></a><strong><u>Authors:</u></strong>  Hongwei Zhang, Ziqi Ye, Xinyuan Wang, Xin Guo, Zenglin Xu, Yuan Cheng, Zixin Hu, Yuan Qi</br><strong><u>Categories:</u></strong> cs.AI, cs.LG, stat.ML</br><strong><u>Comments:</u></strong> ICML 2025</br><p><strong><u>Abstract:</u></strong> We propose Network Automatic Relevance Determination (NARD), an extension of
ARD for linearly probabilistic models, to simultaneously model sparse
relationships between inputs $X \in \mathbb R^{d \times N}$ and outputs $Y \in
\mathbb R^{m \times N}$, while capturing the correlation structure among the
$Y$. NARD employs a matrix normal prior which contains a sparsity-inducing
parameter to identify and discard irrelevant features, thereby promoting
sparsity in the model. Algorithmically, it iteratively updates both the
precision matrix and the relationship between $Y$ and the refined inputs. To
mitigate the computational inefficiencies of the $\mathcal O(m^3 + d^3)$ cost
per iteration, we introduce Sequential NARD, which evaluates features
sequentially, and a Surrogate Function Method, leveraging an efficient
approximation of the marginal likelihood and simplifying the calculation of
determinant and inverse of an intermediate matrix. Combining the Sequential
update with the Surrogate Function method further reduces computational costs.
The computational complexity per iteration for these three methods is reduced
to $\mathcal O(m^3+p^3)$, $\mathcal O(m^3 + d^2)$, $\mathcal O(m^3+p^2)$,
respectively, where $p \ll d$ is the final number of features in the model. Our
methods demonstrate significant improvements in computational efficiency with
comparable performance on both synthetic and real-world datasets.</p></br><a href="http://arxiv.org/pdf/2506.11550v2" target="_blank"><h2>Improving Multimodal Learning Balance and Sufficiency through Data
  Remixing</h2></a><strong><u>Authors:</u></strong>  Xiaoyu Ma, Hao Chen, Yongjian Deng</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> ICML2025</br><p><strong><u>Abstract:</u></strong> Different modalities hold considerable gaps in optimization trajectories,
including speeds and paths, which lead to modality laziness and modality clash
when jointly training multimodal models, resulting in insufficient and
imbalanced multimodal learning. Existing methods focus on enforcing the weak
modality by adding modality-specific optimization objectives, aligning their
optimization speeds, or decomposing multimodal learning to enhance unimodal
learning. These methods fail to achieve both unimodal sufficiency and
multimodal balance. In this paper, we, for the first time, address both
concerns by proposing multimodal Data Remixing, including decoupling multimodal
data and filtering hard samples for each modality to mitigate modality
imbalance; and then batch-level reassembling to align the gradient directions
and avoid cross-modal interference, thus enhancing unimodal learning
sufficiency. Experimental results demonstrate that our method can be seamlessly
integrated with existing approaches, improving accuracy by approximately
6.50%$\uparrow$ on CREMAD and 3.41%$\uparrow$ on Kinetic-Sounds, without
training set expansion or additional computational overhead during inference.
The source code is available at https://github.com/MatthewMaxy/Remix_ICML2025.</p></br><a href="http://arxiv.org/pdf/2506.12378v1" target="_blank"><h2>Component Based Quantum Machine Learning Explainability</h2></a><strong><u>Authors:</u></strong>  Barra White, Krishnendu Guha</br><strong><u>Categories:</u></strong> quant-ph, cs.AI, cs.ET, cs.LG</br><strong><u>Comments:</u></strong> 11 pages</br><p><strong><u>Abstract:</u></strong> Explainable ML algorithms are designed to provide transparency and insight
into their decision-making process. Explaining how ML models come to their
prediction is critical in fields such as healthcare and finance, as it provides
insight into how models can help detect bias in predictions and help comply
with GDPR compliance in these fields. QML leverages quantum phenomena such as
entanglement and superposition, offering the potential for computational
speedup and greater insights compared to classical ML. However, QML models also
inherit the black-box nature of their classical counterparts, requiring the
development of explainability techniques to be applied to these QML models to
help understand why and how a particular output was generated.
  This paper will explore the idea of creating a modular, explainable QML
framework that splits QML algorithms into their core components, such as
feature maps, variational circuits (ansatz), optimizers, kernels, and
quantum-classical loops. Each component will be analyzed using explainability
techniques, such as ALE and SHAP, which have been adapted to analyse the
different components of these QML algorithms. By combining insights from these
parts, the paper aims to infer explainability to the overall QML model.</p></br><a href="http://arxiv.org/pdf/2506.12944v1" target="_blank"><h2>Unsupervised risk factor identification across cancer types and data
  modalities via explainable artificial intelligence</h2></a><strong><u>Authors:</u></strong>  Maximilian Ferle, Jonas Ader, Thomas Wiemers, Nora Grieb, Adrian Lindenmeyer, Hans-Jonas Meyer, Thomas Neumuth, Markus Kreuz, Kristin Reiche, Maximilian Merz</br><strong><u>Categories:</u></strong> cs.LG, q-bio.TO</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Risk stratification is a key tool in clinical decision-making, yet current
approaches often fail to translate sophisticated survival analysis into
actionable clinical criteria. We present a novel method for unsupervised
machine learning that directly optimizes for survival heterogeneity across
patient clusters through a differentiable adaptation of the multivariate
logrank statistic. Unlike most existing methods that rely on proxy metrics, our
approach represents novel methodology for training any neural network
architecture on any data modality to identify prognostically distinct patient
groups. We thoroughly evaluate the method in simulation experiments and
demonstrate its utility in practice by applying it to two distinct cancer
types: analyzing laboratory parameters from multiple myeloma patients and
computed tomography images from non-small cell lung cancer patients,
identifying prognostically distinct patient subgroups with significantly
different survival outcomes in both cases. Post-hoc explainability analyses
uncover clinically meaningful features determining the group assignments which
align well with established risk factors and thus lend strong weight to the
methods utility. This pan-cancer, model-agnostic approach represents a valuable
advancement in clinical risk stratification, enabling the discovery of novel
prognostic signatures across diverse data types while providing interpretable
results that promise to complement treatment personalization and clinical
decision-making in oncology and beyond.</p></br><a href="http://arxiv.org/pdf/2506.11898v1" target="_blank"><h2>Scalable Generalized Bayesian Online Neural Network Training for
  Sequential Decision Making</h2></a><strong><u>Authors:</u></strong>  Gerardo Duran-Martin, Leandro Sánchez-Betancourt, Álvaro Cartea, Kevin Murphy</br><strong><u>Categories:</u></strong> cs.LG, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> We introduce scalable algorithms for online learning and generalized Bayesian
inference of neural network parameters, designed for sequential decision making
tasks. Our methods combine the strengths of frequentist and Bayesian filtering,
which include fast low-rank updates via a block-diagonal approximation of the
parameter error covariance, and a well-defined posterior predictive
distribution that we use for decision making. More precisely, our main method
updates a low-rank error covariance for the hidden layers parameters, and a
full-rank error covariance for the final layer parameters. Although this
characterizes an improper posterior, we show that the resulting posterior
predictive distribution is well-defined. Our methods update all network
parameters online, with no need for replay buffers or offline retraining. We
show, empirically, that our methods achieve a competitive tradeoff between
speed and accuracy on (non-stationary) contextual bandit problems and Bayesian
optimization problems.</p></br><a href="http://arxiv.org/pdf/2506.13244v1" target="_blank"><h2>No-Regret Learning Under Adversarial Resource Constraints: A Spending
  Plan Is All You Need!</h2></a><strong><u>Authors:</u></strong>  Francesco Emanuele Stradi, Matteo Castiglioni, Alberto Marchesi, Nicola Gatti, Christian Kroer</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> We study online decision making problems under resource constraints, where
both reward and cost functions are drawn from distributions that may change
adversarially over time. We focus on two canonical settings: $(i)$ online
resource allocation where rewards and costs are observed before action
selection, and $(ii)$ online learning with resource constraints where they are
observed after action selection, under full feedback or bandit feedback. It is
well known that achieving sublinear regret in these settings is impossible when
reward and cost distributions may change arbitrarily over time. To address this
challenge, we analyze a framework in which the learner is guided by a spending
plan--a sequence prescribing expected resource usage across rounds. We design
general (primal-)dual methods that achieve sublinear regret with respect to
baselines that follow the spending plan. Crucially, the performance of our
algorithms improves when the spending plan ensures a well-balanced distribution
of the budget across rounds. We additionally provide a robust variant of our
methods to handle worst-case scenarios where the spending plan is highly
imbalanced. To conclude, we study the regret of our algorithms when competing
against benchmarks that deviate from the prescribed spending plan.</p></br><a href="http://arxiv.org/pdf/2506.12839v1" target="_blank"><h2>Fair Bayesian Model-Based Clustering</h2></a><strong><u>Authors:</u></strong>  Jihu Lee, Kunwoong Kim, Yongdai Kim</br><strong><u>Categories:</u></strong> stat.ML, cs.AI, cs.LG</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Fair clustering has become a socially significant task with the advancement
of machine learning technologies and the growing demand for trustworthy AI.
Group fairness ensures that the proportions of each sensitive group are similar
in all clusters. Most existing group-fair clustering methods are based on the
$K$-means clustering and thus require the distance between instances and the
number of clusters to be given in advance. To resolve this limitation, we
propose a fair Bayesian model-based clustering called Fair Bayesian Clustering
(FBC). We develop a specially designed prior which puts its mass only on fair
clusters, and implement an efficient MCMC algorithm. Advantages of FBC are that
it can infer the number of clusters and can be applied to any data type as long
as the likelihood is defined (e.g., categorical data). Experiments on
real-world datasets show that FBC (i) reasonably infers the number of clusters,
(ii) achieves a competitive utility-fairness trade-off compared to existing
fair clustering methods, and (iii) performs well on categorical data.</p></br><a href="http://arxiv.org/pdf/2506.13763v1" target="_blank"><h2>Diagnosing and Improving Diffusion Models by Estimating the Optimal Loss
  Value</h2></a><strong><u>Authors:</u></strong>  Yixian Xu, Shengjie Luo, Liwei Wang, Di He, Chang Liu</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.CV, stat.ML</br><strong><u>Comments:</u></strong> 29 pages, 8 figures, 3 tables. Preprint. Work in Progress</br><p><strong><u>Abstract:</u></strong> Diffusion models have achieved remarkable success in generative modeling.
Despite more stable training, the loss of diffusion models is not indicative of
absolute data-fitting quality, since its optimal value is typically not zero
but unknown, leading to confusion between large optimal loss and insufficient
model capacity. In this work, we advocate the need to estimate the optimal loss
value for diagnosing and improving diffusion models. We first derive the
optimal loss in closed form under a unified formulation of diffusion models,
and develop effective estimators for it, including a stochastic variant
scalable to large datasets with proper control of variance and bias. With this
tool, we unlock the inherent metric for diagnosing the training quality of
mainstream diffusion model variants, and develop a more performant training
schedule based on the optimal loss. Moreover, using models with 120M to 1.5B
parameters, we find that the power law is better demonstrated after subtracting
the optimal loss from the actual training loss, suggesting a more principled
setting for investigating the scaling law for diffusion models.</p></br><a href="http://arxiv.org/pdf/2506.13658v1" target="_blank"><h2>Adversarial Disentanglement by Backpropagation with Physics-Informed
  Variational Autoencoder</h2></a><strong><u>Authors:</u></strong>  Ioannis Christoforos Koune, Alice Cicirello</br><strong><u>Categories:</u></strong> stat.ML, cs.LG</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Inference and prediction under partial knowledge of a physical system is
challenging, particularly when multiple confounding sources influence the
measured response. Explicitly accounting for these influences in physics-based
models is often infeasible due to epistemic uncertainty, cost, or time
constraints, resulting in models that fail to accurately describe the behavior
of the system. On the other hand, data-driven machine learning models such as
variational autoencoders are not guaranteed to identify a parsimonious
representation. As a result, they can suffer from poor generalization
performance and reconstruction accuracy in the regime of limited and noisy
data. We propose a physics-informed variational autoencoder architecture that
combines the interpretability of physics-based models with the flexibility of
data-driven models. To promote disentanglement of the known physics and
confounding influences, the latent space is partitioned into physically
meaningful variables that parametrize a physics-based model, and data-driven
variables that capture variability in the domain and class of the physical
system. The encoder is coupled with a decoder that integrates physics-based and
data-driven components, and constrained by an adversarial training objective
that prevents the data-driven components from overriding the known physics,
ensuring that the physics-grounded latent variables remain interpretable. We
demonstrate that the model is able to disentangle features of the input signal
and separate the known physics from confounding influences using supervision in
the form of class and domain observables. The model is evaluated on a series of
synthetic case studies relevant to engineering structures, demonstrating the
feasibility of the proposed approach.</p></br><a href="http://arxiv.org/pdf/2506.12818v1" target="_blank"><h2>Taking the GP Out of the Loop</h2></a><strong><u>Authors:</u></strong>  David Sweet, Siddhant anand Jadhav</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML</br><strong><u>Comments:</u></strong> 12 pages, 11 figures</br><p><strong><u>Abstract:</u></strong> Bayesian optimization (BO) has traditionally solved black box problems where
evaluation is expensive and, therefore, design-evaluation pairs (i.e.,
observations) are few. Recently, there has been growing interest in applying BO
to problems where evaluation is cheaper and, thus, observations are more
plentiful. An impediment to scaling BO to many observations, $N$, is the
$O(N^3)$ scaling of a na{\"i}ve query of the Gaussian process (GP) surrogate.
Modern implementations reduce this to $O(N^2)$, but the GP remains a
bottleneck. We propose Epistemic Nearest Neighbors (ENN), a surrogate that
estimates function values and epistemic uncertainty from $K$ nearest-neighbor
observations. ENN has $O(N)$ query time and omits hyperparameter fitting,
leaving uncertainty uncalibrated. To accommodate the lack of calibration, we
employ an acquisition method based on Pareto-optimal tradeoffs between
predicted value and uncertainty. Our proposed method, TuRBO-ENN, replaces the
GP surrogate in TuRBO with ENN and its Thompson sampling acquisition method
with our Pareto-based alternative. We demonstrate numerically that TuRBO-ENN
can reduce the time to generate proposals by one to two orders of magnitude
compared to TuRBO and scales to thousands of observations.</p></br><a href="http://arxiv.org/pdf/2506.12912v1" target="_blank"><h2>Logit Dynamics in Softmax Policy Gradient Methods</h2></a><strong><u>Authors:</u></strong>  Yingru Li</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML</br><strong><u>Comments:</u></strong> 7 pages</br><p><strong><u>Abstract:</u></strong> We analyzes the logit dynamics of softmax policy gradient methods. We derive
the exact formula for the L2 norm of the logit update vector: $$ \|\Delta
\mathbf{z}\|_2 \propto \sqrt{1-2P_c + C(P)} $$ This equation demonstrates that
update magnitudes are determined by the chosen action's probability ($P_c$) and
the policy's collision probability ($C(P)$), a measure of concentration
inversely related to entropy. Our analysis reveals an inherent self-regulation
mechanism where learning vigor is automatically modulated by policy confidence,
providing a foundational insight into the stability and convergence of these
methods.</p></br><a href="http://arxiv.org/pdf/2506.12439v1" target="_blank"><h2>Interpretable Causal Representation Learning for Biological Data in the
  Pathway Space</h2></a><strong><u>Authors:</u></strong>  Jesus de la Fuente, Robert Lehmann, Carlos Ruiz-Arenas, Jan Voges, Irene Marin-Goñi, Xabier Martinez-de-Morentin, David Gomez-Cabrero, Idoia Ochoa, Jesper Tegner, Vincenzo Lagani, Mikel Hernaez</br><strong><u>Categories:</u></strong> cs.LG, q-bio.QM, stat.ML</br><strong><u>Comments:</u></strong> ICLR 2025, 28 pages, 14 figures, 10 tables</br><p><strong><u>Abstract:</u></strong> Predicting the impact of genomic and drug perturbations in cellular function
is crucial for understanding gene functions and drug effects, ultimately
leading to improved therapies. To this end, Causal Representation Learning
(CRL) constitutes one of the most promising approaches, as it aims to identify
the latent factors that causally govern biological systems, thus facilitating
the prediction of the effect of unseen perturbations. Yet, current CRL methods
fail in reconciling their principled latent representations with known
biological processes, leading to models that are not interpretable. To address
this major issue, we present SENA-discrepancy-VAE, a model based on the
recently proposed CRL method discrepancy-VAE, that produces representations
where each latent factor can be interpreted as the (linear) combination of the
activity of a (learned) set of biological processes. To this extent, we present
an encoder, SENA-{\delta}, that efficiently compute and map biological
processes' activity levels to the latent causal factors. We show that
SENA-discrepancy-VAE achieves predictive performances on unseen combinations of
interventions that are comparable with its original, non-interpretable
counterpart, while inferring causal latent factors that are biologically
meaningful.</p></br><a href="http://arxiv.org/pdf/2506.12350v1" target="_blank"><h2>Theoretical Tensions in RLHF: Reconciling Empirical Success with
  Inconsistencies in Social Choice Theory</h2></a><strong><u>Authors:</u></strong>  Jiancong Xiao, Zhekun Shi, Kaizhao Liu, Qi Long, Weijie J. Su</br><strong><u>Categories:</u></strong> stat.ML, cs.AI, cs.LG</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Despite its empirical success, Reinforcement Learning from Human Feedback
(RLHF) has been shown to violate almost all the fundamental axioms in social
choice theory -- such as majority consistency, pairwise majority consistency,
and Condorcet consistency. This raises a foundational question: why does RLHF
perform so well in practice if it fails these seemingly essential properties?
In this paper, we resolve this paradox by showing that under mild and
empirically plausible assumptions on the preference profile, RLHF does satisfy
pairwise majority and Condorcet consistency. These assumptions are frequently
satisfied in real-world alignment tasks, offering a theoretical explanation for
RLHF's strong practical performance. Furthermore, we show that a slight
modification to the reward modeling objective can ensure pairwise majority or
Condorcet consistency even under general preference profiles, thereby improving
the alignment process. Finally, we go beyond classical axioms in economic and
social choice theory and introduce new alignment criteria -- preference
matching, preference equivalence, and group preference matching -- that better
reflect the goal of learning distributions over responses. We show that while
RLHF satisfies the first two properties, it fails to satisfy the third. We
conclude by discussing how future alignment methods may be designed to satisfy
all three.</p></br><a href="http://arxiv.org/pdf/2506.13018v1" target="_blank"><h2>Symmetry in Neural Network Parameter Spaces</h2></a><strong><u>Authors:</u></strong>  Bo Zhao, Robin Walters, Rose Yu</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> 29 pages, 9 figures</br><p><strong><u>Abstract:</u></strong> Modern deep learning models are highly overparameterized, resulting in large
sets of parameter configurations that yield the same outputs. A significant
portion of this redundancy is explained by symmetries in the parameter
space--transformations that leave the network function unchanged. These
symmetries shape the loss landscape and constrain learning dynamics, offering a
new lens for understanding optimization, generalization, and model complexity
that complements existing theory of deep learning. This survey provides an
overview of parameter space symmetry. We summarize existing literature, uncover
connections between symmetry and learning theory, and identify gaps and
opportunities in this emerging field.</p></br><a href="http://arxiv.org/pdf/2506.12903v1" target="_blank"><h2>Variational Learning Finds Flatter Solutions at the Edge of Stability</h2></a><strong><u>Authors:</u></strong>  Avrajit Ghosh, Bai Cong, Rio Yokota, Saiprasad Ravishankar, Rongrong Wang, Molei Tao, Mohammad Emtiyaz Khan, Thomas Möllenhoff</br><strong><u>Categories:</u></strong> stat.ML, cs.LG</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Variational Learning (VL) has recently gained popularity for training deep
neural networks and is competitive to standard learning methods. Part of its
empirical success can be explained by theories such as PAC-Bayes bounds,
minimum description length and marginal likelihood, but there are few tools to
unravel the implicit regularization in play. Here, we analyze the implicit
regularization of VL through the Edge of Stability (EoS) framework. EoS has
previously been used to show that gradient descent can find flat solutions and
we extend this result to VL to show that it can find even flatter solutions.
This is obtained by controlling the posterior covariance and the number of
Monte Carlo samples from the posterior. These results are derived in a similar
fashion as the standard EoS literature for deep learning, by first deriving a
result for a quadratic problem and then extending it to deep neural networks.
We empirically validate these findings on a wide variety of large networks,
such as ResNet and ViT, to find that the theoretical results closely match the
empirical ones. Ours is the first work to analyze the EoS dynamics in VL.</p></br><a href="http://arxiv.org/pdf/2506.12226v1" target="_blank"><h2>Learning Causality for Modern Machine Learning</h2></a><strong><u>Authors:</u></strong>  Yongqiang Chen</br><strong><u>Categories:</u></strong> cs.LG, stat.ML</br><strong><u>Comments:</u></strong> PhD thesis</br><p><strong><u>Abstract:</u></strong> In the past decades, machine learning with Empirical Risk Minimization (ERM)
has demonstrated great capability in learning and exploiting the statistical
patterns from data, or even surpassing humans. Despite the success, ERM avoids
the modeling of causality the way of understanding and handling changes, which
is fundamental to human intelligence. When deploying models beyond the training
environment, distribution shifts are everywhere. For example, an autopilot
system often needs to deal with new weather conditions that have not been seen
during training, An Al-aided drug discovery system needs to predict the
biochemical properties of molecules with respect to new viruses such as
COVID-19. It renders the problem of Out-of-Distribution (OOD) generalization
challenging to conventional machine learning.
  In this thesis, we investigate how to incorporate and realize the causality
for broader tasks in modern machine learning. In particular, we exploit the
invariance implied by the principle of independent causal mechanisms (ICM),
that is, the causal mechanisms generating the effects from causes do not inform
or influence each other. Therefore, the conditional distribution between the
target variable given its causes is invariant under distribution shifts. With
the causal invariance principle, we first instantiate it to graphs -- a general
data structure ubiquitous in many real-world industry and scientific
applications, such as financial networks and molecules. Then, we shall see how
learning the causality benefits many of the desirable properties of modern
machine learning, in terms of (i) OOD generalization capability; (ii)
interpretability; and (iii) robustness to adversarial attacks.
  Realizing the causality in machine learning, on the other hand, raises a
dilemma for optimization in conventional machine learning, as it often
contradicts the objective of ERM...</p></br><a href="http://arxiv.org/pdf/2506.12404v1" target="_blank"><h2>EXGnet: a single-lead explainable-AI guided multiresolution network with
  train-only quantitative features for trustworthy ECG arrhythmia
  classification</h2></a><strong><u>Authors:</u></strong>  Tushar Talukder Showrav, Soyabul Islam Lincoln, Md. Kamrul Hasan</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> 21 pages, 3 figures</br><p><strong><u>Abstract:</u></strong> Background: Deep learning has significantly advanced ECG arrhythmia
classification, enabling high accuracy in detecting various cardiac conditions.
The use of single-lead ECG systems is crucial for portable devices, as they
offer convenience and accessibility for continuous monitoring in diverse
settings. However, the interpretability and reliability of deep learning models
in clinical applications poses challenges due to their black-box nature.
Methods: To address these challenges, we propose EXGnet, a single-lead,
trustworthy ECG arrhythmia classification network that integrates
multiresolution feature extraction with Explainable Artificial Intelligence
(XAI) guidance and train only quantitative features. Results: Trained on two
public datasets, including Chapman and Ningbo, EXGnet demonstrates superior
performance through key metrics such as Accuracy, F1-score, Sensitivity, and
Specificity. The proposed method achieved average five fold accuracy of
98.762%, and 96.932% and average F1-score of 97.910%, and 95.527% on the
Chapman and Ningbo datasets, respectively. Conclusions: By employing XAI
techniques, specifically Grad-CAM, the model provides visual insights into the
relevant ECG segments it analyzes, thereby enhancing clinician trust in its
predictions. While quantitative features further improve classification
performance, they are not required during testing, making the model suitable
for real-world applications. Overall, EXGnet not only achieves better
classification accuracy but also addresses the critical need for
interpretability in deep learning, facilitating broader adoption in portable
ECG monitoring.</p></br><a href="http://arxiv.org/pdf/2506.13439v1" target="_blank"><h2>Dark Energy Survey Year 3 results: $w$CDM cosmology from
  simulation-based inference with persistent homology on the sphere</h2></a><strong><u>Authors:</u></strong>  J. Prat, M. Gatti, C. Doux, P. Pranav, C. Chang, N. Jeffrey, L. Whiteway, D. Anbajagane, S. Sugiyama, A. Thomsen, A. Alarcon, A. Amon, K. Bechtol, G. M. Bernstein, A. Campos, R. Chen, A. Choi, C. Davis, J. DeRose, S. Dodelson, K. Eckert, J. Elvin-Poole, S. Everett, A. Ferté, D. Gruen, E. M. Huff, I. Harrison, K. Herner, M. Jarvis, N. Kuropatkin, P. -F. Leget, N. MacCrann, J. McCullough, J. Myles, A. Navarro-Alsina, S. Pandey, M. Raveri, R. P. Rollins, A. Roodman, C. Sánchez, L. F. Secco, E. Sheldon, T. Shin, M. A. Troxel, I. Tutusaus, T. N. Varga, B. Yanny, B. Yin, Y. Zhang, J. Zuntz, T. M. C. Abbott, M. Aguena, S. Allam, F. Andrade-Oliveira, J. Blazek, S. Bocquet, D. Brooks, J. Carretero, A. Carnero Rosell, R. Cawthon, J. De Vicente, S. Desai, M. E. da Silva Pereira, H. T. Diehl, B. Flaugher, J. Frieman, J. García-Bellido, R. A. Gruendl, G. Gutierrez, S. R. Hinton, D. L. Hollowood, K. Honscheid, D. J. James, K. Kuehn, L. N. da Costa, O. Lahav, S. Lee, J. L. Marshall, J. Mena-Fernández, R. Miquel, J. J. Mohr, R. L. C. Ogando, A. A. Plazas Malagón, A. Porredon, S. Samuroff, E. Sanchez, B. Santiago, I. Sevilla-Noarbe, M. Smith, E. Suchyta, M. E. C. Swanson, D. Thomas, C. To, V. Vikram, A. R. Walker, N. Weaverdyck, J. Weller</br><strong><u>Categories:</u></strong> astro-ph.CO, astro-ph.IM</br><strong><u>Comments:</u></strong> To be submitted to MNRAS. 18 + 3 pages. 17 figures</br><p><strong><u>Abstract:</u></strong> We present cosmological constraints from Dark Energy Survey Year 3 (DES Y3)
weak lensing data using persistent homology, a topological data analysis
technique that tracks how features like clusters and voids evolve across
density thresholds. For the first time, we apply spherical persistent homology
to galaxy survey data through the algorithm TopoS2, which is optimized for
curved-sky analyses and HEALPix compatibility. Employing a simulation-based
inference framework with the Gower Street simulation suite, specifically
designed to mimic DES Y3 data properties, we extract topological summary
statistics from convergence maps across multiple smoothing scales and redshift
bins. After neural network compression of these statistics, we estimate the
likelihood function and validate our analysis against baryonic feedback
effects, finding minimal biases (under $0.3\sigma$) in the
$\Omega_\mathrm{m}-S_8$ plane. Assuming the $w$CDM model, our combined Betti
numbers and second moments analysis yields $S_8 = 0.821 \pm 0.018$ and
$\Omega_\mathrm{m} = 0.304\pm0.037$-constraints 70% tighter than those from
cosmic shear two-point statistics in the same parameter plane. Our results
demonstrate that topological methods provide a powerful and robust framework
for extracting cosmological information, with our spherical methodology readily
applicable to upcoming Stage IV wide-field galaxy surveys.</p></br><a href="http://arxiv.org/pdf/2506.11882v1" target="_blank"><h2>An Explainable AI Framework for Dynamic Resource Management in Vehicular
  Network Slicing</h2></a><strong><u>Authors:</u></strong>  Haochen Sun, Yifan Liu, Ahmed Al-Tahmeesschi, Swarna Chetty, Syed Ali Raza Zaidi, Avishek Nag, Hamed Ahmadi</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> To appear in Proceedings of IEEE PIMRC 2025. 6 pages, 4 figures</br><p><strong><u>Abstract:</u></strong> Effective resource management and network slicing are essential to meet the
diverse service demands of vehicular networks, including Enhanced Mobile
Broadband (eMBB) and Ultra-Reliable and Low-Latency Communications (URLLC).
This paper introduces an Explainable Deep Reinforcement Learning (XRL)
framework for dynamic network slicing and resource allocation in vehicular
networks, built upon a near-real-time RAN intelligent controller. By
integrating a feature-based approach that leverages Shapley values and an
attention mechanism, we interpret and refine the decisions of our
reinforcementlearning agents, addressing key reliability challenges in
vehicular communication systems. Simulation results demonstrate that our
approach provides clear, real-time insights into the resource allocation
process and achieves higher interpretability precision than a pure attention
mechanism. Furthermore, the Quality of Service (QoS) satisfaction for URLLC
services increased from 78.0% to 80.13%, while that for eMBB services improved
from 71.44% to 73.21%.</p></br><a href="http://arxiv.org/pdf/2506.12183v1" target="_blank"><h2>Temporal cross-validation impacts multivariate time series subsequence
  anomaly detection evaluation</h2></a><strong><u>Authors:</u></strong>  Steven C. Hespeler, Pablo Moriano, Mingyan Li, Samuel C. Hollifield</br><strong><u>Categories:</u></strong> stat.ML, cs.LG, stat.AP, stat.ME</br><strong><u>Comments:</u></strong> 22 pages, 6 figures, 5 tables</br><p><strong><u>Abstract:</u></strong> Evaluating anomaly detection in multivariate time series (MTS) requires
careful consideration of temporal dependencies, particularly when detecting
subsequence anomalies common in fault detection scenarios. While time series
cross-validation (TSCV) techniques aim to preserve temporal ordering during
model evaluation, their impact on classifier performance remains underexplored.
This study systematically investigates the effect of TSCV strategy on the
precision-recall characteristics of classifiers trained to detect fault-like
anomalies in MTS datasets. We compare walk-forward (WF) and sliding window (SW)
methods across a range of validation partition configurations and classifier
types, including shallow learners and deep learning (DL) classifiers. Results
show that SW consistently yields higher median AUC-PR scores and reduced
fold-to-fold performance variance, particularly for deep architectures
sensitive to localized temporal continuity. Furthermore, we find that
classifier generalization is sensitive to the number and structure of temporal
partitions, with overlapping windows preserving fault signatures more
effectively at lower fold counts. A classifier-level stratified analysis
reveals that certain algorithms, such as random forests (RF), maintain stable
performance across validation schemes, whereas others exhibit marked
sensitivity. This study demonstrates that TSCV design in benchmarking anomaly
detection models on streaming time series and provide guidance for selecting
evaluation strategies in temporally structured learning environments.</p></br><a href="http://arxiv.org/pdf/2506.12385v1" target="_blank"><h2>Recent Advances and Future Directions in Literature-Based Discovery</h2></a><strong><u>Authors:</u></strong>  Andrej Kastrin, Bojan Cestnik, Nada Lavrač</br><strong><u>Categories:</u></strong> cs.CL, cs.AI, 68T50 (Primary) 68-02, 68-06 (Secondary), A.1; I.2.7</br><strong><u>Comments:</u></strong> 13 pages, 1 table, 1 figure</br><p><strong><u>Abstract:</u></strong> The explosive growth of scientific publications has created an urgent need
for automated methods that facilitate knowledge synthesis and hypothesis
generation. Literature-based discovery (LBD) addresses this challenge by
uncovering previously unknown associations between disparate domains. This
article surveys recent methodological advances in LBD, focusing on developments
from 2000 to the present. We review progress in three key areas: knowledge
graph construction, deep learning approaches, and the integration of
pre-trained and large language models (LLMs). While LBD has made notable
progress, several fundamental challenges remain unresolved, particularly
concerning scalability, reliance on structured data, and the need for extensive
manual curation. By examining ongoing advances and outlining promising future
directions, this survey underscores the transformative role of LLMs in
enhancing LBD and aims to support researchers and practitioners in harnessing
these technologies to accelerate scientific innovation.</p></br><a href="http://arxiv.org/pdf/2506.11641v1" target="_blank"><h2>Deep Symmetric Autoencoders from the Eckart-Young-Schmidt Perspective</h2></a><strong><u>Authors:</u></strong>  Simone Brivio, Nicola Rares Franco</br><strong><u>Categories:</u></strong> math.NA, cs.LG, cs.NA, 68T07, 47N40</br><strong><u>Comments:</u></strong> 28 pages, 10 figures</br><p><strong><u>Abstract:</u></strong> Deep autoencoders have become a fundamental tool in various machine learning
applications, ranging from dimensionality reduction and reduced order modeling
of partial differential equations to anomaly detection and neural machine
translation. Despite their empirical success, a solid theoretical foundation
for their expressiveness remains elusive, particularly when compared to
classical projection-based techniques. In this work, we aim to take a step
forward in this direction by presenting a comprehensive analysis of what we
refer to as symmetric autoencoders, a broad class of deep learning
architectures ubiquitous in the literature. Specifically, we introduce a formal
distinction between different classes of symmetric architectures, analyzing
their strengths and limitations from a mathematical perspective. For instance,
we show that the reconstruction error of symmetric autoencoders with
orthonormality constraints can be understood by leveraging the well-renowned
Eckart-Young-Schmidt (EYS) theorem. As a byproduct of our analysis, we end up
developing the EYS initialization strategy for symmetric autoencoders, which is
based on an iterated application of the Singular Value Decomposition (SVD). To
validate our findings, we conduct a series of numerical experiments where we
benchmark our proposal against conventional deep autoencoders, discussing the
importance of model design and initialization.</p></br><a href="http://arxiv.org/pdf/2506.13746v1" target="_blank"><h2>Evaluating Large Language Models for Phishing Detection,
  Self-Consistency, Faithfulness, and Explainability</h2></a><strong><u>Authors:</u></strong>  Shova Kuikel, Aritran Piplai, Palvi Aggarwal</br><strong><u>Categories:</u></strong> cs.CR, cs.AI, cs.LG</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Phishing attacks remain one of the most prevalent and persistent
cybersecurity threat with attackers continuously evolving and intensifying
tactics to evade the general detection system. Despite significant advances in
artificial intelligence and machine learning, faithfully reproducing the
interpretable reasoning with classification and explainability that underpin
phishing judgments remains challenging. Due to recent advancement in Natural
Language Processing, Large Language Models (LLMs) show a promising direction
and potential for improving domain specific phishing classification tasks.
However, enhancing the reliability and robustness of classification models
requires not only accurate predictions from LLMs but also consistent and
trustworthy explanations aligning with those predictions. Therefore, a key
question remains: can LLMs not only classify phishing emails accurately but
also generate explanations that are reliably aligned with their predictions and
internally self-consistent? To answer these questions, we have fine-tuned
transformer based models, including BERT, Llama models, and Wizard, to improve
domain relevance and make them more tailored to phishing specific distinctions,
using Binary Sequence Classification, Contrastive Learning (CL) and Direct
Preference Optimization (DPO). To that end, we examined their performance in
phishing classification and explainability by applying the ConsistenCy measure
based on SHAPley values (CC SHAP), which measures prediction explanation token
alignment to test the model's internal faithfulness and consistency and uncover
the rationale behind its predictions and reasoning. Overall, our findings show
that Llama models exhibit stronger prediction explanation token alignment with
higher CC SHAP scores despite lacking reliable decision making accuracy,
whereas Wizard achieves better prediction accuracy but lower CC SHAP scores.</p></br><a href="http://arxiv.org/pdf/2506.11512v1" target="_blank"><h2>Prioritizing Alignment Paradigms over Task-Specific Model Customization
  in Time-Series LLMs</h2></a><strong><u>Authors:</u></strong>  Wei Li, Yunyao Cheng, Xinli Hao, Chaohong Ma, Yuxuan Liang, Bin Yang, Christian S. Jensen, Xiaofeng Meng</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Recent advances in Large Language Models (LLMs) have enabled unprecedented
capabilities for time-series reasoning in diverse real-world applications,
including medical, financial, and spatio-temporal domains. However, existing
approaches typically focus on task-specific model customization, such as
forecasting and anomaly detection, while overlooking the data itself, referred
to as time-series primitives, which are essential for in-depth reasoning. This
position paper advocates a fundamental shift in approaching time-series
reasoning with LLMs: prioritizing alignment paradigms grounded in the intrinsic
primitives of time series data over task-specific model customization. This
realignment addresses the core limitations of current time-series reasoning
approaches, which are often costly, inflexible, and inefficient, by
systematically accounting for intrinsic structure of data before task
engineering. To this end, we propose three alignment paradigms: Injective
Alignment, Bridging Alignment, and Internal Alignment, which are emphasized by
prioritizing different aspects of time-series primitives: domain,
characteristic, and representation, respectively, to activate time-series
reasoning capabilities of LLMs to enable economical, flexible, and efficient
reasoning. We further recommend that practitioners adopt an alignment-oriented
method to avail this instruction to select an appropriate alignment paradigm.
Additionally, we categorize relevant literature into these alignment paradigms
and outline promising research directions.</p></br><a href="http://arxiv.org/pdf/2506.13759v1" target="_blank"><h2>Discrete Diffusion in Large Language and Multimodal Models: A Survey</h2></a><strong><u>Authors:</u></strong>  Runpeng Yu, Qi Li, Xinchao Wang</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> In this work, we provide a systematic survey of Discrete Diffusion Language
Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs).
Unlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token,
parallel decoding paradigm using full attention and a denoising-based
generation strategy. This paradigm naturally enables parallel generation,
fine-grained output controllability, and dynamic, response-aware perception.
These capabilities are previously difficult to achieve with AR models.
Recently, a growing number of industrial-scale proprietary d(M)LLMs, as well as
a large number of open-source academic d(M)LLMs, have demonstrated performance
comparable to their autoregressive counterparts, while achieving up to 10x
acceleration in inference speed.
  The advancement of discrete diffusion LLMs and MLLMs has been largely driven
by progress in two domains. The first is the development of autoregressive LLMs
and MLLMs, which has accumulated vast amounts of data, benchmarks, and
foundational infrastructure for training and inference. The second contributing
domain is the evolution of the mathematical models underlying discrete
diffusion. Together, these advancements have catalyzed a surge in dLLMs and
dMLLMs research in early 2025.
  In this work, we present a comprehensive overview of the research in the dLLM
and dMLLM domains. We trace the historical development of dLLMs and dMLLMs,
formalize the underlying mathematical frameworks, and categorize representative
models. We further analyze key techniques for training and inference, and
summarize emerging applications across language, vision-language, and
biological domains. We conclude by discussing future directions for research
and deployment.
  Paper collection: https://github.com/LiQiiiii/DLLM-Survey</p></br><a href="http://arxiv.org/pdf/2506.11584v1" target="_blank"><h2>A Comparative Analysis of Influence Signals for Data Debugging</h2></a><strong><u>Authors:</u></strong>  Nikolaos Myrtakis, Ioannis Tsamardinos, Vassilis Christophides</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> Accepted and presented at the Data-centric Machine Learning Research (DMLR) Workshop at ICML 2024</br><p><strong><u>Abstract:</u></strong> Improving the quality of training samples is crucial for improving the
reliability and performance of ML models. In this paper, we conduct a
comparative evaluation of influence-based signals for debugging training data.
These signals can potentially identify both mislabeled and anomalous samples
from a potentially noisy training set as we build the models and hence
alleviate the need for dedicated glitch detectors. Although several
influence-based signals (e.g., Self-Influence, Average Absolute Influence,
Marginal Influence, GD-class) have been recently proposed in the literature,
there are no experimental studies for assessing their power in detecting
different glitch types (e.g., mislabeled and anomalous samples) under a common
influence estimator (e.g., TraceIn) for different data modalities (image and
tabular), and deep learning models (trained from scratch or foundation).
Through extensive experiments, we show that signals like Self-Influence
effectively detect mislabeled samples, but none of the existing signals can
detect anomalies. Existing signals do not take into account the training
dynamics, i.e., how the samples' influence on the model changes during
training, while some signals fall into influence cancellation effects, i.e.,
influence score is zero due to unsigned scores accumulation, resulting in
misleading influence attribution.</p></br><a href="http://arxiv.org/pdf/2506.13124v1" target="_blank"><h2>Parameter Recovery Study on IZI -- a Bayesian Analysis Tool for Emission
  Lines from H II Regions and Star-forming Galaxies</h2></a><strong><u>Authors:</u></strong>  Jong-Ho Shinn, Rory Smith, Kyuseok Oh</br><strong><u>Categories:</u></strong> astro-ph.GA, astro-ph.IM</br><strong><u>Comments:</u></strong> 12 pages, MNRAS in press, online supplementary data are included in the source file</br><p><strong><u>Abstract:</u></strong> We present a series of parameter recovery test results of the Bayesian
analysis tool IZI, which analyses emission lines from H II regions and
star-forming galaxies and returns the estimates of the gas metallicity 12 + log
(O/H), ionisation parameter log q, and nebular emission-line colour excess
E(B-V). We created several mock datasets using IZI to represent a few different
ideal or realistic datasets and performed parameter estimation on the mock data
with IZI. We found that IZI underestimated or overestimated the parameters by
approximately 1-$\sigma$ or greater when the model error was included, even
when using all emission lines available in the model grids. We strongly
recommend that IZI users run parameter recovery tests adjusted for their data
before interpreting the IZI estimates. To encourage the appropriate use of IZI,
we also share a script for parameter recovery tests. The cause of IZI's biased
estimation is the substantial model error in the likelihood term, which varies
with the model parameters. We thus note that any parameter estimation with a
substantial, varying model error in the likelihood term could return biased
estimates for the model parameters, such as in the case of NebulaBayes, another
Bayesian analysis tool for photoionisation emission lines. We also note two
issues relevant to setting the log q prior using the observed line ratio [S
III]$\lambda$$\lambda$9068,9532/[S II]$\lambda$6717,6731 (the mismatch of line
flux terms and violation of Bayes' theorem) and propose a way to avoid the
issues.</p></br><a href="http://arxiv.org/pdf/2506.11683v1" target="_blank"><h2>On the performance of multi-fidelity and reduced-dimensional neural
  emulators for inference of physiologic boundary conditions</h2></a><strong><u>Authors:</u></strong>  Chloe H. Choi, Andrea Zanoni, Daniele E. Schiavazzi, Alison L. Marsden</br><strong><u>Categories:</u></strong> stat.ML, cs.CE, cs.LG, math.ST, q-bio.QM, stat.TH</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Solving inverse problems in cardiovascular modeling is particularly
challenging due to the high computational cost of running high-fidelity
simulations. In this work, we focus on Bayesian parameter estimation and
explore different methods to reduce the computational cost of sampling from the
posterior distribution by leveraging low-fidelity approximations. A common
approach is to construct a surrogate model for the high-fidelity simulation
itself. Another is to build a surrogate for the discrepancy between high- and
low-fidelity models. This discrepancy, which is often easier to approximate, is
modeled with either a fully connected neural network or a nonlinear
dimensionality reduction technique that enables surrogate construction in a
lower-dimensional space. A third possible approach is to treat the discrepancy
between the high-fidelity and surrogate models as random noise and estimate its
distribution using normalizing flows. This allows us to incorporate the
approximation error into the Bayesian inverse problem by modifying the
likelihood function. We validate five different methods which are variations of
the above on analytical test cases by comparing them to posterior distributions
derived solely from high-fidelity models, assessing both accuracy and
computational cost. Finally, we demonstrate our approaches on two
cardiovascular examples of increasing complexity: a lumped-parameter Windkessel
model and a patient-specific three-dimensional anatomy.</p></br><a href="http://arxiv.org/pdf/2506.13107v1" target="_blank"><h2>Honesty in Causal Forests: When It Helps and When It Hurts</h2></a><strong><u>Authors:</u></strong>  Yanfang Hou, Carlos Fernández-Loría</br><strong><u>Categories:</u></strong> cs.LG, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Causal forests are increasingly used to personalize decisions based on
estimated treatment effects. A distinctive modeling choice in this method is
honest estimation: using separate data for splitting and for estimating effects
within leaves. This practice is the default in most implementations and is
widely seen as desirable for causal inference. But we show that honesty can
hurt the accuracy of individual-level effect estimates. The reason is a classic
bias-variance trade-off: honesty reduces variance by preventing overfitting,
but increases bias by limiting the model's ability to discover and exploit
meaningful heterogeneity in treatment effects. This trade-off depends on the
signal-to-noise ratio (SNR): honesty helps when effect heterogeneity is hard to
detect (low SNR), but hurts when the signal is strong (high SNR). In essence,
honesty acts as a form of regularization, and like any regularization choice,
it should be guided by out-of-sample performance, not adopted by default.</p></br><a href="http://arxiv.org/pdf/2506.12240v1" target="_blank"><h2>Mind the XAI Gap: A Human-Centered LLM Framework for Democratizing
  Explainable AI</h2></a><strong><u>Authors:</u></strong>  Eva Paraschou, Ioannis Arapakis, Sofia Yfantidou, Sebastian Macaluso, Athena Vakali</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> Accepted for publication at The 3rd World Conference on eXplainable Artificial Intelligence. This version corresponds to the camera-ready manuscript submitted to the conference proceedings</br><p><strong><u>Abstract:</u></strong> Artificial Intelligence (AI) is rapidly embedded in critical decision-making
systems, however their foundational ``black-box'' models require eXplainable AI
(XAI) solutions to enhance transparency, which are mostly oriented to experts,
making no sense to non-experts. Alarming evidence about AI's unprecedented
human values risks brings forward the imperative need for transparent
human-centered XAI solutions. In this work, we introduce a domain-, model-,
explanation-agnostic, generalizable and reproducible framework that ensures
both transparency and human-centered explanations tailored to the needs of both
experts and non-experts. The framework leverages Large Language Models (LLMs)
and employs in-context learning to convey domain- and explainability-relevant
contextual knowledge into LLMs. Through its structured prompt and system
setting, our framework encapsulates in one response explanations understandable
by non-experts and technical information to experts, all grounded in domain and
explainability principles. To demonstrate the effectiveness of our framework,
we establish a ground-truth contextual ``thesaurus'' through a rigorous
benchmarking with over 40 data, model, and XAI combinations for an explainable
clustering analysis of a well-being scenario. Through a comprehensive quality
and human-friendliness evaluation of our framework's explanations, we prove
high content quality through strong correlations with ground-truth explanations
(Spearman rank correlation=0.92) and improved interpretability and
human-friendliness to non-experts through a user study (N=56). Our overall
evaluation confirms trust in LLMs as HCXAI enablers, as our framework bridges
the above Gaps by delivering (i) high-quality technical explanations aligned
with foundational XAI methods and (ii) clear, efficient, and interpretable
human-centered explanations for non-experts.</p></br><a href="http://arxiv.org/pdf/2506.12558v1" target="_blank"><h2>RAW-Explainer: Post-hoc Explanations of Graph Neural Networks on
  Knowledge Graphs</h2></a><strong><u>Authors:</u></strong>  Ryoji Kubo, Djellel Difallah</br><strong><u>Categories:</u></strong> cs.LG, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Graph neural networks have demonstrated state-of-the-art performance on
knowledge graph tasks such as link prediction. However, interpreting GNN
predictions remains a challenging open problem. While many GNN explainability
methods have been proposed for node or graph-level tasks, approaches for
generating explanations for link predictions in heterogeneous settings are
limited. In this paper, we propose RAW-Explainer, a novel framework designed to
generate connected, concise, and thus interpretable subgraph explanations for
link prediction. Our method leverages the heterogeneous information in
knowledge graphs to identify connected subgraphs that serve as patterns of
factual explanation via a random walk objective. Unlike existing methods
tailored to knowledge graphs, our approach employs a neural network to
parameterize the explanation generation process, which significantly speeds up
the production of collective explanations. Furthermore, RAW-Explainer is
designed to overcome the distribution shift issue when evaluating the quality
of an explanatory subgraph which is orders of magnitude smaller than the full
graph, by proposing a robust evaluator that generalizes to the subgraph
distribution. Extensive quantitative results on real-world knowledge graph
datasets demonstrate that our approach strikes a balance between explanation
quality and computational efficiency.</p></br><a href="http://arxiv.org/pdf/2506.11997v1" target="_blank"><h2>pLSTM: parallelizable Linear Source Transition Mark networks</h2></a><strong><u>Authors:</u></strong>  Korbinian Pöppel, Richard Freinschlag, Thomas Schmied, Wei Lin, Sepp Hochreiter</br><strong><u>Categories:</u></strong> cs.LG, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Modern recurrent architectures, such as xLSTM and Mamba, have recently
challenged the Transformer in language modeling. However, their structure
constrains their applicability to sequences only or requires processing
multi-dimensional data structures, such as images or molecular graphs, in a
pre-defined sequential order. In contrast, Multi-Dimensional RNNs (MDRNNs) are
well suited for data with a higher level structure, like 2D grids, trees, and
directed acyclic graphs (DAGs). In this work, we extend the notion of
multi-dimensionality to linear RNNs. We introduce parallelizable Linear Source
Transition Mark networks (pLSTMs) using Source, Transition, and Mark gates that
act on the line graph of a general DAG. This enables parallelization in analogy
to parallel associative scans and the chunkwise-recurrent form of sequential
linear RNNs, but for DAGs. For regular grids (1D and 2D), like images, this
scheme can be efficiently implemented using einsum operations, concatenations,
and padding in logarithmic time. pLSTMs tackle the vanishing/exploding
activation/gradient problem for long distances in DAGs via two distinct modes:
a directed propagation mode (P-mode) and a diffusive distribution mode
(D-mode). To showcase the long-range capabilities of pLSTM, we introduce
arrow-pointing extrapolation as a synthetic computer vision task that contains
long-distance directional information. We demonstrate that pLSTMs generalize
well to larger image sizes, whereas Transformers struggle to extrapolate. On
established molecular graph and computer vision benchmarks, pLSTMs also show
strong performance. Code and Datasets are available at:
https://github.com/ml-jku/plstm_experiments.</p></br><a href="http://arxiv.org/pdf/2506.11901v1" target="_blank"><h2>A Neural Rejection System Against Universal Adversarial Perturbations in
  Radio Signal Classification</h2></a><strong><u>Authors:</u></strong>  Lu Zhang, Sangarapillai Lambotharan, Gan Zheng, Fabio Roli</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Advantages of deep learning over traditional methods have been demonstrated
for radio signal classification in the recent years. However, various
researchers have discovered that even a small but intentional feature
perturbation known as adversarial examples can significantly deteriorate the
performance of the deep learning based radio signal classification. Among
various kinds of adversarial examples, universal adversarial perturbation has
gained considerable attention due to its feature of being data independent,
hence as a practical strategy to fool the radio signal classification with a
high success rate. Therefore, in this paper, we investigate a defense system
called neural rejection system to propose against universal adversarial
perturbations, and evaluate its performance by generating white-box universal
adversarial perturbations. We show that the proposed neural rejection system is
able to defend universal adversarial perturbations with significantly higher
accuracy than the undefended deep neural network.</p></br><a href="http://arxiv.org/pdf/2506.11743v1" target="_blank"><h2>Taxonomy of reduction matrices for Graph Coarsening</h2></a><strong><u>Authors:</u></strong>  Antonin Joly, Nicolas Keriven, Aline Roumy</br><strong><u>Categories:</u></strong> cs.LG, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Graph coarsening aims to diminish the size of a graph to lighten its memory
footprint, and has numerous applications in graph signal processing and machine
learning. It is usually defined using a reduction matrix and a lifting matrix,
which, respectively, allows to project a graph signal from the original graph
to the coarsened one and back. This results in a loss of information measured
by the so-called Restricted Spectral Approximation (RSA). Most coarsening
frameworks impose a fixed relationship between the reduction and lifting
matrices, generally as pseudo-inverses of each other, and seek to define a
coarsening that minimizes the RSA. In this paper, we remark that the roles of
these two matrices are not entirely symmetric: indeed, putting constraints on
the lifting matrix alone ensures the existence of important objects such as the
coarsened graph's adjacency matrix or Laplacian. In light of this, in this
paper, we introduce a more general notion of reduction matrix, that is not
necessarily the pseudo-inverse of the lifting matrix. We establish a taxonomy
of ``admissible'' families of reduction matrices, discuss the different
properties that they must satisfy and whether they admit a closed-form
description or not. We show that, for a fixed coarsening represented by a fixed
lifting matrix, the RSA can be further reduced simply by modifying the
reduction matrix. We explore different examples, including some based on a
constrained optimization process of the RSA. Since this criterion has also been
linked to the performance of Graph Neural Networks, we also illustrate the
impact of this choices on different node classification tasks on coarsened
graphs.</p></br><a href="http://arxiv.org/pdf/2506.13705v1" target="_blank"><h2>TimeMaster: Training Time-Series Multimodal LLMs to Reason via
  Reinforcement Learning</h2></a><strong><u>Authors:</u></strong>  Junru Zhang, Lang Feng, Xu Guo, Yuhan Wu, Yabo Dong, Duanqing Xu</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> Preprint</br><p><strong><u>Abstract:</u></strong> Time-series reasoning remains a significant challenge in multimodal large
language models (MLLMs) due to the dynamic temporal patterns, ambiguous
semantics, and lack of temporal priors. In this work, we introduce TimeMaster,
a reinforcement learning (RL)-based method that enables time-series MLLMs to
perform structured, interpretable reasoning directly over visualized
time-series inputs and task prompts. TimeMaster adopts a three-part structured
output format, reasoning, classification, and domain-specific extension, and is
optimized via a composite reward function that aligns format adherence,
prediction accuracy, and open-ended insight quality. The model is trained using
a two-stage pipeline: we first apply supervised fine-tuning (SFT) to establish
a good initialization, followed by Group Relative Policy Optimization (GRPO) at
the token level to enable stable and targeted reward-driven improvement in
time-series reasoning. We evaluate TimeMaster on the TimerBed benchmark across
six real-world classification tasks based on Qwen2.5-VL-3B-Instruct. TimeMaster
achieves state-of-the-art performance, outperforming both classical time-series
models and few-shot GPT-4o by over 14.6% and 7.3% performance gain,
respectively. Notably, TimeMaster goes beyond time-series classification: it
also exhibits expert-like reasoning behavior, generates context-aware
explanations, and delivers domain-aligned insights. Our results highlight that
reward-driven RL can be a scalable and promising path toward integrating
temporal understanding into time-series MLLMs.</p></br><a href="http://arxiv.org/pdf/2506.11982v1" target="_blank"><h2>Interpretable representation learning of quantum data enabled by
  probabilistic variational autoencoders</h2></a><strong><u>Authors:</u></strong>  Paulin de Schoulepnikoff, Gorka Muñoz-Gil, Hendrik Poulsen Nautrup, Hans J. Briegel</br><strong><u>Categories:</u></strong> quant-ph, cond-mat.stat-mech, cs.LG</br><strong><u>Comments:</u></strong> Main text 10 pages, total document 16 pages, 10 figures</br><p><strong><u>Abstract:</u></strong> Interpretable machine learning is rapidly becoming a crucial tool for
scientific discovery. Among existing approaches, variational autoencoders
(VAEs) have shown promise in extracting the hidden physical features of some
input data, with no supervision nor prior knowledge of the system at study.
Yet, the ability of VAEs to create meaningful, interpretable representations
relies on their accurate approximation of the underlying probability
distribution of their input. When dealing with quantum data, VAEs must hence
account for its intrinsic randomness and complex correlations. While VAEs have
been previously applied to quantum data, they have often neglected its
probabilistic nature, hindering the extraction of meaningful physical
descriptors. Here, we demonstrate that two key modifications enable VAEs to
learn physically meaningful latent representations: a decoder capable of
faithfully reproduce quantum states and a probabilistic loss tailored to this
task. Using benchmark quantum spin models, we identify regimes where standard
methods fail while the representations learned by our approach remain
meaningful and interpretable. Applied to experimental data from Rydberg atom
arrays, the model autonomously uncovers the phase structure without access to
prior labels, Hamiltonian details, or knowledge of relevant order parameters,
highlighting its potential as an unsupervised and interpretable tool for the
study of quantum systems.</p></br><a href="http://arxiv.org/pdf/2506.13519v1" target="_blank"><h2>General-relativistic magnetar magnetospheres in 3D with physics-informed
  neural networks</h2></a><strong><u>Authors:</u></strong>  Petros Stefanou, Arthur G. Suvorov, Jose A. Pons</br><strong><u>Categories:</u></strong> astro-ph.HE</br><strong><u>Comments:</u></strong> 10 pages, 9 figures, submitted to MNRAS</br><p><strong><u>Abstract:</u></strong> Magnetar phenomena are likely intertwined with the location and structure of
magnetospheric currents. While general-relativistic effects may be important in
shaping the force-free equilibria describing static configurations, most
studies have been restricted to axial symmetry. Using a novel methodology based
on physics-informed neural networks, fully three-dimensional configurations of
varying stellar compactness are constructed. Realistic profiles for surface
currents, qualitatively capturing the geometry of observed hotspots, are
applied as boundary conditions to deduce the amount of free energy available to
fuel outburst activity. It is found that the lowest-energy solution branches
permit only a $\approx 30\%$ excess relative to current-starved solutions in
axisymmetric cases with global twists, regardless of compactness, reducing to
$\approx 5\%$ in 3D models with localised spots. Accounting for redshift
reductions to their inferred dipole moments from timing data, explaining
magnetar burst energetics therefore becomes more difficult unless the field
hosts non-negligible multipoles. Discussions on other aspects of magnetar
phenomena are also provided.</p></br><a href="http://arxiv.org/pdf/2506.13139v1" target="_blank"><h2>Random Matrix Theory for Deep Learning: Beyond Eigenvalues of Linear
  Models</h2></a><strong><u>Authors:</u></strong>  Zhenyu Liao, Michael W. Mahoney</br><strong><u>Categories:</u></strong> stat.ML, cs.LG</br><strong><u>Comments:</u></strong> 30 pages, 6 figures</br><p><strong><u>Abstract:</u></strong> Modern Machine Learning (ML) and Deep Neural Networks (DNNs) often operate on
high-dimensional data and rely on overparameterized models, where classical
low-dimensional intuitions break down. In particular, the proportional regime
where the data dimension, sample size, and number of model parameters are all
large and comparable, gives rise to novel and sometimes counterintuitive
behaviors. This paper extends traditional Random Matrix Theory (RMT) beyond
eigenvalue-based analysis of linear models to address the challenges posed by
nonlinear ML models such as DNNs in this regime. We introduce the concept of
High-dimensional Equivalent, which unifies and generalizes both Deterministic
Equivalent and Linear Equivalent, to systematically address three technical
challenges: high dimensionality, nonlinearity, and the need to analyze generic
eigenspectral functionals. Leveraging this framework, we provide precise
characterizations of the training and generalization performance of linear
models, nonlinear shallow networks, and deep networks. Our results capture rich
phenomena, including scaling laws, double descent, and nonlinear learning
dynamics, offering a unified perspective on the theoretical understanding of
deep learning in high dimensions.</p></br><a href="http://arxiv.org/pdf/2506.12602v1" target="_blank"><h2>Exploring the Link between Fast Radio Burst and Binary Neutron Star
  Origins with Spaceborne Gravitational Wave Observations</h2></a><strong><u>Authors:</u></strong>  Yu-xuan Yin, En-kun Li, Bing Zhang, Yi-Ming Hu</br><strong><u>Categories:</u></strong> astro-ph.HE, astro-ph.GA</br><strong><u>Comments:</u></strong> 5 pages, 3 figures</br><p><strong><u>Abstract:</u></strong> The origin of repeating Fast Radio Bursts (FRBs) is an open question, with
observations suggesting that at least some are associated with old stellar
populations. It has been proposed that some repeating FRBs may be produced by
interactions of the binary neutron star magnetospheres decades to centuries
before the coalescence. These systems would also emit centi-Hertz gravitational
waves during this period, which can be detectable by space-borne gravitational
wave detectors. We explore the prospects of using current and future
space-borne gravitational wave detectors, such as TianQin, LISA, and DECIGO, to
test this FRB formation hypothesis. Focusing on nearby galaxies like M81, which
hosts a repeating FRB source in a globular cluster, we calculate the detection
capabilities for binary neutron star systems. Our analysis reveals that while
missions like TianQin and LISA face limitations in horizon distance, changing
detector pointing direction could significantly enhance detection
probabilities. Considering the chance of a Milky Way-like galaxy coincidentally
containing a BNS within 100 years before merger is only $3\times10^{-5}$ to
$5\times10^{-3}$, if a signal is detected originating from M81, we can
establish the link between FRB and binary neutron stars with a significance
level of at least 2.81$\sigma$, or a Bayes factor of $4\times10^6 -
7\times10^8$ / $5\times10^2 - 10^5$ against the background model with
optimistic/realistic assumptions. Next-generation detectors such as DECIGO
offer enhanced capabilities and should easily detect these systems in M81 and
beyond. Our work highlights the critical role of space-borne gravitational wave
missions in unraveling FRB origins.</p></br><a href="http://arxiv.org/pdf/2506.13093v1" target="_blank"><h2>Acceleration and Collimation of the Two-Sided Jets in the Nearby
  Low-luminosity Active Galactic Nucleus NGC 4261 (3C 270)</h2></a><strong><u>Authors:</u></strong>  Xi Yan, Lang Cui, Kazuhiro Hada, Sandor Frey, Ru-sen Lu, Liang Chen, Wancheng Xu, Elika P. Fariyanto, Luis C. Ho</br><strong><u>Categories:</u></strong> astro-ph.GA, astro-ph.HE</br><strong><u>Comments:</u></strong> A revision and a similar version will appear in ApJ. Comments are welcome</br><p><strong><u>Abstract:</u></strong> We study the acceleration and collimation of the two-sided jets in the nearby
low-luminosity active galactic nucleus NGC 4261 (3C 270) using archival
multifrequency, multi-epoch Very Long Baseline Array data. By applying multiple
analysis methods and incorporating results from the literature, we robustly
identify a parabolic-to-conical structural transition in both the jet and
counterjet, with the transition occurring at $(1.23\pm0.24)\,$pc or
$(8.1\pm1.6)\times10^3\,R_{\rm s}$ (Schwarzschild radii) for the jet and
$(0.97\pm0.29)\,$pc or $(6.4\pm1.9)\times10^3\,R_{\rm s}$ for the counterjet.
Assuming that the brightness asymmetry between the twin jets is primarily due
to relativistic Doppler (de)boosting, we derive the jet velocity field at
distances of $\sim (10^3-2\times10^4)\,R_{\rm s}$ based on the
jet-to-counterjet brightness ratio and spectral index. Although local kinematic
variations are present, the jet shows an overall acceleration to relativistic
speeds from $\sim 10^3$ to $\sim8\times10^3\,R_{\rm s}$, with a maximum Lorentz
factor of $\Gamma_{\rm max} \approx 2.6$. Beyond this region, the jet gradually
decelerates to sub-relativistic speeds. These results support the existence of
a (sub)parsec-scale ($\lesssim 1.5\,$pc) acceleration and collimation zone
(ACZ) in NGC 4261, where the jet is accelerated via magnetic-to-kinetic energy
conversion while being confined by external pressure. A brief comparison with M
87 suggests that the ACZ in NGC 4261 may represent a scaled-down analogue of
that in M 87.</p></br><a href="http://arxiv.org/pdf/2506.13064v1" target="_blank"><h2>CoIFNet: A Unified Framework for Multivariate Time Series Forecasting
  with Missing Values</h2></a><strong><u>Authors:</u></strong>  Kai Tang, Ji Zhang, Hua Meng, Minbo Ma, Qi Xiong, Jie Xu, Tianrui Li</br><strong><u>Categories:</u></strong> cs.LG, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Multivariate time series forecasting (MTSF) is a critical task with broad
applications in domains such as meteorology, transportation, and economics.
Nevertheless, pervasive missing values caused by sensor failures or human
errors significantly degrade forecasting accuracy. Prior efforts usually employ
an impute-then-forecast paradigm, leading to suboptimal predictions due to
error accumulation and misaligned objectives between the two stages. To address
this challenge, we propose the Collaborative Imputation-Forecasting Network
(CoIFNet), a novel framework that unifies imputation and forecasting to achieve
robust MTSF in the presence of missing values. Specifically, CoIFNet takes the
observed values, mask matrix and timestamp embeddings as input, processing them
sequentially through the Cross-Timestep Fusion (CTF) and Cross-Variate Fusion
(CVF) modules to capture temporal dependencies that are robust to missing
values. We provide theoretical justifications on how our CoIFNet learning
objective improves the performance bound of MTSF with missing values. Through
extensive experiments on challenging MSTF benchmarks, we demonstrate the
effectiveness and computational efficiency of our proposed approach across
diverse missing-data scenarios, e.g., CoIFNet outperforms the state-of-the-art
method by $\underline{\textbf{24.40}}$% ($\underline{\textbf{23.81}}$%) at a
point (block) missing rate of 0.6, while improving memory and time efficiency
by $\underline{\boldsymbol{4.3\times}}$ and
$\underline{\boldsymbol{2.1\times}}$, respectively.</p></br></body>