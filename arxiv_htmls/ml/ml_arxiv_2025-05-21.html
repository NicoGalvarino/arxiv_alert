<link href='https://fonts.googleapis.com/css?family=Montserrat' rel='stylesheet'><style>     body {font-family: 'Montserrat'; background: #F3F3F3; width: 740px; margin: 0 auto; line-height: 150%; margin-top: 50px; font-size: 15px}         h1 {font-size: 70px}         a {color: #45ABC2}         em {font-size: 120%} </style><h1><center>ArXiv Alert</center></h1><font color='#DDAD5C'><em>Update: from 15 May 2025 to 21 May 2025</em></font><a href="http://arxiv.org/pdf/2505.12751v1" target="_blank"><h2>Structure-based Anomaly Detection and Clustering</h2></a><strong><u>Authors:</u></strong>  Filippo Leveni</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.CV, stat.ML</br><strong><u>Comments:</u></strong> Doctoral dissertation at Politecnico di Milano</br><p><strong><u>Abstract:</u></strong> Anomaly detection is a fundamental problem in domains such as healthcare,
manufacturing, and cybersecurity. This thesis proposes new unsupervised methods
for anomaly detection in both structured and streaming data settings. In the
first part, we focus on structure-based anomaly detection, where normal data
follows low-dimensional manifolds while anomalies deviate from them. We
introduce Preference Isolation Forest (PIF), which embeds data into a
high-dimensional preference space via manifold fitting, and isolates outliers
using two variants: Voronoi-iForest, based on geometric distances, and
RuzHash-iForest, leveraging Locality Sensitive Hashing for scalability. We also
propose Sliding-PIF, which captures local manifold information for streaming
scenarios. Our methods outperform existing techniques on synthetic and real
datasets. We extend this to structure-based clustering with MultiLink, a novel
method for recovering multiple geometric model families in noisy data.
MultiLink merges clusters via a model-aware linkage strategy, enabling robust
multi-class structure recovery. It offers key advantages over existing
approaches, such as speed, reduced sensitivity to thresholds, and improved
robustness to poor initial sampling. The second part of the thesis addresses
online anomaly detection in evolving data streams. We propose Online Isolation
Forest (Online-iForest), which uses adaptive, multi-resolution histograms and
dynamically updates tree structures to track changes over time. It avoids
retraining while achieving accuracy comparable to offline models, with superior
efficiency for real-time applications. Finally, we tackle anomaly detection in
cybersecurity via open-set recognition for malware classification. We enhance a
Gradient Boosting classifier with MaxLogit to detect unseen malware families, a
method now integrated into Cleafy's production system.</p></br><a href="http://arxiv.org/pdf/2505.13518v1" target="_blank"><h2>Data Balancing Strategies: A Survey of Resampling and Augmentation
  Methods</h2></a><strong><u>Authors:</u></strong>  Behnam Yousefimehr, Mehdi Ghatee, Mohammad Amin Seifi, Javad Fazli, Sajed Tavakoli, Zahra Rafei, Shervin Ghaffari, Abolfazl Nikahd, Mahdi Razi Gandomani, Alireza Orouji, Ramtin Mahmoudi Kashani, Sarina Heshmati, Negin Sadat Mousavi</br><strong><u>Categories:</u></strong> stat.ML, cs.AI, cs.LG</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Imbalanced data poses a significant obstacle in machine learning, as an
unequal distribution of class labels often results in skewed predictions and
diminished model accuracy. To mitigate this problem, various resampling
strategies have been developed, encompassing both oversampling and
undersampling techniques aimed at modifying class proportions. Conventional
oversampling approaches like SMOTE enhance the representation of the minority
class, whereas undersampling methods focus on trimming down the majority class.
Advances in deep learning have facilitated the creation of more complex
solutions, such as Generative Adversarial Networks (GANs) and Variational
Autoencoders (VAEs), which are capable of producing high-quality synthetic
examples. This paper reviews a broad spectrum of data balancing methods,
classifying them into categories including synthetic oversampling, adaptive
techniques, generative models, ensemble-based strategies, hybrid approaches,
undersampling, and neighbor-based methods. Furthermore, it highlights current
developments in resampling techniques and discusses practical implementations
and case studies that validate their effectiveness. The paper concludes by
offering perspectives on potential directions for future exploration in this
domain.</p></br><a href="http://arxiv.org/pdf/2505.10873v1" target="_blank"><h2>Hashing for Structure-based Anomaly Detection</h2></a><strong><u>Authors:</u></strong>  Filippo Leveni, Luca Magri, Cesare Alippi, Giacomo Boracchi</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.CV, stat.ML</br><strong><u>Comments:</u></strong> Accepted at International Conference on Image Analysis and Processing (ICIAP 2023)</br><p><strong><u>Abstract:</u></strong> We focus on the problem of identifying samples in a set that do not conform
to structured patterns represented by low-dimensional manifolds. An effective
way to solve this problem is to embed data in a high dimensional space, called
Preference Space, where anomalies can be identified as the most isolated
points. In this work, we employ Locality Sensitive Hashing to avoid explicit
computation of distances in high dimensions and thus improve Anomaly Detection
efficiency. Specifically, we present an isolation-based anomaly detection
technique designed to work in the Preference Space which achieves
state-of-the-art performance at a lower computational cost. Code is publicly
available at
https://github.com/ineveLoppiliF/Hashing-for-Structure-based-Anomaly-Detection.</p></br><a href="http://arxiv.org/pdf/2505.10876v1" target="_blank"><h2>Preference Isolation Forest for Structure-based Anomaly Detection</h2></a><strong><u>Authors:</u></strong>  Filippo Leveni, Luca Magri, Cesare Alippi, Giacomo Boracchi</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.CV, stat.ML</br><strong><u>Comments:</u></strong> Submitted to Pattern Recognition</br><p><strong><u>Abstract:</u></strong> We address the problem of detecting anomalies as samples that do not conform
to structured patterns represented by low-dimensional manifolds. To this end,
we conceive a general anomaly detection framework called Preference Isolation
Forest (PIF), that combines the benefits of adaptive isolation-based methods
with the flexibility of preference embedding. The key intuition is to embed the
data into a high-dimensional preference space by fitting low-dimensional
manifolds, and to identify anomalies as isolated points. We propose three
isolation approaches to identify anomalies: $i$) Voronoi-iForest, the most
general solution, $ii$) RuzHash-iForest, that avoids explicit computation of
distances via Local Sensitive Hashing, and $iii$) Sliding-PIF, that leverages a
locality prior to improve efficiency and effectiveness.</p></br><a href="http://arxiv.org/pdf/2505.11321v1" target="_blank"><h2>Anomaly Detection for Non-stationary Time Series using Recurrent Wavelet
  Probabilistic Neural Network</h2></a><strong><u>Authors:</u></strong>  Pu Yang, J. A. Barria</br><strong><u>Categories:</u></strong> cs.LG, eess.SP</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> In this paper, an unsupervised Recurrent Wavelet Probabilistic Neural Network
(RWPNN) is proposed, which aims at detecting anomalies in non-stationary
environments by modelling the temporal features using a nonparametric density
estimation network. The novel framework consists of two components, a Stacked
Recurrent Encoder-Decoder (SREnc-Dec) module that captures temporal features in
a latent space, and a Multi-Receptive-field Wavelet Probabilistic Network
(MRWPN) that creates an ensemble probabilistic model to characterise the latent
space. This formulation extends the standard wavelet probabilistic networks to
wavelet deep probabilistic networks, which can handle higher data
dimensionality. The MRWPN module can adapt to different rates of data variation
in different datasets without imposing strong distribution assumptions,
resulting in a more robust and accurate detection for Time Series Anomaly
Detection (TSAD) tasks in the non-stationary environment. We carry out the
assessment on 45 real-world time series datasets from various domains, verify
the performance of RWPNN in TSAD tasks with several constraints, and show its
ability to provide early warnings for anomalous events.</p></br><a href="http://arxiv.org/pdf/2505.13544v1" target="_blank"><h2>Multi-head Temporal Latent Attention</h2></a><strong><u>Authors:</u></strong>  Keqi Deng, Philip C. Woodland</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> While Transformer self-attention offers strong parallelism, the Key-Value
(KV) cache grows linearly with sequence length and becomes a bottleneck for
inference efficiency. Multi-head latent attention was recently developed to
compress the KV cache into a low-rank latent space. This paper proposes
Multi-head Temporal Latent Attention (MTLA), which further reduces the KV cache
size along the temporal dimension, greatly lowering the memory footprint of
self-attention inference. MTLA employs a hyper-network to dynamically merge
temporally adjacent KV cache vectors. To address the mismatch between the
compressed KV cache and processed sequence lengths, a stride-aware causal mask
is proposed to ensure efficient parallel training and consistency with
inference behaviour. Experiments across tasks, including speech translation,
speech recognition, speech understanding and text summarisation, demonstrate
that MTLA achieves competitive performance compared to standard Multi-Head
Attention (MHA), while greatly improving inference speed and GPU memory usage.
For example, on a English-German speech translation task, MTLA achieves a 5.3x
speedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA,
while maintaining translation quality.</p></br><a href="http://arxiv.org/pdf/2505.13906v1" target="_blank"><h2>XDementNET: An Explainable Attention Based Deep Convolutional Network to
  Detect Alzheimer Progression from MRI data</h2></a><strong><u>Authors:</u></strong>  Soyabul Islam Lincoln, Mirza Mohd Shahriar Maswood</br><strong><u>Categories:</u></strong> eess.IV, cs.AI, cs.CV</br><strong><u>Comments:</u></strong> 20 pages, 12 figures,</br><p><strong><u>Abstract:</u></strong> A common neurodegenerative disease, Alzheimer's disease requires a precise
diagnosis and efficient treatment, particularly in light of escalating
healthcare expenses and the expanding use of artificial intelligence in medical
diagnostics. Many recent studies shows that the combination of brain Magnetic
Resonance Imaging (MRI) and deep neural networks have achieved promising
results for diagnosing AD. Using deep convolutional neural networks, this paper
introduces a novel deep learning architecture that incorporates multiresidual
blocks, specialized spatial attention blocks, grouped query attention, and
multi-head attention. The study assessed the model's performance on four
publicly accessible datasets and concentrated on identifying binary and
multiclass issues across various categories. This paper also takes into account
of the explainability of AD's progression and compared with state-of-the-art
methods namely Gradient Class Activation Mapping (GradCAM), Score-CAM, Faster
Score-CAM, and XGRADCAM. Our methodology consistently outperforms current
approaches, achieving 99.66\% accuracy in 4-class classification, 99.63\% in
3-class classification, and 100\% in binary classification using Kaggle
datasets. For Open Access Series of Imaging Studies (OASIS) datasets the
accuracies are 99.92\%, 99.90\%, and 99.95\% respectively. The Alzheimer's
Disease Neuroimaging Initiative-1 (ADNI-1) dataset was used for experiments in
three planes (axial, sagittal, and coronal) and a combination of all planes.
The study achieved accuracies of 99.08\% for axis, 99.85\% for sagittal, 99.5\%
for coronal, and 99.17\% for all axis, and 97.79\% and 8.60\% respectively for
ADNI-2. The network's ability to retrieve important information from MRI images
is demonstrated by its excellent accuracy in categorizing AD stages.</p></br><a href="http://arxiv.org/pdf/2505.13519v1" target="_blank"><h2>Continuous Domain Generalization</h2></a><strong><u>Authors:</u></strong>  Zekun Cai, Yiheng Yao, Guangji Bai, Renhe Jiang, Xuan Song, Ryosuke Shibasaki, Liang Zhao</br><strong><u>Categories:</u></strong> stat.ML, cs.AI, cs.LG</br><strong><u>Comments:</u></strong> 22 pages, 9 figures</br><p><strong><u>Abstract:</u></strong> Real-world data distributions often shift continuously across multiple latent
factors such as time, geography, and socioeconomic context. However, existing
domain generalization approaches typically treat domains as discrete or
evolving along a single axis (e.g., time), which fails to capture the complex,
multi-dimensional nature of real-world variation. This paper introduces the
task of Continuous Domain Generalization (CDG), which aims to generalize
predictive models to unseen domains defined by arbitrary combinations of
continuous variation descriptors. We present a principled framework grounded in
geometric and algebraic theory, showing that optimal model parameters across
domains lie on a low-dimensional manifold. To model this structure, we propose
a Neural Lie Transport Operator (NeuralLTO), which enables structured parameter
transitions by enforcing geometric continuity and algebraic consistency. To
handle noisy or incomplete domain descriptors, we introduce a gating mechanism
to suppress irrelevant dimensions and a local chart-based strategy for robust
generalization. Extensive experiments on synthetic and real-world
datasets-including remote sensing, scientific documents, and traffic
forecasting-demonstrate that our method significantly outperforms existing
baselines in generalization accuracy and robustness under descriptor
imperfections.</p></br><a href="http://arxiv.org/pdf/2505.13324v1" target="_blank"><h2>From What Ifs to Insights: Counterfactuals in Causal Inference vs.
  Explainable AI</h2></a><strong><u>Authors:</u></strong>  Galit Shmueli, David Martens, Jaewon Yoo, Travis Greene</br><strong><u>Categories:</u></strong> stat.ML, cs.AI, cs.LG, econ.EM, stat.ME</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Counterfactuals play a pivotal role in the two distinct data science fields
of causal inference (CI) and explainable artificial intelligence (XAI). While
the core idea behind counterfactuals remains the same in both fields--the
examination of what would have happened under different circumstances--there
are key differences in how they are used and interpreted. We introduce a formal
definition that encompasses the multi-faceted concept of the counterfactual in
CI and XAI. We then discuss how counterfactuals are used, evaluated, generated,
and operationalized in CI vs. XAI, highlighting conceptual and practical
differences. By comparing and contrasting the two, we hope to identify
opportunities for cross-fertilization across CI and XAI.</p></br><a href="http://arxiv.org/pdf/2505.11771v1" target="_blank"><h2>Residual Feature Integration is Sufficient to Prevent Negative Transfer</h2></a><strong><u>Authors:</u></strong>  Yichen Xu, Ryumei Nakada, Linjun Zhang, Lexin Li</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, math.ST, stat.ML, stat.TH</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Transfer learning typically leverages representations learned from a source
domain to improve performance on a target task. A common approach is to extract
features from a pre-trained model and directly apply them for target
prediction. However, this strategy is prone to negative transfer where the
source representation fails to align with the target distribution. In this
article, we propose Residual Feature Integration (REFINE), a simple yet
effective method designed to mitigate negative transfer. Our approach combines
a fixed source-side representation with a trainable target-side encoder and
fits a shallow neural network on the resulting joint representation, which
adapts to the target domain while preserving transferable knowledge from the
source domain. Theoretically, we prove that REFINE is sufficient to prevent
negative transfer under mild conditions, and derive the generalization bound
demonstrating its theoretical benefit. Empirically, we show that REFINE
consistently enhances performance across diverse application and data
modalities including vision, text, and tabular data, and outperforms numerous
alternative solutions. Our method is lightweight, architecture-agnostic, and
robust, making it a valuable addition to the existing transfer learning
toolbox.</p></br><a href="http://arxiv.org/pdf/2505.11770v1" target="_blank"><h2>Internal Causal Mechanisms Robustly Predict Language Model
  Out-of-Distribution Behaviors</h2></a><strong><u>Authors:</u></strong>  Jing Huang, Junyi Tao, Thomas Icard, Diyi Yang, Christopher Potts</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.CL, stat.ML</br><strong><u>Comments:</u></strong> ICML 2025</br><p><strong><u>Abstract:</u></strong> Interpretability research now offers a variety of techniques for identifying
abstract internal mechanisms in neural networks. Can such techniques be used to
predict how models will behave on out-of-distribution examples? In this work,
we provide a positive answer to this question. Through a diverse set of
language modeling tasks--including symbol manipulation, knowledge retrieval,
and instruction following--we show that the most robust features for
correctness prediction are those that play a distinctive causal role in the
model's behavior. Specifically, we propose two methods that leverage causal
mechanisms to predict the correctness of model outputs: counterfactual
simulation (checking whether key causal variables are realized) and value
probing (using the values of those variables to make predictions). Both achieve
high AUC-ROC in distribution and outperform methods that rely on
causal-agnostic features in out-of-distribution settings, where predicting
model behaviors is more crucial. Our work thus highlights a novel and
significant application for internal causal analysis of language models.</p></br><a href="http://arxiv.org/pdf/2505.14005v1" target="_blank"><h2>Towards Comprehensive and Prerequisite-Free Explainer for Graph Neural
  Networks</h2></a><strong><u>Authors:</u></strong>  Han Zhang, Yan Wang, Guanfeng Liu, Pengfei Ding, Huaxiong Wang, Kwok-Yan Lam</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> Accepted by IJCAI 2025 AI4Tech Track</br><p><strong><u>Abstract:</u></strong> To enhance the reliability and credibility of graph neural networks (GNNs)
and improve the transparency of their decision logic, a new field of
explainability of GNNs (XGNN) has emerged. However, two major limitations
severely degrade the performance and hinder the generalizability of existing
XGNN methods: they (a) fail to capture the complete decision logic of GNNs
across diverse distributions in the entire dataset's sample space, and (b)
impose strict prerequisites on edge properties and GNN internal accessibility.
To address these limitations, we propose OPEN, a novel c\textbf{O}mprehensive
and \textbf{P}rerequisite-free \textbf{E}xplainer for G\textbf{N}Ns. OPEN, as
the first work in the literature, can infer and partition the entire dataset's
sample space into multiple environments, each containing graphs that follow a
distinct distribution. OPEN further learns the decision logic of GNNs across
different distributions by sampling subgraphs from each environment and
analyzing their predictions, thus eliminating the need for strict
prerequisites. Experimental results demonstrate that OPEN captures nearly
complete decision logic of GNNs, outperforms state-of-the-art methods in
fidelity while maintaining similar efficiency, and enhances robustness in
real-world scenarios.</p></br><a href="http://arxiv.org/pdf/2505.12944v1" target="_blank"><h2>CALM-PDE: Continuous and Adaptive Convolutions for Latent Space Modeling
  of Time-dependent PDEs</h2></a><strong><u>Authors:</u></strong>  Jan Hagnberger, Daniel Musekamp, Mathias Niepert</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.CV, cs.NE, physics.comp-ph</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Solving time-dependent Partial Differential Equations (PDEs) using a densely
discretized spatial domain is a fundamental problem in various scientific and
engineering disciplines, including modeling climate phenomena and fluid
dynamics. However, performing these computations directly in the physical space
often incurs significant computational costs. To address this issue, several
neural surrogate models have been developed that operate in a compressed latent
space to solve the PDE. While these approaches reduce computational complexity,
they often use Transformer-based attention mechanisms to handle irregularly
sampled domains, resulting in increased memory consumption. In contrast,
convolutional neural networks allow memory-efficient encoding and decoding but
are limited to regular discretizations. Motivated by these considerations, we
propose CALM-PDE, a model class that efficiently solves arbitrarily discretized
PDEs in a compressed latent space. We introduce a novel continuous
convolution-based encoder-decoder architecture that uses an
epsilon-neighborhood-constrained kernel and learns to apply the convolution
operator to adaptive and optimized query points. We demonstrate the
effectiveness of CALM-PDE on a diverse set of PDEs with both regularly and
irregularly sampled spatial domains. CALM-PDE is competitive with or
outperforms existing baseline methods while offering significant improvements
in memory and inference time efficiency compared to Transformer-based methods.</p></br><a href="http://arxiv.org/pdf/2505.11625v1" target="_blank"><h2>Nearest Neighbor Multivariate Time Series Forecasting</h2></a><strong><u>Authors:</u></strong>  Huiliang Zhang, Ping Nie, Lijun Sun, Benoit Boulet</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Multivariate time series (MTS) forecasting has a wide range of applications
in both industry and academia. Recently, spatial-temporal graph neural networks
(STGNNs) have gained popularity as MTS forecasting methods. However, current
STGNNs can only use the finite length of MTS input data due to the
computational complexity. Moreover, they lack the ability to identify similar
patterns throughout the entire dataset and struggle with data that exhibit
sparsely and discontinuously distributed correlations among variables over an
extensive historical period, resulting in only marginal improvements. In this
article, we introduce a simple yet effective k-nearest neighbor MTS forecasting
( kNN-MTS) framework, which forecasts with a nearest neighbor retrieval
mechanism over a large datastore of cached series, using representations from
the MTS model for similarity search. This approach requires no additional
training and scales to give the MTS model direct access to the whole dataset at
test time, resulting in a highly expressive model that consistently improves
performance, and has the ability to extract sparse distributed but similar
patterns spanning over multivariables from the entire dataset. Furthermore, a
hybrid spatial-temporal encoder (HSTEncoder) is designed for kNN-MTS which can
capture both long-term temporal and short-term spatial-temporal dependencies
and is shown to provide accurate representation for kNN-MTSfor better
forecasting. Experimental results on several real-world datasets show a
significant improvement in the forecasting performance of kNN-MTS. The
quantitative analysis also illustrates the interpretability and efficiency of
kNN-MTS, showing better application prospects and opening up a new path for
efficiently using the large dataset in MTS models.</p></br><a href="http://arxiv.org/pdf/2505.11621v1" target="_blank"><h2>A Classical View on Benign Overfitting: The Role of Sample Size</h2></a><strong><u>Authors:</u></strong>  Junhyung Park, Patrick Bloebaum, Shiva Prasad Kasiviswanathan</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML</br><strong><u>Comments:</u></strong> The results here subsume:arXiv:2410.06191</br><p><strong><u>Abstract:</u></strong> Benign overfitting is a phenomenon in machine learning where a model
perfectly fits (interpolates) the training data, including noisy examples, yet
still generalizes well to unseen data. Understanding this phenomenon has
attracted considerable attention in recent years. In this work, we introduce a
conceptual shift, by focusing on almost benign overfitting, where models
simultaneously achieve both arbitrarily small training and test errors. This
behavior is characteristic of neural networks, which often achieve low (but
non-zero) training error while still generalizing well. We hypothesize that
this almost benign overfitting can emerge even in classical regimes, by
analyzing how the interaction between sample size and model complexity enables
larger models to achieve both good training fit but still approach
Bayes-optimal generalization. We substantiate this hypothesis with theoretical
evidence from two case studies: (i) kernel ridge regression, and (ii)
least-squares regression using a two-layer fully connected ReLU neural network
trained via gradient flow. In both cases, we overcome the strong assumptions
often required in prior work on benign overfitting.
  Our results on neural networks also provide the first generalization result
in this setting that does not rely on any assumptions about the underlying
regression function or noise, beyond boundedness. Our analysis introduces a
novel proof technique based on decomposing the excess risk into estimation and
approximation errors, interpreting gradient flow as an implicit regularizer,
that helps avoid uniform convergence traps. This analysis idea could be of
independent interest.</p></br><a href="http://arxiv.org/pdf/2505.13828v1" target="_blank"><h2>Multimodal RAG-driven Anomaly Detection and Classification in Laser
  Powder Bed Fusion using Large Language Models</h2></a><strong><u>Authors:</u></strong>  Kiarash Naghavi Khanghah, Zhiling Chen, Lela Romeo, Qian Yang, Rajiv Malhotra, Farhad Imani, Hongyi Xu</br><strong><u>Categories:</u></strong> cs.AI</br><strong><u>Comments:</u></strong> ASME 2025 International Design Engineering Technical Conferences and Computers and Information in Engineering Conference IDETC/CIE2025, August 17-20, 2025, Anaheim, CA (IDETC2025-168615)</br><p><strong><u>Abstract:</u></strong> Additive manufacturing enables the fabrication of complex designs while
minimizing waste, but faces challenges related to defects and process
anomalies. This study presents a novel multimodal Retrieval-Augmented
Generation-based framework that automates anomaly detection across various
Additive Manufacturing processes leveraging retrieved information from
literature, including images and descriptive text, rather than training
datasets. This framework integrates text and image retrieval from scientific
literature and multimodal generation models to perform zero-shot anomaly
identification, classification, and explanation generation in a Laser Powder
Bed Fusion setting. The proposed framework is evaluated on four L-PBF
manufacturing datasets from Oak Ridge National Laboratory, featuring various
printer makes, models, and materials. This evaluation demonstrates the
framework's adaptability and generalizability across diverse images without
requiring additional training. Comparative analysis using Qwen2-VL-2B and
GPT-4o-mini as MLLM within the proposed framework highlights that GPT-4o-mini
outperforms Qwen2-VL-2B and proportional random baseline in manufacturing
anomalies classification. Additionally, the evaluation of the RAG system
confirms that incorporating retrieval mechanisms improves average accuracy by
12% by reducing the risk of hallucination and providing additional information.
The proposed framework can be continuously updated by integrating emerging
research, allowing seamless adaptation to the evolving landscape of AM
technologies. This scalable, automated, and zero-shot-capable framework
streamlines AM anomaly analysis, enhancing efficiency and accuracy.</p></br><a href="http://arxiv.org/pdf/2505.11085v1" target="_blank"><h2>A Fast Kernel-based Conditional Independence test with Application to
  Causal Discovery</h2></a><strong><u>Authors:</u></strong>  Oliver Schacht, Biwei Huang</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML</br><strong><u>Comments:</u></strong> 9 pages, 5 figures</br><p><strong><u>Abstract:</u></strong> Kernel-based conditional independence (KCI) testing is a powerful
nonparametric method commonly employed in causal discovery tasks. Despite its
flexibility and statistical reliability, cubic computational complexity limits
its application to large datasets. To address this computational bottleneck, we
propose \textit{FastKCI}, a scalable and parallelizable kernel-based
conditional independence test that utilizes a mixture-of-experts approach
inspired by embarrassingly parallel inference techniques for Gaussian
processes. By partitioning the dataset based on a Gaussian mixture model over
the conditioning variables, FastKCI conducts local KCI tests in parallel,
aggregating the results using an importance-weighted sampling scheme.
Experiments on synthetic datasets and benchmarks on real-world production data
validate that FastKCI maintains the statistical power of the original KCI test
while achieving substantial computational speedups. FastKCI thus represents a
practical and efficient solution for conditional independence testing in causal
inference on large-scale data.</p></br><a href="http://arxiv.org/pdf/2505.12096v1" target="_blank"><h2>When the Left Foot Leads to the Right Path: Bridging Initial Prejudice
  and Trainability</h2></a><strong><u>Authors:</u></strong>  Alberto Bassi, Carlo Albert, Aurelien Lucchi, Marco Baity-Jesi, Emanuele Francazi</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Understanding the statistical properties of deep neural networks (DNNs) at
initialization is crucial for elucidating both their trainability and the
intrinsic architectural biases they encode prior to data exposure. Mean-field
(MF) analyses have demonstrated that the parameter distribution in randomly
initialized networks dictates whether gradients vanish or explode.
Concurrently, untrained DNNs were found to exhibit an initial-guessing bias
(IGB), in which large regions of the input space are assigned to a single
class. In this work, we derive a theoretical proof establishing the
correspondence between IGB and previous MF theories, thereby connecting a
network prejudice toward specific classes with the conditions for fast and
accurate learning. This connection yields the counter-intuitive conclusion: the
initialization that optimizes trainability is necessarily biased, rather than
neutral. Furthermore, we extend the MF/IGB framework to multi-node activation
functions, offering practical guidelines for designing initialization schemes
that ensure stable optimization in architectures employing max- and
average-pooling layers.</p></br><a href="http://arxiv.org/pdf/2505.12353v1" target="_blank"><h2>Importance Sampling for Nonlinear Models</h2></a><strong><u>Authors:</u></strong>  Prakash Palanivelu Rajmohan, Fred Roosta</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML</br><strong><u>Comments:</u></strong> This work is accepted at ICML 2025</br><p><strong><u>Abstract:</u></strong> While norm-based and leverage-score-based methods have been extensively
studied for identifying "important" data points in linear models, analogous
tools for nonlinear models remain significantly underdeveloped. By introducing
the concept of the adjoint operator of a nonlinear map, we address this gap and
generalize norm-based and leverage-score-based importance sampling to nonlinear
settings. We demonstrate that sampling based on these generalized notions of
norm and leverage scores provides approximation guarantees for the underlying
nonlinear mapping, similar to linear subspace embeddings. As direct
applications, these nonlinear scores not only reduce the computational
complexity of training nonlinear models by enabling efficient sampling over
large datasets but also offer a novel mechanism for model explainability and
outlier detection. Our contributions are supported by both theoretical analyses
and experimental results across a variety of supervised learning scenarios.</p></br><a href="http://arxiv.org/pdf/2505.11785v1" target="_blank"><h2>Improving Coverage in Combined Prediction Sets with Weighted p-values</h2></a><strong><u>Authors:</u></strong>  Gina Wong, Drew Prinster, Suchi Saria, Rama Chellappa, Anqi Liu</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Conformal prediction quantifies the uncertainty of machine learning models by
augmenting point predictions with valid prediction sets, assuming
exchangeability. For complex scenarios involving multiple trials, models, or
data sources, conformal prediction sets can be aggregated to create a
prediction set that captures the overall uncertainty, often improving
precision. However, aggregating multiple prediction sets with individual
$1-\alpha$ coverage inevitably weakens the overall guarantee, typically
resulting in $1-2\alpha$ worst-case coverage. In this work, we propose a
framework for the weighted aggregation of prediction sets, where weights are
assigned to each prediction set based on their contribution. Our framework
offers flexible control over how the sets are aggregated, achieving tighter
coverage bounds that interpolate between the $1-2\alpha$ guarantee of the
combined models and the $1-\alpha$ guarantee of an individual model depending
on the distribution of weights. We extend our framework to data-dependent
weights, and we derive a general procedure for data-dependent weight
aggregation that maintains finite-sample validity. We demonstrate the
effectiveness of our methods through experiments on synthetic and real data in
the mixture-of-experts setting, and we show that aggregation with
data-dependent weights provides a form of adaptive coverage.</p></br><a href="http://arxiv.org/pdf/2505.13188v1" target="_blank"><h2>When a Reinforcement Learning Agent Encounters Unknown Unknowns</h2></a><strong><u>Authors:</u></strong>  Juntian Zhu, Miguel de Carvalho, Zhouwang Yang, Fengxiang He</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> An AI agent might surprisingly find she has reached an unknown state which
she has never been aware of -- an unknown unknown. We mathematically ground
this scenario in reinforcement learning: an agent, after taking an action
calculated from value functions $Q$ and $V$ defined on the {\it {aware
domain}}, reaches a state out of the domain. To enable the agent to handle this
scenario, we propose an {\it episodic Markov decision {process} with growing
awareness} (EMDP-GA) model, taking a new {\it noninformative value expansion}
(NIVE) approach to expand value functions to newly aware areas: when an agent
arrives at an unknown unknown, value functions $Q$ and $V$ whereon are
initialised by noninformative beliefs -- the averaged values on the aware
domain. This design is out of respect for the complete absence of knowledge in
the newly discovered state. The upper confidence bound momentum Q-learning is
then adapted to the growing awareness for training the EMDP-GA model. We prove
that (1) the regret of our approach is asymptotically consistent with the state
of the art (SOTA) without exposure to unknown unknowns in an extremely
uncertain environment, and (2) our computational complexity and space
complexity are comparable with the SOTA -- these collectively suggest that
though an unknown unknown is surprising, it will be asymptotically properly
discovered with decent speed and an affordable cost.</p></br><a href="http://arxiv.org/pdf/2505.11211v1" target="_blank"><h2>Bayesian Hierarchical Invariant Prediction</h2></a><strong><u>Authors:</u></strong>  Francisco Madaleno, Pernille Julie Viuff Sand, Francisco C. Pereira, Sergio Hernan Garrido Mejia</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ME, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> We propose Bayesian Hierarchical Invariant Prediction (BHIP) reframing
Invariant Causal Prediction (ICP) through the lens of Hierarchical Bayes. We
leverage the hierarchical structure to explicitly test invariance of causal
mechanisms under heterogeneous data, resulting in improved computational
scalability for a larger number of predictors compared to ICP. Moreover, given
its Bayesian nature BHIP enables the use of prior information. In this paper,
we test two sparsity inducing priors: horseshoe and spike-and-slab, both of
which allow us a more reliable identification of causal features. We test BHIP
in synthetic and real-world data showing its potential as an alternative
inference method to ICP.</p></br><a href="http://arxiv.org/pdf/2505.11143v1" target="_blank"><h2>Nash: Neural Adaptive Shrinkage for Structured High-Dimensional
  Regression</h2></a><strong><u>Authors:</u></strong>  William R. P. Denault</br><strong><u>Categories:</u></strong> stat.ML, cs.LG</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Sparse linear regression is a fundamental tool in data analysis. However,
traditional approaches often fall short when covariates exhibit structure or
arise from heterogeneous sources. In biomedical applications, covariates may
stem from distinct modalities or be structured according to an underlying
graph. We introduce Neural Adaptive Shrinkage (Nash), a unified framework that
integrates covariate-specific side information into sparse regression via
neural networks. Nash adaptively modulates penalties on a per-covariate basis,
learning to tailor regularization without cross-validation. We develop a
variational inference algorithm for efficient training and establish
connections to empirical Bayes regression. Experiments on real data demonstrate
that Nash can improve accuracy and adaptability over existing methods.</p></br><a href="http://arxiv.org/pdf/2505.12254v1" target="_blank"><h2>MMS-VPR: Multimodal Street-Level Visual Place Recognition Dataset and
  Benchmark</h2></a><strong><u>Authors:</u></strong>  Yiwei Ou, Xiaobin Ren, Ronggui Sun, Guansong Gao, Ziyi Jiang, Kaiqi Zhao, Manfredo Manfredini</br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.LG</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Existing visual place recognition (VPR) datasets predominantly rely on
vehicle-mounted imagery, lack multimodal diversity and underrepresent dense,
mixed-use street-level spaces, especially in non-Western urban contexts. To
address these gaps, we introduce MMS-VPR, a large-scale multimodal dataset for
street-level place recognition in complex, pedestrian-only environments. The
dataset comprises 78,575 annotated images and 2,512 video clips captured across
207 locations in a ~70,800 $\mathrm{m}^2$ open-air commercial district in
Chengdu, China. Each image is labeled with precise GPS coordinates, timestamp,
and textual metadata, and covers varied lighting conditions, viewpoints, and
timeframes. MMS-VPR follows a systematic and replicable data collection
protocol with minimal device requirements, lowering the barrier for scalable
dataset creation. Importantly, the dataset forms an inherent spatial graph with
125 edges, 81 nodes, and 1 subgraph, enabling structure-aware place
recognition. We further define two application-specific subsets --
Dataset_Edges and Dataset_Points -- to support fine-grained and graph-based
evaluation tasks. Extensive benchmarks using conventional VPR models, graph
neural networks, and multimodal baselines show substantial improvements when
leveraging multimodal and structural cues. MMS-VPR facilitates future research
at the intersection of computer vision, geospatial understanding, and
multimodal reasoning. The dataset is publicly available at
https://huggingface.co/datasets/Yiwei-Ou/MMS-VPR.</p></br><a href="http://arxiv.org/pdf/2505.14424v1" target="_blank"><h2>Explaining Neural Networks with Reasons</h2></a><strong><u>Authors:</u></strong>  Levin Hornischer, Hannes Leitgeb</br><strong><u>Categories:</u></strong> cs.LG</br><strong><u>Comments:</u></strong> 28 pages (12 pages main text), 29 figures</br><p><strong><u>Abstract:</u></strong> We propose a new interpretability method for neural networks, which is based
on a novel mathematico-philosophical theory of reasons. Our method computes a
vector for each neuron, called its reasons vector. We then can compute how
strongly this reasons vector speaks for various propositions, e.g., the
proposition that the input image depicts digit 2 or that the input prompt has a
negative sentiment. This yields an interpretation of neurons, and groups
thereof, that combines a logical and a Bayesian perspective, and accounts for
polysemanticity (i.e., that a single neuron can figure in multiple concepts).
We show, both theoretically and empirically, that this method is: (1) grounded
in a philosophically established notion of explanation, (2) uniform, i.e.,
applies to the common neural network architectures and modalities, (3)
scalable, since computing reason vectors only involves forward-passes in the
neural network, (4) faithful, i.e., intervening on a neuron based on its reason
vector leads to expected changes in model output, (5) correct in that the
model's reasons structure matches that of the data source, (6) trainable, i.e.,
neural networks can be trained to improve their reason strengths, (7) useful,
i.e., it delivers on the needs for interpretability by increasing, e.g.,
robustness and fairness.</p></br><a href="http://arxiv.org/pdf/2505.11745v1" target="_blank"><h2>POCAII: Parameter Optimization with Conscious Allocation using Iterative
  Intelligence</h2></a><strong><u>Authors:</u></strong>  Joshua Inman, Tanmay Khandait, Lalitha Sankar, Giulia Pedrielli</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML</br><strong><u>Comments:</u></strong> 21 pages, 4 figures</br><p><strong><u>Abstract:</u></strong> In this paper we propose for the first time the hyperparameter optimization
(HPO) algorithm POCAII. POCAII differs from the Hyperband and Successive
Halving literature by explicitly separating the search and evaluation phases
and utilizing principled approaches to exploration and exploitation principles
during both phases. Such distinction results in a highly flexible scheme for
managing a hyperparameter optimization budget by focusing on search (i.e.,
generating competing configurations) towards the start of the HPO process while
increasing the evaluation effort as the HPO comes to an end.
  POCAII was compared to state of the art approaches SMAC, BOHB and DEHB. Our
algorithm shows superior performance in low-budget hyperparameter optimization
regimes. Since many practitioners do not have exhaustive resources to assign to
HPO, it has wide applications to real-world problems. Moreover, the empirical
evidence showed how POCAII demonstrates higher robustness and lower variance in
the results. This is again very important when considering realistic scenarios
with extremely expensive models to train.</p></br><a href="http://arxiv.org/pdf/2505.13118v1" target="_blank"><h2>Unveil Sources of Uncertainty: Feature Contribution to Conformal
  Prediction Intervals</h2></a><strong><u>Authors:</u></strong>  Marouane Il Idrissi, Agathe Fernandes Machado, Ewen Gallic, Arthur Charpentier</br><strong><u>Categories:</u></strong> cs.AI, cs.LG, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Cooperative game theory methods, notably Shapley values, have significantly
enhanced machine learning (ML) interpretability. However, existing explainable
AI (XAI) frameworks mainly attribute average model predictions, overlooking
predictive uncertainty. This work addresses that gap by proposing a novel,
model-agnostic uncertainty attribution (UA) method grounded in conformal
prediction (CP). By defining cooperative games where CP interval
properties-such as width and bounds-serve as value functions, we systematically
attribute predictive uncertainty to input features. Extending beyond the
traditional Shapley values, we use the richer class of Harsanyi allocations,
and in particular the proportional Shapley values, which distribute attribution
proportionally to feature importance. We propose a Monte Carlo approximation
method with robust statistical guarantees to address computational feasibility,
significantly improving runtime efficiency. Our comprehensive experiments on
synthetic benchmarks and real-world datasets demonstrate the practical utility
and interpretative depth of our approach. By combining cooperative game theory
and conformal prediction, we offer a rigorous, flexible toolkit for
understanding and communicating predictive uncertainty in high-stakes ML
applications.</p></br><a href="http://arxiv.org/pdf/2505.12094v1" target="_blank"><h2>Attribution Projection Calculus: A Novel Framework for Causal Inference
  in Bayesian Networks</h2></a><strong><u>Authors:</u></strong>  M Ruhul Amin</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.IT, math.IT, stat.ML, 60E10, 62R07, 68Q32, 68T07, 94A16, F.2.2; G.3; I.1.2; I.2.6</br><strong><u>Comments:</u></strong> *AI was used to improve Text and collecting Citations</br><p><strong><u>Abstract:</u></strong> This paper introduces Attribution Projection Calculus (AP-Calculus), a novel
mathematical framework for determining causal relationships in structured
Bayesian networks. We investigate a specific network architecture with source
nodes connected to destination nodes through intermediate nodes, where each
input maps to a single label with maximum marginal probability. We prove that
for each label, exactly one intermediate node acts as a deconfounder while
others serve as confounders, enabling optimal attribution of features to their
corresponding labels. The framework formalizes the dual nature of intermediate
nodes as both confounders and deconfounders depending on the context, and
establishes separation functions that maximize distinctions between
intermediate representations. We demonstrate that the proposed network
architecture is optimal for causal inference compared to alternative
structures, including those based on Pearl's causal framework. AP-Calculus
provides a comprehensive mathematical foundation for analyzing feature-label
attributions, managing spurious correlations, quantifying information gain,
ensuring fairness, and evaluating uncertainty in prediction models, including
large language models. Theoretical verification shows that AP-Calculus not only
extends but can also subsume traditional do-calculus for many practical
applications, offering a more direct approach to causal inference in supervised
learning contexts.</p></br><a href="http://arxiv.org/pdf/2505.12992v1" target="_blank"><h2>Fractured Chain-of-Thought Reasoning</h2></a><strong><u>Authors:</u></strong>  Baohao Liao, Hanze Dong, Yuhui Xu, Doyen Sahoo, Christof Monz, Junnan Li, Caiming Xiong</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.CL, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Inference-time scaling techniques have significantly bolstered the reasoning
capabilities of large language models (LLMs) by harnessing additional
computational effort at inference without retraining. Similarly,
Chain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy
by generating rich intermediate reasoning trajectories, but these approaches
incur substantial token costs that impede their deployment in latency-sensitive
settings. In this work, we first show that truncated CoT, which stops reasoning
before completion and directly generates the final answer, often matches full
CoT sampling while using dramatically fewer tokens. Building on this insight,
we introduce Fractured Sampling, a unified inference-time strategy that
interpolates between full CoT and solution-only sampling along three orthogonal
axes: (1) the number of reasoning trajectories, (2) the number of final
solutions per trajectory, and (3) the depth at which reasoning traces are
truncated. Through extensive experiments on five diverse reasoning benchmarks
and several model scales, we demonstrate that Fractured Sampling consistently
achieves superior accuracy-cost trade-offs, yielding steep log-linear scaling
gains in Pass@k versus token budget. Our analysis reveals how to allocate
computation across these dimensions to maximize performance, paving the way for
more efficient and scalable LLM reasoning.</p></br><a href="http://arxiv.org/pdf/2505.11325v1" target="_blank"><h2>Uncertainty Quantification for Prior-Data Fitted Networks using
  Martingale Posteriors</h2></a><strong><u>Authors:</u></strong>  Thomas Nagler, David Rgamer</br><strong><u>Categories:</u></strong> stat.ME, cs.AI, cs.LG, stat.CO, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Prior-data fitted networks (PFNs) have emerged as promising foundation models
for prediction from tabular data sets, achieving state-of-the-art performance
on small to moderate data sizes without tuning. While PFNs are motivated by
Bayesian ideas, they do not provide any uncertainty quantification for
predictive means, quantiles, or similar quantities. We propose a principled and
efficient sampling procedure to construct Bayesian posteriors for such
estimates based on Martingale posteriors, and prove its convergence. Several
simulated and real-world data examples showcase the uncertainty quantification
of our method in inference applications.</p></br><a href="http://arxiv.org/pdf/2505.12225v1" target="_blank"><h2>Reward Inside the Model: A Lightweight Hidden-State Reward Model for
  LLM's Best-of-N sampling</h2></a><strong><u>Authors:</u></strong>  Jizhou Guo, Zhaomin Wu, Philip S. Yu</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.CL, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> High-quality reward models are crucial for unlocking the reasoning potential
of large language models (LLMs), with best-of-N voting demonstrating
significant performance gains. However, current reward models, which typically
operate on the textual output of LLMs, are computationally expensive and
parameter-heavy, limiting their real-world applications. We introduce the
Efficient Linear Hidden State Reward (ELHSR) model - a novel, highly
parameter-efficient approach that leverages the rich information embedded in
LLM hidden states to address these issues. ELHSR systematically outperform
baselines with less than 0.005% of the parameters of baselines, requiring only
a few samples for training. ELHSR also achieves orders-of-magnitude efficiency
improvement with significantly less time and fewer FLOPs per sample than
baseline reward models. Moreover, ELHSR exhibits robust performance even when
trained only on logits, extending its applicability to some closed-source LLMs.
In addition, ELHSR can also be combined with traditional reward models to
achieve additional performance gains.</p></br><a href="http://arxiv.org/pdf/2505.11740v1" target="_blank"><h2>Simple and Effective Specialized Representations for Fair Classifiers</h2></a><strong><u>Authors:</u></strong>  Alberto Sinigaglia, Davide Sartor, Marina Ceccon, Gian Antonio Susto</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Fair classification is a critical challenge that has gained increasing
importance due to international regulations and its growing use in high-stakes
decision-making settings. Existing methods often rely on adversarial learning
or distribution matching across sensitive groups; however, adversarial learning
can be unstable, and distribution matching can be computationally intensive. To
address these limitations, we propose a novel approach based on the
characteristic function distance. Our method ensures that the learned
representation contains minimal sensitive information while maintaining high
effectiveness for downstream tasks. By utilizing characteristic functions, we
achieve a more stable and efficient solution compared to traditional methods.
Additionally, we introduce a simple relaxation of the objective function that
guarantees fairness in common classification models with no performance
degradation. Experimental results on benchmark datasets demonstrate that our
approach consistently matches or achieves better fairness and predictive
accuracy than existing methods. Moreover, our method maintains robustness and
computational efficiency, making it a practical solution for real-world
applications.</p></br><a href="http://arxiv.org/pdf/2505.13770v1" target="_blank"><h2>Ice Cream Doesn't Cause Drowning: Benchmarking LLMs Against Statistical
  Pitfalls in Causal Inference</h2></a><strong><u>Authors:</u></strong>  Jin Du, Li Chen, Xun Xian, An Luo, Fangqiao Tian, Ganghua Wang, Charles Doss, Xiaotong Shen, Jie Ding</br><strong><u>Categories:</u></strong> cs.AI, cs.CL, cs.LG, stat.ME, stat.ML, 62-08, 68T50, 68T05, 68T01, 68T07, 62-07, 68U35, 62C99, I.2.7; I.2.6; I.2.0; I.5.1; I.5.4; F.2.2; H.2.8; G.3</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Reliable causal inference is essential for making decisions in high-stakes
areas like medicine, economics, and public policy. However, it remains unclear
whether large language models (LLMs) can handle rigorous and trustworthy
statistical causal inference. Current benchmarks usually involve simplified
tasks. For example, these tasks might only ask LLMs to identify semantic causal
relationships or draw conclusions directly from raw data. As a result, models
may overlook important statistical pitfalls, such as Simpson's paradox or
selection bias. This oversight limits the applicability of LLMs in the real
world. To address these limitations, we propose CausalPitfalls, a comprehensive
benchmark designed to rigorously evaluate the capability of LLMs in overcoming
common causal inference pitfalls. Our benchmark features structured challenges
across multiple difficulty levels, each paired with grading rubrics. This
approach allows us to quantitatively measure both causal reasoning capabilities
and the reliability of LLMs' responses. We evaluate models using two protocols:
(1) direct prompting, which assesses intrinsic causal reasoning, and (2)
code-assisted prompting, where models generate executable code for explicit
statistical analysis. Additionally, we validate the effectiveness of this judge
by comparing its scoring with assessments from human experts. Our results
reveal significant limitations in current LLMs when performing statistical
causal inference. The CausalPitfalls benchmark provides essential guidance and
quantitative metrics to advance the development of trustworthy causal reasoning
systems.</p></br><a href="http://arxiv.org/pdf/2505.11631v1" target="_blank"><h2>Enhancing Network Anomaly Detection with Quantum GANs and Successive
  Data Injection for Multivariate Time Series</h2></a><strong><u>Authors:</u></strong>  Wajdi Hammami, Soumaya Cherkaoui, Shengrui Wang</br><strong><u>Categories:</u></strong> cs.LG, quant-ph</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Quantum computing may offer new approaches for advancing machine learning,
including in complex tasks such as anomaly detection in network traffic. In
this paper, we introduce a quantum generative adversarial network (QGAN)
architecture for multivariate time-series anomaly detection that leverages
variational quantum circuits (VQCs) in combination with a time-window shifting
technique, data re-uploading, and successive data injection (SuDaI). The method
encodes multivariate time series data as rotation angles. By integrating both
data re-uploading and SuDaI, the approach maps classical data into quantum
states efficiently, helping to address hardware limitations such as the
restricted number of available qubits. In addition, the approach employs an
anomaly scoring technique that utilizes both the generator and the
discriminator output to enhance the accuracy of anomaly detection. The QGAN was
trained using the parameter shift rule and benchmarked against a classical GAN.
Experimental results indicate that the quantum model achieves a accuracy high
along with high recall and F1-scores in anomaly detection, and attains a lower
MSE compared to the classical model. Notably, the QGAN accomplishes this
performance with only 80 parameters, demonstrating competitive results with a
compact architecture. Tests using a noisy simulator suggest that the approach
remains effective under realistic noise-prone conditions.</p></br><a href="http://arxiv.org/pdf/2505.13112v1" target="_blank"><h2>Attention-based clustering</h2></a><strong><u>Authors:</u></strong>  Rodrigo Maulen-Soto, Claire Boyer, Pierre Marion</br><strong><u>Categories:</u></strong> stat.ML, cs.LG</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Transformers have emerged as a powerful neural network architecture capable
of tackling a wide range of learning tasks. In this work, we provide a
theoretical analysis of their ability to automatically extract structure from
data in an unsupervised setting. In particular, we demonstrate their
suitability for clustering when the input data is generated from a Gaussian
mixture model. To this end, we study a simplified two-head attention layer and
define a population risk whose minimization with unlabeled data drives the head
parameters to align with the true mixture centroids.</p></br><a href="http://arxiv.org/pdf/2505.12404v1" target="_blank"><h2>Hyperbolic Residual Quantization: Discrete Representations for Data with
  Latent Hierarchies</h2></a><strong><u>Authors:</u></strong>  Piotr Pikos, Subhradeep Kayal, Alexandros Karatzoglou</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Hierarchical data arise in countless domains, from biological taxonomies and
organizational charts to legal codes and knowledge graphs. Residual
Quantization (RQ) is widely used to generate discrete, multitoken
representations for such data by iteratively quantizing residuals in a
multilevel codebook. However, its reliance on Euclidean geometry can introduce
fundamental mismatches that hinder modeling of hierarchical branching,
necessary for faithful representation of hierarchical data. In this work, we
propose Hyperbolic Residual Quantization (HRQ), which embeds data natively in a
hyperbolic manifold and performs residual quantization using hyperbolic
operations and distance metrics. By adapting the embedding network, residual
computation, and distance metric to hyperbolic geometry, HRQ imparts an
inductive bias that aligns naturally with hierarchical branching. We claim that
HRQ in comparison to RQ can generate more useful for downstream tasks discrete
hierarchical representations for data with latent hierarchies. We evaluate HRQ
on two tasks: supervised hierarchy modeling using WordNet hypernym trees, where
the model is supervised to learn the latent hierarchy - and hierarchy
discovery, where, while latent hierarchy exists in the data, the model is not
directly trained or evaluated on a task related to the hierarchy. Across both
scenarios, HRQ hierarchical tokens yield better performance on downstream tasks
compared to Euclidean RQ with gains of up to $20\%$ for the hierarchy modeling
task. Our results demonstrate that integrating hyperbolic geometry into
discrete representation learning substantially enhances the ability to capture
latent hierarchies.</p></br><a href="http://arxiv.org/pdf/2505.11083v1" target="_blank"><h2>Fault Diagnosis across Heterogeneous Domains via Self-Adaptive
  Temporal-Spatial Attention and Sample Generation</h2></a><strong><u>Authors:</u></strong>  Guangqiang Li, M. Amine Atoui, Xiangshun Li</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> 31 pages, 11 figures</br><p><strong><u>Abstract:</u></strong> Deep learning methods have shown promising performance in fault diagnosis for
multimode process. Most existing studies assume that the collected health state
categories from different operating modes are identical. However, in real
industrial scenarios, these categories typically exhibit only partial overlap.
The incompleteness of the available data and the large distributional
differences between the operating modes pose a significant challenge to
existing fault diagnosis methods. To address this problem, a novel fault
diagnosis model named self-adaptive temporal-spatial attention network
(TSA-SAN) is proposed. First, inter-mode mappings are constructed using healthy
category data to generate multimode samples. To enrich the diversity of the
fault data, interpolation is performed between healthy and fault samples.
Subsequently, the fault diagnosis model is trained using real and generated
data. The self-adaptive instance normalization is established to suppress
irrelevant information while retaining essential statistical features for
diagnosis. In addition, a temporal-spatial attention mechanism is constructed
to focus on the key features, thus enhancing the generalization ability of the
model. The extensive experiments demonstrate that the proposed model
significantly outperforms the state-of-the-art methods. The code will be
available on Github at https://github.com/GuangqiangLi/TSA-SAN.</p></br><a href="http://arxiv.org/pdf/2505.12581v1" target="_blank"><h2>An approach based on class activation maps for investigating the effects
  of data augmentation on neural networks for image classification</h2></a><strong><u>Authors:</u></strong>  Lucas M. Dorneles, Luan Fonseca Garcia, Joel Lus Carbonera</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.CV</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Neural networks have become increasingly popular in the last few years as an
effective tool for the task of image classification due to the impressive
performance they have achieved on this task. In image classification tasks, it
is common to use data augmentation strategies to increase the robustness of
trained networks to changes in the input images and to avoid overfitting.
Although data augmentation is a widely adopted technique, the literature lacks
a body of research analyzing the effects data augmentation methods have on the
patterns learned by neural network models working on complex datasets. The
primary objective of this work is to propose a methodology and set of metrics
that may allow a quantitative approach to analyzing the effects of data
augmentation in convolutional networks applied to image classification. An
important tool used in the proposed approach lies in the concept of class
activation maps for said models, which allow us to identify and measure the
importance these models assign to each individual pixel in an image when
executing the classification task. From these maps, we may then extract metrics
over the similarities and differences between maps generated by these models
trained on a given dataset with different data augmentation strategies.
Experiments made using this methodology suggest that the effects of these data
augmentation techniques not only can be analyzed in this way but also allow us
to identify different impact profiles over the trained models.</p></br><a href="http://arxiv.org/pdf/2505.11793v1" target="_blank"><h2>CL-CaGAN: Capsule differential adversarial continuous learning for
  cross-domain hyperspectral anomaly detection</h2></a><strong><u>Authors:</u></strong>  Jianing Wang, Siying Guo, Zheng Hua, Runhu Huang, Jinyu Hu, Maoguo Gong</br><strong><u>Categories:</u></strong> cs.CV, cs.AI, eess.IV</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Anomaly detection (AD) has attracted remarkable attention in hyperspectral
image (HSI) processing fields, and most existing deep learning (DL)-based
algorithms indicate dramatic potential for detecting anomaly samples through
specific training process under current scenario. However, the limited prior
information and the catastrophic forgetting problem indicate crucial challenges
for existing DL structure in open scenarios cross-domain detection. In order to
improve the detection performance, a novel continual learning-based capsule
differential generative adversarial network (CL-CaGAN) is proposed to elevate
the cross-scenario learning performance for facilitating the real application
of DL-based structure in hyperspectral AD (HAD) task. First, a modified capsule
structure with adversarial learning network is constructed to estimate the
background distribution for surmounting the deficiency of prior information. To
mitigate the catastrophic forgetting phenomenon, clustering-based sample replay
strategy and a designed extra self-distillation regularization are integrated
for merging the history and future knowledge in continual AD task, while the
discriminative learning ability from previous detection scenario to current
scenario is retained by the elaborately designed structure with continual
learning (CL) strategy. In addition, the differentiable enhancement is enforced
to augment the generation performance of the training data. This further
stabilizes the training process with better convergence and efficiently
consolidates the reconstruction ability of background samples. To verify the
effectiveness of our proposed CL-CaGAN, we conduct experiments on several real
HSIs, and the results indicate that the proposed CL-CaGAN demonstrates higher
detection performance and continuous learning capacity for mitigating the
catastrophic forgetting under cross-domain scenarios.</p></br><a href="http://arxiv.org/pdf/2505.13562v1" target="_blank"><h2>Randomised Optimism via Competitive Co-Evolution for Matrix Games with
  Bandit Feedback</h2></a><strong><u>Authors:</u></strong>  Shishen Lin</br><strong><u>Categories:</u></strong> stat.ML, cs.AI, cs.GT, cs.LG, cs.NE</br><strong><u>Comments:</u></strong> 21 pages, 10 figures, accepted at IJCAI 2025</br><p><strong><u>Abstract:</u></strong> Learning in games is a fundamental problem in machine learning and artificial
intelligence, with numerous
applications~\citep{silver2016mastering,schrittwieser2020mastering}. This work
investigates two-player zero-sum matrix games with an unknown payoff matrix and
bandit feedback, where each player observes their actions and the corresponding
noisy payoff. Prior studies have proposed algorithms for this
setting~\citep{o2021matrix,maiti2023query,cai2024uncoupled}, with
\citet{o2021matrix} demonstrating the effectiveness of deterministic optimism
(e.g., \ucb) in achieving sublinear regret. However, the potential of
randomised optimism in matrix games remains theoretically unexplored.
  We propose Competitive Co-evolutionary Bandit Learning (\coebl), a novel
algorithm that integrates evolutionary algorithms (EAs) into the bandit
framework to implement randomised optimism through EA variation operators. We
prove that \coebl achieves sublinear regret, matching the performance of
deterministic optimism-based methods. To the best of our knowledge, this is the
first theoretical regret analysis of an evolutionary bandit learning algorithm
in matrix games.
  Empirical evaluations on diverse matrix game benchmarks demonstrate that
\coebl not only achieves sublinear regret but also consistently outperforms
classical bandit algorithms, including \exptr~\citep{auer2002nonstochastic},
the variant \exptrni~\citep{cai2024uncoupled}, and \ucb~\citep{o2021matrix}.
These results highlight the potential of evolutionary bandit learning,
particularly the efficacy of randomised optimism via evolutionary algorithms in
game-theoretic settings.</p></br><a href="http://arxiv.org/pdf/2505.13100v1" target="_blank"><h2>Time series saliency maps: explaining models across multiple domains</h2></a><strong><u>Authors:</u></strong>  Christodoulos Kechris, Jonathan Dan, David Atienza</br><strong><u>Categories:</u></strong> cs.LG</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Traditional saliency map methods, popularized in computer vision, highlight
individual points (pixels) of the input that contribute the most to the model's
output. However, in time-series they offer limited insights as semantically
meaningful features are often found in other domains. We introduce Cross-domain
Integrated Gradients, a generalization of Integrated Gradients. Our method
enables feature attributions on any domain that can be formulated as an
invertible, differentiable transformation of the time domain. Crucially, our
derivation extends the original Integrated Gradients into the complex domain,
enabling frequency-based attributions. We provide the necessary theoretical
guarantees, namely, path independence and completeness. Our approach reveals
interpretable, problem-specific attributions that time-domain methods cannot
capture, on three real-world tasks: wearable sensor heart rate extraction,
electroencephalography-based seizure detection, and zero-shot time-series
forecasting. We release an open-source Tensorflow/PyTorch library to enable
plug-and-play cross-domain explainability for time-series models. These results
demonstrate the ability of cross-domain integrated gradients to provide
semantically meaningful insights in time-series models that are impossible with
traditional time-domain saliency.</p></br><a href="http://arxiv.org/pdf/2505.12421v1" target="_blank"><h2>Fixed Point Explainability</h2></a><strong><u>Authors:</u></strong>  Emanuele La Malfa, Jon Vadillo, Marco Molinari, Michael Wooldridge</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> Code:this https URL</br><p><strong><u>Abstract:</u></strong> This paper introduces a formal notion of fixed point explanations, inspired
by the "why regress" principle, to assess, through recursive applications, the
stability of the interplay between a model and its explainer. Fixed point
explanations satisfy properties like minimality, stability, and faithfulness,
revealing hidden model behaviours and explanatory weaknesses. We define
convergence conditions for several classes of explainers, from feature-based to
mechanistic tools like Sparse AutoEncoders, and we report quantitative and
qualitative results.</p></br><a href="http://arxiv.org/pdf/2505.11165v1" target="_blank"><h2>Maximizing Asynchronicity in Event-based Neural Networks</h2></a><strong><u>Authors:</u></strong>  Haiqing Hao, Nikola Zubi, Weihua He, Zhipeng Sui, Davide Scaramuzza, Wenhui Wang</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.CL, cs.CV</br><strong><u>Comments:</u></strong> 18 pages, 5 figures, 9 tables</br><p><strong><u>Abstract:</u></strong> Event cameras deliver visual data with high temporal resolution, low latency,
and minimal redundancy, yet their asynchronous, sparse sequential nature
challenges standard tensor-based machine learning (ML). While the recent
asynchronous-to-synchronous (A2S) paradigm aims to bridge this gap by
asynchronously encoding events into learned representations for ML pipelines,
existing A2S approaches often sacrifice representation expressivity and
generalizability compared to dense, synchronous methods. This paper introduces
EVA (EVent Asynchronous representation learning), a novel A2S framework to
generate highly expressive and generalizable event-by-event representations.
Inspired by the analogy between events and language, EVA uniquely adapts
advances from language modeling in linear attention and self-supervised
learning for its construction. In demonstration, EVA outperforms prior A2S
methods on recognition tasks (DVS128-Gesture and N-Cars), and represents the
first A2S framework to successfully master demanding detection tasks, achieving
a remarkable 47.7 mAP on the Gen1 dataset. These results underscore EVA's
transformative potential for advancing real-time event-based vision
applications.</p></br><a href="http://arxiv.org/pdf/2505.13857v1" target="_blank"><h2>Learning Spatio-Temporal Dynamics for Trajectory Recovery via Time-Aware
  Transformer</h2></a><strong><u>Authors:</u></strong>  Tian Sun, Yuqi Chen, Baihua Zheng, Weiwei Sun</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> Accepted as a journal paper in IEEE Transactions on Intelligent Transportation Systems (T-ITS)</br><p><strong><u>Abstract:</u></strong> In real-world applications, GPS trajectories often suffer from low sampling
rates, with large and irregular intervals between consecutive GPS points. This
sparse characteristic presents challenges for their direct use in GPS-based
systems. This paper addresses the task of map-constrained trajectory recovery,
aiming to enhance trajectory sampling rates of GPS trajectories. Previous
studies commonly adopt a sequence-to-sequence framework, where an encoder
captures the trajectory patterns and a decoder reconstructs the target
trajectory. Within this framework, effectively representing the road network
and extracting relevant trajectory features are crucial for overall
performance. Despite advancements in these models, they fail to fully leverage
the complex spatio-temporal dynamics present in both the trajectory and the
road network.
  To overcome these limitations, we categorize the spatio-temporal dynamics of
trajectory data into two distinct aspects: spatial-temporal traffic dynamics
and trajectory dynamics. Furthermore, We propose TedTrajRec, a novel method for
trajectory recovery. To capture spatio-temporal traffic dynamics, we introduce
PD-GNN, which models periodic patterns and learns topologically aware dynamics
concurrently for each road segment. For spatio-temporal trajectory dynamics, we
present TedFormer, a time-aware Transformer that incorporates temporal dynamics
for each GPS location by integrating closed-form neural ordinary differential
equations into the attention mechanism. This allows TedFormer to effectively
handle irregularly sampled data. Extensive experiments on three real-world
datasets demonstrate the superior performance of TedTrajRec. The code is
publicly available at https://github.com/ysygMhdxw/TEDTrajRec/.</p></br><a href="http://arxiv.org/pdf/2505.14252v1" target="_blank"><h2>Hybrid Adaptive Modeling in Process Monitoring: Leveraging Sequence
  Encoders and Physics-Informed Neural Networks</h2></a><strong><u>Authors:</u></strong>  Mouad Elaarabi, Domenico Borzacchiello, Philippe Le Bot, Nathan Lauzeral, Sebastien Comas-Cardona</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> In this work, we explore the integration of Sequence Encoding for Online
Parameter Identification with Physics-Informed Neural Networks to create a
model that, once trained, can be utilized for real time applications with
variable parameters, boundary conditions, and initial conditions. Recently, the
combination of PINNs with Sparse Regression has emerged as a method for
performing dynamical system identification through supervised learning and
sparse regression optimization, while also solving the dynamics using PINNs.
However, this approach can be limited by variations in parameters or boundary
and initial conditions, requiring retraining of the model whenever changes
occur. In this work, we introduce an architecture that employs Deep Sets or
Sequence Encoders to encode dynamic parameters, boundary conditions, and
initial conditions, using these encoded features as inputs for the PINN,
enabling the model to adapt to changes in parameters, BCs, and ICs. We apply
this approach to three different problems. First, we analyze the Rossler ODE
system, demonstrating the robustness of the model with respect to noise and its
ability to generalize. Next, we explore the model's capability in a 2D
Navier-Stokes PDE problem involving flow past a cylinder with a parametric
sinusoidal inlet velocity function, showing that the model can encode pressure
data from a few points to identify the inlet velocity profile and utilize
physics to compute velocity and pressure throughout the domain. Finally, we
address a 1D heat monitoring problem using real data from the heating of glass
fiber and thermoplastic composite plates.</p></br><a href="http://arxiv.org/pdf/2505.10774v1" target="_blank"><h2>Context-Aware Probabilistic Modeling with LLM for Multimodal Time Series
  Forecasting</h2></a><strong><u>Authors:</u></strong>  Yueyang Yao, Jiajun Li, Xingyuan Dai, MengMeng Zhang, Xiaoyan Gong, Fei-Yue Wang, Yisheng Lv</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> 13 pages, 2 figures</br><p><strong><u>Abstract:</u></strong> Time series forecasting is important for applications spanning energy
markets, climate analysis, and traffic management. However, existing methods
struggle to effectively integrate exogenous texts and align them with the
probabilistic nature of large language models (LLMs). Current approaches either
employ shallow text-time series fusion via basic prompts or rely on
deterministic numerical decoding that conflict with LLMs' token-generation
paradigm, which limits contextual awareness and distribution modeling. To
address these limitations, we propose CAPTime, a context-aware probabilistic
multimodal time series forecasting method that leverages text-informed
abstraction and autoregressive LLM decoding. Our method first encodes temporal
patterns using a pretrained time series encoder, then aligns them with textual
contexts via learnable interactions to produce joint multimodal
representations. By combining a mixture of distribution experts with frozen
LLMs, we enable context-aware probabilistic forecasting while preserving LLMs'
inherent distribution modeling capabilities. Experiments on diverse time series
forecasting tasks demonstrate the superior accuracy and generalization of
CAPTime, particularly in multimodal scenarios. Additional analysis highlights
its robustness in data-scarce scenarios through hybrid probabilistic decoding.</p></br><a href="http://arxiv.org/pdf/2505.11671v1" target="_blank"><h2>Humble your Overconfident Networks: Unlearning Overfitting via
  Sequential Monte Carlo Tempered Deep Ensembles</h2></a><strong><u>Authors:</u></strong>  Andrew Millard, Zheng Zhao, Joshua Murphy, Simon Maskell</br><strong><u>Categories:</u></strong> stat.ML, cs.LG, stat.CO</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Sequential Monte Carlo (SMC) methods offer a principled approach to Bayesian
uncertainty quantification but are traditionally limited by the need for
full-batch gradient evaluations. We introduce a scalable variant by
incorporating Stochastic Gradient Hamiltonian Monte Carlo (SGHMC) proposals
into SMC, enabling efficient mini-batch based sampling. Our resulting SMCSGHMC
algorithm outperforms standard stochastic gradient descent (SGD) and deep
ensembles across image classification, out-of-distribution (OOD) detection, and
transfer learning tasks. We further show that SMCSGHMC mitigates overfitting
and improves calibration, providing a flexible, scalable pathway for converting
pretrained neural networks into well-calibrated Bayesian models.</p></br><a href="http://arxiv.org/pdf/2505.14659v1" target="_blank"><h2>Explainable AI for Securing Healthcare in IoT-Integrated 6G Wireless
  Networks</h2></a><strong><u>Authors:</u></strong>  Navneet Kaur, Lav Gupta</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> As healthcare systems increasingly adopt advanced wireless networks and
connected devices, securing medical applications has become critical. The
integration of Internet of Medical Things devices, such as robotic surgical
tools, intensive care systems, and wearable monitors has enhanced patient care
but introduced serious security risks. Cyberattacks on these devices can lead
to life threatening consequences, including surgical errors, equipment failure,
and data breaches. While the ITU IMT 2030 vision highlights 6G's transformative
role in healthcare through AI and cloud integration, it also raises new
security concerns. This paper explores how explainable AI techniques like SHAP,
LIME, and DiCE can uncover vulnerabilities, strengthen defenses, and improve
trust and transparency in 6G enabled healthcare. We support our approach with
experimental analysis and highlight promising results.</p></br><a href="http://arxiv.org/pdf/2505.12880v1" target="_blank"><h2>AdS-GNN -- a Conformally Equivariant Graph Neural Network</h2></a><strong><u>Authors:</u></strong>  Maksim Zhdanov, Nabil Iqbal, Erik Bekkers, Patrick Forr</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, hep-th</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Conformal symmetries, i.e.\ coordinate transformations that preserve angles,
play a key role in many fields, including physics, mathematics, computer vision
and (geometric) machine learning. Here we build a neural network that is
equivariant under general conformal transformations. To achieve this, we lift
data from flat Euclidean space to Anti de Sitter (AdS) space. This allows us to
exploit a known correspondence between conformal transformations of flat space
and isometric transformations on the AdS space. We then build upon the fact
that such isometric transformations have been extensively studied on general
geometries in the geometric deep learning literature. We employ message-passing
layers conditioned on the proper distance, yielding a computationally efficient
framework. We validate our model on tasks from computer vision and statistical
physics, demonstrating strong performance, improved generalization capacities,
and the ability to extract conformal data such as scaling dimensions from the
trained network.</p></br><a href="http://arxiv.org/pdf/2505.14428v1" target="_blank"><h2>Interpretable Neural System Dynamics: Combining Deep Learning with
  System Dynamics Modeling to Support Critical Applications</h2></a><strong><u>Authors:</u></strong>  Riccardo D'Elia</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> To be submitted tothis http URLfor publication in the Doctoral Consortium Proceedings of XAI 2025, The World Conference on Explainable Artificial Intelligence</br><p><strong><u>Abstract:</u></strong> The objective of this proposal is to bridge the gap between Deep Learning
(DL) and System Dynamics (SD) by developing an interpretable neural system
dynamics framework. While DL excels at learning complex models and making
accurate predictions, it lacks interpretability and causal reliability.
Traditional SD approaches, on the other hand, provide transparency and causal
insights but are limited in scalability and require extensive domain knowledge.
To overcome these limitations, this project introduces a Neural System Dynamics
pipeline, integrating Concept-Based Interpretability, Mechanistic
Interpretability, and Causal Machine Learning. This framework combines the
predictive power of DL with the interpretability of traditional SD models,
resulting in both causal reliability and scalability. The efficacy of the
proposed pipeline will be validated through real-world applications of the
EU-funded AutoMoTIF project, which is focused on autonomous multimodal
transportation systems. The long-term goal is to collect actionable insights
that support the integration of explainability and safety in autonomous
systems.</p></br><a href="http://arxiv.org/pdf/2505.11072v1" target="_blank"><h2>The Giant Arc -- Filament of Figment?</h2></a><strong><u>Authors:</u></strong>  Till Sawala, Meri Teeriaho</br><strong><u>Categories:</u></strong> astro-ph.CO, astro-ph.GA</br><strong><u>Comments:</u></strong> 4 pages, 2 figures. Full code provided. Comments welcome!</br><p><strong><u>Abstract:</u></strong> The so-called "Giant Arc" is a sparse pattern of MgII absorbers spanning
approximately 740 comoving Mpc, whose discovery has been claimed to contradict
the large-scale homogeneity inherent to the standard cosmological model. We
previously showed that, with the same algorithm and parameters used for its
discovery, very similar patterns are abundant in uniform random distributions,
and among equivalent halo samples in a cosmological simulation of the standard
model. In a response, the original discoverers of the "Giant Arc" have argued
that these parameters were only appropriate for their specific observational
data, but that a smaller linking length should be used for control studies, in
which case far fewer patterns are detected. We briefly review and disprove
these arguments, and demonstrate that large patterns like the "Giant Arc" are
indeed ubiquitous in a statistically homogeneous universe.</p></br><a href="http://arxiv.org/pdf/2505.11576v1" target="_blank"><h2>Concept-Guided Interpretability via Neural Chunking</h2></a><strong><u>Authors:</u></strong>  Shuchen Wu, Stephan Alaniz, Shyamgopal Karthik, Peter Dayan, Eric Schulz, Zeynep Akata</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.CV</br><strong><u>Comments:</u></strong> 35 pages, 32 figures. arXiv admin note: text overlap witharXiv:2502.01803</br><p><strong><u>Abstract:</u></strong> Neural networks are often black boxes, reflecting the significant challenge
of understanding their internal workings. We propose a different perspective
that challenges the prevailing view: rather than being inscrutable, neural
networks exhibit patterns in their raw population activity that mirror
regularities in the training data. We refer to this as the Reflection
Hypothesis and provide evidence for this phenomenon in both simple recurrent
neural networks (RNNs) and complex large language models (LLMs). Building on
this insight, we propose to leverage cognitively-inspired methods of chunking
to segment high-dimensional neural population dynamics into interpretable units
that reflect underlying concepts. We propose three methods to extract these
emerging entities, complementing each other based on label availability and
dimensionality. Discrete sequence chunking (DSC) creates a dictionary of
entities; population averaging (PA) extracts recurring entities that correspond
to known labels; and unsupervised chunk discovery (UCD) can be used when labels
are absent. We demonstrate the effectiveness of these methods in extracting
entities across varying model sizes, ranging from inducing compositionality in
RNNs to uncovering recurring neural population states in large models with
diverse architectures, and illustrate their advantage over other methods.
Throughout, we observe a robust correspondence between the extracted entities
and concrete or abstract concepts. Artificially inducing the extracted entities
in neural populations effectively alters the network's generation of associated
concepts. Our work points to a new direction for interpretability, one that
harnesses both cognitive principles and the structure of naturalistic data to
reveal the hidden computations of complex learning systems, gradually
transforming them from black boxes into systems we can begin to understand.</p></br><a href="http://arxiv.org/pdf/2505.11725v1" target="_blank"><h2>CLT and Edgeworth Expansion for m-out-of-n Bootstrap Estimators of The
  Studentized Median</h2></a><strong><u>Authors:</u></strong>  Imon Banerjee, Sayak Chakrabarty</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.CE, math.ST, stat.ME, stat.ML, stat.TH</br><strong><u>Comments:</u></strong> 48 pages</br><p><strong><u>Abstract:</u></strong> The m-out-of-n bootstrap, originally proposed by Bickel, Gotze, and Zwet
(1992), approximates the distribution of a statistic by repeatedly drawing m
subsamples (with m much smaller than n) without replacement from an original
sample of size n. It is now routinely used for robust inference with
heavy-tailed data, bandwidth selection, and other large-sample applications.
Despite its broad applicability across econometrics, biostatistics, and machine
learning, rigorous parameter-free guarantees for the soundness of the
m-out-of-n bootstrap when estimating sample quantiles have remained elusive.
  This paper establishes such guarantees by analyzing the estimator of sample
quantiles obtained from m-out-of-n resampling of a dataset of size n. We first
prove a central limit theorem for a fully data-driven version of the estimator
that holds under a mild moment condition and involves no unknown nuisance
parameters. We then show that the moment assumption is essentially tight by
constructing a counter-example in which the CLT fails. Strengthening the
assumptions slightly, we derive an Edgeworth expansion that provides exact
convergence rates and, as a corollary, a Berry Esseen bound on the bootstrap
approximation error. Finally, we illustrate the scope of our results by
deriving parameter-free asymptotic distributions for practical statistics,
including the quantiles for random walk Metropolis-Hastings and the rewards of
ergodic Markov decision processes, thereby demonstrating the usefulness of our
theory in modern estimation and learning tasks.</p></br><a href="http://arxiv.org/pdf/2505.11054v1" target="_blank"><h2>NeuralSurv: Deep Survival Analysis with Bayesian Uncertainty
  Quantification</h2></a><strong><u>Authors:</u></strong>  Mlodie Monod, Alessandro Micheli, Samir Bhatt</br><strong><u>Categories:</u></strong> cs.LG, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> We introduce NeuralSurv, the first deep survival model to incorporate
Bayesian uncertainty quantification. Our non-parametric, architecture-agnostic
framework flexibly captures time-varying covariate-risk relationships in
continuous time via a novel two-stage data-augmentation scheme, for which we
establish theoretical guarantees. For efficient posterior inference, we
introduce a mean-field variational algorithm with coordinate-ascent updates
that scale linearly in model size. By locally linearizing the Bayesian neural
network, we obtain full conjugacy and derive all coordinate updates in closed
form. In experiments, NeuralSurv delivers superior calibration compared to
state-of-the-art deep survival models, while matching or exceeding their
discriminative performance across both synthetic benchmarks and real-world
datasets. Our results demonstrate the value of Bayesian principles in
data-scarce regimes by enhancing model calibration and providing robust,
well-calibrated uncertainty estimates for the survival function.</p></br><a href="http://arxiv.org/pdf/2505.13102v1" target="_blank"><h2>Lightweight Transformer via Unrolling of Mixed Graph Algorithms for
  Traffic Forecast</h2></a><strong><u>Authors:</u></strong>  Ji Qi, Tam Thuc Do, Mingxiao Liu, Zhuoshi Pan, Yuzhe Li, Gene Cheung, H. Vicky Zhao</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, eess.SP</br><strong><u>Comments:</u></strong> 19 pages, 5 figures, 8 tables</br><p><strong><u>Abstract:</u></strong> To forecast traffic with both spatial and temporal dimensions, we unroll a
mixed-graph-based optimization algorithm into a lightweight and interpretable
transformer-like neural net. Specifically, we construct two graphs: an
undirected graph $\mathcal{G}^u$ capturing spatial correlations across
geography, and a directed graph $\mathcal{G}^d$ capturing sequential
relationships over time. We formulate a prediction problem for the future
samples of signal $\mathbf{x}$, assuming it is "smooth" with respect to both
$\mathcal{G}^u$ and $\mathcal{G}^d$, where we design new $\ell_2$ and
$\ell_1$-norm variational terms to quantify and promote signal smoothness
(low-frequency reconstruction) on a directed graph. We construct an iterative
algorithm based on alternating direction method of multipliers (ADMM), and
unroll it into a feed-forward network for data-driven parameter learning. We
insert graph learning modules for $\mathcal{G}^u$ and $\mathcal{G}^d$, which
are akin to the self-attention mechanism in classical transformers. Experiments
show that our unrolled networks achieve competitive traffic forecast
performance as state-of-the-art prediction schemes, while reducing parameter
counts drastically. Our code is available in
https://github.com/SingularityUndefined/Unrolling-GSP-STForecast.</p></br><a href="http://arxiv.org/pdf/2505.13580v1" target="_blank"><h2>OMGPT: A Sequence Modeling Framework for Data-driven Operational
  Decision Making</h2></a><strong><u>Authors:</u></strong>  Hanzhao Wang, Guanting Chen, Kalyan Talluri, Xiaocheng Li</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> arXiv admin note: text overlap witharXiv:2405.14219</br><p><strong><u>Abstract:</u></strong> We build a Generative Pre-trained Transformer (GPT) model from scratch to
solve sequential decision making tasks arising in contexts of operations
research and management science which we call OMGPT. We first propose a general
sequence modeling framework to cover several operational decision making tasks
as special cases, such as dynamic pricing, inventory management, resource
allocation, and queueing control. Under the framework, all these tasks can be
viewed as a sequential prediction problem where the goal is to predict the
optimal future action given all the historical information. Then we train a
transformer-based neural network model (OMGPT) as a natural and powerful
architecture for sequential modeling. This marks a paradigm shift compared to
the existing methods for these OR/OM tasks in that (i) the OMGPT model can take
advantage of the huge amount of pre-trained data; (ii) when tackling these
problems, OMGPT does not assume any analytical model structure and enables a
direct and rich mapping from the history to the future actions. Either of these
two aspects, to the best of our knowledge, is not achieved by any existing
method. We establish a Bayesian perspective to theoretically understand the
working mechanism of the OMGPT on these tasks, which relates its performance
with the pre-training task diversity and the divergence between the testing
task and pre-training tasks. Numerically, we observe a surprising performance
of the proposed model across all the above tasks.</p></br><a href="http://arxiv.org/pdf/2505.13033v1" target="_blank"><h2>TSPulse: Dual Space Tiny Pre-Trained Models for Rapid Time-Series
  Analysis</h2></a><strong><u>Authors:</u></strong>  Vijay Ekambaram, Subodh Kumar, Arindam Jati, Sumanta Mukherjee, Tomoya Sakai, Pankaj Dayama, Wesley M. Gifford, Jayant Kalagnanam</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> The rise of time-series pre-trained models has advanced temporal
representation learning, but current state-of-the-art models are often
large-scale, requiring substantial compute. We introduce TSPulse, ultra-compact
time-series pre-trained models with only 1M parameters, specialized to perform
strongly across classification, anomaly detection, imputation, and retrieval
tasks. TSPulse introduces innovations at both the architecture and task levels.
At the architecture level, it employs a dual-space masked reconstruction,
learning from both time and frequency domains to capture complementary signals.
This is further enhanced by a dual-embedding disentanglement, generating both
detailed embeddings for fine-grained analysis and high-level semantic
embeddings for broader task understanding. Notably, TSPulse's semantic
embeddings are robust to shifts in time, magnitude, and noise, which is
important for robust retrieval. At the task level, TSPulse incorporates TSLens,
a fine-tuning component enabling task-specific feature attention. It also
introduces a multi-head triangulation technique that correlates deviations from
multiple prediction heads, enhancing anomaly detection by fusing complementary
model outputs. Additionally, a hybrid mask pretraining is proposed to improves
zero-shot imputation by reducing pre-training bias. These architecture and task
innovations collectively contribute to TSPulse's significant performance gains:
5-16% on the UEA classification benchmarks, +20% on the TSB-AD anomaly
detection leaderboard, +50% in zero-shot imputation, and +25% in time-series
retrieval. Remarkably, these results are achieved with just 1M parameters,
making TSPulse 10-100X smaller than existing pre-trained models. Its efficiency
enables GPU-free inference and rapid pre-training, setting a new standard for
efficient time-series pre-trained models. Models will be open-sourced soon.</p></br><a href="http://arxiv.org/pdf/2505.12419v1" target="_blank"><h2>Embedding principle of homogeneous neural network for classification
  problem</h2></a><strong><u>Authors:</u></strong>  Jiahan Zhang, Tao Luo, Yaoyu Zhang</br><strong><u>Categories:</u></strong> cs.LG, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Understanding the convergence points and optimization landscape of neural
networks is crucial, particularly for homogeneous networks where
Karush-Kuhn-Tucker (KKT) points of the associated maximum-margin problem often
characterize solutions. This paper investigates the relationship between such
KKT points across networks of different widths generated via neuron splitting.
We introduce and formalize the \textbf{KKT point embedding principle},
establishing that KKT points of a homogeneous network's max-margin problem
($P_{\Phi}$) can be embedded into the KKT points of a larger network's problem
($P_{\tilde{\Phi}}$) via specific linear isometric transformations
corresponding to neuron splitting. We rigorously prove this principle holds for
neuron splitting in both two-layer and deep homogeneous networks. Furthermore,
we connect this static embedding to the dynamics of gradient flow training with
smooth losses. We demonstrate that trajectories initiated from appropriately
mapped points remain mapped throughout training and that the resulting
$\omega$-limit sets of directions are correspondingly mapped ($T(L(\theta(0)))
= L(\boldsymbol{\eta}(0))$), thereby preserving the alignment with KKT
directions dynamically when directional convergence occurs. Our findings offer
insights into the effects of network width, parameter redundancy, and the
structural connections between solutions found via optimization in homogeneous
networks of varying sizes.</p></br><a href="http://arxiv.org/pdf/2505.10880v1" target="_blank"><h2>Approximation and Generalization Abilities of Score-based Neural Network
  Generative Models for Sub-Gaussian Distributions</h2></a><strong><u>Authors:</u></strong>  Guoji Fu, Wee Sun Lee</br><strong><u>Categories:</u></strong> cs.LG, stat.ML</br><strong><u>Comments:</u></strong> 94 pages</br><p><strong><u>Abstract:</u></strong> This paper studies the approximation and generalization abilities of
score-based neural network generative models (SGMs) in estimating an unknown
distribution $P_0$ from $n$ i.i.d. observations in $d$ dimensions. Assuming
merely that $P_0$ is $\alpha$-sub-Gaussian, we prove that for any time step $t
\in [t_0, n^{O(1)}]$, where $t_0 \geq O(\alpha^2n^{-2/d}\log n)$, there exists
a deep ReLU neural network with width $\leq O(\log^3n)$ and depth $\leq
O(n^{3/d}\log_2n)$ that can approximate the scores with $\tilde{O}(n^{-1})$
mean square error and achieve a nearly optimal rate of
$\tilde{O}(n^{-1}t_0^{-d/2})$ for score estimation, as measured by the score
matching loss. Our framework is universal and can be used to establish
convergence rates for SGMs under milder assumptions than previous work. For
example, assuming further that the target density function $p_0$ lies in
Sobolev or Besov classes, with an appropriately early stopping strategy, we
demonstrate that neural network-based SGMs can attain nearly minimax
convergence rates up to logarithmic factors. Our analysis removes several
crucial assumptions, such as Lipschitz continuity of the score function or a
strictly positive lower bound on the target density.</p></br><a href="http://arxiv.org/pdf/2505.11416v1" target="_blank"><h2>MID-L: Matrix-Interpolated Dropout Layer with Layer-wise Neuron
  Selection</h2></a><strong><u>Authors:</u></strong>  Pouya Shaeri, Ariane Middel</br><strong><u>Categories:</u></strong> cs.NE, cs.AI, cs.LG</br><strong><u>Comments:</u></strong> Submitted in a Computer Science Conference, currently in Review</br><p><strong><u>Abstract:</u></strong> Modern neural networks often activate all neurons for every input, leading to
unnecessary computation and inefficiency. We introduce Matrix-Interpolated
Dropout Layer (MID-L), a novel module that dynamically selects and activates
only the most informative neurons by interpolating between two transformation
paths via a learned, input-dependent gating vector. Unlike conventional dropout
or static sparsity methods, MID-L employs a differentiable Top-k masking
strategy, enabling per-input adaptive computation while maintaining end-to-end
differentiability. MID-L is model-agnostic and integrates seamlessly into
existing architectures. Extensive experiments on six benchmarks, including
MNIST, CIFAR-10, CIFAR-100, SVHN, UCI Adult, and IMDB, show that MID-L achieves
up to average 55\% reduction in active neurons, 1.7$\times$ FLOPs savings, and
maintains or exceeds baseline accuracy. We further validate the informativeness
and selectivity of the learned neurons via Sliced Mutual Information (SMI) and
observe improved robustness under overfitting and noisy data conditions.
Additionally, MID-L demonstrates favorable inference latency and memory usage
profiles, making it suitable for both research exploration and deployment on
compute-constrained systems. These results position MID-L as a general-purpose,
plug-and-play dynamic computation layer, bridging the gap between dropout
regularization and efficient inference.</p></br><a href="http://arxiv.org/pdf/2505.14512v1" target="_blank"><h2>Just One Layer Norm Guarantees Stable Extrapolation</h2></a><strong><u>Authors:</u></strong>  Juliusz Ziomek, George Whittle, Michael A. Osborne</br><strong><u>Categories:</u></strong> cs.LG, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> In spite of their prevalence, the behaviour of Neural Networks when
extrapolating far from the training distribution remains poorly understood,
with existing results limited to specific cases. In this work, we prove general
results -- the first of their kind -- by applying Neural Tangent Kernel (NTK)
theory to analyse infinitely-wide neural networks trained until convergence and
prove that the inclusion of just one Layer Norm (LN) fundamentally alters the
induced NTK, transforming it into a bounded-variance kernel. As a result, the
output of an infinitely wide network with at least one LN remains bounded, even
on inputs far from the training data. In contrast, we show that a broad class
of networks without LN can produce pathologically large outputs for certain
inputs. We support these theoretical findings with empirical experiments on
finite-width networks, demonstrating that while standard NNs often exhibit
uncontrolled growth outside the training domain, a single LN layer effectively
mitigates this instability. Finally, we explore real-world implications of this
extrapolatory stability, including applications to predicting residue sizes in
proteins larger than those seen during training and estimating age from facial
images of underrepresented ethnicities absent from the training set.</p></br><a href="http://arxiv.org/pdf/2505.12591v1" target="_blank"><h2>First Light And Reionization Epoch Simulations (FLARES) -- XIX:
  Supermassive black hole mergers in the early Universe and their environmental
  dependence</h2></a><strong><u>Authors:</u></strong>  Shihong Liao, Dimitrios Irodotou, Maxwell G. A. Maltz, Christopher C. Lovell, Zhen Jiang, Sophie L. Newman, Aswin P. Vijayan, Paurush Punyasheel, William J. Roper, Louise T. C. Seeyave, Sonja Soininen, Peter A. Thomas, Stephen M. Wilkins</br><strong><u>Categories:</u></strong> astro-ph.GA, astro-ph.CO</br><strong><u>Comments:</u></strong> 14 pages, 12 figures, submitted to MNRAS</br><p><strong><u>Abstract:</u></strong> The upcoming space-based gravitational wave (GW) observatory, LISA, is
expected to detect GW signals from supermassive black hole (SMBH) mergers
occurring at high redshifts. However, understanding the origin and growth of
SMBHs in the early Universe remains an open problem in astrophysics. In this
work, we utilize the First Light And Reionization Epoch Simulations (FLARES), a
suite of cosmological hydrodynamical zoom-in simulations, to study SMBH mergers
at $5 \lesssim z \lesssim 10$ across a wide range of environments. Most mergers
in FLARES involve secondary SMBHs near the seed mass ($m_{seed} \approx 1.5
\times 10^{5} M_{\odot}$) while primary SMBHs span up to $10^{9} M_{\odot}$,
resulting in mass ratios from $q \sim 10^{-4}$ to $1$, with a peak at $q \sim
1$. The number of mergers increases rapidly towards lower redshifts, and the
comoving total number density scales with overdensity as $n_{merger} =
10^{-3.80} (1 + \delta)^{4.56}$. Denser regions host more massive mergers, with
higher merger redshifts and lower mass ratios. Within the FLARES redshift
range, LISA is expected to detect mergers with $10^{5} \lesssim M_{tot} /
M_{\odot} \lesssim 10^{8}$ and $q \gtrsim 10^{-2}$, corresponding to a
detection rate of 0.030 $yr^{-1}$ for events with signal-to-noise ratio $SNR
\geq 10$. Our study demonstrates the sensitivity of GW predictions at high
redshifts to SMBH seed models and merger time delays, highlighting the need for
improved modeling in future cosmological simulations to maximize LISA's
scientific return.</p></br><a href="http://arxiv.org/pdf/2505.13742v1" target="_blank"><h2>Understanding Task Representations in Neural Networks via Bayesian
  Ablation</h2></a><strong><u>Authors:</u></strong>  Andrew Nam, Declan Campbell, Thomas Griffiths, Jonathan Cohen, Sarah-Jane Leslie</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Neural networks are powerful tools for cognitive modeling due to their
flexibility and emergent properties. However, interpreting their learned
representations remains challenging due to their sub-symbolic semantics. In
this work, we introduce a novel probabilistic framework for interpreting latent
task representations in neural networks. Inspired by Bayesian inference, our
approach defines a distribution over representational units to infer their
causal contributions to task performance. Using ideas from information theory,
we propose a suite of tools and metrics to illuminate key model properties,
including representational distributedness, manifold complexity, and
polysemanticity.</p></br><a href="http://arxiv.org/pdf/2505.14027v1" target="_blank"><h2>CSAGC-IDS: A Dual-Module Deep Learning Network Intrusion Detection Model
  for Complex and Imbalanced Data</h2></a><strong><u>Authors:</u></strong>  Yifan Zeng</br><strong><u>Categories:</u></strong> cs.CR, cs.AI</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> As computer networks proliferate, the gravity of network intrusions has
escalated, emphasizing the criticality of network intrusion detection systems
for safeguarding security. While deep learning models have exhibited promising
results in intrusion detection, they face challenges in managing
high-dimensional, complex traffic patterns and imbalanced data categories. This
paper presents CSAGC-IDS, a network intrusion detection model based on deep
learning techniques. CSAGC-IDS integrates SC-CGAN, a self-attention-enhanced
convolutional conditional generative adversarial network that generates
high-quality data to mitigate class imbalance. Furthermore, CSAGC-IDS
integrates CSCA-CNN, a convolutional neural network enhanced through cost
sensitive learning and channel attention mechanism, to extract features from
complex traffic data for precise detection. Experiments conducted on the
NSL-KDD dataset. CSAGC-IDS achieves an accuracy of 84.55% and an F1-score of
84.52% in five-class classification task, and an accuracy of 91.09% and an F1
score of 92.04% in binary classification task.Furthermore, this paper provides
an interpretability analysis of the proposed model, using SHAP and LIME to
explain the decision-making mechanisms of the model.</p></br><a href="http://arxiv.org/pdf/2505.10856v1" target="_blank"><h2>ImputeINR: Time Series Imputation via Implicit Neural Representations
  for Disease Diagnosis with Missing Data</h2></a><strong><u>Authors:</u></strong>  Mengxuan Li, Ke Liu, Jialong Guo, Jiajun Bu, Hongwei Wang, Haishuai Wang</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> Accepted by IJCAI 2025</br><p><strong><u>Abstract:</u></strong> Healthcare data frequently contain a substantial proportion of missing
values, necessitating effective time series imputation to support downstream
disease diagnosis tasks. However, existing imputation methods focus on discrete
data points and are unable to effectively model sparse data, resulting in
particularly poor performance for imputing substantial missing values. In this
paper, we propose a novel approach, ImputeINR, for time series imputation by
employing implicit neural representations (INR) to learn continuous functions
for time series. ImputeINR leverages the merits of INR in that the continuous
functions are not coupled to sampling frequency and have infinite sampling
frequency, allowing ImputeINR to generate fine-grained imputations even on
extremely sparse observed values. Extensive experiments conducted on eight
datasets with five ratios of masked values show the superior imputation
performance of ImputeINR, especially for high missing ratios in time series
data. Furthermore, we validate that applying ImputeINR to impute missing values
in healthcare data enhances the performance of downstream disease diagnosis
tasks. Codes are available.</p></br><a href="http://arxiv.org/pdf/2505.13595v1" target="_blank"><h2>Revealing the intricacies of radio galaxies and filaments in the merging
  galaxy cluster Abell 2255. I. Insights from deep LOFAR-VLBI sub-arcsecond
  resolution images</h2></a><strong><u>Authors:</u></strong>  E. De Rubeis, M. Bondi, A. Botteon, R. J. van Weeren, J. M. G. H. J. de Jong, L. Rudnick, G. Brunetti, K. Rajpurohit, C. Gheller, H. J. A. Rttgering</br><strong><u>Categories:</u></strong> astro-ph.GA, astro-ph.CO</br><strong><u>Comments:</u></strong> 11 pages, 8 figures, 2 tables. Accepted for publication in A&A</br><p><strong><u>Abstract:</u></strong> High sensitivity of modern interferometers is revealing a plethora of
filaments surrounding radio galaxies, especially in galaxy cluster
environments. The morphology and spectral characteristics of these thin
structures require the combination of high-resolution and low frequency
observations, which is best obtained using the LOw Frequency ARray (LOFAR)
international stations. In this paper, we aim to detect and characterize
non-thermal filaments observed close or as part of the radio galaxies in Abell
2255 using deep, LOFAR-VLBI observations at 144 MHz. These structures can be
used to disentangle possible scenarios for the origin of the non-thermal
filaments and connection to the motion of the host galaxy within the dense and
turbulent intracluster medium (ICM), and consequent interaction between the ICM
and radio jets. Combining multiple observations, we produced the deepest images
ever obtained with LOFAR-VLBI targeting a galaxy cluster, using 56 hours of
observations, reaching $0.3-0.5"$ resolution. We detailed throughout the paper
the calibration and imaging strategy for the different targets, as well as the
multitude of morphological features discovered. Thanks to the high-sensitivity
of LOFAR-VLBI, we revealed unprecedented details for the main cluster radio
galaxies, recovering in most cases also their more extended structure observed
only at such low frequencies. In particular, we focused on the Original Tailed
Radio Galaxy (Original TRG) where we distinguished many filaments constituting
its tail with varying lengths ($80-110$ kpc) and widths ($3-10$ kpc). The final
radio images showcase the potential of deep, high-resolution observations for
galaxy clusters. With such approach, we enabled the study of these thin,
elongated radio filaments: after being discovered, these filaments now require
spectral studies to determine their formation mechanisms.</p></br><a href="http://arxiv.org/pdf/2505.14293v1" target="_blank"><h2>The first direct imaging of the silhouette of a damped Lyman $$
  system along the line-of-sight to a background galaxy</h2></a><strong><u>Authors:</u></strong>  Fuga Komori, Akio K. Inoue, Ken Mawatari, Yuma Sugahara, Hideki Umehata, Rhythm Shimakawa, Satoshi Yamanaka, Takuya Hashimoto, Jorryt Matthee, Toru Misawa</br><strong><u>Categories:</u></strong> astro-ph.CO, astro-ph.GA</br><strong><u>Comments:</u></strong> Submitted to MNRAS</br><p><strong><u>Abstract:</u></strong> The H~{\sc i} gas distribution in damped Lyman $\alpha$ absorbers (DLAs) has
remained elusive due to the point-source nature of background quasar emission.
Observing DLAs against spatially extended background galaxies provides a new
method for constraining their size and structure. Using the Keck Cosmic Web
Imager, we present the first ``silhouette'' image of a DLA at $z=3.34$,
identified in the spectrum of a background galaxy at $z=3.61$. Although the
silhouette remains unresolved due to limited spatial resolution, this
represents a successful proof-of-concept for studying DLA morphology using
extended background sources. Possible residual emission in the DLA trough
suggests an optical depth contrast exceeding $10^7$ in the internal structure,
implying a sharp edge or patchy structure. A Lyman $\alpha$ emitter (LAE) at
$z_{\rm LAE}=3.3433\pm0.0005$, consistent with the DLA redshift, is detected at
an angular separation of $1.''73\pm0.''28$ ($12.9\pm2.1$ kpc). The DLA is
surrounded by three galaxies within 140 kpc in projected distance and 500 km
s$^{-1}$ in line-of-sight velocity, indicating that it resides in the
circumgalactic medium of the LAE or within a galaxy group/protocluster
environment. An O~{\sc i} $\lambda1302$ absorption at $z_{\rm
OI}=3.3288\pm0.0004$ is also detected along the line of sight. This absorber
may trace metal-enriched outflow from the LAE or a gas-rich galaxy exhibiting
the highest star formation activity among the surrounding galaxies. Future
large spectroscopic surveys of galaxies will expand such a DLA sample, and
three-dimensional spectroscopy for it will shed new light on the role of
intergalactic dense gas in galaxy formation and evolution.</p></br><a href="http://arxiv.org/pdf/2505.12473v1" target="_blank"><h2>Multi-modal contrastive learning adapts to intrinsic dimensions of
  shared latent variables</h2></a><strong><u>Authors:</u></strong>  Yu Gui, Cong Ma, Zongming Ma</br><strong><u>Categories:</u></strong> stat.ML, cs.LG, math.ST, stat.TH</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Multi-modal contrastive learning as a self-supervised representation learning
technique has achieved great success in foundation model training, such as
CLIP~\citep{radford2021learning}. In this paper, we study the theoretical
properties of the learned representations from multi-modal contrastive learning
beyond linear representations and specific data distributions. Our analysis
reveals that, enabled by temperature optimization, multi-modal contrastive
learning not only maximizes mutual information between modalities but also
adapts to intrinsic dimensions of data, which can be much lower than
user-specified dimensions for representation vectors. Experiments on both
synthetic and real-world datasets demonstrate the ability of contrastive
learning to learn low-dimensional and informative representations, bridging
theoretical insights and practical performance.</p></br><a href="http://arxiv.org/pdf/2505.12626v1" target="_blank"><h2>scSiameseClu: A Siamese Clustering Framework for Interpreting
  single-cell RNA Sequencing Data</h2></a><strong><u>Authors:</u></strong>  Ping Xu, Zhiyuan Ning, Pengjiang Li, Wenhao Liu, Pengyang Wang, Jiaxu Cui, Yuanchun Zhou, Pengfei Wang</br><strong><u>Categories:</u></strong> q-bio.GN, cs.AI, cs.LG</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Single-cell RNA sequencing (scRNA-seq) reveals cell heterogeneity, with cell
clustering playing a key role in identifying cell types and marker genes.
Recent advances, especially graph neural networks (GNNs)-based methods, have
significantly improved clustering performance. However, the analysis of
scRNA-seq data remains challenging due to noise, sparsity, and high
dimensionality. Compounding these challenges, GNNs often suffer from
over-smoothing, limiting their ability to capture complex biological
information. In response, we propose scSiameseClu, a novel Siamese Clustering
framework for interpreting single-cell RNA-seq data, comprising of 3 key steps:
(1) Dual Augmentation Module, which applies biologically informed perturbations
to the gene expression matrix and cell graph relationships to enhance
representation robustness; (2) Siamese Fusion Module, which combines
cross-correlation refinement and adaptive information fusion to capture complex
cellular relationships while mitigating over-smoothing; and (3) Optimal
Transport Clustering, which utilizes Sinkhorn distance to efficiently align
cluster assignments with predefined proportions while maintaining balance.
Comprehensive evaluations on seven real-world datasets demonstrate
that~\methodname~outperforms state-of-the-art methods in single-cell
clustering, cell type annotation, and cell type classification, providing a
powerful tool for scRNA-seq data interpretation.</p></br><a href="http://arxiv.org/pdf/2505.13575v1" target="_blank"><h2>An Overview of Arithmetic Adaptations for Inference of Convolutional
  Neural Networks on Re-configurable Hardware</h2></a><strong><u>Authors:</u></strong>  Ilkay Wunderlich, Benjamin Koch, Sven Schnfeld</br><strong><u>Categories:</u></strong> cs.LG</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Convolutional Neural Networks (CNNs) have gained high popularity as a tool
for computer vision tasks and for that reason are used in various applications.
There are many different concepts, like single shot detectors, that have been
published for detecting objects in images or video streams. However, CNNs
suffer from disadvantages regarding the deployment on embedded platforms such
as re-configurable hardware like Field Programmable Gate Arrays (FPGAs). Due to
the high computational intensity, memory requirements and arithmetic
conditions, a variety of strategies for running CNNs on FPGAs have been
developed. The following methods showcase our best practice approaches for a
TinyYOLOv3 detector network on a XILINX Artix-7 FPGA using techniques like
fusion of batch normalization, filter pruning and post training network
quantization.</p></br><a href="http://arxiv.org/pdf/2505.13677v1" target="_blank"><h2>Unveiling Electron Density Profile in Nearby Galaxies using SDSS MaNGA</h2></a><strong><u>Authors:</u></strong>  Shivam Burman, Sunil Malik, Suprit Singh, Yogesh Wadadekar</br><strong><u>Categories:</u></strong> astro-ph.GA, astro-ph.CO</br><strong><u>Comments:</u></strong> 15 pages, 8 figures, 2 tables</br><p><strong><u>Abstract:</u></strong> Most observational studies of galactic-scale magnetic fields using Faraday
rotation rely on estimates of thermal electron densities in galaxies and their
radial variations. However, the spatial distribution of electrons in the
interstellar medium (ISM) is not clearly known. In this study, we propose and
utilize collision-excited doublet emission line ratios of [S\,\textsc{ii}]
$\lambda\lambda$ 6716, 6731 \AA~and [O\,\textsc{ii}] $\lambda\lambda$ 3726,
3729 \AA \ to estimate the electron densities ($n_e$). To map their
distribution in the galaxies, we employ Integral Field Unit (IFU) spectroscopic
observations from the SDSS Mapping Nearby Galaxies at Apache Point Observatory
(MaNGA) survey, utilising data products from both the \texttt{Pipe3D} and MaNGA
Data Analysis Pipeline (DAP). We present a spatially resolved analysis of $13$
face-on galaxies (inclination, $i \leq 10^\circ$), including $9$ star-forming
galaxies (SFGs) and $4$ Non-SFGs. Azimuthally averaged radial profiles of $n_e$
are obtained using two different binning schemes: linear and non-linear. For
the \texttt{Pipe3D} case, both SFGs and non-SFGs exhibit $n_e$ gradients, with
higher densities of $n_e$(S\,\textsc{ii}) = $165.6 \pm 20.8$ cm$^{-3}$ in the
inner disk region (r/R$_e$ $\leq$ 1.5), which decrease to $31 \pm 4.5$
cm$^{-3}$ in the outer disk region (r/R$_e$ $>$ 1.5). We also translate $n_e$
to the electron column density $N_e$ assuming an evenly distributed thin disk
profile, fairly excluding the central bulge regions. These electron density
estimates at different radii provide valuable insights for resolving
ambiguities in current and future studies of magnetic fields in galaxies.</p></br><a href="http://arxiv.org/pdf/2505.11567v1" target="_blank"><h2>Beyond Time: Cross-Dimensional Frequency Supervision for Time Series
  Forecasting</h2></a><strong><u>Authors:</u></strong>  Tianyi Shi, Zhu Meng, Yue Chen, Siyang Zheng, Fei Su, Jin Huang, Changrui Ren, Zhicheng Zhao</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Time series forecasting plays a crucial role in various fields, and the
methods based on frequency domain analysis have become an important branch.
However, most existing studies focus on the design of elaborate model
architectures and are often tailored for limited datasets, still lacking
universality. Besides, the assumption of independent and identically
distributed (IID) data also contradicts the strong correlation of the time
domain labels. To address these issues, abandoning time domain supervision, we
propose a purely frequency domain supervision approach named cross-dimensional
frequency (X-Freq) loss. Specifically, based on a statistical phenomenon, we
first prove that the information entropy of the time series is higher than its
spectral entropy, which implies higher certainty in frequency domain and thus
can provide better supervision. Secondly, the Fourier Transform and the Wavelet
Transform are applied to the time dimension and the channel dimension of the
time series respectively, to capture the long-term and short-term frequency
variations as well as the spatial configuration features. Thirdly, the loss
between predictions and targets is uniformly computed in the frequency domain.
Moreover, we plug-and-play incorporate X-Freq into multiple advanced
forecasting models and compare on 14 real-world datasets. The experimental
results demonstrate that, without making any modification to the original
architectures or hyperparameters, X-Freq can improve the forecasting
performance by an average of 3.3% on long-term forecasting datasets and 27.7%
on short-term ones, showcasing superior generality and practicality. The code
will be released publicly.</p></br><a href="http://arxiv.org/pdf/2505.13593v1" target="_blank"><h2>The High-redshift Blazar MG3 J163554+3629: Physical Properties and the
  Enigma of Its Unexpected Supermassive Black Hole Growth</h2></a><strong><u>Authors:</u></strong>  Jose Maria Sanchez Zaballa, Eugenio Bottacini, Andrea Tramacere</br><strong><u>Categories:</u></strong> astro-ph.GA, astro-ph.CO</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> There is general consensus that active galactic nuclei (AGNs) derive their
radiating power from a supermassive black hole (SMBH) that accretes matter.
Yet, their precise powering mechanisms and the resulting growth of the SMBH are
poorly understood, especially for AGNs at high redshift. Blazars are AGNs
pointing their jet toward the observer, thus being detectable from radio
through gamma rays at high redshift due to Doppler boosting. The blazar MG3
J163554+3629 is located at redshift z=3.65 and it is a flat spectrum radio
quasar (FSRQ). In this work, we show the results of the modeling of its
spectral energy distribution (SED) from radio to gamma rays with a one-zone
leptonic model. We estimate the uncertainties through a Markov Chain Monte
Carlo approach. As a result, we infer the black hole mass M_BH = 1.1(+0.2,-0.1)
x 10^9 Msun and a modest magnetic field of B = 6.56(+0.13,-0.09) x 10^-2 G in
line with the Compton dominance observed in high-redshift FSRQs. The emitting
region is outside the broad line region but within the region of the dust torus
radius. The rather small accretion efficiency of eta=0.083 is not solely
inferred through the SED modeling but also through the energetics. An evolution
study suggests that in an Eddington-limited accretion process the SMBH did not
have time enough to grow from an initial seed mass of ~10^6 Msun at z~30 into a
mass of M_BH ~ 10^9 Msun at z=3.65. Faster mass growth might be obtained in a
super-Eddington process throughout frequent episodes. Alternative scenarios
propose that the existence of the jet itself can facilitate a more rapid
growth.</p></br><a href="http://arxiv.org/pdf/2505.13650v1" target="_blank"><h2>Self-Reinforced Graph Contrastive Learning</h2></a><strong><u>Authors:</u></strong>  Chou-Ying Hsieh, Chun-Fu Jang, Cheng-En Hsieh, Qian-Hui Chen, Sy-Yen Kuo</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Graphs serve as versatile data structures in numerous real-world
domains-including social networks, molecular biology, and knowledge graphs-by
capturing intricate relational information among entities. Among graph-based
learning techniques, Graph Contrastive Learning (GCL) has gained significant
attention for its ability to derive robust, self-supervised graph
representations through the contrasting of positive and negative sample pairs.
However, a critical challenge lies in ensuring high-quality positive pairs so
that the intrinsic semantic and structural properties of the original graph are
preserved rather than distorted. To address this issue, we propose SRGCL
(Self-Reinforced Graph Contrastive Learning), a novel framework that leverages
the model's own encoder to dynamically evaluate and select high-quality
positive pairs. We designed a unified positive pair generator employing
multiple augmentation strategies, and a selector guided by the manifold
hypothesis to maintain the underlying geometry of the latent space. By adopting
a probabilistic mechanism for selecting positive pairs, SRGCL iteratively
refines its assessment of pair quality as the encoder's representational power
improves. Extensive experiments on diverse graph-level classification tasks
demonstrate that SRGCL, as a plug-in module, consistently outperforms
state-of-the-art GCL methods, underscoring its adaptability and efficacy across
various domains.</p></br><a href="http://arxiv.org/pdf/2505.11749v1" target="_blank"><h2>Missing Data Imputation by Reducing Mutual Information with Rectified
  Flows</h2></a><strong><u>Authors:</u></strong>  Jiahao Yu, Qizhen Ying, Leyang Wang, Ziyue Jiang, Song Liu</br><strong><u>Categories:</u></strong> stat.ML, cs.LG</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> This paper introduces a novel iterative method for missing data imputation
that sequentially reduces the mutual information between data and their
corresponding missing mask. Inspired by GAN-based approaches, which train
generators to decrease the predictability of missingness patterns, our method
explicitly targets the reduction of mutual information. Specifically, our
algorithm iteratively minimizes the KL divergence between the joint
distribution of the imputed data and missing mask, and the product of their
marginals from the previous iteration. We show that the optimal imputation
under this framework corresponds to solving an ODE, whose velocity field
minimizes a rectified flow training objective. We further illustrate that some
existing imputation techniques can be interpreted as approximate special cases
of our mutual-information-reducing framework. Comprehensive experiments on
synthetic and real-world datasets validate the efficacy of our proposed
approach, demonstrating superior imputation performance.</p></br><a href="http://arxiv.org/pdf/2505.11878v1" target="_blank"><h2>AdaptMol: Adaptive Fusion from Sequence String to Topological Structure
  for Few-shot Drug Discovery</h2></a><strong><u>Authors:</u></strong>  Yifan Dai, Xuanbai Ren, Tengfei Ma, Qipeng Yan, Yiping Liu, Yuansheng Liu, Xiangxiang Zeng</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, q-bio.MN, J.3; I.2.7</br><strong><u>Comments:</u></strong> 15 pages, 6 figures</br><p><strong><u>Abstract:</u></strong> Accurate molecular property prediction (MPP) is a critical step in modern
drug development. However, the scarcity of experimental validation data poses a
significant challenge to AI-driven research paradigms. Under few-shot learning
scenarios, the quality of molecular representations directly dictates the
theoretical upper limit of model performance. We present AdaptMol, a
prototypical network integrating Adaptive multimodal fusion for Molecular
representation. This framework employs a dual-level attention mechanism to
dynamically integrate global and local molecular features derived from two
modalities: SMILES sequences and molecular graphs. (1) At the local level,
structural features such as atomic interactions and substructures are extracted
from molecular graphs, emphasizing fine-grained topological information; (2) At
the global level, the SMILES sequence provides a holistic representation of the
molecule. To validate the necessity of multimodal adaptive fusion, we propose
an interpretable approach based on identifying molecular active substructures
to demonstrate that multimodal adaptive fusion can efficiently represent
molecules. Extensive experiments on three commonly used benchmarks under 5-shot
and 10-shot settings demonstrate that AdaptMol achieves state-of-the-art
performance in most cases. The rationale-extracted method guides the fusion of
two modalities and highlights the importance of both modalities.</p></br><a href="http://arxiv.org/pdf/2505.13631v1" target="_blank"><h2>Learning (Approximately) Equivariant Networks via Constrained
  Optimization</h2></a><strong><u>Authors:</u></strong>  Andrei Manolache, Luiz F. O. Chamon, Mathias Niepert</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Equivariant neural networks are designed to respect symmetries through their
architecture, boosting generalization and sample efficiency when those
symmetries are present in the data distribution. Real-world data, however,
often departs from perfect symmetry because of noise, structural variation,
measurement bias, or other symmetry-breaking effects. Strictly equivariant
models may struggle to fit the data, while unconstrained models lack a
principled way to leverage partial symmetries. Even when the data is fully
symmetric, enforcing equivariance can hurt training by limiting the model to a
restricted region of the parameter space. Guided by homotopy principles, where
an optimization problem is solved by gradually transforming a simpler problem
into a complex one, we introduce Adaptive Constrained Equivariance (ACE), a
constrained optimization approach that starts with a flexible, non-equivariant
model and gradually reduces its deviation from equivariance. This gradual
tightening smooths training early on and settles the model at a data-driven
equilibrium, balancing between equivariance and non-equivariance. Across
multiple architectures and tasks, our method consistently improves performance
metrics, sample efficiency, and robustness to input perturbations compared with
strictly equivariant models and heuristic equivariance relaxations.</p></br><a href="http://arxiv.org/pdf/2505.12908v1" target="_blank"><h2>Dynamic Graph Induced Contour-aware Heat Conduction Network for
  Event-based Object Detection</h2></a><strong><u>Authors:</u></strong>  Xiao Wang, Yu Jin, Lan Chen, Bo Jiang, Lin Zhu, Yonghong Tian, Jin Tang, Bin Luo</br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.LG</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Event-based Vision Sensors (EVS) have demonstrated significant advantages
over traditional RGB frame-based cameras in low-light conditions, high-speed
motion capture, and low latency. Consequently, object detection based on EVS
has attracted increasing attention from researchers. Current event stream
object detection algorithms are typically built upon Convolutional Neural
Networks (CNNs) or Transformers, which either capture limited local features
using convolutional filters or incur high computational costs due to the
utilization of self-attention. Recently proposed vision heat conduction
backbone networks have shown a good balance between efficiency and accuracy;
however, these models are not specifically designed for event stream data. They
exhibit weak capability in modeling object contour information and fail to
exploit the benefits of multi-scale features. To address these issues, this
paper proposes a novel dynamic graph induced contour-aware heat conduction
network for event stream based object detection, termed CvHeat-DET. The
proposed model effectively leverages the clear contour information inherent in
event streams to predict the thermal diffusivity coefficients within the heat
conduction model, and integrates hierarchical structural graph features to
enhance feature learning across multiple scales. Extensive experiments on three
benchmark datasets for event stream-based object detection fully validated the
effectiveness of the proposed model. The source code of this paper will be
released on https://github.com/Event-AHU/OpenEvDET.</p></br><a href="http://arxiv.org/pdf/2505.12138v1" target="_blank"><h2>Transformer learns the cross-task prior and regularization for
  in-context learning</h2></a><strong><u>Authors:</u></strong>  Fei Lu, Yue Yu</br><strong><u>Categories:</u></strong> cs.LG, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Transformers have shown a remarkable ability for in-context learning (ICL),
making predictions based on contextual examples. However, while theoretical
analyses have explored this prediction capability, the nature of the inferred
context and its utility for downstream predictions remain open questions. This
paper aims to address these questions by examining ICL for inverse linear
regression (ILR), where context inference can be characterized by unsupervised
learning of underlying weight vectors. Focusing on the challenging scenario of
rank-deficient inverse problems, where context length is smaller than the
number of unknowns in the weight vectors and regularization is necessary, we
introduce a linear transformer to learn the inverse mapping from contextual
examples to the underlying weight vector. Our findings reveal that the
transformer implicitly learns both a prior distribution and an effective
regularization strategy, outperforming traditional ridge regression and
regularization methods. A key insight is the necessity of low task
dimensionality relative to the context length for successful learning.
Furthermore, we numerically verify that the error of the transformer estimator
scales linearly with the noise level, the ratio of task dimension to context
length, and the condition number of the input data. These results not only
demonstrate the potential of transformers for solving ill-posed inverse
problems, but also provide a new perspective towards understanding the
knowledge extraction mechanism within transformers.</p></br><a href="http://arxiv.org/pdf/2505.14102v1" target="_blank"><h2>High-dimensional Nonparametric Contextual Bandit Problem</h2></a><strong><u>Authors:</u></strong>  Shogo Iwazaki, Junpei Komiyama, Masaaki Imaizumi</br><strong><u>Categories:</u></strong> stat.ML, cs.LG, stat.ME</br><strong><u>Comments:</u></strong> 38 pages</br><p><strong><u>Abstract:</u></strong> We consider the kernelized contextual bandit problem with a large feature
space. This problem involves $K$ arms, and the goal of the forecaster is to
maximize the cumulative rewards through learning the relationship between the
contexts and the rewards. It serves as a general framework for various
decision-making scenarios, such as personalized online advertising and
recommendation systems. Kernelized contextual bandits generalize the linear
contextual bandit problem and offers a greater modeling flexibility. Existing
methods, when applied to Gaussian kernels, yield a trivial bound of $O(T)$ when
we consider $\Omega(\log T)$ feature dimensions. To address this, we introduce
stochastic assumptions on the context distribution and show that no-regret
learning is achievable even when the number of dimensions grows up to the
number of samples. Furthermore, we analyze lenient regret, which allows a
per-round regret of at most $\Delta > 0$. We derive the rate of lenient regret
in terms of $\Delta$.</p></br><a href="http://arxiv.org/pdf/2505.12761v2" target="_blank"><h2>Enhancing Channel-Independent Time Series Forecasting via Cross-Variate
  Patch Embedding</h2></a><strong><u>Authors:</u></strong>  Donghwa Shin, Edwin Zhang</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Transformers have recently gained popularity in time series forecasting due
to their ability to capture long-term dependencies. However, many existing
models focus only on capturing temporal dependencies while omitting intricate
relationships between variables. Recent models have tried tackling this by
explicitly modeling both cross-time and cross-variate dependencies through a
sequential or unified attention mechanism, but they are entirely channel
dependent (CD) across all layers, making them potentially susceptible to
overfitting. To address this, we propose Cross-Variate Patch Embeddings (CVPE),
a lightweight CD module that injects cross-variate context into
channel-independent (CI) models by simply modifying the patch embedding
process. We achieve this by adding a learnable positional encoding and a
lightweight router-attention block to the vanilla patch embedding layer. We
then integrate CVPE into Time-LLM, a multimodal CI forecasting model, to
demonstrate its effectiveness in capturing cross-variate dependencies and
enhance the CI model's performance. Extensive experimental results on seven
real-world datasets show that our enhanced Time-LLM outperforms the original
baseline model simply by incorporating the CVPE module, with no other changes.</p></br><a href="http://arxiv.org/pdf/2505.11034v1" target="_blank"><h2>CleanPatrick: A Benchmark for Image Data Cleaning</h2></a><strong><u>Authors:</u></strong>  Fabian Grger, Simone Lionetti, Philippe Gottfrois, Alvaro Gonzalez-Jimenez, Ludovic Amruthalingam, Elisabeth Victoria Goessinger, Hanna Lindemann, Marie Bargiela, Marie Hofbauer, Omar Badri, Philipp Tschandl, Arash Koochek, Matthew Groh, Alexander A. Navarini, Marc Pouly</br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.LG</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Robust machine learning depends on clean data, yet current image data
cleaning benchmarks rely on synthetic noise or narrow human studies, limiting
comparison and real-world relevance. We introduce CleanPatrick, the first
large-scale benchmark for data cleaning in the image domain, built upon the
publicly available Fitzpatrick17k dermatology dataset. We collect 496,377
binary annotations from 933 medical crowd workers, identify off-topic samples
(4%), near-duplicates (21%), and label errors (22%), and employ an aggregation
model inspired by item-response theory followed by expert review to derive
high-quality ground truth. CleanPatrick formalizes issue detection as a ranking
task and adopts typical ranking metrics mirroring real audit workflows.
Benchmarking classical anomaly detectors, perceptual hashing, SSIM, Confident
Learning, NoiseRank, and SelfClean, we find that, on CleanPatrick,
self-supervised representations excel at near-duplicate detection, classical
methods achieve competitive off-topic detection under constrained review
budgets, and label-error detection remains an open challenge for fine-grained
medical classification. By releasing both the dataset and the evaluation
framework, CleanPatrick enables a systematic comparison of image-cleaning
strategies and paves the way for more reliable data-centric artificial
intelligence.</p></br><a href="http://arxiv.org/pdf/2505.10960v1" target="_blank"><h2>Relational Graph Transformer</h2></a><strong><u>Authors:</u></strong>  Vijay Prakash Dwivedi, Sri Jaladi, Yangyi Shen, Federico Lpez, Charilaos I. Kanatsoulis, Rishi Puri, Matthias Fey, Jure Leskovec</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.DB</br><strong><u>Comments:</u></strong> Code:this https URL</br><p><strong><u>Abstract:</u></strong> Relational Deep Learning (RDL) is a promising approach for building
state-of-the-art predictive models on multi-table relational data by
representing it as a heterogeneous temporal graph. However, commonly used Graph
Neural Network models suffer from fundamental limitations in capturing complex
structural patterns and long-range dependencies that are inherent in relational
data. While Graph Transformers have emerged as powerful alternatives to GNNs on
general graphs, applying them to relational entity graphs presents unique
challenges: (i) Traditional positional encodings fail to generalize to massive,
heterogeneous graphs; (ii) existing architectures cannot model the temporal
dynamics and schema constraints of relational data; (iii) existing tokenization
schemes lose critical structural information. Here we introduce the Relational
Graph Transformer (RelGT), the first graph transformer architecture designed
specifically for relational tables. RelGT employs a novel multi-element
tokenization strategy that decomposes each node into five components (features,
type, hop distance, time, and local structure), enabling efficient encoding of
heterogeneity, temporality, and topology without expensive precomputation. Our
architecture combines local attention over sampled subgraphs with global
attention to learnable centroids, incorporating both local and database-wide
representations. Across 21 tasks from the RelBench benchmark, RelGT
consistently matches or outperforms GNN baselines by up to 18%, establishing
Graph Transformers as a powerful architecture for Relational Deep Learning.</p></br><a href="http://arxiv.org/pdf/2505.13072v1" target="_blank"><h2>Orthogonal Survival Learners for Estimating Heterogeneous Treatment
  Effects from Time-to-Event Data</h2></a><strong><u>Authors:</u></strong>  Dennis Frauen, Maresa Schrder, Konstantin Hess, Stefan Feuerriegel</br><strong><u>Categories:</u></strong> cs.LG, stat.ML</br><strong><u>Comments:</u></strong> Preprint</br><p><strong><u>Abstract:</u></strong> Estimating heterogeneous treatment effects (HTEs) is crucial for personalized
decision-making. However, this task is challenging in survival analysis, which
includes time-to-event data with censored outcomes (e.g., due to study
dropout). In this paper, we propose a toolbox of novel orthogonal survival
learners to estimate HTEs from time-to-event data under censoring. Our learners
have three main advantages: (i) we show that learners from our toolbox are
guaranteed to be orthogonal and thus come with favorable theoretical
properties; (ii) our toolbox allows for incorporating a custom weighting
function, which can lead to robustness against different types of low overlap,
and (iii) our learners are model-agnostic (i.e., they can be combined with
arbitrary machine learning models). We instantiate the learners from our
toolbox using several weighting functions and, as a result, propose various
neural orthogonal survival learners. Some of these coincide with existing
survival learners (including survival versions of the DR- and R-learner), while
others are novel and further robust w.r.t. low overlap regimes specific to the
survival setting (i.e., survival overlap and censoring overlap). We then
empirically verify the effectiveness of our learners for HTE estimation in
different low-overlap regimes through numerical experiments. In sum, we provide
practitioners with a large toolbox of learners that can be used for randomized
and observational studies with censored time-to-event data.</p></br><a href="http://arxiv.org/pdf/2505.13745v1" target="_blank"><h2>Synthetic Non-stationary Data Streams for Recognition of the Unknown</h2></a><strong><u>Authors:</u></strong>  Joanna Komorniczak</br><strong><u>Categories:</u></strong> cs.LG, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> The problem of data non-stationarity is commonly addressed in data stream
processing. In a dynamic environment, methods should continuously be ready to
analyze time-varying data -- hence, they should enable incremental training and
respond to concept drifts. An equally important variability typical for
non-stationary data stream environments is the emergence of new, previously
unknown classes. Often, methods focus on one of these two phenomena --
detection of concept drifts or detection of novel classes -- while both
difficulties can be observed in data streams. Additionally, concerning
previously unknown observations, the topic of open set of classes has become
particularly important in recent years, where the goal of methods is to
efficiently classify within known classes and recognize objects outside the
model competence. This article presents a strategy for synthetic data stream
generation in which both concept drifts and the emergence of new classes
representing unknown objects occur. The presented research shows how
unsupervised drift detectors address the task of detecting novelty and concept
drifts and demonstrates how the generated data streams can be utilized in the
open set recognition task.</p></br><a href="http://arxiv.org/pdf/2505.13264v1" target="_blank"><h2>Net-Zero: A Comparative Study on Neural Network Design for
  Climate-Economic PDEs Under Uncertainty</h2></a><strong><u>Authors:</u></strong>  Carlos Rodriguez-Pardo, Louis Daumas, Leonardo Chiani, Massimo Tavoni</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.NE, cs.PF, math.AP, 68T07 (Primary) 35Q91, 91B76 (Secondary), I.2.1; I.5.1; J.4</br><strong><u>Comments:</u></strong> Under review</br><p><strong><u>Abstract:</u></strong> Climate-economic modeling under uncertainty presents significant
computational challenges that may limit policymakers' ability to address
climate change effectively. This paper explores neural network-based approaches
for solving high-dimensional optimal control problems arising from models that
incorporate ambiguity aversion in climate mitigation decisions. We develop a
continuous-time endogenous-growth economic model that accounts for multiple
mitigation pathways, including emission-free capital and carbon intensity
reductions. Given the inherent complexity and high dimensionality of these
models, traditional numerical methods become computationally intractable. We
benchmark several neural network architectures against finite-difference
generated solutions, evaluating their ability to capture the dynamic
interactions between uncertainty, technology transitions, and optimal climate
policy. Our findings demonstrate that appropriate neural architecture selection
significantly impacts both solution accuracy and computational efficiency when
modeling climate-economic systems under uncertainty. These methodological
advances enable more sophisticated modeling of climate policy decisions,
allowing for better representation of technology transitions and
uncertainty-critical elements for developing effective mitigation strategies in
the face of climate change.</p></br><a href="http://arxiv.org/pdf/2505.12171v1" target="_blank"><h2>Learning to Dissipate Energy in Oscillatory State-Space Models</h2></a><strong><u>Authors:</u></strong>  Jared Boyer, T. Konstantin Rusch, Daniela Rus</br><strong><u>Categories:</u></strong> cs.LG, stat.ML</br><strong><u>Comments:</u></strong> 18 pages, 2 figures</br><p><strong><u>Abstract:</u></strong> State-space models (SSMs) are a class of networks for sequence learning that
benefit from fixed state size and linear complexity with respect to sequence
length, contrasting the quadratic scaling of typical attention mechanisms.
Inspired from observations in neuroscience, Linear Oscillatory State-Space
models (LinOSS) are a recently proposed class of SSMs constructed from layers
of discretized forced harmonic oscillators. Although these models perform
competitively, leveraging fast parallel scans over diagonal recurrent matrices
and achieving state-of-the-art performance on tasks with sequence length up to
50k, LinOSS models rely on rigid energy dissipation ("forgetting") mechanisms
that are inherently coupled to the timescale of state evolution. As forgetting
is a crucial mechanism for long-range reasoning, we demonstrate the
representational limitations of these models and introduce Damped Linear
Oscillatory State-Space models (D-LinOSS), a more general class of oscillatory
SSMs that learn to dissipate latent state energy on multiple timescales. We
analyze the spectral distribution of the model's recurrent matrices and prove
that the SSM layers exhibit stable dynamics under simple, flexible
parameterizations. D-LinOSS consistently outperforms previous LinOSS methods on
long-range learning tasks, without introducing additional complexity, and
simultaneously reduces the hyperparameter search space by 50%.</p></br><a href="http://arxiv.org/pdf/2505.14535v1" target="_blank"><h2>Spiking Neural Networks with Temporal Attention-Guided Adaptive Fusion
  for imbalanced Multi-modal Learning</h2></a><strong><u>Authors:</u></strong>  Jiangrong Shen, Yulin Xie, Qi Xu, Gang Pan, Huajin Tang, Badong Chen</br><strong><u>Categories:</u></strong> cs.LG, cs.HC</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Multimodal spiking neural networks (SNNs) hold significant potential for
energy-efficient sensory processing but face critical challenges in modality
imbalance and temporal misalignment. Current approaches suffer from
uncoordinated convergence speeds across modalities and static fusion mechanisms
that ignore time-varying cross-modal interactions. We propose the temporal
attention-guided adaptive fusion framework for multimodal SNNs with two
synergistic innovations: 1) The Temporal Attention-guided Adaptive Fusion
(TAAF) module that dynamically assigns importance scores to fused spiking
features at each timestep, enabling hierarchical integration of temporally
heterogeneous spike-based features; 2) The temporal adaptive balanced fusion
loss that modulates learning rates per modality based on the above attention
scores, preventing dominant modalities from monopolizing optimization. The
proposed framework implements adaptive fusion, especially in the temporal
dimension, and alleviates the modality imbalance during multimodal learning,
mimicking cortical multisensory integration principles. Evaluations on CREMA-D,
AVE, and EAD datasets demonstrate state-of-the-art performance (77.55\%,
70.65\% and 97.5\%accuracy, respectively) with energy efficiency. The system
resolves temporal misalignment through learnable time-warping operations and
faster modality convergence coordination than baseline SNNs. This work
establishes a new paradigm for temporally coherent multimodal learning in
neuromorphic systems, bridging the gap between biological sensory processing
and efficient machine intelligence.</p></br><a href="http://arxiv.org/pdf/2505.12952v1" target="_blank"><h2>LoD: Loss-difference OOD Detection by Intentionally Label-Noisifying
  Unlabeled Wild Data</h2></a><strong><u>Authors:</u></strong>  Chuanxing Geng, Qifei Li, Xinrui Wang, Dong Liang, Songcan Chen, Pong C. Yuen</br><strong><u>Categories:</u></strong> cs.LG, stat.ML</br><strong><u>Comments:</u></strong> Accepted by IJCAI2025</br><p><strong><u>Abstract:</u></strong> Using unlabeled wild data containing both in-distribution (ID) and
out-of-distribution (OOD) data to improve the safety and reliability of models
has recently received increasing attention. Existing methods either design
customized losses for labeled ID and unlabeled wild data then perform joint
optimization, or first filter out OOD data from the latter then learn an OOD
detector. While achieving varying degrees of success, two potential issues
remain: (i) Labeled ID data typically dominates the learning of models,
inevitably making models tend to fit OOD data as IDs; (ii) The selection of
thresholds for identifying OOD data in unlabeled wild data usually faces
dilemma due to the unavailability of pure OOD samples. To address these issues,
we propose a novel loss-difference OOD detection framework (LoD) by
\textit{intentionally label-noisifying} unlabeled wild data. Such operations
not only enable labeled ID data and OOD data in unlabeled wild data to jointly
dominate the models' learning but also ensure the distinguishability of the
losses between ID and OOD samples in unlabeled wild data, allowing the classic
clustering technique (e.g., K-means) to filter these OOD samples without
requiring thresholds any longer. We also provide theoretical foundation for
LoD's viability, and extensive experiments verify its superiority.</p></br><a href="http://arxiv.org/pdf/2505.13060v1" target="_blank"><h2>Automatic mixed precision for optimizing gained time with constrained
  loss mean-squared-error based on model partition to sequential sub-graphs</h2></a><strong><u>Authors:</u></strong>  Shmulik Markovich-Golan, Daniel Ohayon, Itay Niv, Yair Hanani</br><strong><u>Categories:</u></strong> cs.LG</br><strong><u>Comments:</u></strong> Preprint, under review</br><p><strong><u>Abstract:</u></strong> Quantization is essential for Neural Network (NN) compression, reducing model
size and computational demands by using lower bit-width data types, though
aggressive reduction often hampers accuracy. Mixed Precision (MP) mitigates
this tradeoff by varying the numerical precision across network layers. This
study focuses on automatically selecting an optimal MP configuration within
Post-Training Quantization (PTQ) for inference. The first key contribution is a
novel sensitivity metric derived from a first-order Taylor series expansion of
the loss function as a function of quantization errors in weights and
activations. This metric, based on the Mean Square Error (MSE) of the loss, is
efficiently calculated per layer using high-precision forward and backward
passes over a small calibration dataset. The metric is additive across layers,
with low calibration memory overhead as weight optimization is unnecessary. The
second contribution is an accurate hardware-aware method for predicting MP time
gain by modeling it as additive for sequential sub-graphs. An algorithm
partitions the model graph into sequential subgraphs, measuring time gain for
each configuration using a few samples. After calibrating per-layer sensitivity
and time gain, an Integer Programming (IP) problem is formulated to maximize
time gain while keeping loss MSE below a set threshold. Memory gain and
theoretical time gain based on Multiply and Accumulate (MAC) operations are
also considered. Rigorous experiments on the Intel Gaudi 2 accelerator validate
the approach on several Large Language Models (LLMs).</p></br><a href="http://arxiv.org/pdf/2505.12109v1" target="_blank"><h2>SAINT: Attention-Based Modeling of Sub-Action Dependencies in
  Multi-Action Policies</h2></a><strong><u>Authors:</u></strong>  Matthew Landers, Taylor W. Killian, Thomas Hartvigsen, Afsaneh Doryab</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> The combinatorial structure of many real-world action spaces leads to
exponential growth in the number of possible actions, limiting the
effectiveness of conventional reinforcement learning algorithms. Recent
approaches for combinatorial action spaces impose factorized or sequential
structures over sub-actions, failing to capture complex joint behavior. We
introduce the Sub-Action Interaction Network using Transformers (SAINT), a
novel policy architecture that represents multi-component actions as unordered
sets and models their dependencies via self-attention conditioned on the global
state. SAINT is permutation-invariant, sample-efficient, and compatible with
standard policy optimization algorithms. In 15 distinct combinatorial
environments across three task domains, including environments with nearly 17
million joint actions, SAINT consistently outperforms strong baselines.</p></br><a href="http://arxiv.org/pdf/2505.13007v1" target="_blank"><h2>Generative Modeling of Random Fields from Limited Data via Constrained
  Latent Flow Matching</h2></a><strong><u>Authors:</u></strong>  James E. Warner, Tristan A. Shah, Patrick E. Leser, Geoffrey F. Bomarito, Joshua D. Pribe, Michael C. Stanley</br><strong><u>Categories:</u></strong> cs.LG, cs.CE</br><strong><u>Comments:</u></strong> 10 pages plus references and appendices, 17 figures</br><p><strong><u>Abstract:</u></strong> Deep generative models are promising tools for science and engineering, but
their reliance on abundant, high-quality data limits applicability. We present
a novel framework for generative modeling of random fields (probability
distributions over continuous functions) that incorporates domain knowledge to
supplement limited, sparse, and indirect data. The foundation of the approach
is latent flow matching, where generative modeling occurs on compressed
function representations in the latent space of a pre-trained variational
autoencoder (VAE). Innovations include the adoption of a function decoder
within the VAE and integration of physical/statistical constraints into the VAE
training process. In this way, a latent function representation is learned that
yields continuous random field samples satisfying domain-specific constraints
when decoded, even in data-limited regimes. Efficacy is demonstrated on two
challenging applications: wind velocity field reconstruction from sparse
sensors and material property inference from a limited number of indirect
measurements. Results show that the proposed framework achieves significant
improvements in reconstruction accuracy compared to unconstrained methods and
enables effective inference with relatively small training datasets that is
intractable without constraints.</p></br><a href="http://arxiv.org/pdf/2505.14505v1" target="_blank"><h2>ModRWKV: Transformer Multimodality in Linear Time</h2></a><strong><u>Authors:</u></strong>  Jiale Kang, Ziyin Yue, Qingyu Yin, Jiang Rui, Weile Li, Zening Lu, Zhouran Ji</br><strong><u>Categories:</u></strong> cs.CL, cs.AI</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Currently, most multimodal studies are based on large language models (LLMs)
with quadratic-complexity Transformer architectures. While linear models like
RNNs enjoy low inference costs, their application has been largely limited to
the text-only modality. This work explores the capabilities of modern RNN
architectures in multimodal contexts. We propose ModRWKV-a decoupled multimodal
framework built upon the RWKV7 architecture as its LLM backbone-which achieves
multi-source information fusion through dynamically adaptable heterogeneous
modality encoders. We designed the multimodal modules in ModRWKV with an
extremely lightweight architecture and, through extensive experiments,
identified a configuration that achieves an optimal balance between performance
and computational efficiency. ModRWKV leverages the pretrained weights of the
RWKV7 LLM for initialization, which significantly accelerates multimodal
training. Comparative experiments with different pretrained checkpoints further
demonstrate that such initialization plays a crucial role in enhancing the
model's ability to understand multimodal signals. Supported by extensive
experiments, we conclude that modern RNN architectures present a viable
alternative to Transformers in the domain of multimodal large language models
(MLLMs). Furthermore, we identify the optimal configuration of the ModRWKV
architecture through systematic exploration.</p></br><a href="http://arxiv.org/pdf/2505.14513v1" target="_blank"><h2>Latent Flow Transformer</h2></a><strong><u>Authors:</u></strong>  Yen-Chen Wu, Feng-Ting Liao, Meng-Hsi Chen, Pei-Chen Ho, Farhang Nabiei, Da-shan Shiu</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Transformers, the standard implementation for large language models (LLMs),
typically consist of tens to hundreds of discrete layers. While more layers can
lead to better performance, this approach has been challenged as far from
efficient, especially given the superiority of continuous layers demonstrated
by diffusion and flow-based models for image generation. We propose the Latent
Flow Transformer (LFT), which replaces a block of layers with a single learned
transport operator trained via flow matching, offering significant compression
while maintaining compatibility with the original architecture. Additionally,
we address the limitations of existing flow-based methods in \textit{preserving
coupling} by introducing the Flow Walking (FW) algorithm. On the Pythia-410M
model, LFT trained with flow matching compresses 6 of 24 layers and outperforms
directly skipping 2 layers (KL Divergence of LM logits at 0.407 vs. 0.529),
demonstrating the feasibility of this design. When trained with FW, LFT further
distills 12 layers into one while reducing the KL to 0.736 surpassing that from
skipping 3 layers (0.932), significantly narrowing the gap between
autoregressive and flow-based generation paradigms.</p></br><a href="http://arxiv.org/pdf/2505.11702v1" target="_blank"><h2>Invariant Representations via Wasserstein Correlation Maximization</h2></a><strong><u>Authors:</u></strong>  Keenan Eikenberry, Lizuo Liu, Yoonsang Lee</br><strong><u>Categories:</u></strong> cs.LG, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> This work investigates the use of Wasserstein correlation -- a normalized
measure of statistical dependence based on the Wasserstein distance between a
joint distribution and the product of its marginals -- for unsupervised
representation learning. Unlike, for example, contrastive methods, which
naturally cluster classes in the latent space, we find that an (auto)encoder
trained to maximize Wasserstein correlation between the input and encoded
distributions instead acts as a compressor, reducing dimensionality while
approximately preserving the topological and geometric properties of the input
distribution. More strikingly, we show that Wasserstein correlation
maximization can be used to arrive at an (auto)encoder -- either trained from
scratch, or else one that extends a frozen, pretrained model -- that is
approximately invariant to a chosen augmentation, or collection of
augmentations, and that still approximately preserves the structural properties
of the non-augmented input distribution. To do this, we first define the notion
of an augmented encoder using the machinery of Markov-Wasserstein kernels. When
the maximization objective is then applied to the augmented encoder, as opposed
to the underlying, deterministic encoder, the resulting model exhibits the
desired invariance properties. Finally, besides our experimental results, which
show that even simple feedforward networks can be imbued with invariants or
can, alternatively, be used to impart invariants to pretrained models under
this training process, we additionally establish various theoretical results
for optimal transport-based dependence measures. Code is available at
https://github.com/keenan-eikenberry/wasserstein_correlation_maximization .</p></br><a href="http://arxiv.org/pdf/2505.12825v1" target="_blank"><h2>Theoretical Investigation on Inductive Bias of Isolation Forest</h2></a><strong><u>Authors:</u></strong>  Qin-Cheng Zheng, Shao-Qun Zhang, Shen-Huan Lyu, Yuan Jiang, Zhi-Hua Zhou</br><strong><u>Categories:</u></strong> cs.LG, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Isolation Forest (iForest) stands out as a widely-used unsupervised anomaly
detector valued for its exceptional runtime efficiency and performance on
large-scale tasks. Despite its widespread adoption, a theoretical foundation
explaining iForest's success remains unclear. This paper theoretically
investigates the conditions and extent of iForest's effectiveness by analyzing
its inductive bias through the formulation of depth functions and growth
processes. Since directly analyzing the depth function proves intractable due
to iForest's random splitting mechanism, we model the growth process of iForest
as a random walk, enabling us to derive the expected depth function using
transition probabilities. Our case studies reveal key inductive biases: iForest
exhibits lower sensitivity to central anomalies while demonstrating greater
parameter adaptability compared to $k$-Nearest Neighbor anomaly detectors. Our
study provides theoretical understanding of the effectiveness of iForest and
establishes a foundation for further theoretical exploration.</p></br><a href="http://arxiv.org/pdf/2505.12801v1" target="_blank"><h2>Testing Identifiability and Transportability with Observational and
  Experimental Data</h2></a><strong><u>Authors:</u></strong>  Konstantina Lelova, Gregory F. Cooper, Sofia Triantafillou</br><strong><u>Categories:</u></strong> stat.ML, cs.LG</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Transporting causal information learned from experiments in one population to
another is a critical challenge in clinical research and decision-making.
Causal transportability uses causal graphs to model differences between the
source and target populations and identifies conditions under which causal
effects learned from experiments can be reused in a different population.
Similarly, causal identifiability identifies conditions under which causal
effects can be estimated from observational data. However, these approaches
rely on knowing the causal graph, which is often unavailable in real-world
settings. In this work, we propose a Bayesian method for assessing whether
Z-specific (conditional) causal effects are both identifiable and
transportable, without knowing the causal graph. Our method combines
experimental data from the source population with observational data from the
target population to compute the probability that a causal effect is both
identifiable from observational data and transportable. When this holds, we
leverage both observational data from the target domain and experimental data
from the source domain to obtain an unbiased, efficient estimator of the causal
effect in the target population. Using simulations, we demonstrate that our
method correctly identifies transportable causal effects and improves causal
effect estimation.</p></br><a href="http://arxiv.org/pdf/2505.13855v1" target="_blank"><h2>Domain Gating Ensemble Networks for AI-Generated Text Detection</h2></a><strong><u>Authors:</u></strong>  Arihant Tripathi, Liam Dugan, Charis Gao, Maggie Huan, Emma Jin, Peter Zhang, David Zhang, Julia Zhao, Chris Callison-Burch</br><strong><u>Categories:</u></strong> cs.CL, cs.AI, cs.LG</br><strong><u>Comments:</u></strong> Submitted to EMNLP 2025</br><p><strong><u>Abstract:</u></strong> As state-of-the-art language models continue to improve, the need for robust
detection of machine-generated text becomes increasingly critical. However,
current state-of-the-art machine text detectors struggle to adapt to new unseen
domains and generative models. In this paper we present DoGEN (Domain Gating
Ensemble Networks), a technique that allows detectors to adapt to unseen
domains by ensembling a set of domain expert detector models using weights from
a domain classifier. We test DoGEN on a wide variety of domains from leading
benchmarks and find that it achieves state-of-the-art performance on in-domain
detection while outperforming models twice its size on out-of-domain detection.
We release our code and trained models to assist in future research in
domain-adaptive AI detection.</p></br><a href="http://arxiv.org/pdf/2505.13614v1" target="_blank"><h2>Deterministic Bounds and Random Estimates of Metric Tensors on
  Neuromanifolds</h2></a><strong><u>Authors:</u></strong>  Ke Sun</br><strong><u>Categories:</u></strong> cs.LG, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> The high dimensional parameter space of modern deep neural networks -- the
neuromanifold -- is endowed with a unique metric tensor defined by the Fisher
information, estimating which is crucial for both theory and practical methods
in deep learning. To analyze this tensor for classification networks, we return
to a low dimensional space of probability distributions -- the core space --
and carefully analyze the spectrum of its Riemannian metric. We extend our
discoveries there into deterministic bounds of the metric tensor on the
neuromanifold. We introduce an unbiased random estimate of the metric tensor
and its bounds based on Hutchinson's trace estimator. It can be evaluated
efficiently through a single backward pass and can be used to estimate the
diagonal, or block diagonal, or the full tensor. Its quality is guaranteed with
a standard deviation bounded by the true value up to scaling.</p></br><a href="http://arxiv.org/pdf/2505.14533v1" target="_blank"><h2>Energy-Efficient Deep Reinforcement Learning with Spiking Transformers</h2></a><strong><u>Authors:</u></strong>  Mohammad Irfan Uddin, Nishad Tasnim, Md Omor Faruk, Zejian Zhou</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> No comments</br><p><strong><u>Abstract:</u></strong> Agent-based Transformers have been widely adopted in recent reinforcement
learning advances due to their demonstrated ability to solve complex tasks.
However, the high computational complexity of Transformers often results in
significant energy consumption, limiting their deployment in real-world
autonomous systems. Spiking neural networks (SNNs), with their biologically
inspired structure, offer an energy-efficient alternative for machine learning.
In this paper, a novel Spike-Transformer Reinforcement Learning (STRL)
algorithm that combines the energy efficiency of SNNs with the powerful
decision-making capabilities of reinforcement learning is developed.
Specifically, an SNN using multi-step Leaky Integrate-and-Fire (LIF) neurons
and attention mechanisms capable of processing spatio-temporal patterns over
multiple time steps is designed. The architecture is further enhanced with
state, action, and reward encodings to create a Transformer-like structure
optimized for reinforcement learning tasks. Comprehensive numerical experiments
conducted on state-of-the-art benchmarks demonstrate that the proposed SNN
Transformer achieves significantly improved policy performance compared to
conventional agent-based Transformers. With both enhanced energy efficiency and
policy optimality, this work highlights a promising direction for deploying
bio-inspired, low-cost machine learning models in complex real-world
decision-making scenarios.</p></br></body>