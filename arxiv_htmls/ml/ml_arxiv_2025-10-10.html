<link href='https://fonts.googleapis.com/css?family=Montserrat' rel='stylesheet'>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$','$']],
            processEscapes: true
        },
        "HTML-CSS": {
            availableFonts: ["TeX"]
        }
    });
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <style>     body {font-family: 'Montserrat'; background: #F3F3F3; width: 740px; margin: 0 auto; line-height: 150%; margin-top: 50px; font-size: 15px}         h1 {font-size: 70px}         a {color: #45ABC2}         em {font-size: 120%} </style><h1><center>ArXiv Alert</center></h1><font color='#DDAD5C'><em>Update: from 08 Oct 2025 to 10 Oct 2025</em></font><a href="http://arxiv.org/pdf/2510.07847v1" target="_blank"><h2>Meta-Learning Based Few-Shot Graph-Level Anomaly Detection</h2></a><strong><u>Authors:</u></strong>  Liting Li, Yumeng Wang, Yueheng Sun</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> Accepted by ARRML2025</br><strong><u>Matching Keywords:</u></strong> anomaly detection (title, abstract), neural network (abstract)</br><p><strong><u>Abstract:</u></strong> Graph-level anomaly detection aims to identify anomalous graphs or subgraphs
within graph datasets, playing a vital role in various fields such as fraud
detection, review classification, and biochemistry. While Graph Neural Networks
(GNNs) have made significant progress in this domain, existing methods rely
heavily on large amounts of labeled data, which is often unavailable in
real-world scenarios. Additionally, few-shot anomaly detection methods based on
GNNs are prone to noise interference, resulting in poor embedding quality and
reduced model robustness. To address these challenges, we propose a novel
meta-learning-based graph-level anomaly detection framework (MA-GAD),
incorporating a graph compression module that reduces the graph size,
mitigating noise interference while retaining essential node information. We
also leverage meta-learning to extract meta-anomaly information from similar
networks, enabling the learning of an initialization model that can rapidly
adapt to new tasks with limited samples. This improves the anomaly detection
performance on target graphs, and a bias network is used to enhance the
distinction between anomalous and normal nodes. Our experimental results, based
on four real-world biochemical datasets, demonstrate that MA-GAD outperforms
existing state-of-the-art methods in graph-level anomaly detection under
few-shot conditions. Experiments on both graph anomaly and subgraph anomaly
detection tasks validate the framework's effectiveness on real-world datasets.</p></br><a href="http://arxiv.org/pdf/2510.08573v1" target="_blank"><h2>Reconstructing the local density field with combined convolutional and
  point cloud architecture</h2></a><strong><u>Authors:</u></strong>  Baptiste Barthe-Gold, Nhat-Minh Nguyen, Leander Thiele</br><strong><u>Categories:</u></strong> astro-ph.CO, cs.LG, stat.ML</br><strong><u>Comments:</u></strong> 6 pages, 4 figures, 1 table. Accepted at the NeurIPS 2025 Workshop: ML4PS. Comments welcome!</br><strong><u>Matching Keywords:</u></strong> convolutional (title, abstract), neural network (abstract)</br><p><strong><u>Abstract:</u></strong> We construct a neural network to perform regression on the local dark-matter
density field given line-of-sight peculiar velocities of dark-matter halos,
biased tracers of the dark matter field. Our architecture combines a
convolutional U-Net with a point-cloud DeepSets. This combination enables
efficient use of small-scale information and improves reconstruction quality
relative to a U-Net-only approach. Specifically, our hybrid network recovers
both clustering amplitudes and phases better than the U-Net on small scales.</p></br><a href="http://arxiv.org/pdf/2510.07513v1" target="_blank"><h2>MLLM4TS: Leveraging Vision and Multimodal Language Models for General
  Time-Series Analysis</h2></a><strong><u>Authors:</u></strong>  Qinghua Liu, Sam Heshmati, Zheda Mai, Zubin Abraham, John Paparrizos, Liu Ren</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.CV, cs.DB</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> anomaly detection (abstract), multimodal (title, abstract)</br><p><strong><u>Abstract:</u></strong> Effective analysis of time series data presents significant challenges due to
the complex temporal dependencies and cross-channel interactions in
multivariate data. Inspired by the way human analysts visually inspect time
series to uncover hidden patterns, we ask: can incorporating visual
representations enhance automated time-series analysis? Recent advances in
multimodal large language models have demonstrated impressive generalization
and visual understanding capability, yet their application to time series
remains constrained by the modality gap between continuous numerical data and
discrete natural language. To bridge this gap, we introduce MLLM4TS, a novel
framework that leverages multimodal large language models for general
time-series analysis by integrating a dedicated vision branch. Each time-series
channel is rendered as a horizontally stacked color-coded line plot in one
composite image to capture spatial dependencies across channels, and a
temporal-aware visual patch alignment strategy then aligns visual patches with
their corresponding time segments. MLLM4TS fuses fine-grained temporal details
from the numerical data with global contextual information derived from the
visual representation, providing a unified foundation for multimodal
time-series analysis. Extensive experiments on standard benchmarks demonstrate
the effectiveness of MLLM4TS across both predictive tasks (e.g.,
classification) and generative tasks (e.g., anomaly detection and forecasting).
These results underscore the potential of integrating visual modalities with
pretrained language models to achieve robust and generalizable time-series
analysis.</p></br><a href="http://arxiv.org/pdf/2510.07477v1" target="_blank"><h2>HEMERA: A Human-Explainable Transformer Model for Estimating Lung Cancer
  Risk using GWAS Data</h2></a><strong><u>Authors:</u></strong>  Maria Mahbub, Robert J. Klein, Myvizhi Esai Selvan, Rowena Yip, Claudia Henschke, Providencia Morales, Ian Goethert, Olivera Kotevska, Mayanka Chandra Shekar, Sean R. Wilkinson, Eileen McAllister, Samuel M. Aguayo, Zeynep H. Gümüş, Ioana Danciu, VA Million Veteran Program</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> 18 pages, 6 figures, 3 tables</br><strong><u>Matching Keywords:</u></strong> explainability (abstract), explainable (title, abstract), transformer (title, abstract)</br><p><strong><u>Abstract:</u></strong> Lung cancer (LC) is the third most common cancer and the leading cause of
cancer deaths in the US. Although smoking is the primary risk factor, the
occurrence of LC in never-smokers and familial aggregation studies highlight a
genetic component. Genetic biomarkers identified through genome-wide
association studies (GWAS) are promising tools for assessing LC risk. We
introduce HEMERA (Human-Explainable Transformer Model for Estimating Lung
Cancer Risk using GWAS Data), a new framework that applies explainable
transformer-based deep learning to GWAS data of single nucleotide polymorphisms
(SNPs) for predicting LC risk. Unlike prior approaches, HEMERA directly
processes raw genotype data without clinical covariates, introducing additive
positional encodings, neural genotype embeddings, and refined variant
filtering. A post hoc explainability module based on Layer-wise Integrated
Gradients enables attribution of model predictions to specific SNPs, aligning
strongly with known LC risk loci. Trained on data from 27,254 Million Veteran
Program participants, HEMERA achieved >99% AUC (area under receiver
characteristics) score. These findings support transparent,
hypothesis-generating models for personalized LC risk assessment and early
intervention.</p></br><a href="http://arxiv.org/pdf/2510.08450v1" target="_blank"><h2>gLSTM: Mitigating Over-Squashing by Increasing Storage Capacity</h2></a><strong><u>Authors:</u></strong>  Hugh Blayney, Álvaro Arroyo, Xiaowen Dong, Michael M. Bronstein</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML</br><strong><u>Comments:</u></strong> 22 pages, 22 figures, 7 tables</br><strong><u>Matching Keywords:</u></strong> neural network (abstract)</br><p><strong><u>Abstract:</u></strong> Graph Neural Networks (GNNs) leverage the graph structure to transmit
information between nodes, typically through the message-passing mechanism.
While these models have found a wide variety of applications, they are known to
suffer from over-squashing, where information from a large receptive field of
node representations is collapsed into a single fixed sized vector, resulting
in an information bottleneck. In this paper, we re-examine the over-squashing
phenomenon through the lens of model storage and retrieval capacity, which we
define as the amount of information that can be stored in a node's
representation for later use. We study some of the limitations of existing
tasks used to measure over-squashing and introduce a new synthetic task to
demonstrate that an information bottleneck can saturate this capacity.
Furthermore, we adapt ideas from the sequence modeling literature on
associative memories, fast weight programmers, and the xLSTM model to develop a
novel GNN architecture with improved capacity. We demonstrate strong
performance of this architecture both on our capacity synthetic task, as well
as a range of real-world graph benchmarks.</p></br><a href="http://arxiv.org/pdf/2510.08409v1" target="_blank"><h2>Optimal Stopping in Latent Diffusion Models</h2></a><strong><u>Authors:</u></strong>  Yu-Han Wu, Quentin Berthet, Gérard Biau, Claire Boyer, Romuald Elie, Pierre Marion</br><strong><u>Categories:</u></strong> stat.ML, cs.LG</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> dimensionality reduction (abstract), latent space (abstract)</br><p><strong><u>Abstract:</u></strong> We identify and analyze a surprising phenomenon of Latent Diffusion Models
(LDMs) where the final steps of the diffusion can degrade sample quality. In
contrast to conventional arguments that justify early stopping for numerical
stability, this phenomenon is intrinsic to the dimensionality reduction in
LDMs. We provide a principled explanation by analyzing the interaction between
latent dimension and stopping time. Under a Gaussian framework with linear
autoencoders, we characterize the conditions under which early stopping is
needed to minimize the distance between generated and target distributions.
More precisely, we show that lower-dimensional representations benefit from
earlier termination, whereas higher-dimensional latent spaces require later
stopping time. We further establish that the latent dimension interplays with
other hyperparameters of the problem such as constraints in the parameters of
score matching. Experiments on synthetic and real datasets illustrate these
properties, underlining that early stopping can improve generative quality.
Together, our results offer a theoretical foundation for understanding how the
latent dimension influences the sample quality, and highlight stopping time as
a key hyperparameter in LDMs.</p></br><a href="http://arxiv.org/pdf/2510.08123v1" target="_blank"><h2>High-dimensional Analysis of Synthetic Data Selection</h2></a><strong><u>Authors:</u></strong>  Parham Rezaei, Filip Kovacevic, Francesco Locatello, Marco Mondelli</br><strong><u>Categories:</u></strong> stat.ML, cs.LG</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> neural network (abstract)</br><p><strong><u>Abstract:</u></strong> Despite the progress in the development of generative models, their
usefulness in creating synthetic data that improve prediction performance of
classifiers has been put into question. Besides heuristic principles such as
"synthetic data should be close to the real data distribution", it is actually
not clear which specific properties affect the generalization error. Our paper
addresses this question through the lens of high-dimensional regression.
Theoretically, we show that, for linear models, the covariance shift between
the target distribution and the distribution of the synthetic data affects the
generalization error but, surprisingly, the mean shift does not. Furthermore we
prove that, in some settings, matching the covariance of the target
distribution is optimal. Remarkably, the theoretical insights from linear
models carry over to deep neural networks and generative models. We empirically
demonstrate that the covariance matching procedure (matching the covariance of
the synthetic data with that of the data coming from the target distribution)
performs well against several recent approaches for synthetic data selection,
across training paradigms, architectures, datasets and generative models used
for augmentation.</p></br><a href="http://arxiv.org/pdf/2510.07578v1" target="_blank"><h2>Accuracy, Memory Efficiency and Generalization: A Comparative Study on
  Liquid Neural Networks and Recurrent Neural Networks</h2></a><strong><u>Authors:</u></strong>  Shilong Zong, Alex Bierly, Almuatazbellah Boker, Hoda Eldardiry</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.SY, eess.SY, I.2.6; I.2.8</br><strong><u>Comments:</u></strong> 13 pages, 12 figures. Submitted to IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract), sequential data (abstract)</br><p><strong><u>Abstract:</u></strong> This review aims to conduct a comparative analysis of liquid neural networks
(LNNs) and traditional recurrent neural networks (RNNs) and their variants,
such as long short-term memory networks (LSTMs) and gated recurrent units
(GRUs). The core dimensions of the analysis include model accuracy, memory
efficiency, and generalization ability. By systematically reviewing existing
research, this paper explores the basic principles, mathematical models, key
characteristics, and inherent challenges of these neural network architectures
in processing sequential data. Research findings reveal that LNN, as an
emerging, biologically inspired, continuous-time dynamic neural network,
demonstrates significant potential in handling noisy, non-stationary data, and
achieving out-of-distribution (OOD) generalization. Additionally, some LNN
variants outperform traditional RNN in terms of parameter efficiency and
computational speed. However, RNN remains a cornerstone in sequence modeling
due to its mature ecosystem and successful applications across various tasks.
This review identifies the commonalities and differences between LNNs and RNNs,
summarizes their respective shortcomings and challenges, and points out
valuable directions for future research, particularly emphasizing the
importance of improving the scalability of LNNs to promote their application in
broader and more complex scenarios.</p></br></body>