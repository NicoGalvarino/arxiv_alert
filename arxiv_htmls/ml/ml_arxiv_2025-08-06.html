<link href='https://fonts.googleapis.com/css?family=Montserrat' rel='stylesheet'>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$','$']],
            processEscapes: true
        },
        "HTML-CSS": {
            availableFonts: ["TeX"]
        }
    });
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <style>     body {font-family: 'Montserrat'; background: #F3F3F3; width: 740px; margin: 0 auto; line-height: 150%; margin-top: 50px; font-size: 15px}         h1 {font-size: 70px}         a {color: #45ABC2}         em {font-size: 120%} </style><h1><center>ArXiv Alert</center></h1><font color='#DDAD5C'><em>Update: from 04 Aug 2025 to 06 Aug 2025</em></font><a href="http://arxiv.org/pdf/2508.03546v1" target="_blank"><h2>Supervised Dynamic Dimension Reduction with Deep Neural Network</h2></a><strong><u>Authors:</u></strong>  Zhanye Luo, Yuefeng Han, Xiufan Yu</br><strong><u>Categories:</u></strong> stat.ML, cs.AI, cs.LG</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> dimension reduction (title, abstract), neural network (title, abstract)</br><p><strong><u>Abstract:</u></strong> This paper studies the problem of dimension reduction, tailored to improving
time series forecasting with high-dimensional predictors. We propose a novel
Supervised Deep Dynamic Principal component analysis (SDDP) framework that
incorporates the target variable and lagged observations into the factor
extraction process. Assisted by a temporal neural network, we construct
target-aware predictors by scaling the original predictors in a supervised
manner, with larger weights assigned to predictors with stronger forecasting
power. A principal component analysis is then performed on the target-aware
predictors to extract the estimated SDDP factors. This supervised factor
extraction not only improves predictive accuracy in the downstream forecasting
task but also yields more interpretable and target-specific latent factors.
Building upon SDDP, we propose a factor-augmented nonlinear dynamic forecasting
model that unifies a broad family of factor-model-based forecasting approaches.
To further demonstrate the broader applicability of SDDP, we extend our studies
to a more challenging scenario when the predictors are only partially
observable. We validate the empirical performance of the proposed method on
several real-world public datasets. The results show that our algorithm
achieves notable improvements in forecasting accuracy compared to
state-of-the-art methods.</p></br><a href="http://arxiv.org/pdf/2508.03661v1" target="_blank"><h2>Automated Algorithmic Discovery for Gravitational-Wave Detection Guided
  by LLM-Informed Evolutionary Monte Carlo Tree Search</h2></a><strong><u>Authors:</u></strong>  He Wang, Liang Zeng</br><strong><u>Categories:</u></strong> cs.AI, astro-ph.HE, astro-ph.IM, gr-qc</br><strong><u>Comments:</u></strong> 89 pages (37 main), 6+6 figures, 1 table. Initial submission; subject to revision</br><strong><u>Matching Keywords:</u></strong> neural network (abstract)</br><p><strong><u>Abstract:</u></strong> Computational scientific discovery increasingly relies on algorithms to
process complex data and identify meaningful patterns - yet faces persistent
challenges in gravitational-wave signal identification. While existing
algorithmic approaches like matched filtering (MF) and deep neural networks
(DNNs) have achieved partial success, their limitations directly stem from
fundamental limitations: MF's excessive computational demands arise from its
reliance on predefined theoretical waveform templates, while DNNs' black-box
architectures obscure decision logic and introduce hidden biases. We propose
Evolutionary Monte Carlo Tree Search (Evo-MCTS), a framework that addresses
these limitations through systematic algorithm space exploration guided by
domain-aware physical constraints. Our approach combines tree-structured search
with evolutionary optimization and large language model heuristics to create
interpretable algorithmic solutions. Our Evo-MCTS framework demonstrates
substantial improvements, achieving a 20.2\% improvement over state-of-the-art
gravitational wave detection algorithms on the MLGWSC-1 benchmark dataset.
High-performing algorithm variants consistently exceed thresholds. The
framework generates human-interpretable algorithmic pathways that reveal
distinct performance patterns. Beyond performance improvements, our framework
discovers novel algorithmic combinations, thereby establishing a transferable
methodology for automated algorithmic discovery across computational science
domains.</p></br><a href="http://arxiv.org/pdf/2508.02871v1" target="_blank"><h2>Evaluation and Analysis of Deep Neural Transformers and Convolutional
  Neural Networks on Modern Remote Sensing Datasets</h2></a><strong><u>Authors:</u></strong>  J. Alex Hurt, Trevor M. Bajkowski, Grant J. Scott, Curt H. Davis</br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.LG</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> convolutional (title, abstract), neural network (title, abstract), transformer (title, abstract)</br><p><strong><u>Abstract:</u></strong> In 2012, AlexNet established deep convolutional neural networks (DCNNs) as
the state-of-the-art in CV, as these networks soon led in visual tasks for many
domains, including remote sensing. With the publication of Visual Transformers,
we are witnessing the second modern leap in computational vision, and as such,
it is imperative to understand how various transformer-based neural networks
perform on satellite imagery. While transformers have shown high levels of
performance in natural language processing and CV applications, they have yet
to be compared on a large scale to modern remote sensing data. In this paper,
we explore the use of transformer-based neural networks for object detection in
high-resolution electro-optical satellite imagery, demonstrating
state-of-the-art performance on a variety of publicly available benchmark data
sets. We compare eleven distinct bounding-box detection and localization
algorithms in this study, of which seven were published since 2020, and all
eleven since 2015. The performance of five transformer-based architectures is
compared with six convolutional networks on three state-of-the-art opensource
high-resolution remote sensing imagery datasets ranging in size and complexity.
Following the training and evaluation of thirty-three deep neural models, we
then discuss and analyze model performance across various feature extraction
methodologies and detection algorithms.</p></br><a href="http://arxiv.org/pdf/2508.02874v1" target="_blank"><h2>Beyond Least Squares: Robust Regression Transformer (R2T)</h2></a><strong><u>Authors:</u></strong>  Roman Gutierrez, Tony Kai Tang, Isabel Gutierrez</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML, 68T30, 65D10, 62J02, 68T07, 62F35, 62J02, I.2.6; G.1.2; G.3</br><strong><u>Comments:</u></strong> 10 pages, 4 figures, 1 table</br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract)</br><p><strong><u>Abstract:</u></strong> Robust regression techniques rely on least-squares optimization, which works
well for Gaussian noise but fails in the presence of asymmetric structured
noise. We propose a hybrid neural-symbolic architecture where a transformer
encoder processes numerical sequences, a compression NN predicts symbolic
parameters, and a fixed symbolic equation reconstructs the original sequence.
Using synthetic data, the training objective is to recover the original
sequence after adding asymmetric structured noise, effectively learning a
symbolic fit guided by neural parameter estimation. Our model achieves a median
regression MSE of 6e-6 to 3.5e-5 on synthetic wearable data, which is a 10-300
times improvement when compared with ordinary least squares fit and robust
regression techniques such as Huber loss or SoftL1.</p></br><a href="http://arxiv.org/pdf/2508.03047v1" target="_blank"><h2>TF-MLPNet: Tiny Real-Time Neural Speech Separation</h2></a><strong><u>Authors:</u></strong>  Malek Itani, Tuochao Chen, Shyamnath Gollakota</br><strong><u>Categories:</u></strong> cs.SD, cs.LG, eess.AS</br><strong><u>Comments:</u></strong> The 6th Clarity Workshop on Improving Speech-in-Noise for Hearing Devices (Clarity 2025)</br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), time sequence (abstract)</br><p><strong><u>Abstract:</u></strong> Speech separation on hearable devices can enable transformative augmented and
enhanced hearing capabilities. However, state-of-the-art speech separation
networks cannot run in real-time on tiny, low-power neural accelerators
designed for hearables, due to their limited compute capabilities. We present
TF-MLPNet, the first speech separation network capable of running in real-time
on such low-power accelerators while outperforming existing streaming models
for blind speech separation and target speech extraction. Our network operates
in the time-frequency domain, processing frequency sequences with stacks of
fully connected layers that alternate along the channel and frequency
dimensions, and independently processing the time sequence at each frequency
bin using convolutional layers. Results show that our mixed-precision
quantization-aware trained (QAT) model can process 6 ms audio chunks in
real-time on the GAP9 processor, achieving a 3.5-4x runtime reduction compared
to prior speech separation models.</p></br><a href="http://arxiv.org/pdf/2508.03046v1" target="_blank"><h2>A Novel Multimodal Framework for Early Detection of Alzheimers Disease
  Using Deep Learning</h2></a><strong><u>Authors:</u></strong>  Tatwadarshi P Nagarhalli, Sanket Patil, Vishal Pande, Uday Aswalekar, Prafulla Patil</br><strong><u>Categories:</u></strong> cs.LG</br><strong><u>Comments:</u></strong> Journal paper, 14 pages</br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract), multimodal (title, abstract)</br><p><strong><u>Abstract:</u></strong> Alzheimers Disease (AD) is a progressive neurodegenerative disorder that
poses significant challenges in its early diagnosis, often leading to delayed
treatment and poorer outcomes for patients. Traditional diagnostic methods,
typically reliant on single data modalities, fall short of capturing the
multifaceted nature of the disease. In this paper, we propose a novel
multimodal framework for the early detection of AD that integrates data from
three primary sources: MRI imaging, cognitive assessments, and biomarkers. This
framework employs Convolutional Neural Networks (CNN) for analyzing MRI images
and Long Short-Term Memory (LSTM) networks for processing cognitive and
biomarker data. The system enhances diagnostic accuracy and reliability by
aggregating results from these distinct modalities using advanced techniques
like weighted averaging, even in incomplete data. The multimodal approach not
only improves the robustness of the detection process but also enables the
identification of AD at its earliest stages, offering a significant advantage
over conventional methods. The integration of biomarkers and cognitive tests is
particularly crucial, as these can detect Alzheimer's long before the onset of
clinical symptoms, thereby facilitating earlier intervention and potentially
altering the course of the disease. This research demonstrates that the
proposed framework has the potential to revolutionize the early detection of
AD, paving the way for more timely and effective treatments</p></br><a href="http://arxiv.org/pdf/2508.03008v1" target="_blank"><h2>ClinicalFMamba: Advancing Clinical Assessment using Mamba-based
  Multimodal Neuroimaging Fusion</h2></a><strong><u>Authors:</u></strong>  Meng Zhou, Farzad Khalvati</br><strong><u>Categories:</u></strong> eess.IV, cs.AI, cs.CV</br><strong><u>Comments:</u></strong> Accepted at MICCAI MLMI 2025 Workshop</br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract), multimodal (title, abstract), transformer (abstract)</br><p><strong><u>Abstract:</u></strong> Multimodal medical image fusion integrates complementary information from
different imaging modalities to enhance diagnostic accuracy and treatment
planning. While deep learning methods have advanced performance, existing
approaches face critical limitations: Convolutional Neural Networks (CNNs)
excel at local feature extraction but struggle to model global context
effectively, while Transformers achieve superior long-range modeling at the
cost of quadratic computational complexity, limiting clinical deployment.
Recent State Space Models (SSMs) offer a promising alternative, enabling
efficient long-range dependency modeling in linear time through selective scan
mechanisms. Despite these advances, the extension to 3D volumetric data and the
clinical validation of fused images remains underexplored. In this work, we
propose ClinicalFMamba, a novel end-to-end CNN-Mamba hybrid architecture that
synergistically combines local and global feature modeling for 2D and 3D
images. We further design a tri-plane scanning strategy for effectively
learning volumetric dependencies in 3D images. Comprehensive evaluations on
three datasets demonstrate the superior fusion performance across multiple
quantitative metrics while achieving real-time fusion. We further validate the
clinical utility of our approach on downstream 2D/3D brain tumor classification
tasks, achieving superior performance over baseline methods. Our method
establishes a new paradigm for efficient multimodal medical image fusion
suitable for real-time clinical deployment.</p></br><a href="http://arxiv.org/pdf/2508.03688v1" target="_blank"><h2>Learning quadratic neural networks in high dimensions: SGD dynamics and
  scaling laws</h2></a><strong><u>Authors:</u></strong>  GÃ©rard Ben Arous, Murat A. Erdogdu, N. Mert Vural, Denny Wu</br><strong><u>Categories:</u></strong> stat.ML, cs.LG</br><strong><u>Comments:</u></strong> 84 pages</br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)</br><p><strong><u>Abstract:</u></strong> We study the optimization and sample complexity of gradient-based training of
a two-layer neural network with quadratic activation function in the
high-dimensional regime, where the data is generated as $y \propto
\sum_{j=1}^{r}\lambda_j \sigma\left(\langle \boldsymbol{\theta_j},
\boldsymbol{x}\rangle\right), \boldsymbol{x} \sim N(0,\boldsymbol{I}_d)$,
$\sigma$ is the 2nd Hermite polynomial, and $\lbrace\boldsymbol{\theta}_j
\rbrace_{j=1}^{r} \subset \mathbb{R}^d$ are orthonormal signal directions. We
consider the extensive-width regime $r \asymp d^\beta$ for $\beta \in [0, 1)$,
and assume a power-law decay on the (non-negative) second-layer coefficients
$\lambda_j\asymp j^{-\alpha}$ for $\alpha \geq 0$. We present a sharp analysis
of the SGD dynamics in the feature learning regime, for both the population
limit and the finite-sample (online) discretization, and derive scaling laws
for the prediction risk that highlight the power-law dependencies on the
optimization time, sample size, and model width. Our analysis combines a
precise characterization of the associated matrix Riccati differential equation
with novel matrix monotonicity arguments to establish convergence guarantees
for the infinite-dimensional effective dynamics.</p></br><a href="http://arxiv.org/pdf/2508.03170v1" target="_blank"><h2>Quantum Spectral Reasoning: A Non-Neural Architecture for Interpretable
  Machine Learning</h2></a><strong><u>Authors:</u></strong>  Andrew Kiruluta</br><strong><u>Categories:</u></strong> cs.LG</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> anomaly detection (abstract), neural network (abstract), time-domain (abstract)</br><p><strong><u>Abstract:</u></strong> We propose a novel machine learning architecture that departs from
conventional neural network paradigms by leveraging quantum spectral methods,
specifically Pade approximants and the Lanczos algorithm, for interpretable
signal analysis and symbolic reasoning. The core innovation of our approach
lies in its ability to transform raw time-domain signals into sparse,
physically meaningful spectral representations without the use of
backpropagation, high-dimensional embeddings, or data-intensive black-box
models. Through rational spectral approximation, the system extracts resonant
structures that are then mapped into symbolic predicates via a kernel
projection function, enabling logical inference through a rule-based reasoning
engine. This architecture bridges mathematical physics, sparse approximation
theory, and symbolic artificial intelligence, offering a transparent and
physically grounded alternative to deep learning models. We develop the full
mathematical formalism underlying each stage of the pipeline, provide a modular
algorithmic implementation, and demonstrate the system's effectiveness through
comparative evaluations on time-series anomaly detection, symbolic
classification, and hybrid reasoning tasks. Our results show that this
spectral-symbolic architecture achieves competitive accuracy while maintaining
interpretability and data efficiency, suggesting a promising new direction for
physically-informed, reasoning-capable machine learning.</p></br><a href="http://arxiv.org/pdf/2508.03291v1" target="_blank"><h2>Investigation on deep learning-based galaxy image translation models</h2></a><strong><u>Authors:</u></strong>  Hengxin Ruan, Qiufan Lin, Shupei Chen, Yang Wang, Wei Zhang</br><strong><u>Categories:</u></strong> astro-ph.IM, astro-ph.GA, cs.CV</br><strong><u>Comments:</u></strong> Accepted at A&A; 18+6 pages; 12+6 figures</br><strong><u>Matching Keywords:</u></strong> transformer (abstract)</br><p><strong><u>Abstract:</u></strong> Galaxy image translation is an important application in galaxy physics and
cosmology. With deep learning-based generative models, image translation has
been performed for image generation, data quality enhancement, information
extraction, and generalized for other tasks such as deblending and anomaly
detection. However, most endeavors on image translation primarily focus on the
pixel-level and morphology-level statistics of galaxy images. There is a lack
of discussion on the preservation of complex high-order galaxy physical
information, which would be more challenging but crucial for studies that rely
on high-fidelity image translation. Therefore, we investigated the
effectiveness of generative models in preserving high-order physical
information (represented by spectroscopic redshift) along with pixel-level and
morphology-level information. We tested four representative models, i.e. a Swin
Transformer, an SRGAN, a capsule network, and a diffusion model, using the SDSS
and CFHTLS galaxy images. We found that these models show different levels of
incapabilities in retaining redshift information, even if the global structures
of galaxies and morphology-level statistics can be roughly reproduced. In
particular, the cross-band peak fluxes of galaxies were found to contain
meaningful redshift information, whereas they are subject to noticeable
uncertainties in the translation of images, which may substantially be due to
the nature of many-to-many mapping. Nonetheless, imperfect translated images
may still contain a considerable amount of information and thus hold promise
for downstream applications for which high image fidelity is not strongly
required. Our work can facilitate further research on how complex physical
information is manifested on galaxy images, and it provides implications on the
development of image translation models for scientific use.</p></br><a href="http://arxiv.org/pdf/2508.02911v1" target="_blank"><h2>Neural Approximators for Low-Thrust Trajectory Transfer Cost and
  Reachability</h2></a><strong><u>Authors:</u></strong>  Zhong Zhang, Francesco Topputo</br><strong><u>Categories:</u></strong> cs.LG, cs.SY, eess.SY, math.OC</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> neural network (abstract)</br><p><strong><u>Abstract:</u></strong> In trajectory design, fuel consumption and trajectory reachability are two
key performance indicators for low-thrust missions. This paper proposes
general-purpose pretrained neural networks to predict these metrics. The
contributions of this paper are as follows: Firstly, based on the confirmation
of the Scaling Law applicable to low-thrust trajectory approximation, the
largest dataset is constructed using the proposed homotopy ray method, which
aligns with mission-design-oriented data requirements. Secondly, the data are
transformed into a self-similar space, enabling the neural network to adapt to
arbitrary semi-major axes, inclinations, and central bodies. This extends the
applicability beyond existing studies and can generalize across diverse mission
scenarios without retraining. Thirdly, to the best of our knowledge, this work
presents the current most general and accurate low-thrust trajectory
approximator, with implementations available in C++, Python, and MATLAB. The
resulting neural network achieves a relative error of 0.78% in predicting
velocity increments and 0.63% in minimum transfer time estimation. The models
have also been validated on a third-party dataset, multi-flyby mission design
problem, and mission analysis scenario, demonstrating their generalization
capability, predictive accuracy, and computational efficiency.</p></br><a href="http://arxiv.org/pdf/2508.03484v1" target="_blank"><h2>Semantic-aware Graph-guided Behavior Sequences Generation with Large
  Language Models for Smart Homes</h2></a><strong><u>Authors:</u></strong>  Zhiyao Xu, Dan Zhao, Qingsong Zou, Qing Li, Yong Jiang, Yuhang Wang, Jingyu Xiao</br><strong><u>Categories:</u></strong> cs.AI</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> anomaly detection (abstract), latent space (abstract)</br><p><strong><u>Abstract:</u></strong> As smart homes become increasingly prevalent, intelligent models are widely
used for tasks such as anomaly detection and behavior prediction. These models
are typically trained on static datasets, making them brittle to behavioral
drift caused by seasonal changes, lifestyle shifts, or evolving routines.
However, collecting new behavior data for retraining is often impractical due
to its slow pace, high cost, and privacy concerns. In this paper, we propose
SmartGen, an LLM-based framework that synthesizes context-aware user behavior
data to support continual adaptation of downstream smart home models. SmartGen
consists of four key components. First, we design a Time and Semantic-aware
Split module to divide long behavior sequences into manageable, semantically
coherent subsequences under dual time-span constraints. Second, we propose
Semantic-aware Sequence Compression to reduce input length while preserving
representative semantics by clustering behavior mapping in latent space. Third,
we introduce Graph-guided Sequence Synthesis, which constructs a behavior
relationship graph and encodes frequent transitions into prompts, guiding the
LLM to generate data aligned with contextual changes while retaining core
behavior patterns. Finally, we design a Two-stage Outlier Filter to identify
and remove implausible or semantically inconsistent outputs, aiming to improve
the factual coherence and behavioral validity of the generated sequences.
Experiments on three real-world datasets demonstrate that SmartGen
significantly enhances model performance on anomaly detection and behavior
prediction tasks under behavioral drift, with anomaly detection improving by
85.43% and behavior prediction by 70.51% on average. The code is available at
https://github.com/horizonsinzqs/SmartGen.</p></br></body>