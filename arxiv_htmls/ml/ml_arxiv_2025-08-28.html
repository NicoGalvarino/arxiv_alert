<link href='https://fonts.googleapis.com/css?family=Montserrat' rel='stylesheet'>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$','$']],
            processEscapes: true
        },
        "HTML-CSS": {
            availableFonts: ["TeX"]
        }
    });
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <style>     body {font-family: 'Montserrat'; background: #F3F3F3; width: 740px; margin: 0 auto; line-height: 150%; margin-top: 50px; font-size: 15px}         h1 {font-size: 70px}         a {color: #45ABC2}         em {font-size: 120%} </style><h1><center>ArXiv Alert</center></h1><font color='#DDAD5C'><em>Update: from 26 Aug 2025 to 28 Aug 2025</em></font><a href="http://arxiv.org/pdf/2508.19441v1" target="_blank"><h2>Data-Augmented Few-Shot Neural Stencil Emulation for System
  Identification of Computer Models</h2></a><strong><u>Authors:</u></strong>  Sanket Jantre, Deepak Akhare, Xiaoning Qian, Nathan M. Urban</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ME, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> neural network (abstract)</br><p><strong><u>Abstract:</u></strong> Partial differential equations (PDEs) underpin the modeling of many natural
and engineered systems. It can be convenient to express such models as neural
PDEs rather than using traditional numerical PDE solvers by replacing part or
all of the PDE's governing equations with a neural network representation.
Neural PDEs are often easier to differentiate, linearize, reduce, or use for
uncertainty quantification than the original numerical solver. They are usually
trained on solution trajectories obtained by long time integration of the PDE
solver. Here we propose a more sample-efficient data-augmentation strategy for
generating neural PDE training data from a computer model by space-filling
sampling of local "stencil" states. This approach removes a large degree of
spatiotemporal redundancy present in trajectory data and oversamples states
that may be rarely visited but help the neural PDE generalize across the state
space. We demonstrate that accurate neural PDE stencil operators can be learned
from synthetic training data generated by the computational equivalent of 10
timesteps' worth of numerical simulation. Accuracy is further improved if we
assume access to a single full-trajectory simulation from the computer model,
which is typically available in practice. Across several PDE systems, we show
that our data-augmented synthetic stencil data yield better trained neural
stencil operators, with clear performance gains compared with naively sampled
stencil data from simulation trajectories.</p></br><a href="http://arxiv.org/pdf/2508.19683v1" target="_blank"><h2>Topological Uncertainty for Anomaly Detection in the Neural-network EoS
  Inference with Neutron Star Data</h2></a><strong><u>Authors:</u></strong>  Kenji Fukushima, Syo Kamata</br><strong><u>Categories:</u></strong> nucl-th, cs.AI, cs.LG</br><strong><u>Comments:</u></strong> 23 pages, 7 figures, 2 tables</br><strong><u>Matching Keywords:</u></strong> anomaly detection (title, abstract), neural network (abstract)</br><p><strong><u>Abstract:</u></strong> We study the performance of the Topological Uncertainty (TU) constructed with
a trained feedforward neural network (FNN) for Anomaly Detection. Generally,
meaningful information can be stored in the hidden layers of the trained FNN,
and the TU implementation is one tractable recipe to extract buried information
by means of the Topological Data Analysis. We explicate the concept of the TU
and the numerical procedures. Then, for a concrete demonstration of the
performance test, we employ the Neutron Star data used for inference of the
equation of state (EoS). For the training dataset consisting of the input
(Neutron Star data) and the output (EoS parameters), we can compare the
inferred EoSs and the exact answers to classify the data with the label $k$.
The subdataset with $k=0$ leads to the normal inference for which the inferred
EoS approximates the answer well, while the subdataset with $k=1$ ends up with
the unsuccessful inference. Once the TU is prepared based on the $k$-labled
subdatasets, we introduce the cross-TU to quantify the uncertainty of
characterizing the $k$-labeled data with the label $j$. The anomaly or
unsuccessful inference is correctly detected if the cross-TU for $j=k=1$ is
smaller than that for $j=0$ and $k=1$. In our numerical experiment, for various
input data, we calculate the cross-TU and estimate the performance of Anomaly
Detection. We find that performance depends on FNN hyperparameters, and the
success rate of Anomaly Detection exceeds $90\%$ in the best case. We finally
discuss further potential of the TU application to retrieve the information
hidden in the trained FNN.</p></br><a href="http://arxiv.org/pdf/2508.19772v1" target="_blank"><h2>Using normal to find abnormal: AI-based anomaly detection in
  gravitational wave data</h2></a><strong><u>Authors:</u></strong>  Yi-Yang Guo, Soumya D. Mohanty, Xie Qunying, Yu-Xiao Liu</br><strong><u>Categories:</u></strong> gr-qc, astro-ph.IM</br><strong><u>Comments:</u></strong> 24 pages; 22 figures</br><strong><u>Matching Keywords:</u></strong> anomaly detection (title, abstract), neural network (abstract)</br><p><strong><u>Abstract:</u></strong> The detection and classification of anomalies in gravitational wave data
plays a critical role in improving the sensitivity of searches for signals of
astrophysical origins. We present ABNORMAL (AI Based Nonstationarity Observer
for Resectioning and Marking AnomaLies), a deep neural network (DNN) model for
anomaly detection that is trained exclusively on simulated Gaussian noise. By
removing dependence on real data for training, the method resolves a circular
paradox in anomaly detection: training on real data implicitly involves prior
segregation of stationary from non-stationary data but this is not possible
unless all anomalies are detected first. ABNORMAL is an autoencoder-based DNN,
commonly used in anomaly detection, with the key innovation that it is trained
to predict statistical features of noise rather than reconstructing the noise
time series themselves. The statistical features are obtained by applying Gabor
and Wavelet filter banks, which implement time-frequency analysis, and are
subsequently combined through multi-view fusion using a dual-path architecture.
We quantify the performance of our method on simulated and real LIGO data.
Application to data from the O1 to O3b observational runs uncovers a rich
landscape of anomalies over different timescales, including many that do not
fit within known classes.</p></br><a href="http://arxiv.org/pdf/2508.19563v1" target="_blank"><h2>Just Because You Can, Doesn't Mean You Should: LLMs for Data Fitting</h2></a><strong><u>Authors:</u></strong>  Hejia Liu, Mochen Yang, Gediminas Adomavicius</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.AP, stat.ML</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> attention (abstract)</br><p><strong><u>Abstract:</u></strong> Large Language Models (LLMs) are being applied in a wide array of settings,
well beyond the typical language-oriented use cases. In particular, LLMs are
increasingly used as a plug-and-play method for fitting data and generating
predictions. Prior work has shown that LLMs, via in-context learning or
supervised fine-tuning, can perform competitively with many tabular supervised
learning techniques in terms of predictive performance. However, we identify a
critical vulnerability of using LLMs for data fitting -- making changes to data
representation that are completely irrelevant to the underlying learning task
can drastically alter LLMs' predictions on the same data. For example, simply
changing variable names can sway the size of prediction error by as much as 82%
in certain settings. Such prediction sensitivity with respect to
task-irrelevant variations manifests under both in-context learning and
supervised fine-tuning, for both close-weight and open-weight general-purpose
LLMs. Moreover, by examining the attention scores of an open-weight LLM, we
discover a non-uniform attention pattern: training examples and variable
names/values which happen to occupy certain positions in the prompt receive
more attention when output tokens are generated, even though different
positions are expected to receive roughly the same attention. This partially
explains the sensitivity in the presence of task-irrelevant variations. We also
consider a state-of-the-art tabular foundation model (TabPFN) trained
specifically for data fitting. Despite being explicitly designed to achieve
prediction robustness, TabPFN is still not immune to task-irrelevant
variations. Overall, despite LLMs' impressive predictive capabilities,
currently they lack even the basic level of robustness to be used as a
principled data-fitting tool.</p></br><a href="http://arxiv.org/pdf/2508.19361v1" target="_blank"><h2>Atrial Fibrillation Prediction Using a Lightweight Temporal
  Convolutional and Selective State Space Architecture</h2></a><strong><u>Authors:</u></strong>  Yongbin Lee, Ki H. Chon</br><strong><u>Categories:</u></strong> cs.LG, cs.AI</br><strong><u>Comments:</u></strong> 4 pages, 2 figures, 4 table, IEEE-EMBS International Conference on Body Sensor Networks (IEEE-EMBS BSN 2025)</br><strong><u>Matching Keywords:</u></strong> convolutional (title, abstract), neural network (abstract)</br><p><strong><u>Abstract:</u></strong> Atrial fibrillation (AF) is the most common arrhythmia, increasing the risk
of stroke, heart failure, and other cardiovascular complications. While AF
detection algorithms perform well in identifying persistent AF, early-stage
progression, such as paroxysmal AF (PAF), often goes undetected due to its
sudden onset and short duration. However, undetected PAF can progress into
sustained AF, increasing the risk of mortality and severe complications. Early
prediction of AF offers an opportunity to reduce disease progression through
preventive therapies, such as catecholamine-sparing agents or beta-blockers. In
this study, we propose a lightweight deep learning model using only RR
Intervals (RRIs), combining a Temporal Convolutional Network (TCN) for
positional encoding with Mamba, a selective state space model, to enable early
prediction of AF through efficient parallel sequence modeling. In subject-wise
testing results, our model achieved a sensitivity of 0.908, specificity of
0.933, F1-score of 0.930, AUROC of 0.972, and AUPRC of 0.932. Additionally, our
method demonstrates high computational efficiency, with only 73.5 thousand
parameters and 38.3 MFLOPs, outperforming traditional Convolutional Neural
Network-Recurrent Neural Network (CNN-RNN) approaches in both accuracy and
model compactness. Notably, the model can predict AF up to two hours in advance
using just 30 minutes of input data, providing enough lead time for preventive
interventions.</p></br><a href="http://arxiv.org/pdf/2508.19842v1" target="_blank"><h2>Symplectic convolutional neural networks</h2></a><strong><u>Authors:</u></strong>  Süleyman Yıldız, Konrad Janik, Peter Benner</br><strong><u>Categories:</u></strong> cs.LG</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> convolutional (title, abstract), neural network (title, abstract)</br><p><strong><u>Abstract:</u></strong> We propose a new symplectic convolutional neural network (CNN) architecture
by leveraging symplectic neural networks, proper symplectic decomposition, and
tensor techniques. Specifically, we first introduce a mathematically equivalent
form of the convolution layer and then, using symplectic neural networks, we
demonstrate a way to parameterize the layers of the CNN to ensure that the
convolution layer remains symplectic. To construct a complete autoencoder, we
introduce a symplectic pooling layer. We demonstrate the performance of the
proposed neural network on three examples: the wave equation, the nonlinear
Schr\"odinger (NLS) equation, and the sine-Gordon equation. The numerical
results indicate that the symplectic CNN outperforms the linear symplectic
autoencoder obtained via proper symplectic decomposition.</p></br><a href="http://arxiv.org/pdf/2508.19567v1" target="_blank"><h2>Counterfactual Reward Model Training for Bias Mitigation in Multimodal
  Reinforcement Learning</h2></a><strong><u>Authors:</u></strong>  Sheryl Mathew, N Harshit</br><strong><u>Categories:</u></strong> cs.LG</br><strong><u>Comments:</u></strong> No comments</br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)</br><p><strong><u>Abstract:</u></strong> In reinforcement learning with human feedback (RLHF), reward models can
efficiently learn and amplify latent biases within multimodal datasets, which
can lead to imperfect policy optimization through flawed reward signals and
decreased fairness. Bias mitigation studies have often applied passive
constraints, which can fail under causal confounding. Here, we present a
counterfactual reward model that introduces causal inference with multimodal
representation learning to provide an unsupervised, bias-resilient reward
signal. The heart of our contribution is the Counterfactual Trust Score, an
aggregated score consisting of four components: (1) counterfactual shifts that
decompose political framing bias from topical bias; (2) reconstruction
uncertainty during counterfactual perturbations; (3) demonstrable violations of
fairness rules for each protected attribute; and (4) temporal reward shifts
aligned with dynamic trust measures. We evaluated the framework on a multimodal
fake versus true news dataset, which exhibits framing bias, class imbalance,
and distributional drift. Following methodologies similar to unsupervised drift
detection from representation-based distances [1] and temporal robustness
benchmarking in language models [2], we also inject synthetic bias across
sequential batches to test robustness. The resulting system achieved an
accuracy of 89.12% in fake news detection, outperforming the baseline reward
models. More importantly, it reduced spurious correlations and unfair
reinforcement signals. This pipeline outlines a robust and interpretable
approach to fairness-aware RLHF, offering tunable bias reduction thresholds and
increasing reliability in dynamic real-time policy making.</p></br><a href="http://arxiv.org/pdf/2508.19366v1" target="_blank"><h2>Grounding the Ungrounded: A Spectral-Graph Framework for Quantifying
  Hallucinations in multimodal LLMs</h2></a><strong><u>Authors:</u></strong>  Supratik Sarkar, Swagatam Das</br><strong><u>Categories:</u></strong> cs.LG, cs.AI, 53B21, 46E22 (Primary), 68R10 (Secondary)</br><strong><u>Comments:</u></strong> 29 pages, 3 figures, 1 table</br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)</br><p><strong><u>Abstract:</u></strong> Hallucinations in large language models (LLMs) remain a fundamental obstacle
to trustworthy AI, particularly in high-stakes multimodal domains such as
medicine, law, and finance. Existing evaluation techniques are largely
heuristic -- anchored in qualitative benchmarking or ad-hoc empirical
mitigation -- providing neither principled quantification nor actionable
theoretical guarantees. This gap leaves a critical blind spot in understanding
how hallucinations arise, propagate, and interact across modalities. We
introduce the first (to our knowledge) rigorous information geometric framework
in diffusion dynamics for quantifying hallucinations in multimodal LLMs
(MLLMs), advancing the field from qualitative detection to mathematically
grounded measurement. Our approach represents MLLM outputs as the spectral
embeddings over multimodal graph Laplacians and characterizes the manifold gaps
of truth vs inconsistencies as the semantic distortion, enabling the tight
Rayleigh--Ritz bounds on the multimodal hallucination energy as a functional of
time-dependent temperature profiles. By leveraging eigenmode decompositions in
Reproducing Kernel Hilbert Space (RKHS) embeddings, our framework delivers
modality-aware, theoretically interpretable metrics that capture the evolution
of hallucinations across time and input prompts through temperature annealing.
This work establishes a principled foundation for quantifying and bounding
hallucinations, transforming them from a qualitative risk to a tractable,
analyzable phenomenon.</p></br><a href="http://arxiv.org/pdf/2508.19311v1" target="_blank"><h2>Identification of Strongly Lensed Gravitational Wave Events Using
  Squeeze-and-Excitation Multilayer Perceptron Data-efficient Image Transformer</h2></a><strong><u>Authors:</u></strong>  Dejiang Li, Tonghua Liu, Ao Liu, Cuihong Wen, Jieci Wang, Kai Liao, Jiaxing Cui</br><strong><u>Categories:</u></strong> astro-ph.IM, astro-ph.CO</br><strong><u>Comments:</u></strong> 11 pages, 5 figures, comments are welcome</br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract), attention (abstract)</br><p><strong><u>Abstract:</u></strong> With the advancement of third-generation gravitational wave detectors, the
identification of strongly lensed gravitational wave (GW) events is expected to
play an increasingly vital role in cosmology and fundamental physics. However,
traditional Bayesian inference methods suffer from combinatorial computational
overhead as the number of events grows, making real-time analysis infeasible.
To address this, we propose a deep learning model named Squeeze-and-Excitation
Multilayer Perceptron Data-efficient Image Transformer (SEMD), based on Vision
Transformers, which classifies strongly lensed GW events by modeling
morphological similarity between time-frequency spectrogram pairs. By
integrating Squeeze-and-Excitation attention mechanisms and multilayer
perceptrons , SEMD achieves strong feature extraction and discrimination.
Trained and evaluated on simulated datasets using Advanced LIGO and Einstein
Telescope noise, the model demonstrates robustness and generalization across
different detector sensitivities and physical conditions, highlighting the
promise of deep learning for rapid identification of strongly lensed GW
signals.</p></br></body>