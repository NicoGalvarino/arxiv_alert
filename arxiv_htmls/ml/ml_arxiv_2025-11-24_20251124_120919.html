<!DOCTYPE html><html><head><meta charset='utf-8'><link href='https://fonts.googleapis.com/css?family=Montserrat' rel='stylesheet'>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$','$']],
            processEscapes: true
        },
        "HTML-CSS": {
            availableFonts: ["TeX"]
        }
    });
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <style>
    body {font-family: 'Montserrat', sans-serif; background: #F3F3F3; width: 740px; margin: 0 auto; line-height: 150%; margin-top: 50px; font-size: 15px}
    h1 {font-size: 70px}
    a {color: #45ABC2}
    em {font-size: 120%}
    </style>
    </head><body><h1><center>ArXiv Alert</center></h1><font color='#DDAD5C'><em>Update: from 21 Nov 2025 to 23 Nov 2025</em></font><br><br><a href="https://arxiv.org/pdf/2511.17489v1" target="_blank"><h2>Harnessing Data from Clustered LQR Systems: Personalized and Collaborative Policy Optimization <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Vinay Kanakeri, Shivam Bajaj, Ashwin Verma, Vijay Gupta, Aritra Mitra<br><strong><u>Categories:</u></strong> cs.LG, eess.SY, math.OC<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> It is known that reinforcement learning (RL) is data-hungry. To improve sample-efficiency of RL, it has been proposed that the learning algorithm utilize data from 'approximately similar' processes. However, since the process models are unknown, identifying which other processes are similar poses a challenge. In this work, we study this problem in the context of the benchmark Linear Quadratic Regulator (LQR) setting. Specifically, we consider a setting with multiple agents, each corresponding to a copy of a linear process to be controlled. The agents' local processes can be partitioned into clusters based on similarities in dynamics and tasks. Combining ideas from sequential elimination and zeroth-order policy optimization, we propose a new algorithm that performs simultaneous clustering and learning to output a personalized policy (controller) for each cluster. Under a suitable notion of cluster separation that captures differences in closed-loop performance across systems, we prove that our approach guarantees correct clustering with high probability. Furthermore, we show that the sub-optimality gap of the policy learned for each cluster scales inversely with the size of the cluster, with no additional bias, unlike in prior works on collaborative learning-based control. Our work is the first to reveal how clustering can be used in data-driven control to learn personalized policies that enjoy statistical gains from collaboration but do not suffer sub-optimality due to inclusion of data from dissimilar processes. From a distributed implementation perspective, our method is attractive as it incurs only a mild logarithmic communication overhead.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17477v1" target="_blank"><h2>Enhancing Quranic Learning: A Multimodal Deep Learning Approach for Arabic Phoneme Recognition <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Ayhan Kucukmanisa, Derya Gelmez, Sukru Selim Calik, Zeynep Hilal Kilimci<br><strong><u>Categories:</u></strong> cs.SD, cs.AI<br><strong><u>Comments:</u></strong> 11 pages, 2 figures, 3 tables<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract), transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Recent advances in multimodal deep learning have greatly enhanced the capability of systems for speech analysis and pronunciation assessment. Accurate pronunciation detection remains a key challenge in Arabic, particularly in the context of Quranic recitation, where subtle phonetic differences can alter meaning. Addressing this challenge, the present study proposes a transformer-based multimodal framework for Arabic phoneme mispronunciation detection that combines acoustic and textual representations to achieve higher precision and robustness. The framework integrates UniSpeech-derived acoustic embeddings with BERT-based textual embeddings extracted from Whisper transcriptions, creating a unified representation that captures both phonetic detail and linguistic context. To determine the most effective integration strategy, early, intermediate, and late fusion methods were implemented and evaluated on two datasets containing 29 Arabic phonemes, including eight hafiz sounds, articulated by 11 native speakers. Additional speech samples collected from publicly available YouTube recordings were incorporated to enhance data diversity and generalization. Model performance was assessed using standard evaluation metrics: accuracy, precision, recall, and F1-score, allowing a detailed comparison of the fusion strategies. Experimental findings show that the UniSpeech-BERT multimodal configuration provides strong results and that fusion-based transformer architectures are effective for phoneme-level mispronunciation detection. The study contributes to the development of intelligent, speaker-independent, and multimodal Computer-Aided Language Learning (CALL) systems, offering a practical step toward technology-supported Quranic pronunciation training and broader speech-based educational applications.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17475v1" target="_blank"><h2>Addressing A Posteriori Performance Degradation in Neural Network Subgrid Stress Models <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Andy Wu, Sanjiva K. Lele<br><strong><u>Categories:</u></strong> physics.flu-dyn, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract), data augmentation (abstract)<br><p><strong><u>Abstract:</u></strong> Neural network subgrid stress models often have a priori performance that is far better than the a posteriori performance, leading to neural network models that look very promising a priori completely failing in a posteriori Large Eddy Simulations (LES). This performance gap can be decreased by combining two different methods, training data augmentation and reducing input complexity to the neural network. Augmenting the training data with two different filters before training the neural networks has no performance degradation a priori as compared to a neural network trained with one filter. A posteriori, neural networks trained with two different filters are far more robust across two different LES codes with different numerical schemes. In addition, by ablating away the higher order terms input into the neural network, the a priori versus a posteriori performance changes become less apparent. When combined, neural networks that use both training data augmentation and a less complex set of inputs have a posteriori performance far more reflective of their a priori evaluation.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17446v1" target="_blank"><h2>Unmasking Airborne Threats: Guided-Transformers for Portable Aerosol Mass Spectrometry <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Kyle M. Regan, Michael McLoughlin, Wayne A. Bryden, Gonzalo R. Arce<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 13 pages, 9 figures. Preprint. Submitted to Computers in Biology and Medicine<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> Matrix Assisted Laser Desorption/Ionization Mass Spectrometry (MALDI-MS) is a cornerstone in biomolecular analysis, offering precise identification of pathogens through unique mass spectral signatures. Yet, its reliance on labor-intensive sample preparation and multi-shot spectral averaging restricts its use to laboratory settings, rendering it impractical for real-time environmental monitoring. These limitations are especially pronounced in emerging aerosol MALDI-MS systems, where autonomous sampling generates noisy spectra for unknown aerosol analytes, requiring single-shot detection for effective analysis. Addressing these challenges, we propose the Mass Spectral Dictionary-Guided Transformer (MS-DGFormer): a data-driven framework that redefines spectral analysis by directly processing raw, minimally prepared mass spectral data. MS-DGFormer leverages a transformer architecture, designed to capture the long-range dependencies inherent in these time-series spectra. To enhance feature extraction, we introduce a novel dictionary encoder that integrates denoised spectral information derived from Singular Value Decomposition (SVD), enabling the model to discern critical biomolecular patterns from single-shot spectra with robust performance. This innovation provides a system to achieve superior pathogen identification from aerosol samples, facilitating autonomous, real-time analysis in field conditions. By eliminating the need for extensive preprocessing, our method unlocks the potential for portable, deployable MALDI-MS platforms, revolutionizing environmental pathogen detection and rapid response to biological threats.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17443v1" target="_blank"><h2>GRAPHIC--Guidelines for Reviewing Algorithmic Practices in Human-centred Design and Interaction for Creativity <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Joana Rovira Martins, Pedro Martins, Ana Boavida<br><strong><u>Categories:</u></strong> cs.HC, cs.AI, cs.GR<br><strong><u>Comments:</u></strong> 20 pages, 16 figures<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> explainable (abstract)<br><p><strong><u>Abstract:</u></strong> Artificial Intelligence (AI) has been increasingly applied to creative domains, leading to the development of systems that collaborate with humans in design processes. In Graphic Design, integrating computational systems into co-creative workflows presents specific challenges, as it requires balancing scientific rigour with the subjective and visual nature of design practice. Following the PRISMA methodology, we identified 872 articles, resulting in a final corpus of 71 publications describing 68 unique systems. Based on this review, we introduce GRAPHIC (Guidelines for Reviewing Algorithmic Practices in Human-centred Design and Interaction for Creativity), a framework for analysing AI-based systems applied to Graphic Design. Its goal is to understand how current systems support human-AI collaboration in the Graphic Design discipline. The framework comprises main dimensions, which our analysis revealed to be essential across diverse system types: (1) Collaborative Panorama, (2) Processes and Modalities, and (3) Graphic Design Principles. Its application revealed research gaps, including the need to balance initiative and control between agents, improve communication through explainable interaction models, and promote systems that support transformational creativity grounded in core design principles.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17442v1" target="_blank"><h2>REMSA: An LLM Agent for Foundation Model Selection in Remote Sensing <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Binger Chen, Tacettin Emre Bök, Behnood Rasti, Volker Markl, Begüm Demir<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> Code and data available atthis https URL<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Foundation Models (FMs) are increasingly used in remote sensing (RS) for tasks such as environmental monitoring, disaster assessment, and land-use mapping. These models include unimodal vision encoders trained on a single data modality and multimodal architectures trained on combinations of SAR, multispectral, hyperspectral, and image-text data. They support diverse RS tasks including semantic segmentation, image classification, change detection, and visual question answering. However, selecting an appropriate remote sensing foundation model (RSFM) remains difficult due to scattered documentation, heterogeneous formats, and varied deployment constraints. We introduce the RSFM Database (RS-FMD), a structured resource covering over 150 RSFMs spanning multiple data modalities, resolutions, and learning paradigms. Built on RS-FMD, we present REMSA, the first LLM-based agent for automated RSFM selection from natural language queries. REMSA interprets user requirements, resolves missing constraints, ranks candidate models using in-context learning, and provides transparent justifications. We also propose a benchmark of 75 expert-verified RS query scenarios, producing 900 configurations under an expert-centered evaluation protocol. REMSA outperforms several baselines, including naive agents, dense retrieval, and unstructured RAG-based LLMs. It operates entirely on publicly available metadata and does not access private or sensitive data.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17439v1" target="_blank"><h2>InTAct: Interval-based Task Activation Consolidation for Continual Learning <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Patryk Krukowski, Jan Miksa, Piotr Helm, Jacek Tabor, Paweł Wawrzyński, Przemysław Spurek<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Continual learning aims to enable neural networks to acquire new knowledge without forgetting previously learned information. While recent prompt-based methods perform strongly in class-incremental settings, they remain vulnerable under domain shifts, where the input distribution changes but the label space remains fixed. This exposes a persistent problem known as representation drift. Shared representations evolve in ways that overwrite previously useful features and cause forgetting even when prompts isolate task-specific parameters. To address this issue, we introduce InTAct, a method that preserves functional behavior in shared layers without freezing parameters or storing past data. InTAct captures the characteristic activation ranges associated with previously learned tasks and constrains updates to ensure the network remains consistent within these regions, while still allowing for flexible adaptation elsewhere. In doing so, InTAct stabilizes the functional role of important neurons rather than directly restricting parameter values. The approach is architecture-agnostic and integrates seamlessly into existing prompt-based continual learning frameworks. By regulating representation changes where past knowledge is encoded, InTAct achieves a principled balance between stability and plasticity. Across diverse domain-incremental benchmarks, including DomainNet and ImageNet-R, InTAct consistently reduces representation drift and improves performance, increasing Average Accuracy by up to 8 percentage points over state-of-the-art baselines.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17435v1" target="_blank"><h2>Multi-Agent Pointer Transformer: Seq-to-Seq Reinforcement Learning for Multi-Vehicle Dynamic Pickup-Delivery Problems <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Zengyu Zou, Jingyuan Wang, Yixuan Huang, Junjie Wu<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 15 pages<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> This paper addresses the cooperative Multi-Vehicle Dynamic Pickup and Delivery Problem with Stochastic Requests (MVDPDPSR) and proposes an end-to-end centralized decision-making framework based on sequence-to-sequence, named Multi-Agent Pointer Transformer (MAPT). MVDPDPSR is an extension of the vehicle routing problem and a spatio-temporal system optimization problem, widely applied in scenarios such as on-demand delivery. Classical operations research methods face bottlenecks in computational complexity and time efficiency when handling large-scale dynamic problems. Although existing reinforcement learning methods have achieved some progress, they still encounter several challenges: 1) Independent decoding across multiple vehicles fails to model joint action distributions; 2) The feature extraction network struggles to capture inter-entity relationships; 3) The joint action space is exponentially large. To address these issues, we designed the MAPT framework, which employs a Transformer Encoder to extract entity representations, combines a Transformer Decoder with a Pointer Network to generate joint action sequences in an AutoRegressive manner, and introduces a Relation-Aware Attention module to capture inter-entity relationships. Additionally, we guide the model's decision-making using informative priors to facilitate effective exploration. Experiments on 8 datasets demonstrate that MAPT significantly outperforms existing baseline methods in terms of performance and exhibits substantial computational time advantages compared to classical operations research methods.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17408v1" target="_blank"><h2>That's not natural: The Impact of Off-Policy Training Data on Probe Performance <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Nathalie Kirch, Samuel Dower, Adrians Skapars, Ekdeep Singh Lubana, Dmitrii Krasheninnikov<br><strong><u>Categories:</u></strong> cs.AI, cs.LG<br><strong><u>Comments:</u></strong> 10 pages, EurIPS 2025 Workshop on Private AI Governance<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Probing has emerged as a promising method for monitoring Large Language Models (LLMs), enabling inference-time detection of concerning behaviours such as deception and sycophancy. However, natural examples of many behaviours are rare, forcing researchers to rely on synthetic or off-policy LLM responses for training probes. We systematically evaluate how the use of synthetic and off-policy data influences probe generalisation across eight distinct LLM behaviours. Testing linear and attention probes across multiple LLMs, we find that the response generation strategy can significantly affect probe performance, though the magnitude of this effect varies by behaviour. We find that successful generalisation from off-policy data, to test sets where the model is incentivised to produce the target behaviour, is predictive of successful on-policy generalisation. Leveraging this result, we predict that Deception and Sandbagging probes may fail to generalise from off-policy to on-policy data when used in real monitoring scenarios. Notably, shifts in the training data domain still cause even larger performance degradation, with different-domain test scores being consistently lower than the same-domain ones. These results indicate that, in the absence of on-policy data, using same-domain off-policy data yields more reliable probes than using on-policy data from a different domain, emphasizing the need for methods that can better handle distribution shifts in LLM monitoring.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17405v1" target="_blank"><h2>Beyond Multiple Choice: A Hybrid Framework for Unifying Robust Evaluation and Verifiable Reasoning Training <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Yesheng Liu, Hao Li, Haiyu Xu, Baoqi Pei, Jiahao Wang, Mingxuan Zhao, Jingshu Zheng, Zheqi He, JG Yao, Bowen Qin, Xi Yang, Jiajun Zhang<br><strong><u>Categories:</u></strong> cs.CL, cs.AI<br><strong><u>Comments:</u></strong> Project url:this https URL<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Multiple-choice question answering (MCQA) has been a popular format for evaluating and reinforcement fine-tuning (RFT) of modern multimodal language models. Its constrained output format allows for simplified, deterministic automatic verification. However, we find that the options may leak exploitable signals, which makes the accuracy metrics unreliable for indicating real capabilities and encourages explicit or implicit answer guessing behaviors during RFT. We propose ReVeL (Rewrite and Verify by LLM), a framework that rewrites multiple-choice questions into open-form questions while keeping answers verifiable whenever possible. The framework categorizes questions according to different answer types, apply different rewriting and verification schemes, respectively. When applied for RFT, we converted 20k MCQA examples and use GRPO to finetune Qwen2.5-VL models. Models trained on ReVeL-OpenQA match MCQA accuracy on multiple-choice benchmarks and improve OpenQA accuracy by about six percentage points, indicating better data efficiency and more robust reward signals than MCQA-based training. When used for evaluation, ReVeL also reveals up to 20 percentage points of score inflation in MCQA benchmarks (relative to OpenQA), improves judging accuracy, and reduces both cost and latency. We will release code and data publicly.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17400v1" target="_blank"><h2>Sparse Mixture-of-Experts for Multi-Channel Imaging: Are All Channel Interactions Required? <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Sukwon Yun, Heming Yao, Burkhard Hoeckendorf, David Richmond, Aviv Regev, Russell Littman<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> This has been accepted at the NeurIPS AI4Science Workshop 2025<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Vision Transformers ($\text{ViTs}$) have become the backbone of vision foundation models, yet their optimization for multi-channel domains - such as cell painting or satellite imagery - remains underexplored. A key challenge in these domains is capturing interactions between channels, as each channel carries different information. While existing works have shown efficacy by treating each channel independently during tokenization, this approach naturally introduces a major computational bottleneck in the attention block - channel-wise comparisons leads to a quadratic growth in attention, resulting in excessive $\text{FLOPs}$ and high training cost. In this work, we shift focus from efficacy to the overlooked efficiency challenge in cross-channel attention and ask: "Is it necessary to model all channel interactions?". Inspired by the philosophy of Sparse Mixture-of-Experts ($\text{MoE}$), we propose MoE-ViT, a Mixture-of-Experts architecture for multi-channel images in $\text{ViTs}$, which treats each channel as an expert and employs a lightweight router to select only the most relevant experts per patch for attention. Proof-of-concept experiments on real-world datasets - JUMP-CP and So2Sat - demonstrate that $\text{MoE-ViT}$ achieves substantial efficiency gains without sacrificing, and in some cases enhancing, performance, making it a practical and attractive backbone for multi-channel imaging.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17388v1" target="_blank"><h2>Selective Rotary Position Embedding <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Sajad Movahedi, Timur Carstensen, Arshia Afzal, Frank Hutter, Antonio Orvieto, Volkan Cevher<br><strong><u>Categories:</u></strong> cs.CL, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Position information is essential for language modeling. In softmax transformers, Rotary Position Embeddings (\textit{RoPE}) encode positions through \textit{fixed-angle} rotations, while in linear transformers, order is handled via input-dependent (selective) gating that decays past key-value associations. Selectivity has generally been shown to improve language-related tasks. Inspired by this, we introduce \textit{Selective RoPE}, an \textit{input-dependent} rotary embedding mechanism, that generalizes \textit{RoPE}, and enables rotation in \textit{arbitrary angles} for both linear and softmax transformers. We show that softmax attention already performs a hidden form of these rotations on query-key pairs, uncovering an implicit positional structure. We further show that in state-space models and gated linear transformers, the real part manages forgetting while the imaginary part encodes positions through rotations. We validate our method by equipping gated transformers with \textit{Selective RoPE}, demonstrating that its input-dependent rotations improve performance in language modeling and on difficult sequence tasks like copying, state tracking, and retrieval.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17367v1" target="_blank"><h2>R2PS: Worst-Case Robust Real-Time Pursuit Strategies under Partial Observability <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Runyu Lu, Ruochuan Shi, Yuanheng Zhu, Dongbin Zhao<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Computing worst-case robust strategies in pursuit-evasion games (PEGs) is time-consuming, especially when real-world factors like partial observability are considered. While important for general security purposes, real-time applicable pursuit strategies for graph-based PEGs are currently missing when the pursuers only have imperfect information about the evader's position. Although state-of-the-art reinforcement learning (RL) methods like Equilibrium Policy Generalization (EPG) and Grasper provide guidelines for learning graph neural network (GNN) policies robust to different game dynamics, they are restricted to the scenario of perfect information and do not take into account the possible case where the evader can predict the pursuers' actions. This paper introduces the first approach to worst-case robust real-time pursuit strategies (R2PS) under partial observability. We first prove that a traditional dynamic programming (DP) algorithm for solving Markov PEGs maintains optimality under the asynchronous moves by the evader. Then, we propose a belief preservation mechanism about the evader's possible positions, extending the DP pursuit strategies to a partially observable setting. Finally, we embed the belief preservation into the state-of-the-art EPG framework to finish our R2PS learning scheme, which leads to a real-time pursuer policy through cross-graph reinforcement learning against the asynchronous-move DP evasion strategies. After reinforcement learning, our policy achieves robust zero-shot generalization to unseen real-world graph structures and consistently outperforms the policy directly trained on the test graphs by the existing game RL approach.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17339v1" target="_blank"><h2>ReBaPL: Repulsive Bayesian Prompt Learning <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Yassir Bendou, Omar Ezzahir, Eduardo Fernandes Montesuma, Gabriel Mahuas, Victoria Shevchenko, Mike Gartrell<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> Under review<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Prompt learning has emerged as an effective technique for fine-tuning large-scale foundation models for downstream tasks. However, conventional prompt tuning methods are prone to overfitting and can struggle with out-of-distribution generalization. To address these limitations, Bayesian prompt learning has been proposed, which frames prompt optimization as a Bayesian inference problem to enhance robustness. This paper introduces Repulsive Bayesian Prompt Learning (ReBaPL), a novel method for Bayesian prompt learning, designed to efficiently explore the complex and often multimodal posterior landscape of prompts. Our method integrates a cyclical step-size schedule with a stochastic gradient Hamiltonian Monte Carlo (SGHMC) algorithm, enabling alternating phases of exploration to discover new modes, and exploitation to refine existing modes. Furthermore, we introduce a repulsive force derived from a potential function over probability metrics (including Maximum Mean Discrepancy and Wasserstein distance) computed on the distributions of representations produced by different prompts. This representation-space repulsion diversifies exploration and prevents premature collapse to a single mode. Our approach allows for a more comprehensive characterization of the prompt posterior distribution, leading to improved generalization. In contrast to prior Bayesian prompt learning methods, our method provides a modular plug-and-play Bayesian extension of any existing prompt learning method based on maximum likelihood estimation. We demonstrate the efficacy of ReBaPL on several benchmark datasets, showing superior performance over state-of-the-art methods for prompt learning.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17332v1" target="_blank"><h2>Agentifying Agentic AI <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Virginia Dignum, Frank Dignum<br><strong><u>Categories:</u></strong> cs.AI, cs.MA<br><strong><u>Comments:</u></strong> 10 pages; 1 figure<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> Agentic AI seeks to endow systems with sustained autonomy, reasoning, and interaction capabilities. To realize this vision, its assumptions about agency must be complemented by explicit models of cognition, cooperation, and governance. This paper argues that the conceptual tools developed within the Autonomous Agents and Multi-Agent Systems (AAMAS) community, such as BDI architectures, communication protocols, mechanism design, and institutional modelling, provide precisely such a foundation. By aligning adaptive, data-driven approaches with structured models of reasoning and coordination, we outline a path toward agentic systems that are not only capable and flexible, but also transparent, cooperative, and accountable. The result is a perspective on agency that bridges formal theory and practical autonomy.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17331v1" target="_blank"><h2>AI Workers, Geopolitics, and Algorithmic Collective Action <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Sydney Reis<br><strong><u>Categories:</u></strong> cs.CY, cs.AI, cs.HC<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> According to the theory of International Political Economy (IPE), states are often incentivized to rely on rather than constrain powerful corporations. For this reason, IPE provides a useful lens to explain why efforts to govern Artificial Intelligence (AI) at the international and national levels have thus far been developed, applied, and enforced unevenly. Building on recent work that explores how AI companies engage in geopolitics, this position paper argues that some AI workers can be considered actors of geopolitics. It makes the timely case that governance alone cannot ensure responsible, ethical, or robust AI development and use, and greater attention should be paid to bottom-up interventions at the site of AI development. AI workers themselves should be situated as individual agents of change, especially when considering their potential to foster Algorithmic Collective Action (ACA). Drawing on methods of Participatory Design (PD), this paper proposes engaging AI workers as sources of knowledge, relative power, and intentionality to encourage more responsible and just AI development and create the conditions that can facilitate ACA.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17323v1" target="_blank"><h2>MusicAIR: A Multimodal AI Music Generation Framework Powered by an Algorithm-Driven Core <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Callie C. Liao, Duoduo Liao, Ellie L. Zhang<br><strong><u>Categories:</u></strong> cs.SD, cs.AI, cs.CL, cs.MM<br><strong><u>Comments:</u></strong> Accepted by IEEE Big Data 2025<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Recent advances in generative AI have made music generation a prominent research focus. However, many neural-based models rely on large datasets, raising concerns about copyright infringement and high-performance costs. In contrast, we propose MusicAIR, an innovative multimodal AI music generation framework powered by a novel algorithm-driven symbolic music core, effectively mitigating copyright infringement risks. The music core algorithms connect critical lyrical and rhythmic information to automatically derive musical features, creating a complete, coherent melodic score solely from the lyrics. The MusicAIR framework facilitates music generation from lyrics, text, and images. The generated score adheres to established principles of music theory, lyrical structure, and rhythmic conventions. We developed Generate AI Music (GenAIM), a web tool using MusicAIR for lyric-to-song, text-to-music, and image-to-music generation. In our experiments, we evaluated AI-generated music scores produced by the system using both standard music metrics and innovative analysis that compares these compositions with original works. The system achieves an average key confidence of 85%, outperforming human composers at 79%, and aligns closely with established music theory standards, demonstrating its ability to generate diverse, human-like compositions. As a co-pilot tool, GenAIM can serve as a reliable music composition assistant and a possible educational composition tutor while simultaneously lowering the entry barrier for all aspiring musicians, which is innovative and significantly contributes to AI for music generation.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17318v1" target="_blank"><h2>FORWARD: Dataset of a forwarder operating in rough terrain <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Mikael Lundbäck, Erik Wallin, Carola Häggström, Mattias Nyström, Andreas Grönlund, Mats Richardson, Petrus Jönsson, William Arnvik, Lucas Hedström, Arvid Fälldin, Martin Servin<br><strong><u>Categories:</u></strong> cs.RO, cs.AI, cs.CE, cs.LG, physics.app-ph<br><strong><u>Comments:</u></strong> 25 pages, 22 figures<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> We present FORWARD, a high-resolution multimodal dataset of a cut-to-length forwarder operating in rough terrain on two harvest sites in the middle part of Sweden. The forwarder is a large Komatsu model equipped with a variety of sensors, including RTK-GNSS, 360-camera, operator vibration sensors, internal CAN-bus signal recording, and multiple IMUs. The data includes event time logs recorded in 5 Hz with e.g., driving speed, fuel consumption, vehicle position with centimeter accuracy, and crane use while the vehicle operates in forest areas laser-scanned with very high-resolution, $\sim$1500 points per square meter. Production log files (StanForD standard) with time-stamped machine events, extensive video material, and terrain data in various formats are included as well. About 18 hours of regular wood extraction work during three days is annotated from 360-video material into individual work elements and included in the dataset. We also include scenario specifications of conducted experiments on forest roads and in terrain. Scenarios include repeatedly driving the same routes with and without steel tracks, different load weight, and different target driving speeds. The dataset is intended for developing models and algorithms for trafficability, perception, and autonomous control of forest machines using artificial intelligence, simulation, and experiments on physical testbeds. In part, we focus on forwarders traversing terrain, avoiding obstacles, and loading or unloading logs, with consideration for efficiency, fuel consumption, safety, and environmental impact. Other benefits of the open dataset include the ability to explore auto-generation and calibration of forestry machine simulators and automation scenario descriptions using the data recorded in the field.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17309v1" target="_blank"><h2>MuM: Multi-View Masked Image Modeling for 3D Vision <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> David Nordström, Johan Edstedt, Fredrik Kahl, Georg Bökman<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Self-supervised learning on images seeks to extract meaningful visual representations from unlabeled data. When scaled to large datasets, this paradigm has achieved state-of-the-art performance and the resulting trained models such as DINOv3 have seen widespread adoption. However, most prior efforts are optimized for semantic understanding rather than geometric reasoning. One important exception is Cross-View Completion, CroCo, which is a form of masked autoencoding (MAE) tailored for 3D understanding. In this work, we continue on the path proposed by CroCo and focus on learning features tailored for 3D vision. In a nutshell, we extend MAE to arbitrarily many views of the same scene. By uniformly masking all views and employing a lightweight decoder with inter-frame attention, our approach is inherently simpler and more scalable than CroCo. We evaluate the resulting model, MuM, extensively on downstream tasks including feedforward reconstruction, dense image matching and relative pose estimation, finding that it outperforms the state-of-the-art visual encoders DINOv3 and CroCo v2.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17298v1" target="_blank"><h2>SAVeD: Semantic Aware Version Discovery <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Artem Frenk, Roee Shraga<br><strong><u>Categories:</u></strong> cs.LG, stat.ML<br><strong><u>Comments:</u></strong> 11 pages, 6 figures<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> latent space (abstract), transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Our work introduces SAVeD (Semantically Aware Version Detection), a contrastive learning-based framework for identifying versions of structured datasets without relying on metadata, labels, or integration-based assumptions. SAVeD addresses a common challenge in data science of repeated labor due to a difficulty of similar work or transformations on datasets. SAVeD employs a modified SimCLR pipeline, generating augmented table views through random transformations (e.g., row deletion, encoding perturbations). These views are embedded via a custom transformer encoder and contrasted in latent space to optimize semantic similarity. Our model learns to minimize distances between augmented views of the same dataset and maximize those between unrelated tables. We evaluate performance using validation accuracy and separation, defined respectively as the proportion of correctly classified version/non-version pairs on a hold-out set, and the difference between average similarities of versioned and non-versioned tables (defined by a benchmark, and not provided to the model). Our experiments span five canonical datasets from the Semantic Versioning in Databases Benchmark, and demonstrate substantial gains post-training. SAVeD achieves significantly higher accuracy on completely unseen tables in, and a significant boost in separation scores, confirming its capability to distinguish semantically altered versions. Compared to untrained baselines and prior state-of-the-art dataset-discovery methods like Starmie, our custom encoder achieves competitive or superior results.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17276v1" target="_blank"><h2>Leveraging CVAE for Joint Configuration Estimation of Multifingered Grippers from Point Cloud Data <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Julien Merand, Boris Meden, Mathieu Grossard<br><strong><u>Categories:</u></strong> cs.RO, cs.AI, cs.CV<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> VAE (title, abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> This paper presents an efficient approach for determining the joint configuration of a multifingered gripper solely from the point cloud data of its poly-articulated chain, as generated by visual sensors, simulations or even generative neural networks. Well-known inverse kinematics (IK) techniques can provide mathematically exact solutions (when they exist) for joint configuration determination based solely on the fingertip pose, but often require post-hoc decision-making by considering the positions of all intermediate phalanges in the gripper's fingers, or rely on algorithms to numerically approximate solutions for more complex kinematics. In contrast, our method leverages machine learning to implicitly overcome these challenges. This is achieved through a Conditional Variational Auto-Encoder (CVAE), which takes point cloud data of key structural elements as input and reconstructs the corresponding joint configurations. We validate our approach on the MultiDex grasping dataset using the Allegro Hand, operating within 0.05 milliseconds and achieving accuracy comparable to state-of-the-art methods. This highlights the effectiveness of our pipeline for joint configuration estimation within the broader context of AI-driven techniques for grasp planning.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17254v1" target="_blank"><h2>Intervene-All-Paths: Unified Mitigation of LVLM Hallucinations across Alignment Formats <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Jiaye Qian, Ge Zheng, Yuchen Zhu, Sibei Yang<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> Accepted to NeurIPS 2025, Project Page:this https URL<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Despite their impressive performance across a wide range of tasks, Large Vision-Language Models (LVLMs) remain prone to hallucination. In this study, we propose a comprehensive intervention framework aligned with the transformer's causal architecture in LVLMs, integrating the effects of different intervention paths on hallucination. We find that hallucinations in LVLMs do not arise from a single causal path, but rather from the interplay among image-to-input-text, image-to-output-text, and text-to-text pathways. For the first time, we also find that LVLMs rely on different pathways depending on the question-answer alignment format. Building on these insights, we propose simple yet effective methods to identify and intervene on critical hallucination heads within each pathway, tailored to discriminative and generative formats. Experiments across multiple benchmarks demonstrate that our approach consistently reduces hallucinations across diverse alignment types.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17242v1" target="_blank"><h2>Equivariant-Aware Structured Pruning for Efficient Edge Deployment: A Comprehensive Framework with Adaptive Fine-Tuning <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Mohammed Alnemari<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> 8 pages, 5 tables, 1 figure. Accepted at IEEE EdgeCom 2025 (11th IEEE International Conference on Edge Computing and Scalable Cloud)<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> This paper presents a novel framework combining group equivariant convolutional neural networks (G-CNNs) with equivariant-aware structured pruning to produce compact, transformation-invariant models for resource-constrained environments. Equivariance to rotations is achieved through the C4 cyclic group via the e2cnn library,enabling consistent performance under geometric transformations while reducing computational overhead.
  Our approach introduces structured pruning that preserves equivariant properties by analyzing e2cnn layer structure and applying neuron-level pruning to fully connected components. To mitigate accuracy degradation, we implement adaptive fine-tuning that automatically triggers when accuracy drop exceeds 2%, using early stopping and learning rate scheduling for efficient recovery. The framework includes dynamic INT8 quantization and a comprehensive pipeline encompassing training, knowledge distillation, structured pruning, fine-tuning, and quantization.
  We evaluate our method on satellite imagery (EuroSAT) and standard benchmarks (CIFAR-10, Rotated MNIST) demonstrating effectiveness across diverse domains. Experimental results show 29.3% parameter reduction with significant accuracy recovery, demonstrating that structured pruning of equivariant networks achieves substantial compression while maintaining geometric robustness. Our pipeline provides a reproducible framework for optimizing equivariant models, bridging the gap between group-theoretic network design and practical deployment constraints, with particular relevance to satellite imagery analysis and geometric vision tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17233v1" target="_blank"><h2>Algorithmic design and implementation considerations of deep MPC <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Prabhat K. Mishra, Mateus V. Gasparino, Girish Chowdhary<br><strong><u>Categories:</u></strong> eess.SY, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Deep Model Predictive Control (Deep MPC) is an evolving field that integrates model predictive control and deep learning. This manuscript is focused on a particular approach, which employs deep neural network in the loop with MPC. This class of approaches distributes control authority between a neural network and an MPC controller, in such a way that the neural network learns the model uncertainties while the MPC handles constraints. The approach is appealing because training data collected while the system is in operation can be used to fine-tune the neural network, and MPC prevents unsafe behavior during those learning transients. This manuscript explains implementation challenges of Deep MPC, algorithmic way to distribute control authority and argues that a poor choice in distributing control authority may lead to poor performance. A reason of poor performance is explained through a numerical experiment on a four-wheeled skid-steer dynamics.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17228v1" target="_blank"><h2>Intrinsic preservation of plasticity in continual quantum learning <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Yu-Qin Chen, Shi-Xin Zhang<br><strong><u>Categories:</u></strong> quant-ph, cs.LG<br><strong><u>Comments:</u></strong> 11 pages, 5 figures and supplementary information<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Artificial intelligence in dynamic, real-world environments requires the capacity for continual learning. However, standard deep learning suffers from a fundamental issue: loss of plasticity, in which networks gradually lose their ability to learn from new data. Here we show that quantum learning models naturally overcome this limitation, preserving plasticity over long timescales. We demonstrate this advantage systematically across a broad spectrum of tasks from multiple learning paradigms, including supervised learning and reinforcement learning, and diverse data modalities, from classical high-dimensional images to quantum-native datasets. Although classical models exhibit performance degradation correlated with unbounded weight and gradient growth, quantum neural networks maintain consistent learning capabilities regardless of the data or task. We identify the origin of the advantage as the intrinsic physical constraints of quantum models. Unlike classical networks where unbounded weight growth leads to landscape ruggedness or saturation, the unitary constraints confine the optimization to a compact manifold. Our results suggest that the utility of quantum computing in machine learning extends beyond potential speedups, offering a robust pathway for building adaptive artificial intelligence and lifelong learners.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17219v1" target="_blank"><h2>DelTriC: A Novel Clustering Method with Accurate Outlier <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Tomas Javurek, Michal Gregor, Sebastian Kula, Marian Simko<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 10 pages, submitted to AISTATS<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> anomaly detection (abstract)<br><p><strong><u>Abstract:</u></strong> The paper introduces DelTriC (Delaunay Triangulation Clustering), a clustering algorithm which integrates PCA/UMAP-based projection, Delaunay triangulation, and a novel back-projection mechanism to form clusters in the original high-dimensional space. DelTriC decouples neighborhood construction from decision-making by first triangulating in a low-dimensional proxy to index local adjacency, and then back-projecting to the original space to perform robust edge pruning, merging, and anomaly detection. DelTriC can outperform traditional methods such as k-means, DBSCAN, and HDBSCAN in many scenarios; it is both scalable and accurate, and it also significantly improves outlier detection.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17184v1" target="_blank"><h2>Attention-Guided Feature Fusion (AGFF) Model for Integrating Statistical and Semantic Features in News Text Classification <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Mohammad Zare<br><strong><u>Categories:</u></strong> cs.CL, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> News text classification is a crucial task in natural language processing, essential for organizing and filtering the massive volume of digital content. Traditional methods typically rely on statistical features like term frequencies or TF-IDF values, which are effective at capturing word-level importance but often fail to reflect contextual meaning. In contrast, modern deep learning approaches utilize semantic features to understand word usage within context, yet they may overlook simple, high-impact statistical indicators. This paper introduces an Attention-Guided Feature Fusion (AGFF) model that combines statistical and semantic features in a unified framework. The model applies an attention-based mechanism to dynamically determine the relative importance of each feature type, enabling more informed classification decisions. Through evaluation on benchmark news datasets, the AGFF model demonstrates superior performance compared to both traditional statistical models and purely semantic deep learning models. The results confirm that strategic integration of diverse feature types can significantly enhance classification accuracy. Additionally, ablation studies validate the contribution of each component in the fusion process. The findings highlight the model's ability to balance and exploit the complementary strengths of statistical and semantic representations, making it a practical and effective solution for real-world news classification tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17181v1" target="_blank"><h2>Investigating self-supervised representations for audio-visual deepfake detection <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Dragos-Alexandru Boldisor, Stefan Smeu, Dan Oneata, Elisabeta Oneata<br><strong><u>Categories:</u></strong> cs.CV, cs.LG, cs.SD<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Self-supervised representations excel at many vision and speech tasks, but their potential for audio-visual deepfake detection remains underexplored. Unlike prior work that uses these features in isolation or buried within complex architectures, we systematically evaluate them across modalities (audio, video, multimodal) and domains (lip movements, generic visual content). We assess three key dimensions: detection effectiveness, interpretability of encoded information, and cross-modal complementarity. We find that most self-supervised features capture deepfake-relevant information, and that this information is complementary. Moreover, models primarily attend to semantically meaningful regions rather than spurious artifacts. Yet none generalize reliably across datasets. This generalization failure likely stems from dataset characteristics, not from the features themselves latching onto superficial patterns. These results expose both the promise and fundamental challenges of self-supervised representations for deepfake detection: while they learn meaningful patterns, achieving robust cross-domain performance remains elusive.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17171v1" target="_blank"><h2>FireScope: Wildfire Risk Prediction with a Chain-of-Thought Oracle <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Mario Markov, Stefan Maria Ailuro, Luc Van Gool, Konrad Schindler, Danda Pani Paudel<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Predicting wildfire risk is a reasoning-intensive spatial problem that requires the integration of visual, climatic, and geographic factors to infer continuous risk maps. Existing methods lack the causal reasoning and multimodal understanding required for reliable generalization. We introduce $\textbf{FireScope-Bench}$, a large-scale dataset and benchmark that couples Sentinel-2 imagery and climate data with expert-defined risk rasters across the USA, and real wildfire events in Europe for cross-continental evaluation. Building on this dataset, we propose $\textbf{FireScope}$, a VLM-based reasoning-to-generation framework that learns from both reinforcement learning and visual supervision to predict risk rasters with complementary reasoning traces. When trained in the USA and tested in Europe, $\textbf{FireScope}$ achieves substantial performance gains, while expert feedback and automated analysis confirm that its reasoning traces are faithful and semantically meaningful. Our findings demonstrate that reasoning can ground raster prediction models, improving both generalization and interpretability. To our knowledge, this is the first framework to (1) demonstrate that language-based reasoning can improve generalization in visual generation, (2) propose a high-resolution wildfire risk model that can be applied across continents, and (3) enable systematic studies of robust cross-continental generalization for multimodal fire risk models. We believe that $\textbf{FireScope-Bench}$ has the potential to serve as a foundation for advancing reasoning-driven, interpretable and generalizable spatial modeling. Data and source code will be made publicly available.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17162v1" target="_blank"><h2>The Belief-Desire-Intention Ontology for modelling mental reality and agency <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Sara Zuppiroli, Carmelo Fabio Longo, Anna Sofia Lippolis, Rocco Paolillo, Lorenzo Giammei, Miguel Ceriani, Francesco Poggi, Antonio Zinilli, Andrea Giovanni Nuzzolese<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> explainable (abstract)<br><p><strong><u>Abstract:</u></strong> The Belief-Desire-Intention (BDI) model is a cornerstone for representing rational agency in artificial intelligence and cognitive sciences. Yet, its integration into structured, semantically interoperable knowledge representations remains limited. This paper presents a formal BDI Ontology, conceived as a modular Ontology Design Pattern (ODP) that captures the cognitive architecture of agents through beliefs, desires, intentions, and their dynamic interrelations. The ontology ensures semantic precision and reusability by aligning with foundational ontologies and best practices in modular design. Two complementary lines of experimentation demonstrate its applicability: (i) coupling the ontology with Large Language Models (LLMs) via Logic Augmented Generation (LAG) to assess the contribution of ontological grounding to inferential coherence and consistency; and (ii) integrating the ontology within the Semas reasoning platform, which implements the Triples-to-Beliefs-to-Triples (T2B2T) paradigm, enabling a bidirectional flow between RDF triples and agent mental states. Together, these experiments illustrate how the BDI Ontology acts as both a conceptual and operational bridge between declarative and procedural intelligence, paving the way for cognitively grounded, explainable, and semantically interoperable multi-agent and neuro-symbolic systems operating within the Web of Data.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17161v1" target="_blank"><h2>The PLLuM Instruction Corpus <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Piotr Pęzik, Filip Żarnecki, Konrad Kaczyński, Anna Cichosz, Zuzanna Deckert, Monika Garnys, Izabela Grabarczyk, Wojciech Janowski, Sylwia Karasińska, Aleksandra Kujawiak, Piotr Misztela, Maria Szymańska, Karolina Walkusz, Igor Siek, Maciej Chrabąszcz, Anna Kołos, Agnieszka Karlińska, Karolina Seweryn, Aleksandra Krasnodębska, Paula Betscher, Zofia Cieślińska, Katarzyna Kowol, Artur Wilczek, Maciej Trzciński, Katarzyna Dziewulska, Roman Roszko, Tomasz Bernaś, Jurgita Vaičenonienė, Danuta Roszko, Paweł Levchuk, Paweł Kowalski, Irena Prawdzic-Jankowska, Marek Kozłowski, Sławomir Dadas, Rafał Poświata, Alina Wróblewska, Katarzyna Krasnowska-Kieraś, Maciej Ogrodniczuk, Michał Rudolf, Piotr Rybak, Karolina Saputa, Joanna Wołoszyn, Marcin Oleksy, Bartłomiej Koptyra, Teddy Ferdinan, Stanisław Woźniak, Maciej Piasecki, Paweł Walkowiak, Konrad Wojtasik, Arkadiusz Janz, Przemysław Kazienko, Julia Moska, Jan Kocoń<br><strong><u>Categories:</u></strong> cs.CL, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> This paper describes the instruction dataset used to fine-tune a set of transformer-based large language models (LLMs) developed in the PLLuM (Polish Large Language Model) project. We present a functional typology of the organic, converted, and synthetic instructions used in PLLuM and share some observations about the implications of using human-authored versus synthetic instruction datasets in the linguistic adaptation of base LLMs. Additionally, we release the first representative subset of the PLLuM instruction corpus (PLLuMIC), which we believe to be useful in guiding and planning the development of similar datasets for other LLMs.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17147v1" target="_blank"><h2>A lightweight detector for real-time detection of remote sensing images <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Qianyi Wang, Guoqiang Ren<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> none<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Remote sensing imagery is widely used across various fields, yet real-time detection remains challenging due to the prevalence of small objects and the need to balance accuracy with efficiency. To address this, we propose DMG-YOLO, a lightweight real-time detector tailored for small object detection in remote sensing images. Specifically, we design a Dual-branch Feature Extraction (DFE) module in the backbone, which partitions feature maps into two parallel branches: one extracts local features via depthwise separable convolutions, and the other captures global context using a vision transformer with a gating mechanism. Additionally, a Multi-scale Feature Fusion (MFF) module with dilated convolutions enhances multi-scale integration while preserving fine details. In the neck, we introduce the Global and Local Aggregate Feature Pyramid Network (GLAFPN) to further boost small object detection through global-local feature fusion. Extensive experiments on the VisDrone2019 and NWPU VHR-10 datasets show that DMG-YOLO achieves competitive performance in terms of mAP, model size, and other key metrics.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17136v1" target="_blank"><h2>Device-Guided Music Transfer <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Manh Pham Hung, Changshuo Hu, Ting Dang, Dong Ma<br><strong><u>Categories:</u></strong> cs.SD, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Device-guided music transfer adapts playback across unseen devices for users who lack them. Existing methods mainly focus on modifying the timbre, rhythm, harmony, or instrumentation to mimic genres or artists, overlooking the diverse hardware properties of the playback device (i.e., speaker). Therefore, we propose DeMT, which processes a speaker's frequency response curve as a line graph using a vision-language model to extract device embeddings. These embeddings then condition a hybrid transformer via feature-wise linear modulation. Fine-tuned on a self-collected dataset, DeMT enables effective speaker-style transfer and robust few-shot adaptation for unseen devices, supporting applications like device-style augmentation and quality enhancement.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17127v1" target="_blank"><h2>Training Foundation Models on a Full-Stack AMD Platform: Compute, Networking, and System Design <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Quentin Anthony, Yury Tokpanov, Skyler Szot, Srivatsan Rajagopal, Praneeth Medepalli, Rishi Iyer, Vasu Shyam, Anna Golubeva, Ansh Chaurasia, Xiao Yang, Tomas Figliolia, Robert Washbourne, Drew Thorstensen, Amartey Pearson, Zack Grossbart, Jason van Patten, Emad Barsoum, Zhenyu Gu, Yao Fu, Beren Millidge<br><strong><u>Categories:</u></strong> cs.CL, cs.AI, cs.DC<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> We report on the first large-scale mixture-of-experts (MoE) pretraining study on pure AMD hardware, utilizing both MI300X GPUs with Pollara interconnect. We distill practical guidance for both systems and model design. On the systems side, we deliver a comprehensive cluster and networking characterization: microbenchmarks for all core collectives (all-reduce, reduce-scatter, all-gather, broadcast) across message sizes and GPU counts on Pollara. To our knowledge, this is the first at this scale. We further provide MI300X microbenchmarks on kernel sizing and memory bandwidth to inform model design. On the modeling side, we introduce and apply MI300X-aware transformer sizing rules for attention and MLP blocks and justify MoE widths that jointly optimize training throughput and inference latency. We describe our training stack in depth, including often-ignored utilities such as fault-tolerance and checkpoint-reshaping, as well as detailed information on our training recipe. We also provide a preview of our model architecture and base model - ZAYA1 (760M active, 8.3B total parameters MoE) - which will be further improved upon in forthcoming papers. ZAYA1-base achieves performance comparable to leading base models such as Qwen3-4B and Gemma3-12B at its scale and larger, and outperforms models including Llama-3-8B and OLMoE across reasoning, mathematics, and coding benchmarks. Together, these results demonstrate that the AMD hardware, network, and software stack are mature and optimized enough for competitive large-scale pretraining.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17126v1" target="_blank"><h2>OmniLens++: Blind Lens Aberration Correction via Large LensLib Pre-Training and Latent PSF Representation <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Qi Jiang, Xiaolong Qian, Yao Gao, Lei Sun, Kailun Yang, Zhonghua Yi, Wenyong Li, Ming-Hsuan Yang, Luc Van Gool, Kaiwei Wang<br><strong><u>Categories:</u></strong> eess.IV, cs.CV, cs.LG, physics.optics<br><strong><u>Comments:</u></strong> The source code and datasets will be made publicly available atthis https URL<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> VAE (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Emerging deep-learning-based lens library pre-training (LensLib-PT) pipeline offers a new avenue for blind lens aberration correction by training a universal neural network, demonstrating strong capability in handling diverse unknown optical degradations. This work proposes the OmniLens++ framework, which resolves two challenges that hinder the generalization ability of existing pipelines: the difficulty of scaling data and the absence of prior guidance characterizing optical degradation. To improve data scalability, we expand the design specifications to increase the degradation diversity of the lens source, and we sample a more uniform distribution by quantifying the spatial-variation patterns and severity of optical degradation. In terms of model design, to leverage the Point Spread Functions (PSFs), which intuitively describe optical degradation, as guidance in a blind paradigm, we propose the Latent PSF Representation (LPR). The VQVAE framework is introduced to learn latent features of LensLib's PSFs, which is assisted by modeling the optical degradation process to constrain the learning of degradation priors. Experiments on diverse aberrations of real-world lenses and synthetic LensLib show that OmniLens++ exhibits state-of-the-art generalization capacity in blind aberration correction. Beyond performance, the AODLibpro is verified as a scalable foundation for more effective training across diverse aberrations, and LPR can further tap the potential of large-scale LensLib. The source code and datasets will be made publicly available at https://github.com/zju-jiangqi/OmniLens2.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17123v1" target="_blank"><h2>Layer-wise Weight Selection for Power-Efficient Neural Network Acceleration <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Jiaxun Fang, Li Zhang, Shaoyi Huang<br><strong><u>Categories:</u></strong> cs.AR, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> neural network (title)<br><p><strong><u>Abstract:</u></strong> Systolic array accelerators execute CNNs with energy dominated by the switching activity of multiply accumulate (MAC) units. Although prior work exploits weight dependent MAC power for compression, existing methods often use global activation models, coarse energy proxies, or layer-agnostic policies, which limits their effectiveness on real hardware. We propose an energy aware, layer-wise compression framework that explicitly leverages MAC and layer level energy characteristics. First, we build a layer-aware MAC energy model that combines per-layer activation statistics with an MSB-Hamming distance grouping of 22-bit partial sum transitions, and integrate it with a tile-level systolic mapping to estimate convolution-layer energy. On top of this model, we introduce an energy accuracy co-optimized weight selection algorithm within quantization aware training and an energy-prioritized layer-wise schedule that compresses high energy layers more aggressively under a global accuracy constraint. Experiments on different CNN models demonstrate up to 58.6\% energy reduction with 2-3\% accuracy drop, outperforming a state-of-the-art power-aware baseline.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17113v1" target="_blank"><h2>AutoGraphAD: A novel approach using Variational Graph Autoencoders for anomalous network flow detection <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Georgios Anyfantis, Pere Barlet-Ros<br><strong><u>Categories:</u></strong> cs.CR, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> 11 pages, 9 figures<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> anomaly detection (abstract)<br><p><strong><u>Abstract:</u></strong> Network Intrusion Detection Systems (NIDS) are essential tools for detecting network attacks and intrusions. While extensive research has explored the use of supervised Machine Learning for attack detection and characterisation, these methods require accurately labelled datasets, which are very costly to obtain. Moreover, existing public datasets have limited and/or outdated attacks, and many of them suffer from mislabelled data. To reduce the reliance on labelled data, we propose AutoGraphAD, a novel unsupervised anomaly detection approach based on a Heterogeneous Variational Graph Autoencoder. AutoGraphAD operates on heterogeneous graphs, made from connection and IP nodes that capture network activity within a time window. The model is trained using unsupervised and contrastive learning, without relying on any labelled data. The reconstruction, structural loss, and KL divergence are then weighted and combined in an anomaly score that is then used for anomaly detection. Overall, AutoGraphAD yields the same, and in some cases better, results than previous unsupervised approaches, such as Anomal-E, but without requiring costly downstream anomaly detectors. As a result, AutoGraphAD achieves around 1.18 orders of magnitude faster training and 1.03 orders of magnitude faster inference, which represents a significant advantage for operational deployment.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17084v1" target="_blank"><h2>Spectral synthesis techniques for supernovae and kilonovae <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Anders Jerkstrand<br><strong><u>Categories:</u></strong> astro-ph.HE, astro-ph.SR<br><strong><u>Comments:</u></strong> Invited review article for Living Reviews in Computational Astrophysics, August 2025<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> VAE (title, abstract)<br><p><strong><u>Abstract:</u></strong> Supernovae (SNe) and kilonovae (KNe) are the most violent explosions in cosmos, signalling the destruction of a massive star (core-collapse SN), a white dwarf (thermonuclear SN) and a neutron star (KN), respectively. The ejected debris in these explosions is believed to be the main cosmic source of most elements in the periodic table. However, decoding the spectra of these transients is a challenging task requiring sophisticated spectral synthesis modelling. Here, the techniques for such modelling is reviewed, with particular focus on the computational aspects. We build from a historical review of how methodologies evolved from modelling of stellar winds, to supernovae, to kilonovae, studying various approximations in use for the central physical processes. Similarities and differences in the numeric schemes employed by current codes are discussed, and the path towards improved models is laid out.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17061v1" target="_blank"><h2>Morphological Image Similarity Search on the ALMA Science Archive Query Interface Using Deep Unsupervised Contrastive Representation Learning <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Felix Stoehr, Andrea Farago, Stefan Curiban, Alisdair Manning, Jorge Garcia, Pei-Ying Hsieh, Andrew Lipnicky, Adele Plunkett<br><strong><u>Categories:</u></strong> astro-ph.IM<br><strong><u>Comments:</u></strong> 3 pages, 2 figures<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> With the exponential growth of astronomical data over time, finding the needles in the haystack is becoming increasingly difficult. The next frontier for science archives is to enable searches not only on observational metadata, but also on the content of the observations themselves. As a step in this direction, we have implemented morphological image similarity search into the ALMA Science Archive (ASA). To achieve this we use self-supervised contrastive affine-transformation-independent representation learning of source morphologies with a deep neural network. For a given image on the ASA web interface, astronomers are presented with a summary view of the morphologically most similar images. Each time an astronomer selects an additional image from that view, the display is instantly updated to show the images most similar to the combination of the selected images. Each selection thus refines the similarity display according to the scientific needs of the astronomer. This is the first time image similarity search has been offered in an astronomical science archive.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17056v1" target="_blank"><h2>Patient-level Information Extraction by Consistent Integration of Textual and Tabular Evidence with Bayesian Networks <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Paloma Rabaey, Adrick Tench, Stefan Heytens, Thomas Demeester<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> multi-modal (abstract)<br><p><strong><u>Abstract:</u></strong> Electronic health records (EHRs) form an invaluable resource for training clinical decision support systems. To leverage the potential of such systems in high-risk applications, we need large, structured tabular datasets on which we can build transparent feature-based models. While part of the EHR already contains structured information (e.g. diagnosis codes, medications, and lab results), much of the information is contained within unstructured text (e.g. discharge summaries and nursing notes). In this work, we propose a method for multi-modal patient-level information extraction that leverages both the tabular features available in the patient's EHR (using an expert-informed Bayesian network) as well as clinical notes describing the patient's symptoms (using neural text classifiers). We propose the use of virtual evidence augmented with a consistency node to provide an interpretable, probabilistic fusion of the models' predictions. The consistency node improves the calibration of the final predictions compared to virtual evidence alone, allowing the Bayesian network to better adjust the neural classifier's output to handle missing information and resolve contradictions between the tabular and text data. We show the potential of our method on the SimSUM dataset, a simulated benchmark linking tabular EHRs with clinical notes through expert knowledge.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17045v1" target="_blank"><h2>RacketVision: A Multiple Racket Sports Benchmark for Unified Ball and Racket Analysis <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Linfeng Dong, Yuchen Yang, Hao Wu, Wei Wang, Yuenan HouZhihang Zhong, Xiao Sun<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.MM<br><strong><u>Comments:</u></strong> Accepted to AAAI 2026 (Oral)<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract), multi-modal (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> We introduce RacketVision, a novel dataset and benchmark for advancing computer vision in sports analytics, covering table tennis, tennis, and badminton. The dataset is the first to provide large-scale, fine-grained annotations for racket pose alongside traditional ball positions, enabling research into complex human-object interactions. It is designed to tackle three interconnected tasks: fine-grained ball tracking, articulated racket pose estimation, and predictive ball trajectory forecasting. Our evaluation of established baselines reveals a critical insight for multi-modal fusion: while naively concatenating racket pose features degrades performance, a CrossAttention mechanism is essential to unlock their value, leading to trajectory prediction results that surpass strong unimodal baselines. RacketVision provides a versatile resource and a strong starting point for future research in dynamic object tracking, conditional motion forecasting, and multimodal analysis in sports. Project page at https://github.com/OrcustD/RacketVision</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17043v1" target="_blank"><h2>MedImageInsight for Thoracic Cavity Health Classification from Chest X-rays <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Rama Krishna Boya, Mohan Kireeti Magalanadu, Azaruddin Palavalli, Rupa Ganesh Tekuri, Amrit Pattanayak, Prasanthi Enuga, Vignesh Esakki Muthu, Vivek Aditya Boya<br><strong><u>Categories:</u></strong> eess.IV, cs.AI, cs.CV<br><strong><u>Comments:</u></strong> 9 pages, 5 figures and 3 tables<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> transfer learning (abstract)<br><p><strong><u>Abstract:</u></strong> Chest radiography remains one of the most widely used imaging modalities for thoracic diagnosis, yet increasing imaging volumes and radiologist workload continue to challenge timely interpretation. In this work, we investigate the use of MedImageInsight, a medical imaging foundational model, for automated binary classification of chest X-rays into Normal and Abnormal categories. Two approaches were evaluated: (1) fine-tuning MedImageInsight for end-to-end classification, and (2) employing the model as a feature extractor for a transfer learning pipeline using traditional machine learning classifiers. Experiments were conducted using a combination of the ChestX-ray14 dataset and real-world clinical data sourced from partner hospitals. The fine-tuned classifier achieved the highest performance, with an ROC-AUC of 0.888 and superior calibration compared to the transfer learning models, demonstrating performance comparable to established architectures such as CheXNet. These results highlight the effectiveness of foundational medical imaging models in reducing task-specific training requirements while maintaining diagnostic reliability. The system is designed for integration into web-based and hospital PACS workflows to support triage and reduce radiologist burden. Future work will extend the model to multi-label pathology classification to provide preliminary diagnostic interpretation in clinical environments.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17040v1" target="_blank"><h2>Step-E: A Differentiable Data Cleaning Framework for Robust Learning with Noisy Labels <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Wenzhang Du<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 12 pages, 4 figures<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Training data collected in the wild often contain noisy labels and outliers that substantially degrade the performance and reliability of deep neural networks. While data cleaning is commonly applied as a separate preprocessing stage, such two-stage pipelines neither fully exploit feedback from the downstream model nor adapt to unknown noise patterns. We propose Step-E, a simple framework that integrates sample selection and model learning into a single optimization process. At each epoch, Step-E ranks samples by loss and gradually increases the fraction of high-loss examples that are excluded from gradient updates after a brief warm-up stage, yielding an online curriculum that focuses on easy and consistent examples and eventually ignores persistent outliers. On CIFAR-100N, Step-E improves the test accuracy of a ResNet-18 model from 43.3% (+/- 0.7%) to 50.4% (+/- 0.9%), clearly outperforming loss truncation, self-paced learning, and one-shot filtering while approaching the clean-label oracle at 60.5% (+/- 0.2%). On CIFAR-10N (aggre), Step-E also improves over the noisy baseline (85.3% vs. 83.9%) and nearly matches the clean-label oracle (85.9%), with only moderate training-time overhead.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17038v1" target="_blank"><h2>DAPS++: Rethinking Diffusion Inverse Problems with Decoupled Posterior Annealing <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Hao Chen, Renzheng Zhang, Scott S. Howard<br><strong><u>Categories:</u></strong> cs.AI, stat.ML<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> From a Bayesian perspective, score-based diffusion solves inverse problems through joint inference, embedding the likelihood with the prior to guide the sampling process. However, this formulation fails to explain its practical behavior: the prior offers limited guidance, while reconstruction is largely driven by the measurement-consistency term, leading to an inference process that is effectively decoupled from the diffusion dynamics. To clarify this structure, we reinterpret the role of diffusion in inverse problem solving as an initialization stage within an expectation--maximization (EM)--style framework, where the diffusion stage and the data-driven refinement are fully decoupled. We introduce \textbf{DAPS++}, which allows the likelihood term to guide inference more directly while maintaining numerical stability and providing insight into why unified diffusion trajectories remain effective in practice. By requiring fewer function evaluations (NFEs) and measurement-optimization steps, \textbf{DAPS++} achieves high computational efficiency and robust reconstruction performance across diverse image restoration tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17008v1" target="_blank"><h2>Mask the Redundancy: Evolving Masking Representation Learning for Multivariate Time-Series Clustering <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Zexi Tan, Xiaopeng Luo, Yunlin Liu, Yiqun Zhang<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> Accepted to AAAI 2026<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Multivariate Time-Series (MTS) clustering discovers intrinsic grouping patterns of temporal data samples. Although time-series provide rich discriminative information, they also contain substantial redundancy, such as steady-state machine operation records and zero-output periods of solar power generation. Such redundancy diminishes the attention given to discriminative timestamps in representation learning, thus leading to performance bottlenecks in MTS clustering. Masking has been widely adopted to enhance the MTS representation, where temporal reconstruction tasks are designed to capture critical information from MTS. However, most existing masking strategies appear to be standalone preprocessing steps, isolated from the learning process, which hinders dynamic adaptation to the importance of clustering-critical timestamps. Accordingly, this paper proposes the Evolving-masked MTS Clustering (EMTC) method, with its model architecture composed of Importance-aware Variate-wise Masking (IVM) and Multi-Endogenous Views (MEV) representation learning modules. IVM adaptively guides the model in learning more discriminative representations for clustering, while the MEV-based reconstruction and contrastive learning pathways enhance the generalization. That is, the MEV reconstruction facilitates multi-perspective complementary to prevent the masking from premature convergence, and the clustering-guided contrastive learning facilitates the joint optimization of representation and clustering. Extensive experiments on 15 real benchmark datasets demonstrate the superiority of EMTC in comparison with eight SOTA methods, where the EMTC achieves an average improvement of 4.85% over the strongest baselines.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17007v1" target="_blank"><h2>Generative MIMO Beam Map Construction for Location Recovery and Beam Tracking <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Wangqian Chen, Junting Chen, Shuguang Cui<br><strong><u>Categories:</u></strong> eess.SP, cs.LG, eess.SY<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), convolutional (abstract), latent space (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Machine learning (ML) has greatly advanced data-driven channel modeling and resource optimization in wireless communication systems. However, most existing ML-based methods rely on large, accurately labeled datasets with location information, which are often difficult and costly to obtain. This paper proposes a generative framework to recover location labels directly from sequences of sparse channel state information (CSI) measurements, without explicit location labels for radio map construction. Instead of directly storing raw CSI, we learn a compact low-dimensional radio map embedding and leverage a generative model to reconstruct the high-dimensional CSI. Specifically, to address the uncertainty of sparse CSI, a dual-scale feature extraction scheme is designed to enhance feature representation by jointly exploiting correlations from angular space and across neighboring samples. We develop a hybrid recurrent-convolutional encoder to learn mobility patterns, which combines a truncation strategy and multi-scale convolutions in the recurrent neural network (RNN) to ensure feature robustness against short-term fluctuations. Unlike conventional Gaussian priors in latent space, we embed a learnable radio map to capture the location information by encoding high-level positional features from CSI measurements. Finally, a diffusion-based generative decoder reconstructs the full CSI with high fidelity by conditioning on the positional features in the radio map. Numerical experiments demonstrate that the proposed model can improve localization accuracy by over 30% and achieve a 20% capacity gain in non-line-of-sight (NLOS) scenarios compared with model-based Kalman filter approaches.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17005v1" target="_blank"><h2>FLUID: Training-Free Face De-identification via Latent Identity Substitution <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Jinhyeong Park, Shaheryar Muhammad, Seangmin Lee, Jong Taek Lee, Soon Ki Jung<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> latent space (abstract)<br><p><strong><u>Abstract:</u></strong> We present FLUID (Face de-identification in the Latent space via Utility-preserving Identity Displacement), a training-free framework that directly substitutes identity in the latent space of pretrained diffusion models. Inspired by substitution mechanisms in chemistry, we reinterpret identity editing as semantic displacement in the latent h-space of a pretrained unconditional diffusion model. Our framework discovers identity-editing directions through optimization guided by novel reagent losses, which supervise for attribute preservation and identity suppression. We further propose both linear and geodesic (tangent-based) editing schemes to effectively navigate the latent manifold. Experimental results on CelebA-HQ and FFHQ demonstrate that FLUID achieves a superior trade-off between identity suppression and attribute preservation, outperforming state-of-the-art de-identification methods in both qualitative and quantitative metrics.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16982v1" target="_blank"><h2>A Diversity-optimized Deep Ensemble Approach for Accurate Plant Leaf Disease Detection <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Sai Nath Chowdary Medikonduru, Hongpeng Jin, Yanzhao Wu<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Plant diseases pose a significant threat to global agriculture, causing over $220 billion in annual economic losses and jeopardizing food security. The timely and accurate detection of these diseases from plant leaf images is critical to mitigating their adverse effects. Deep neural network Ensembles (Deep Ensembles) have emerged as a powerful approach to enhancing prediction accuracy by leveraging the strengths of diverse Deep Neural Networks (DNNs). However, selecting high-performing ensemble member models is challenging due to the inherent difficulty in measuring ensemble diversity. In this paper, we introduce the Synergistic Diversity (SQ) framework to enhance plant disease detection accuracy. First, we conduct a comprehensive analysis of the limitations of existing ensemble diversity metrics (denoted as Q metrics), which often fail to identify optimal ensemble teams. Second, we present the SQ metric, a novel measure that captures the synergy between ensemble members and consistently aligns with ensemble accuracy. Third, we validate our SQ approach through extensive experiments on a plant leaf image dataset, which demonstrates that our SQ metric substantially improves ensemble selection and enhances detection accuracy. Our findings pave the way for a more reliable and efficient image-based plant disease detection.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16976v1" target="_blank"><h2>Gradient flow for deep equilibrium single-index models <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Sanjit Dandapanthula, Aaditya Ramdas<br><strong><u>Categories:</u></strong> cs.LG, math.ST, stat.ML<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Deep equilibrium models (DEQs) have recently emerged as a powerful paradigm for training infinitely deep weight-tied neural networks that achieve state of the art performance across many modern machine learning tasks. Despite their practical success, theoretically understanding the gradient descent dynamics for training DEQs remains an area of active research. In this work, we rigorously study the gradient descent dynamics for DEQs in the simple setting of linear models and single-index models, filling several gaps in the literature. We prove a conservation law for linear DEQs which implies that the parameters remain trapped on spheres during training and use this property to show that gradient flow remains well-conditioned for all time. We then prove linear convergence of gradient descent to a global minimizer for linear DEQs and deep equilibrium single-index models under appropriate initialization and with a sufficiently small step size. Finally, we validate our theoretical findings through experiments.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16943v1" target="_blank"><h2>RASTP: Representation-Aware Semantic Token Pruning for Generative Recommendation with Semantic Identifiers <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Tianyu Zhan, Kairui Fu, Zheqi Lv, Shengyu Zhang<br><strong><u>Categories:</u></strong> cs.IR, cs.AI<br><strong><u>Comments:</u></strong> 4 pages<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Generative recommendation systems typically leverage Semantic Identifiers (SIDs), which represent each item as a sequence of tokens that encode semantic information. However, representing item ID with multiple SIDs significantly increases input sequence length, which is a major determinant of computational complexity and memory consumption. While existing efforts primarily focus on optimizing attention computation and KV cache, we propose RASTP (Representation-Aware Semantic Token Pruning), which directly prunes less informative tokens in the input sequence. Specifically, RASTP evaluates token importance by combining semantic saliency, measured via representation magnitude, and attention centrality, derived from cumulative attention weights. Since RASTP dynamically prunes low-information or irrelevant semantic tokens, experiments on three real-world Amazon datasets show that RASTP reduces training time by 26.7\%, while maintaining or slightly improving recommendation performance. The code has been open-sourced at https://github.com/Yuzt-zju/RASTP.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16937v1" target="_blank"><h2>OmniGround: A Comprehensive Spatio-Temporal Grounding Benchmark for Real-World Complex Scenarios <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Hong Gao, Jingyu Wu, Xiangkai Xu, Kangni Xie, Yunchen Zhang, Bin Zhong, Xurui Gao, Min-Ling Zhang<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> 20 pages<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Spatio-Temporal Video Grounding (STVG) aims to localize target objects in videos based on natural language descriptions. Despite recent advances in Multimodal Large Language Models, a significant gap remains between current models and real-world demands involving diverse objects and complex queries. We attribute this to limited benchmark scope, causing models to exhibit category bias, oversimplified reasoning, and poor linguistic robustness. To address these limitations, we introduce OmniGround, a comprehensive benchmark with 3,475 videos spanning 81 categories and complex real-world queries. We propose the Forward-Backward-Refinement annotation pipeline that combines multi-directional tracking with intelligent error correction for high-quality labels. We further introduce DeepSTG, a systematic evaluation framework quantifying dataset quality across four complementary dimensions beyond superficial statistics. Evaluations reveal performance average drop of 10.4% on complex real-world scenes, particularly with small/occluded objects and intricate spatial relations. Motivated by these, we propose PG-TAF, a training-free two-stage framework decomposing STVG into high-level temporal grounding and fine-grained spatio-temporal propagation. Experiments demonstrate PG-TAF achieves 25.6% and 35.6% improvements in m\_tIoU and m\_vIoU on OmniGround with consistent gains across four benchmarks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16929v1" target="_blank"><h2>CroTad: A Contrastive Reinforcement Learning Framework for Online Trajectory Anomaly Detection <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Rui Xue, Dan He, Fengmei Jin, Chen Zhang, Xiaofang Zhou<br><strong><u>Categories:</u></strong> cs.LG, cs.DB<br><strong><u>Comments:</u></strong> 18 pages, 4 figures, will be submitted to VLDBJ<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> anomaly detection (title, abstract)<br><p><strong><u>Abstract:</u></strong> Detecting trajectory anomalies is a vital task in modern Intelligent Transportation Systems (ITS), enabling the identification of unsafe, inefficient, or irregular travel behaviours. While deep learning has emerged as the dominant approach, several key challenges remain unresolved. First, sub-trajectory anomaly detection, capable of pinpointing the precise segments where anomalies occur, remains underexplored compared to whole-trajectory analysis. Second, many existing methods depend on carefully tuned thresholds, limiting their adaptability in real-world applications. Moreover, the irregular sampling of trajectory data and the presence of noise in training sets further degrade model performance, making it difficult to learn reliable representations of normal routes. To address these challenges, we propose a contrastive reinforcement learning framework for online trajectory anomaly detection, CroTad. Our method is threshold-free and robust to noisy, irregularly sampled data. By incorporating contrastive learning, CroTad learns to extract diverse normal travel patterns for different itineraries and effectively distinguish anomalous behaviours at both sub-trajectory and point levels. The detection module leverages deep reinforcement learning to perform online, real-time anomaly scoring, enabling timely and fine-grained identification of abnormal segments. Extensive experiments on two real-world datasets demonstrate the effectiveness and robustness of our framework across various evaluation scenarios.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16905v1" target="_blank"><h2>Predicting Talent Breakout Rate using Twitter and TV data <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Bilguun Batsaikhan, Hiroyuki Fukuda<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 4 pages. Presented at the 34th Annual Conference of the Japanese Society for Artificial Intelligence (JSAI 2020), paper ID 1K3-ES-2-02<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Early detection of rising talents is of paramount importance in the field of advertising. In this paper, we define a concept of talent breakout and propose a method to detect Japanese talents before their rise to stardom. The main focus of the study is to determine the effectiveness of combining Twitter and TV data on predicting time-dependent changes in social data. Although traditional time-series models are known to be robust in many applications, the success of neural network models in various fields (e.g.\ Natural Language Processing, Computer Vision, Reinforcement Learning) continues to spark an interest in the time-series community to apply new techniques in practice. Therefore, in order to find the best modeling approach, we have experimented with traditional, neural network and ensemble learning methods. We observe that ensemble learning methods outperform traditional and neural network models based on standard regression metrics. However, by utilizing the concept of talent breakout, we are able to assess the true forecasting ability of the models, where neural networks outperform traditional and ensemble learning methods in terms of precision and recall.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16884v1" target="_blank"><h2>Generative AI in Sociological Research: State of the Discipline <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> AJ Alvero, Dustin S. Stoltz, Oscar Stuhler, Marshall Taylor<br><strong><u>Categories:</u></strong> cs.CY, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Generative artificial intelligence (GenAI) has garnered considerable attention for its potential utility in research and scholarship, even among those who typically do not rely on computational tools. Early commentators, however, have also articulated concerns about how GenAI usage comes with enormous environmental costs, serious social risks, and a tendency to produce low-quality content. In the midst of both excitement and skepticism, it is crucial to take stock of how GenAI is actually being used. Our study focuses on sociological research as our site, and here we present findings from a survey of 433 authors of articles published in 50 sociology journals in the last five years. The survey provides an overview of the state of the discipline with regard to the use of GenAI by providing answers to fundamental questions: how (much) do scholars use the technology for their research; what are their reasons for using it; and how concerned, trustful, and optimistic are they about the technology? Of the approximately one third ofrespondents who self-report using GenAI at least weekly, the primary uses are for writing assistance and comparatively less so in planning, data collection, or data analysis. In both use and attitudes, there are surprisingly few differences between self-identified computational and non-computational researchers. Generally, respondents are very concerned about the social and environmental consequences of GenAI. Trust in GenAI outputs is low, regardless of expertise or frequency of use. While optimism that GenAI will improve is high, scholars are divided on whether GenAI will have a positive impact on the field.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16871v1" target="_blank"><h2>Topologic Attention Networks: Attending to Direct and Indirect Neighbors through Gaussian Belief Propagation <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Marshall Rosenhoover, Huaming Zhang<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 15 pages, 13 Figures<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Graph Neural Networks rely on local message passing, which limits their ability to model long-range dependencies in graphs. Existing approaches extend this range through continuous-time dynamics or dense self-attention, but both suffer from high computational cost and limited scalability. We propose Topologic Attention Networks, a new framework that applies topologic attention, a probabilistic mechanism that learns how information should flow through both direct and indirect connections in a graph. Unlike conventional attention that depends on explicit pairwise interactions, topologic attention emerges from the learned information propagation of the graph, enabling unified reasoning over local and global relationships. This method achieves provides state-of-the-art performance across all measured baseline models. Our implementation is available at https://github.com/Marshall-Rosenhoover/Topologic-Attention-Networks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16856v1" target="_blank"><h2>The use of vocal biomarkers in the detection of Parkinson's disease: a robust statistical performance comparison of classic machine learning models <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Katia Pires Nascimento do Sacramento, Elliot Q. C. Garcia, Nicéias Silva Vilela, Vinicius P. Sacramento, Tiago A. E. Ferreira<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 18 pages, 3 figures<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Parkinson's disease (PD) is a progressive neurodegenerative disorder that, in addition to directly impairing functional mobility, is frequently associated with vocal impairments such as hypophonia and dysarthria, which typically manifest in the early stages. The use of vocal biomarkers to support the early diagnosis of PD presents a non-invasive, low-cost, and accessible alternative in clinical settings. Thus, the objective of this cross-sectional study was to consistently evaluate the effectiveness of a Deep Neural Network (DNN) in distinguishing individuals with Parkinson's disease from healthy controls, in comparison with traditional Machine Learning (ML) methods, using vocal biomarkers. Two publicly available voice datasets were used. Mel-frequency cepstral coefficients (MFCCs) were extracted from the samples, and model robustness was assessed using a validation strategy with 1000 independent random executions. Performance was evaluated using classification statistics. Since normality assumptions were not satisfied, non-parametric tests (Kruskal-Wallis and Bonferroni post-hoc tests) were applied to verify whether the tested classification models were similar or different in the classification of PD. With an average accuracy of $98.65\%$ and $92.11\%$ on the Italian Voice dataset and Parkinson's Telemonitoring dataset, respectively, the DNN demonstrated superior performance and efficiency compared to traditional ML models, while also achieving competitive results when benchmarked against relevant studies. Overall, this study confirms the efficiency of DNNs and emphasizes their potential to provide greater accuracy and reliability for the early detection of neurodegenerative diseases using voice-based biomarkers.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16849v1" target="_blank"><h2>Better audio representations are more brain-like: linking model-brain alignment with performance in downstream auditory tasks <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Leonardo Pepino, Pablo Riera, Juan Kamienkowski, Luciana Ferrer<br><strong><u>Categories:</u></strong> cs.LG, cs.SD<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Artificial neural networks (ANNs) are increasingly powerful models of brain computation, yet it remains unclear whether improving their task performance also makes their internal representations more similar to brain signals. To address this question in the auditory domain, we quantified the alignment between the internal representations of 36 different audio models and brain activity from two independent fMRI datasets. Using voxel-wise and component-wise regression, and representation similarity analysis (RSA), we found that recent self-supervised audio models with strong performance in diverse downstream tasks are better predictors of auditory cortex activity than older and more specialized models. To assess the quality of the audio representations, we evaluated these models in 6 auditory tasks from the HEAREval benchmark, spanning music, speech, and environmental sounds. This revealed strong positive Pearson correlations ($r>0.7$) between a model's overall task performance and its alignment with brain representations. Finally, we analyzed the evolution of the similarity between audio and brain representations during the pretraining of EnCodecMAE. We discovered that brain similarity increases progressively and emerges early during pretraining, despite the model not being explicitly optimized for this objective. This suggests that brain-like representations can be an emergent byproduct of learning to reconstruct missing information from naturalistic audio data.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16839v1" target="_blank"><h2>Analysis of heart failure patient trajectories using sequence modeling <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Falk Dippela, Yinan Yu, Annika Rosengren, Martin Lindgren, Christina E. Lundberg, Erik Aerts, Martin Adiels, Helen Sjöland<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Transformers have defined the state-of-the-art for clinical prediction tasks involving electronic health records (EHRs). The recently introduced Mamba architecture outperformed an advanced Transformer (Transformer++) based on Llama in handling long context lengths, while using fewer model parameters. Despite the impressive performance of these architectures, a systematic approach to empirically analyze model performance and efficiency under various settings is not well established in the medical domain. The performances of six sequence models were investigated across three architecture classes (Transformers, Transformers++, Mambas) in a large Swedish heart failure (HF) cohort (N = 42820), providing a clinically relevant case study. Patient data included diagnoses, vital signs, laboratories, medications and procedures extracted from in-hospital EHRs. The models were evaluated on three one-year prediction tasks: clinical instability (a readmission phenotype) after initial HF hospitalization, mortality after initial HF hospitalization and mortality after latest hospitalization. Ablations account for modifications of the EHR-based input patient sequence, architectural model configurations, and temporal preprocessing techniques for data collection. Llama achieves the highest predictive discrimination, best calibration, and showed robustness across all tasks, followed by Mambas. Both architectures demonstrate efficient representation learning, with tiny configurations surpassing other large-scaled Transformers. At equal model size, Llama and Mambas achieve superior performance using 25% less training data. This paper presents a first ablation study with systematic design choices for input tokenization, model configuration and temporal data preprocessing. Future model development in clinical prediction tasks using EHRs could build upon this study's recommendation as a starting point.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16828v1" target="_blank"><h2>ManifoldFormer: Geometric Deep Learning for Neural Dynamics on Riemannian Manifolds <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Yihang Fu, Lifang He, Qingyu Chen<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 5 pages, under review by ICASSP<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> VAE (abstract), transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Existing EEG foundation models mainly treat neural signals as generic time series in Euclidean space, ignoring the intrinsic geometric structure of neural dynamics that constrains brain activity to low-dimensional manifolds. This fundamental mismatch between model assumptions and neural geometry limits representation quality and cross-subject generalization. ManifoldFormer addresses this limitation through a novel geometric deep learning framework that explicitly learns neural manifold representations. The architecture integrates three key innovations: a Riemannian VAE for manifold embedding that preserves geometric structure, a geometric Transformer with geodesic-aware attention mechanisms operating directly on neural manifolds, and a dynamics predictor leveraging neural ODEs for manifold-constrained temporal evolution. Extensive evaluation across four public datasets demonstrates substantial improvements over state-of-the-art methods, with 4.6-4.8% higher accuracy and 6.2-10.2% higher Cohen's Kappa, while maintaining robust cross-subject generalization. The geometric approach reveals meaningful neural patterns consistent with neurophysiological principles, establishing geometric constraints as essential for effective EEG foundation models.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16786v1" target="_blank"><h2>Revisiting Multimodal KV Cache Compression: A Frequency-Domain-Guided Outlier-KV-Aware Approach <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Yaoxin Yang, Peng Ye, Xudong Tan, Chongjun Tu, Maosen Zhao, Jia Hao, Tao Chen<br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.CV<br><strong><u>Comments:</u></strong> Under Review<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Multimodal large language models suffer from substantial inference overhead since multimodal KV Cache grows proportionally with the visual input length. Existing multimodal KV Cache compression methods mostly rely on attention score to reduce cache size, which makes them are incompatible with established efficient attention kernels (e.g., FlashAttention) and ignores the contribution of value vectors to the attention output. In this work, we revisit multimodal KV Cache compression from the perspective of the KV matrices' distribution. First, we observe that frequency-domain energy of multimodal KV matrices is predominantly concentrated in low-frequency and extract this principal energy via a low-pass filter. Further, we find that removing KV pairs that deviate substantially from this principal energy leads to a pronounced performance drop, which we define as Outlier KVs. Considering Outlier KVs are more likely to encode features critical for inference, we propose FlashCache, a frequency-domain-guided, Outlier-KV-aware KV Cache compression framework. First, we introduce an Outlier KV Recognition Module that models the principal component of multimodal KV matrices in the frequency domain and preferentially retains KV pairs that significantly deviate from it. Furthermore, Dynamic Budget Allocation Module is designed to adaptively determine the per-layer KV Cache size to retain more Outlier KVs. Experiments on multiple MLLMs and benchmarks demonstrate that FlashCache outperforms state-of-the-art multimoal KV compression methods, achieving up to 1.69 times faster decoding with 80% lower KV memory usage while maintaining task performance.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16778v1" target="_blank"><h2>GCL-OT: Graph Contrastive Learning with Optimal Transport for Heterophilic Text-Attributed Graphs <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Yating Ren, Yikun Ban, Huobin Tan<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> AAAI 2026<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Recently, structure-text contrastive learning has shown promising performance on text-attributed graphs by leveraging the complementary strengths of graph neural networks and language models. However, existing methods typically rely on homophily assumptions in similarity estimation and hard optimization objectives, which limit their applicability to heterophilic graphs. Although existing methods can mitigate heterophily through structural adjustments or neighbor aggregation, they usually treat textual embeddings as static targets, leading to suboptimal alignment. In this work, we identify the multi-granular heterophily in text-attributed graphs, including complete heterophily, partial heterophily, and latent homophily, which makes structure-text alignment particularly challenging due to mixed, noisy, and missing semantic correlations. To achieve flexible and bidirectional alignment, we propose GCL-OT, a novel graph contrastive learning framework with optimal transport, equipped with tailored mechanisms for each type of heterophily. Specifically, for partial heterophily, we design a RealSoftMax-based similarity estimator to emphasize key neighbor-word interactions while easing background noise. For complete heterophily, we introduce a prompt-based filter that adaptively excludes irrelevant noise during optimal transport alignment. Furthermore, we incorporate OT-guided soft supervision to uncover potential neighbors with similar semantics, enhancing the learning of latent homophily. Theoretical analysis shows that GCL-OT can improve the mutual information bound and Bayes error guarantees. Extensive experiments on nine benchmarks show that GCL-OT consistently outperforms state-of-the-art methods, verifying its effectiveness and robustness.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16767v1" target="_blank"><h2>When Structure Doesn't Help: LLMs Do Not Read Text-Attributed Graphs as Effectively as We Expected <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Haotian Xu, Yuning You, Tengfei Ma<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Graphs provide a unified representation of semantic content and relational structure, making them a natural fit for domains such as molecular modeling, citation networks, and social graphs. Meanwhile, large language models (LLMs) have excelled at understanding natural language and integrating cross-modal signals, sparking interest in their potential for graph reasoning. Recent work has explored this by either designing template-based graph templates or using graph neural networks (GNNs) to encode structural information. In this study, we investigate how different strategies for encoding graph structure affect LLM performance on text-attributed graphs. Surprisingly, our systematic experiments reveal that: (i) LLMs leveraging only node textual descriptions already achieve strong performance across tasks; and (ii) most structural encoding strategies offer marginal or even negative gains. We show that explicit structural priors are often unnecessary and, in some cases, counterproductive when powerful language models are involved. This represents a significant departure from traditional graph learning paradigms and highlights the need to rethink how structure should be represented and utilized in the LLM era. Our study is to systematically challenge the foundational assumption that structure is inherently beneficial for LLM-based graph reasoning, opening the door to new, semantics-driven approaches for graph learning.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16755v1" target="_blank"><h2>Turbulence in Core-Collapse Supernovae <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> David Calvert, Michael Redle, Bibek Gautam, Charles J. Stapleford, Carla Fröhlich, James P. Kneller, Matthias Liebendorfer<br><strong><u>Categories:</u></strong> astro-ph.HE<br><strong><u>Comments:</u></strong> 27 pages, 16 figures. Accepted to ApJ. Data behind the figures available atthis https URL<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> VAE (title)<br><p><strong><u>Abstract:</u></strong> It is understood in a general sense that turbulent fluid motion below the shock front in a core-collapse supernova stiffens the effective equation of state of the fluid and aids in the revival of the explosion. However, when one wishes to be precise and quantify the amount of turbulence in a supernova simulation, one immediately encounters the problem that turbulence is difficult to define and measure. Using the 3D magnetohydrodynamic code ELEPHANT, we study how different definitions of turbulence change one's conclusions about the amount of turbulence in a supernova and the extent to which it helps the explosion. We find that, while all the definitions of turbulence we use lead to a qualitatively similar growth pattern over time of the turbulent kinetic energy in the gain region, the total amount of turbulent kinetic energy, and especially the ratios of turbulent to total kinetic energy, distinguish them. Some of the definitions appear to indicate turbulence is a necessary contributor to the explosion, and others indicate it is not. The different definitions also produce turbulence maps with different correlations with maps of the enstrophy, a quantity widely regarded as also indicating the presence of turbulence. We also compute the turbulent adiabatic index and observe that in regions of low enstrophy, this quantity is sensitive to the definition used. As a consequence, the effective adiabatic index depends upon the method used to measure the turbulence and thus alter one's conclusions regarding the impact of turbulence within the supernova.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16754v1" target="_blank"><h2>Data-Driven Stellar Spectral Modelling with GSPICE <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Douglas P. Finkbeiner, Joshua S. Speagle, Tanveer Karim<br><strong><u>Categories:</u></strong> astro-ph.IM<br><strong><u>Comments:</u></strong> 18 pages, 12 figures<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> data-driven (title, abstract)<br><p><strong><u>Abstract:</u></strong> Spectral data reduction pipelines deal with a wide variety of challenges including masking cosmic rays, calibrating wavelength solutions, and estimating background noise while trying to remain model-agnostic. Traditional methods rely on hardware-specific code or pre-calculated stellar model templates to solve this problem, making them model-dependent and not suitable for large datasets that may contain new classes of objects. To solve this problem, we present a flexible, data-driven method: the GausSian PIxelwise Conditional Estimator (GSPICE) that models an ensemble of spectra as a multivariate Gaussian and estimates the expected value and expected variance of each pixel in each spectrum conditional on others. GSPICE compares observed fluxes and errors to its own flux and error estimates to reveal outliers, which then can be completely masked or replaced by their estimates. We apply GSPICE to 3.9 million stellar spectra from the LAMOST survey, and show that variations of the method can directly identify and correct both individual pixel-level outliers (e.g., from cosmic ray hits) as well as extended systematic features (e.g., from incorrect wavelength calibrations), while still providing a novel characterization of the true per-pixel measurement uncertainties. We also demonstrate how GSPICE can take advantage of data partitioning with an application to diffuse interstellar bands. Implementations of GSPICE in both Python and IDL can be found here http://github.com/dfink/gspice.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16750v1" target="_blank"><h2>CLAWDIA: A dictionary learning framework for gravitational-wave data analysis <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Miquel Llorens-Monteagudo, Alejandro Torres-Forné, José A. Font<br><strong><u>Categories:</u></strong> astro-ph.IM<br><strong><u>Comments:</u></strong> 20 pages, 6 figures<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> time-domain (abstract)<br><p><strong><u>Abstract:</u></strong> Deep-learning methods are becoming increasingly important in gravitational-wave data analysis, yet their performance often relies on large training datasets and models whose internal representations are difficult to interpret. Sparse dictionary learning (SDL) offers a complementary approach: it performs well in scarce-data regimes and yields physically interpretable representations of gravitational-wave morphology. Here we present CLAWDIA (Comprehensive Library for the Analysis of Waves via Dictionary-based Algorithms), an open-source Python framework that integrates SDL-based denoising and classification under realistic detector noise. We systematise previously isolated SDL workflows into a unified, modular environment with a consistent, user-friendly interface. The current release provides several time-domain denoising strategies based on LASSO-regularised sparse coding and a classifier based on Low-Rank Shared Dictionary Learning. A companion toolbox, GWADAMA, supports dataset construction and realistic conditioning of real and simulated interferometer data. We demonstrate CLAWDIA's performance by denoising the signal from binary neutron star event GW170817 and by classifying families of instrumental glitches from LIGO's third observing run, highlighting robustness at low signal-to-noise ratios. CLAWDIA is intended as a community-driven, interoperable library extensible to additional tasks, including detection and parameter estimation.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16741v1" target="_blank"><h2>Fermions and Supersymmetry in Neural Network Field Theories <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Samuel Frank, James Halverson, Anindita Maiti, Fabian Ruehle<br><strong><u>Categories:</u></strong> hep-th, cs.LG<br><strong><u>Comments:</u></strong> 34 pages + appendices<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> We introduce fermionic neural network field theories via Grassmann-valued neural networks. Free theories are obtained by a generalization of the Central Limit Theorem to Grassmann variables. This enables the realization of the free Dirac spinor at infinite width and a four fermion interaction at finite width. Yukawa couplings are introduced by breaking the statistical independence of the output weights for the fermionic and bosonic fields. A large class of interacting supersymmetric quantum mechanics and field theory models are introduced by super-affine transformations on the input that realize a superspace formalism.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16671v1" target="_blank"><h2>Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Ziyu Guo, Renrui Zhang, Hongyu Li, Manyuan Zhang, Xinyan Chen, Sifan Wang, Yan Feng, Peng Pei, Pheng-Ann Heng<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.CL<br><strong><u>Comments:</u></strong> Project Page:this https URLCode:this https URL<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process. As visual content is progressively generating, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones. This dynamic interplay produces more context-aware and semantically rich visual outputs. To unveil the potential of this framework, we investigate three candidate strategies, zero-shot prompting, supervised fine-tuning (SFT) on our curated TwiG-50K dataset, and reinforcement learning (RL) via a customized TwiG-GRPO strategy, each offering unique insights into the dynamics of interleaved reasoning. We hope this work inspires further research into interleaving textual reasoning for enhanced visual generation. Code will be released at: https://github.com/ZiyuGuo99/Thinking-while-Generating.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16658v1" target="_blank"><h2>Prospects for Neutrino Observation and Mass Measurement from Binary Neutron Star Mergers <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Vedran Brdar, Dibya S. Chattopadhyay, Samiur R. Mir, Tousif Raza, Marc S. Romanowski<br><strong><u>Categories:</u></strong> hep-ph, astro-ph.HE, hep-ex<br><strong><u>Comments:</u></strong> 13 pages, 8 figures<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> VAE (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Over the next decade, $\mathcal{O}(100)$ diffuse supernova neutrino background (DSNB) events are expected in Hyper-Kamiokande. Another neutrino source that has received far less attention is binary neutron star mergers. Including the data from recent simulations, we find that detection in current and near-future neutrino experiments is not feasible, and a megaton-scale detector with $\mathcal{O}(10)$ MeV threshold, such as the proposed Deep-TITAND, MEMPHYS, or MICA, will be required. This is due to the updated binary neutron star merger rate and the time-of-flight delay caused by the nonzero neutrino mass. Regarding the former, recent results from LIGO, Virgo, and KAGRA has significantly lowered the upper limit on the neutron star merger rate. As for the latter, neutrino events from neutron star mergers are expected to be recorded shortly after the gravitational wave signal. Limiting the analysis to such short time windows can significantly reduce background rates. While this approach has been qualitatively discussed in the literature, the effect of the time delay caused by neutrino mass, which can substantially extend the observation windows, has been disregarded. We present a refined analysis employing energy-dependent time windows and luminosity distance cuts for the mergers and provide realistic estimates of the detector runtime required to record neutrinos from binary neutron star mergers with small background contamination. The relative timing between the neutrino and gravitational wave signals can also be employed to probe the scale of neutrino mass. We find that the sensitivity to the lightest neutrino mass exceeds both the most stringent terrestrial bounds from KATRIN and the projections based on galactic supernovae. This level of sensitivity may become particularly relevant in the future if terrestrial and supernova constraints are not significantly improved.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16717v1" target="_blank"><h2>A Machine Learning-Driven Solution for Denoising Inertial Confinement Fusion Images <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Asya Y. Akkus, Bradley T. Wolfe, Pinghan Chu, Chengkun Huang, Chris S. Campbell, Mariana Alvarado Alvarez, Petr Volegov, David Fittinghoff, Robert Reinovsky, Zhehui Wang<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> latent space (abstract)<br><p><strong><u>Abstract:</u></strong> Neutron imaging is important in optimizing analysis of inertial confinement fusion (ICF) events such as those at the National Ignition Facility (NIF) and improving current and future ICF platforms. However, images of neutron sources are often degraded by various types of noise. Most commonly, Gaussian and Poisson noise often coexist within one image, obscuring fine details and blurring edges. These noise types often overlap, making them difficult to distinguish and remove using conventional filtering and thresholding methods. As a result, noise removal techniques that preserve image fidelity are important for analyzing and interpreting images of a neutron source. Current solutions include a combination of filtering and thresholding methodologies. In the past, machine learning approaches were rarely implemented due to a lack of ground truth neutron imaging data for ICF processes. However, recent advances in synthetic data production, particularly in the fusion imaging field, have opened opportunities to investigate new denoising procedures using both supervised and unsupervised machine learning methods. In this study, we implement an unsupervised autoencoder with a Cohen-Daubechies- Feauveau (CDF 97) wavelet transform in the latent space for mixed Gaussian-Poisson denoising. The network successfully denoises neutron imaging data. Additionally, it demonstrates lower reconstruction error and superior edge preservation metrics when benchmarked with data generated by a forward model and compared to non-ML-based filtering mechanisms such as Block-matching and 3D filtering (BM3D). This approach presents a promising advancement in neutron image noise reduction and three-dimensional reconstruction analysis of ICF experiments.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16652v1" target="_blank"><h2>Evolution Strategies at the Hyperscale <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Bidipta Sarkar, Mattie Fellows, Juan Agustin Duque, Alistair Letcher, Antonio León Villares, Anya Sims, Dylan Cope, Jarek Liesen, Lukas Seier, Theo Wolf, Uljad Berdica, Alexander David Goldie, Aaron Courville, Karin Sevegnani, Shimon Whiteson, Jakob Nicolaus Foerster<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 48 pages, 12 figures, Website atthis https URL<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> We introduce Evolution Guided General Optimization via Low-rank Learning (EGGROLL), an evolution strategies (ES) algorithm designed to scale backprop-free optimization to large population sizes for modern large neural network architectures with billions of parameters. ES is a set of powerful blackbox optimisation methods that can handle non-differentiable or noisy objectives with excellent scaling potential through parallelisation. Na{ï}ve ES becomes prohibitively expensive at scale due to the computational and memory costs associated with generating matrix perturbations $E\in\mathbb{R}^{m\times n}$ and the batched matrix multiplications needed to compute per-member forward passes. EGGROLL overcomes these bottlenecks by generating random matrices $A\in \mathbb{R}^{m\times r},\ B\in \mathbb{R}^{n\times r}$ with $r\ll \min(m,n)$ to form a low-rank matrix perturbation $A B^\top$ that are used in place of the full-rank perturbation $E$. As the overall update is an average across a population of $N$ workers, this still results in a high-rank update but with significant memory and computation savings, reducing the auxiliary storage from $mn$ to $r(m+n)$ per layer and the cost of a forward pass from $\mathcal{O}(mn)$ to $\mathcal{O}(r(m+n))$ when compared to full-rank ES. A theoretical analysis reveals our low-rank update converges to the full-rank update at a fast $\mathcal{O}\left(\frac{1}{r}\right)$ rate. Our experiments show that (1) EGGROLL does not compromise the performance of ES in tabula-rasa RL settings, despite being faster, (2) it is competitive with GRPO as a technique for improving LLM reasoning, and (3) EGGROLL enables stable pre-training of nonlinear recurrent language models that operate purely in integer datatypes.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16653v1" target="_blank"><h2>Teacher-Guided One-Shot Pruning via Context-Aware Knowledge Distillation <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Md. Samiul Alim, Sharjil Khan, Amrijit Biswas, Fuad Rahman, Shafin Rahman, Nabeel Mohammed<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> Accepted at 2025 IEEE International Conference on Big Data (IEEE BigData 2025)<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Unstructured pruning remains a powerful strategy for compressing deep neural networks, yet it often demands iterative train-prune-retrain cycles, resulting in significant computational overhead. To address this challenge, we introduce a novel teacher-guided pruning framework that tightly integrates Knowledge Distillation (KD) with importance score estimation. Unlike prior approaches that apply KD as a post-pruning recovery step, our method leverages gradient signals informed by the teacher during importance score calculation to identify and retain parameters most critical for both task performance and knowledge transfer. Our method facilitates a one-shot global pruning strategy that efficiently eliminates redundant weights while preserving essential representations. After pruning, we employ sparsity-aware retraining with and without KD to recover accuracy without reactivating pruned connections. Comprehensive experiments across multiple image classification benchmarks, including CIFAR-10, CIFAR-100, and TinyImageNet, demonstrate that our method consistently achieves high sparsity levels with minimal performance degradation. Notably, our approach outperforms state-of-the-art baselines such as EPG and EPSD at high sparsity levels, while offering a more computationally efficient alternative to iterative pruning schemes like COLT. The proposed framework offers a computation-efficient, performance-preserving solution well suited for deployment in resource-constrained environments.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16631v1" target="_blank"><h2>A Core-Collapse Supernova Neutrino Parameterization with Enhanced Physical Interpretability <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Haihao Shi, Zhenyang Huang, Junda Zhou, Guoliang Lü, Xuefei Chen<br><strong><u>Categories:</u></strong> astro-ph.HE<br><strong><u>Comments:</u></strong> 48 pages, 31 figures, It has been accepted by APJS<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> We introduce a novel parameterization of supernova neutrino energy spectra with a clear physical motivation. Its central parameter, $τ(t)$, quantifies the characteristic thermal-diffusion area during the explosion. When applied to the historic SN1987A data, this parameterization yields statistically significant fits and provides robust constraints on the unobserved low-energy portion of the spectrum. Beyond this specific application, we demonstrate the model's power on a suite of 3D core-collapse supernova simulations, finding that the temporal evolution of $τ(t)$ distinctly separates successful from failed explosions. Furthermore, we constrain the progenitor mass of SN 1987A to approximately 19 solar masses by applying Smoothed Isotonic Regression, while noting the sensitivity of this estimate to observational uncertainties. Moreover, in these simulations, $τ(t)$ and the gravitational-wave strain amplitude display a strong, synergistic co-evolution, directly linking the engine's energetic evolution to its geometric asymmetry. This implies that the thermodynamic state of the explosion is imprinted not only on the escaping neutrino flux, but also recorded in the shape of the energy spectrum. Our framework therefore offers a valuable tool for decoding the detailed core dynamics and multi-messenger processes of future galactic supernovae.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16625v1" target="_blank"><h2>MedBayes-Lite: Bayesian Uncertainty Quantification for Safe Clinical Decision Support <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Elias Hossain, Md Mehedi Hasan Nipu, Maleeha Sheikh, Rajib Rana, Subash Neupane, Niloofar Yousefi<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> We propose MedBayes-Lite, a lightweight Bayesian enhancement for transformer-based clinical language models designed to produce reliable, uncertainty-aware predictions. Although transformers show strong potential for clinical decision support, they remain prone to overconfidence, especially in ambiguous medical cases where calibrated uncertainty is critical. MedBayes-Lite embeds uncertainty quantification directly into existing transformer pipelines without any retraining or architectural rewiring, adding no new trainable layers and keeping parameter overhead under 3 percent. The framework integrates three components: (i) Bayesian Embedding Calibration using Monte Carlo dropout for epistemic uncertainty, (ii) Uncertainty-Weighted Attention that marginalizes over token reliability, and (iii) Confidence-Guided Decision Shaping inspired by clinical risk minimization. Across biomedical QA and clinical prediction benchmarks (MedQA, PubMedQA, MIMIC-III), MedBayes-Lite consistently improves calibration and trustworthiness, reducing overconfidence by 32 to 48 percent. In simulated clinical settings, it can prevent up to 41 percent of diagnostic errors by flagging uncertain predictions for human review. These results demonstrate its effectiveness in enabling reliable uncertainty propagation and improving interpretability in medical AI systems.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16614v1" target="_blank"><h2>Deep Learning Framework for Enhanced Neutrino Reconstruction of Single-line Events in the ANTARES Telescope <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> A. Albert, S. Alves, M. André, M. Ardid, S. Ardid, J. -J. Aubert, J. Aublin, B. Baret, S. Basa, Y. Becherini, B. Belhorma, F. Benfenati, V. Bertin, S. Biagi, J. Boumaaza, M. Bouta, M. C. Bouwhuis, H. Brânzaş, R. Bruijn, J. Brunner, J. Busto, B. Caiffi, D. Calvo, S. Campion, A. Capone, F. Carenini, J. Carr, V. Carretero, T. Cartraud, S. Celli, L. Cerisy, M. Chabab, R. Cherkaoui El Moursli, T. Chiarusi, M. Circella, J. A. B. Coelho, A. Coleiro, R. Coniglione, P. Coyle, A. Creusot, A. F. Díaz, B. De Martino, C. Distefano, I. Di Palma, C. Donzaud, D. Dornic, D. Drouhin, T. Eberl, A. Eddymaoui, T. van Eeden, D. van Eijk, S. El Hedri, N. El Khayati, A. Enzenhöfer, P. Fermani, G. Ferrara, F. Filippini, L. Fusco, S. Gagliardini, J. García-Méndez, C. Gatius Oliver, P. Gay, N. Geißelbrecht, H. Glotin, R. Gozzini, R. Gracia Ruiz, K. Graf, C. Guidi, L. Haegel, H. van Haren, A. J. Heijboer, Y. Hello, L. Hennig, J. J. Hernández-Rey, J. Hößl, F. Huang, G. Illuminati, B. Jisse-Jung, M. de Jong, P. de Jong, M. Kadler, O. Kalekin, U. Katz, A. Kouchner, I. Kreykenbohm, V. Kulikovskiy, R. Lahmann, M. Lamoureux, A. Lazo, D. Lefèvre, E. Leonora, G. Levi, S. Le Stum, S. Loucatos, J. Manczak, M. Marcelin, A. Margiotta, A. Marinelli, J. A. Martínez-Mora, P. Migliozzi, A. Moussa, R. Muller, S. Navas, E. Nezri, B. Ó Fearraigh, E. Oukacha, A. M. Păun, G. E. Păvălaş, S. Peña-Martínez, M. Perrin-Terrin, P. Piattelli, C. Poiré, V. Popa, T. Pradier, N. Randazzo, D. Real, G. Riccobene, A. Romanov, A. Sánchez Losa, A. Saina, F. Salesa Greus, D. F. E. Samtleben, M. Sanguineti, P. Sapienza, F. Schüssler, J. Seneca, M. Spurio, Th. Stolarczyk, M. Taiuti, Y. Tayalati, B. Vallage, G. Vannoye, V. Van Elewyck, S. Viola, D. Vivolo, J. Wilms, S. Zavatarelli, A. Zegarelli, J. D. Zornoza, J. Zúñiga<br><strong><u>Categories:</u></strong> physics.comp-ph, astro-ph.IM<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract), transfer learning (abstract)<br><p><strong><u>Abstract:</u></strong> We present the $N$-fit algorithm designed to improve the reconstruction of neutrino events detected by a single line of the ANTARES underwater telescope, usually associated with low energy neutrino events ($\sim$ 100 GeV). $N$-Fit is a neural network model that relies on deep learning and combines several advanced techniques in machine learning --deep convolutional layers, mixture density output layers, and transfer learning. This framework divides the reconstruction process into two dedicated branches for each neutrino event topology --tracks and showers-- composed of sub-models for spatial estimation --direction and position-- and energy inference, which later on are combined for event classification. Regarding the direction of single-line events, the $N$-Fit algorithm significantly refines the estimation of the zenithal angle, and delivers reliable azimuthal angle predictions that were previously unattainable with traditional $χ^2$-fit methods. Improving on energy estimation of single-line events is a tall order; $N$-Fit benefits from transfer learning to efficiently integrate key characteristics, such as the estimation of the closest distance from the event to the detector. $N$-Fit also takes advantage from transfer learning in event topology classification by freezing convolutional layers of the pretrained branches. Tests on Monte Carlo simulations and data demonstrate a significant reduction in mean and median absolute errors across all reconstructed parameters. The improvements achieved by $N$-Fit highlight its potential for advancing multimessenger astrophysics and enhancing our ability to probe fundamental physics beyond the Standard Model using single-line events from ANTARES data.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16600v2" target="_blank"><h2>You Only Forward Once: An Efficient Compositional Judging Paradigm <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Tianlong Zhang, Hongwei Xue, Shilin Yan, Di Wu, Chen Xu, Yunyun Yang<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Multimodal large language models (MLLMs) show strong potential as judges. However, existing approaches face a fundamental trade-off: adapting MLLMs to output a single score misaligns with the generative nature of MLLMs and limits fine-grained requirement understanding, whereas autoregressively generating judging analyses is prohibitively slow in high-throughput settings. Observing that judgment reduces to verifying whether inputs satisfy a set of structured requirements, we propose YOFO, a template-conditioned method that judges all requirements in a single forward pass. Built on an autoregressive model, YOFO accepts a structured requirement template and, in one inference step, produces a binary yes/no decision for each requirement by reading the logits of the final token associated with that requirement. This design yields orders-of-magnitude speedups while preserving interpretability. Extensive experiments show that YOFO not only achieves state-of-the-art results on standard recommendation datasets, but also supports dependency-aware analysis -- where subsequent judgments are conditioned on previous ones -- and further benefits from post-hoc CoT.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16595v1" target="_blank"><h2>TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Boshen Xu, Zihan Xiao, Jiaze Li, Jianzhong Ju, Zhenbo Luo, Jian Luan, Qin Jin<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.CL<br><strong><u>Comments:</u></strong> Project page:this https URL<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract), transformer (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> We introduce TimeViper, a hybrid vision-language model designed to tackle challenges of long video understanding. Processing long videos demands both an efficient model architecture and an effective mechanism for handling extended temporal contexts. To this end, TimeViper adopts a hybrid Mamba-Transformer backbone that combines the efficiency of state-space models with the expressivity of attention mechanisms. Through this hybrid design, we reveal the vision-to-text information aggregation phenomenon, where information progressively flows from vision tokens to text tokens across increasing LLM depth, resulting in severe vision token redundancy. Motivated by this observation, we propose TransV, a token information transfer module that transfers and compresses vision tokens into instruction tokens while maintaining multimodal understanding capabilities. This design enables TimeViper to process hour-long videos exceeding 10,000 frames. Extensive experiments across multiple benchmarks demonstrate that TimeViper competes with state-of-the-art models while extending frame numbers. We further analyze attention behaviors of both Mamba and Transformer layers, offering new insights into hybrid model interpretability. This work represents an initial step towards developing, interpreting, and compressing hybrid Mamba-Transformer architectures.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16588v1" target="_blank"><h2>Formal Abductive Latent Explanations for Prototype-Based Networks <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Jules Soria, Zakaria Chihani, Julien Girard-Satabin, Alban Grastien, Romain Xu-Darme, Daniela Cancila<br><strong><u>Categories:</u></strong> cs.AI, cs.LO<br><strong><u>Comments:</u></strong> Accepted at AAAI-26<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> explainable (abstract)<br><p><strong><u>Abstract:</u></strong> Case-based reasoning networks are machine-learning models that make predictions based on similarity between the input and prototypical parts of training samples, called prototypes. Such models are able to explain each decision by pointing to the prototypes that contributed the most to the final outcome. As the explanation is a core part of the prediction, they are often qualified as ``interpretable by design". While promising, we show that such explanations are sometimes misleading, which hampers their usefulness in safety-critical contexts. In particular, several instances may lead to different predictions and yet have the same explanation. Drawing inspiration from the field of formal eXplainable AI (FXAI), we propose Abductive Latent Explanations (ALEs), a formalism to express sufficient conditions on the intermediate (latent) representation of the instance that imply the prediction. Our approach combines the inherent interpretability of case-based reasoning models and the guarantees provided by formal XAI. We propose a solver-free and scalable algorithm for generating ALEs based on three distinct paradigms, compare them, and present the feasibility of our approach on diverse datasets for both standard and fine-grained image classification. The associated code can be found at https://github.com/julsoria/ale</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16573v1" target="_blank"><h2>An Exterior-Embedding Neural Operator Framework for Preserving Conservation Laws <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Huanshuo Dong, Hong Wang, Hao Wu, Zhiwei Zhuang, Xuanze Yang, Ruiqi Shu, Yuan Gao, Xiaomeng Huang<br><strong><u>Categories:</u></strong> cs.OH, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Neural operators have demonstrated considerable effectiveness in accelerating the solution of time-dependent partial differential equations (PDEs) by directly learning governing physical laws from data. However, for PDEs governed by conservation laws(e.g., conservation of mass, energy, or matter), existing neural operators fail to satisfy conservation properties, which leads to degraded model performance and limited generalizability. Moreover, we observe that distinct PDE problems generally require different optimal neural network architectures. This finding underscores the inherent limitations of specialized models in generalizing across diverse problem domains.
  To address these limitations, we propose Exterior-Embedded Conservation Framework (ECF), a universal conserving framework that can be integrated with various data-driven neural operators to enforce conservation laws strictly in predictions. The framework consists of two key components: a conservation quantity encoder that extracts conserved quantities from input data, and a conservation quantity decoder that adjusts the neural operator's predictions using these quantities to ensure strict conservation compliance in the final output. Since our architecture enforces conservation laws, we theoretically prove that it enhances model performance. To validate the performance of our method, we conduct experiments on multiple conservation-law-constrained PDE scenarios, including adiabatic systems, shallow water equations, and the Allen-Cahn problem. These baselines demonstrate that our method effectively improves model accuracy while strictly enforcing conservation laws in the predictions.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16571v1" target="_blank"><h2>Boosting Predictive Performance on Tabular Data through Data Augmentation with Latent-Space Flow-Based Diffusion <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Md. Tawfique Ihsan, Md. Rakibul Hasan Rafi, Ahmed Shoyeb Raihan, Imtiaz Ahmed, Abdullahil Azeem<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 35 Pages<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> VAE (abstract), latent space (abstract), attention (abstract), data augmentation (title, abstract)<br><p><strong><u>Abstract:</u></strong> Severe class imbalance is common in real-world tabular learning, where rare but important minority classes are essential for reliable prediction. Existing generative oversampling methods such as GANs, VAEs, and diffusion models can improve minority-class performance, but they often struggle with tabular heterogeneity, training stability, and privacy concerns. We propose a family of latent-space, tree-driven diffusion methods for minority oversampling that use conditional flow matching with gradient-boosted trees as the vector-field learner. The models operate in compact latent spaces to preserve tabular structure and reduce computation. We introduce three variants: PCAForest, which uses linear PCA embedding; EmbedForest, which uses a learned nonlinear embedding; and AttentionForest, which uses an attention-augmented embedding. Each method couples a GBT-based flow with a decoder back to the original feature space. Across 11 datasets from healthcare, finance, and manufacturing, AttentionForest achieves the best average minority recall while maintaining competitive precision, calibration, and distributional similarity. PCAForest and EmbedForest reach similar utility with much faster generation, offering favorable accuracy-efficiency trade-offs. Privacy evaluated with nearest-neighbor distance ratio and distance-to-closest-record is comparable to or better than the ForestDiffusion baseline. Ablation studies show that smaller embeddings tend to improve minority recall, while aggressive learning rates harm stability. Overall, latent-space, tree-driven diffusion provides an efficient and privacy-aware approach to high-fidelity tabular data augmentation under severe class imbalance.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16566v1" target="_blank"><h2>NutriScreener: Retrieval-Augmented Multi-Pose Graph Attention Network for Malnourishment Screening <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Misaal Khan, Mayank Vatsa, Kuldeep Singh, Richa Singh<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> Accepted in AAAI 2026 Special Track on AI for Social Impact<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Child malnutrition remains a global crisis, yet existing screening methods are laborious and poorly scalable, hindering early intervention. In this work, we present NutriScreener, a retrieval-augmented, multi-pose graph attention network that combines CLIP-based visual embeddings, class-boosted knowledge retrieval, and context awareness to enable robust malnutrition detection and anthropometric prediction from children's images, simultaneously addressing generalizability and class imbalance. In a clinical study, doctors rated it 4.3/5 for accuracy and 4.6/5 for efficiency, confirming its deployment readiness in low-resource settings. Trained and tested on 2,141 children from AnthroVision and additionally evaluated on diverse cross-continent populations, including ARAN and an in-house collected CampusPose dataset, it achieves 0.79 recall, 0.82 AUC, and significantly lower anthropometric RMSEs, demonstrating reliable measurement in unconstrained pediatric settings. Cross-dataset results show up to 25% recall gain and up to 3.5 cm RMSE reduction using demographically matched knowledge bases. NutriScreener offers a scalable and accurate solution for early malnutrition detection in low-resource environments.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16554v1" target="_blank"><h2>Dark Matter-Dark Radiation Interactions and the Hubble Tension <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Manuel A. Buen-Abad, Zackaria Chacko, Ina Flood, Can Kilic, Gustavo Marques-Tavares, Taewook Youn<br><strong><u>Categories:</u></strong> astro-ph.CO, hep-ph<br><strong><u>Comments:</u></strong> 41 pages, 19 figures<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> Models in which a subcomponent of dark matter interacts with dark radiation have been proposed as a solution to the Hubble tension. In this framework, the interacting subcomponent of dark matter is in thermal equilibrium with the dark radiation in the early universe, but decouples from it around the time of matter-radiation equality. We study this general class of models and evaluate the quality of fit to recent cosmological data on the cosmic microwave background (from Planck 2018 and ACT DR6), baryon acoustic oscillations, large-scale structure, supernovae type Ia, and Cepheid variables. We focus on three benchmark scenarios that differ in the rate at which the dark matter decouples from the dark radiation, resulting in different patterns of dark acoustic oscillations. Fitting without ACT DR6 data, we find that all three scenarios significantly reduce the Hubble tension relative to $Λ$CDM, with an exponentially fast decoupling being the most preferred. The tension is reduced to less than $2 \, σ$ in fits that don't include the SH0ES collaboration results as part of the data and to less than $1 \, σ$ when these are included. When ACT DR6 data is included, the fit is significantly worsened. We find that the largest $H_0$ value at the $95 \%$ confidence region is $70.1$ km/s/Mpc without the SH0ES data, leading to only a mild reduction in the tension. This increases to $72.5$ km/s/Mpc, corresponding to a reduction in the tension to less than $3 \, σ$, if the SH0ES results are included in the fit.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16551v1" target="_blank"><h2>Toward Valid Generative Clinical Trial Data with Survival Endpoints <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Perrine Chassat, Van Tuan Nguyen, Lucas Ducrot, Emilie Lanoy, Agathe Guilloux<br><strong><u>Categories:</u></strong> cs.LG, stat.AP, stat.ME, stat.ML<br><strong><u>Comments:</u></strong> P. Chassat and V.T. Nguyen contributed equally to this work<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> variational autoencoder (abstract), VAE (abstract)<br><p><strong><u>Abstract:</u></strong> Clinical trials face mounting challenges: fragmented patient populations, slow enrollment, and unsustainable costs, particularly for late phase trials in oncology and rare diseases. While external control arms built from real-world data have been explored, a promising alternative is the generation of synthetic control arms using generative AI. A central challenge is the generation of time-to-event outcomes, which constitute primary endpoints in oncology and rare disease trials, but are difficult to model under censoring and small sample sizes. Existing generative approaches, largely GAN-based, are data-hungry, unstable, and rely on strong assumptions such as independent censoring. We introduce a variational autoencoder (VAE) that jointly generates mixed-type covariates and survival outcomes within a unified latent variable framework, without assuming independent censoring. Across synthetic and real trial datasets, we evaluate our model in two realistic scenarios: (i) data sharing under privacy constraints, where synthetic controls substitute for original data, and (ii) control-arm augmentation, where synthetic patients mitigate imbalances between treated and control groups. Our method outperforms GAN baselines on fidelity, utility, and privacy metrics, while revealing systematic miscalibration of type I error and power. We propose a post-generation selection procedure that improves calibration, highlighting both progress and open challenges for generative survival modeling.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16550v1" target="_blank"><h2>Broad stochastic configuration residual learning system for norm-convergent universal approximation <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Han Su, Zhongyan Li, Wanquan Liu<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Universal approximation serves as the foundation of neural network learning algorithms. However, some networks establish their universal approximation property by demonstrating that the iterative errors converge in probability measure rather than the more rigorous norm convergence, which makes the universal approximation property of randomized learning networks highly sensitive to random parameter selection, Broad residual learning system (BRLS), as a member of randomized learning models, also encounters this issue. We theoretically demonstrate the limitation of its universal approximation property, that is, the iterative errors do not satisfy norm convergence if the selection of random parameters is inappropriate and the convergence rate meets certain conditions. To address this issue, we propose the broad stochastic configuration residual learning system (BSCRLS) algorithm, which features a novel supervisory mechanism adaptively constraining the range settings of random parameters on the basis of BRLS framework, Furthermore, we prove the universal approximation theorem of BSCRLS based on the more stringent norm convergence. Three versions of incremental BSCRLS algorithms are presented to satisfy the application requirements of various network updates. Solar panels dust detection experiments are performed on publicly available dataset and compared with 13 deep and broad learning algorithms. Experimental results reveal the effectiveness and superiority of BSCRLS algorithms.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16543v1" target="_blank"><h2>The Oracle and The Prism: A Decoupled and Efficient Framework for Generative Recommendation Explanation <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Jiaheng Zhang, Daqiang Zhang<br><strong><u>Categories:</u></strong> cs.IR, cs.AI, cs.CL, cs.LG<br><strong><u>Comments:</u></strong> 11 pages,3 figures<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> explainable (abstract)<br><p><strong><u>Abstract:</u></strong> The integration of Large Language Models (LLMs) into explainable recommendation systems often leads to a performance-efficiency trade-off in end-to-end architectures, where joint optimization of ranking and explanation can result in suboptimal compromises. To resolve this, we propose Prism, a novel decoupled framework that rigorously separates the recommendation process into a dedicated ranking stage and an explanation generation stage.
  Inspired by knowledge distillation, Prism leverages a powerful teacher LLM (e.g., FLAN-T5-XXL) as an Oracle to produce high-fidelity explanatory knowledge. A compact, fine-tuned student model (e.g., BART-Base), the Prism, then specializes in synthesizing this knowledge into personalized explanations. This decomposition ensures that each component is optimized for its specific objective, eliminating inherent conflicts in coupled models.
  Extensive experiments on benchmark datasets demonstrate that our 140M-parameter Prism model significantly outperforms its 11B-parameter teacher in human evaluations of faithfulness and personalization, while achieving a 24 times speedup and a 10 times reduction in memory consumption during inference. These results validate that decoupling, coupled with targeted distillation, provides an efficient and effective pathway to high-quality explainable recommendation.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16527v1" target="_blank"><h2>Contrastive vision-language learning with paraphrasing and negation <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Kwun Ho Ngan, Saman Sadeghi Afgeh, Joe Townsend, Artur d'Avila Garcez<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> latent space (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Contrastive vision-language models continue to be the dominant approach for image and text retrieval. Contrastive Language-Image Pre-training (CLIP) trains two neural networks in contrastive manner to align their image and text embeddings in a shared latent space. Recent results evaluating CLIP on negated or paraphrased text have shown mixed performance because negation changes meaning radically with minimal lexical changes, while paraphrasing can create very different textual expressions with the same intended meaning. This poses a significant challenge for improving the evaluation results and alignment of vision-language models. To address this challenge, this paper evaluates the combination of paraphrasing and negation, proposes a new CLIP contrastive loss function accounting for both paraphrasing and negation, and applies LLM-generated training triples consisting of original, paraphrased and negated textual captions to CLIP-like training models. The approach, called SemCLIP, is shown to move paraphrased captions towards the original image embeddings while pushing negated captions further away in embedding space. Empirically, SemCLIP is shown to be capable of preserving CLIP's performance while increasing considerably the distances to negated captions. On the CC-Neg benchmark using an original over negation image-retrieval accuracy metric, SemCLIP improves accuracy from 68.1% to 78.1%. Although results are mixed when compared with CLIP on the Sugarcrepe++ benchmark, SemCLIP's performance is generally better than the models trained with negated captions. This robustness to negation extends to downstream zero-shot classification tasks where SemCLIP pre-trained on Sugarcrepe++ performs better than CLIP on all tested downstream tasks. These results indicate that SemCLIP can achieve significant robustness to semantic transformations.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16506v1" target="_blank"><h2>Two-beam Multiparticle Many-body simulations of Inhomogeneous FFI <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Zoha Laraib, Sherwood Richers<br><strong><u>Categories:</u></strong> astro-ph.HE<br><strong><u>Comments:</u></strong> 15 pages, 14 figures<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> Neutrino flavor evolution in dense astrophysical environments is inherently nonlinear and sensitive to many-body (MB) quantum effects beyond the mean-field (MF) approximation. Existing MB studies are constrained by small system sizes, closed boundaries, and highly idealized symmetry assumptions. We present a unified tensor-network framework that enables simulations of inhomogeneous and anisotropic flavor evolution under conditions relevant to core-collapse supernovae and neutron-star mergers. Within this framework, we examine the effects of inhomogeneity, boundary conditions, and convergence with resolution for multiple neutrino distributions, allowing direct comparison of these setups under one consistent formulation. In our simulations, many-body systems equilibrate earlier than their mean-field counterparts while approaching similar final flavor states. Enlarging the interaction region allows open boundaries to reproduce closed-system behavior, but only when the beams begin superimposed and interact continuously. By contrast, initially separated configurations develop entanglement more slowly, interact over longer times, and equilibrate to a flavor content that differs from that obtained from initially superimposed calculations.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16501v1" target="_blank"><h2>ODE-ViT: Plug & Play Attention Layer from the Generalization of the ViT as an Ordinary Differential Equation <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Carlos Boned Riera, David Romero Sanchez, Oriol Ramos Terrades<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), transformer (abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> In recent years, increasingly large models have achieved outstanding performance across CV tasks. However, these models demand substantial computational resources and storage, and their growing complexity limits our understanding of how they make decisions. Most of these architectures rely on the attention mechanism within Transformer-based designs. Building upon the connection between residual neural networks and ordinary differential equations (ODEs), we introduce ODE-ViT, a Vision Transformer reformulated as an ODE system that satisfies the conditions for well-posed and stable dynamics. Experiments on CIFAR-10 and CIFAR-100 demonstrate that ODE-ViT achieves stable, interpretable, and competitive performance with up to one order of magnitude fewer parameters, surpassing prior ODE-based Transformer approaches in classification tasks. We further propose a plug-and-play teacher-student framework in which a discrete ViT guides the continuous trajectory of ODE-ViT by treating the intermediate representations of the teacher as solutions of the ODE. This strategy improves performance by more than 10% compared to training a free ODE-ViT from scratch.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16494v1" target="_blank"><h2>Physics-Informed Machine Learning for Efficient Sim-to-Real Data Augmentation in Micro-Object Pose Estimation <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Zongcai Tan, Lan Wei, Dandan Zhang<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> data augmentation (title, abstract)<br><p><strong><u>Abstract:</u></strong> Precise pose estimation of optical microrobots is essential for enabling high-precision object tracking and autonomous biological studies. However, current methods rely heavily on large, high-quality microscope image datasets, which are difficult and costly to acquire due to the complexity of microrobot fabrication and the labour-intensive labelling. Digital twin systems offer a promising path for sim-to-real data augmentation, yet existing techniques struggle to replicate complex optical microscopy phenomena, such as diffraction artifacts and depth-dependent imaging.This work proposes a novel physics-informed deep generative learning framework that, for the first time, integrates wave optics-based physical rendering and depth alignment into a generative adversarial network (GAN), to synthesise high-fidelity microscope images for microrobot pose estimation efficiently. Our method improves the structural similarity index (SSIM) by 35.6% compared to purely AI-driven methods, while maintaining real-time rendering speeds (0.022 s/frame).The pose estimator (CNN backbone) trained on our synthetic data achieves 93.9%/91.9% (pitch/roll) accuracy, just 5.0%/5.4% (pitch/roll) below that of an estimator trained exclusively on real data. Furthermore, our framework generalises to unseen poses, enabling data augmentation and robust pose estimation for novel microrobot configurations without additional training data.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16482v1" target="_blank"><h2>Correlation-Aware Feature Attribution Based Explainable AI <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Poushali Sengupta, Yan Zhang, Frank Eliassen, Sabita Maharjan<br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML<br><strong><u>Comments:</u></strong> Accepted, 2026 International Conference on Advances in Artificial Intelligence and Machine Learning (AAIML 2026)<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), explainability (abstract), explainable (title, abstract)<br><p><strong><u>Abstract:</u></strong> Explainable AI (XAI) is increasingly essential as modern models become more complex and high-stakes applications demand transparency, trust, and regulatory compliance. Existing global attribution methods often incur high computational costs, lack stability under correlated inputs, and fail to scale efficiently to large or heterogeneous datasets. We address these gaps with \emph{ExCIR} (Explainability through Correlation Impact Ratio), a correlation-aware attribution score equipped with a lightweight transfer protocol that reproduces full-model rankings using only a fraction of the data. ExCIR quantifies sign-aligned co-movement between features and model outputs after \emph{robust centering} (subtracting a robust location estimate, e.g., median or mid-mean, from features and outputs). We further introduce \textsc{BlockCIR}, a \emph{groupwise} extension of ExCIR that scores \emph{sets} of correlated features as a single unit. By aggregating the same signed-co-movement numerators and magnitudes over predefined or data-driven groups, \textsc{BlockCIR} mitigates double-counting in collinear clusters (e.g., synonyms or duplicated sensors) and yields smoother, more stable rankings when strong dependencies are present. Across diverse text, tabular, signal, and image datasets, ExCIR shows trustworthy agreement with established global baselines and the full model, delivers consistent top-$k$ rankings across settings, and reduces runtime via lightweight evaluation on a subset of rows. Overall, ExCIR provides \emph{computationally efficient}, \emph{consistent}, and \emph{scalable} explainability for real-world deployment.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16475v1" target="_blank"><h2>A Comparison Between Decision Transformers and Traditional Offline Reinforcement Learning Algorithms <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Ali Murtaza Caunhye, Asad Jeewa<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 15 pages, 4 figures, published in the Proceedings of the 46th Annual conference of the South African Institute of Computer Scientists and Information Technologists (SIACSIT 2025)<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> The field of Offline Reinforcement Learning (RL) aims to derive effective policies from pre-collected datasets without active environment interaction. While traditional offline RL algorithms like Conservative Q-Learning (CQL) and Implicit Q-Learning (IQL) have shown promise, they often face challenges in balancing exploration and exploitation, especially in environments with varying reward densities. The recently proposed Decision Transformer (DT) approach, which reframes offline RL as a sequence modelling problem, has demonstrated impressive results across various benchmarks. This paper presents a comparative study evaluating the performance of DT against traditional offline RL algorithms in dense and sparse reward settings for the ANT continous control environment. Our research investigates how these algorithms perform when faced with different reward structures, examining their ability to learn effective policies and generalize across varying levels of feedback. Through empirical analysis in the ANT environment, we found that DTs showed less sensitivity to varying reward density compared to other methods and particularly excelled with medium-expert datasets in sparse reward scenarios. In contrast, traditional value-based methods like IQL showed improved performance in dense reward settings with high-quality data, while CQL offered balanced performance across different data qualities. Additionally, DTs exhibited lower variance in performance but required significantly more computational resources compared to traditional approaches. These findings suggest that sequence modelling approaches may be more suitable for scenarios with uncertain reward structures or mixed-quality data, while value-based methods remain competitive in settings with dense rewards and high-quality demonstrations.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16468v1" target="_blank"><h2>Optimizing Quantum Key Distribution Network Performance using Graph Neural Networks <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Akshit Pramod Anchan, Ameiy Acharya, Leki Chom Thungon<br><strong><u>Categories:</u></strong> quant-ph, cs.CR, cs.LG, cs.NI<br><strong><u>Comments:</u></strong> 11 pages, 4 figures, and 2 tables<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> This paper proposes an optimization of Quantum Key Distribution (QKD) Networks using Graph Neural Networks (GNN) framework. Today, the development of quantum computers threatens the security systems of classical cryptography. Moreover, as QKD networks are designed for protecting secret communication, they suffer from multiple operational difficulties: adaptive to dynamic conditions, optimization for multiple parameters and effective resource utilization. In order to overcome these obstacles, we propose a GNN-based framework which can model QKD networks as dynamic graphs and extracts exploitable characteristics from these networks' structure. The graph contains not only topological information but also specific characteristics associated with quantum communication (the number of edges between nodes, etc). Experimental results demonstrate that the GNN-optimized QKD network achieves a substantial increase in total key rate (from 27.1 Kbits/s to 470 Kbits/s), a reduced average QBER (from 6.6% to 6.0%), and maintains path integrity with a slight reduction in average transmission distance (from 7.13 km to 6.42 km). Furthermore, we analyze network performance across varying scales (10 to 250 nodes), showing improved link prediction accuracy and enhanced key generation rate in medium-sized networks. This work introduces a novel operation mode for QKD networks, shifting the paradigm of network optimization through adaptive and scalable quantum communication systems that enhance security and performance.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16467v1" target="_blank"><h2>Anatomy of an Idiom: Tracing Non-Compositionality in Language Models <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Andrew Gomes<br><strong><u>Categories:</u></strong> cs.CL, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> We investigate the processing of idiomatic expressions in transformer-based language models using a novel set of techniques for circuit discovery and analysis. First discovering circuits via a modified path patching algorithm, we find that idiom processing exhibits distinct computational patterns. We identify and investigate ``Idiom Heads,'' attention heads that frequently activate across different idioms, as well as enhanced attention between idiom tokens due to earlier processing, which we term ``augmented reception.'' We analyze these phenomena and the general features of the discovered circuits as mechanisms by which transformers balance computational efficiency and robustness. Finally, these findings provide insights into how transformers handle non-compositional language and suggest pathways for understanding the processing of more complex grammatical constructions.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16449v2" target="_blank"><h2>VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Ziyan Liu, Yeqiu Chen, Hongyi Cai, Tao Lin, Shuo Yang, Zheng Liu, Bo Zhao<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Vision-Language-Action (VLA) models have shown great promise for embodied AI, yet the heavy computational cost of processing continuous visual streams severely limits their real-time deployment. Token pruning (keeping salient visual tokens and dropping redundant ones) has emerged as an effective approach for accelerating Vision-Language Models (VLMs), offering a solution for efficient VLA. However, these VLM-specific token pruning methods select tokens based solely on semantic salience metrics (e.g., prefill attention), while overlooking the VLA's intrinsic dual-system nature of high-level semantic understanding and low-level action execution. Consequently, these methods bias token retention toward semantic cues, discard critical information for action generation, and significantly degrade VLA performance. To bridge this gap, we propose VLA-Pruner, a versatile plug-and-play VLA-specific token prune method that aligns with the dual-system nature of VLA models and exploits the temporal continuity in robot manipulation. Specifically, VLA-Pruner adopts a dual-level importance criterion for visual token retention: vision-language prefill attention for semantic-level relevance and action decode attention, estimated via temporal smoothing, for action-level importance. Based on this criterion, VLA-Pruner proposes a novel dual-level token selection strategy that adaptively preserves a compact, informative set of visual tokens for both semantic understanding and action execution under given compute budget. Experiments show that VLA-Pruner achieves state-of-the-art performance across multiple VLA architectures and diverse robotic tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16445v1" target="_blank"><h2>PersonaDrift: A Benchmark for Temporal Anomaly Detection in Language-Based Dementia Monitoring <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Joy Lai, Alex Mihailidis<br><strong><u>Categories:</u></strong> cs.AI, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> anomaly detection (title, abstract)<br><p><strong><u>Abstract:</u></strong> People living with dementia (PLwD) often show gradual shifts in how they communicate, becoming less expressive, more repetitive, or drifting off-topic in subtle ways. While caregivers may notice these changes informally, most computational tools are not designed to track such behavioral drift over time. This paper introduces PersonaDrift, a synthetic benchmark designed to evaluate machine learning and statistical methods for detecting progressive changes in daily communication, focusing on user responses to a digital reminder system. PersonaDrift simulates 60-day interaction logs for synthetic users modeled after real PLwD, based on interviews with caregivers. These caregiver-informed personas vary in tone, modality, and communication habits, enabling realistic diversity in behavior. The benchmark focuses on two forms of longitudinal change that caregivers highlighted as particularly salient: flattened sentiment (reduced emotional tone and verbosity) and off-topic replies (semantic drift). These changes are injected progressively at different rates to emulate naturalistic cognitive trajectories, and the framework is designed to be extensible to additional behaviors in future use cases. To explore this novel application space, we evaluate several anomaly detection approaches, unsupervised statistical methods (CUSUM, EWMA, One-Class SVM), sequence models using contextual embeddings (GRU + BERT), and supervised classifiers in both generalized and personalized settings. Preliminary results show that flattened sentiment can often be detected with simple statistical models in users with low baseline variability, while detecting semantic drift requires temporal modeling and personalized baselines. Across both tasks, personalized classifiers consistently outperform generalized ones, highlighting the importance of individual behavioral context.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16432v1" target="_blank"><h2>From generative AI to the brain: five takeaways <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Claudius Gros<br><strong><u>Categories:</u></strong> cs.AI, q-bio.NC<br><strong><u>Comments:</u></strong> Frontiers in Computational Neuroscience, in press<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> The big strides seen in generative AI are not based on somewhat obscure algorithms, but due to clearly defined generative principles. The resulting concrete implementations have proven themselves in large numbers of applications. We suggest that it is imperative to thoroughly investigate which of these generative principles may be operative also in the brain, and hence relevant for cognitive neuroscience. In addition, ML research led to a range of interesting characterizations of neural information processing systems. We discuss five examples, the shortcomings of world modelling, the generation of thought processes, attention, neural scaling laws, and quantization, that illustrate how much neuroscience could potentially learn from ML research.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16430v1" target="_blank"><h2>Graph Neural Networks for Surgical Scene Segmentation <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Yihan Li, Nikhil Churamani, Maria Robu, Imanol Luengo, Danail Stoyanov<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> 12 pages, 4 figures, 3 tables<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (title, abstract), transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Purpose: Accurate identification of hepatocystic anatomy is critical to preventing surgical complications during laparoscopic cholecystectomy. Deep learning models often struggle with occlusions, long-range dependencies, and capturing the fine-scale geometry of rare structures. This work addresses these challenges by introducing graph-based segmentation approaches that enhance spatial and semantic understanding in surgical scene analyses.
  Methods: We propose two segmentation models integrating Vision Transformer (ViT) feature encoders with Graph Neural Networks (GNNs) to explicitly model spatial relationships between anatomical regions. (1) A static k Nearest Neighbours (k-NN) graph with a Graph Convolutional Network with Initial Residual and Identity Mapping (GCNII) enables stable long-range information propagation. (2) A dynamic Differentiable Graph Generator (DGG) with a Graph Attention Network (GAT) supports adaptive topology learning. Both models are evaluated on the Endoscapes-Seg50 and CholecSeg8k benchmarks.
  Results: The proposed approaches achieve up to 7-8% improvement in Mean Intersection over Union (mIoU) and 6% improvement in Mean Dice (mDice) scores over state-of-the-art baselines. It produces anatomically coherent predictions, particularly on thin, rare and safety-critical structures.
  Conclusion: The proposed graph-based segmentation methods enhance both performance and anatomical consistency in surgical scene segmentation. By combining ViT-based global context with graph-based relational reasoning, the models improve interpretability and reliability, paving the way for safer laparoscopic and robot-assisted surgery through a precise identification of critical anatomical features.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16423v1" target="_blank"><h2>TOFA: Training-Free One-Shot Federated Adaptation for Vision-Language Models <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Li Zhang, Zhongxuan Han, XiaoHua Feng, Jiaming Zhang, Yuyuan Li, Linbo Jiang, Jianan Lin, Chaochao Chen<br><strong><u>Categories:</u></strong> cs.AI, cs.CL<br><strong><u>Comments:</u></strong> Accepted by AAAI 2026<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Efficient and lightweight adaptation of pre-trained Vision-Language Models (VLMs) to downstream tasks through collaborative interactions between local clients and a central server is a rapidly emerging research topic in federated learning. Existing adaptation algorithms are typically trained iteratively, which incur significant communication costs and increase the susceptibility to potential attacks. Motivated by the one-shot federated training techniques that reduce client-server exchanges to a single round, developing a lightweight one-shot federated VLM adaptation method to alleviate these issues is particularly attractive. However, current one-shot approaches face certain challenges in adapting VLMs within federated settings: (1) insufficient exploitation of the rich multimodal information inherent in VLMs; (2) lack of specialized adaptation strategies to systematically handle the severe data heterogeneity; and (3) requiring additional training resource of clients or server. To bridge these gaps, we propose a novel Training-free One-shot Federated Adaptation framework for VLMs, named TOFA. To fully leverage the generalizable multimodal features in pre-trained VLMs, TOFA employs both visual and textual pipelines to extract task-relevant representations. In the visual pipeline, a hierarchical Bayesian model learns personalized, class-specific prototype distributions. For the textual pipeline, TOFA evaluates and globally aligns the generated local text prompts for robustness. An adaptive weight calibration mechanism is also introduced to combine predictions from both modalities, balancing personalization and robustness to handle data heterogeneity. Our method is training-free, not relying on additional training resources on either the client or server side. Extensive experiments across 9 datasets in various federated settings demonstrate the effectiveness of the proposed TOFA method.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16417v1" target="_blank"><h2>Pharos-ESG: A Framework for Multimodal Parsing, Contextual Narration, and Hierarchical Labeling of ESG Report <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Yan Chen, Yu Zou, Jialei Zeng, Haoran You, Xiaorui Zhou, Aixi Zhong<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> Accepted to AAAI 26:main technical track Oral<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Environmental, Social, and Governance (ESG) principles are reshaping the foundations of global financial gover- nance, transforming capital allocation architectures, regu- latory frameworks, and systemic risk coordination mecha- nisms. However, as the core medium for assessing corpo- rate ESG performance, the ESG reports present significant challenges for large-scale understanding, due to chaotic read- ing order from slide-like irregular layouts and implicit hier- archies arising from lengthy, weakly structured content. To address these challenges, we propose Pharos-ESG, a uni- fied framework that transforms ESG reports into structured representations through multimodal parsing, contextual nar- ration, and hierarchical labeling. It integrates a reading-order modeling module based on layout flow, hierarchy-aware seg- mentation guided by table-of-contents anchors, and a multi- modal aggregation pipeline that contextually transforms vi- sual elements into coherent natural language. The framework further enriches its outputs with ESG, GRI, and sentiment labels, yielding annotations aligned with the analytical de- mands of financial research. Extensive experiments on anno- tated benchmarks demonstrate that Pharos-ESG consistently outperforms both dedicated document parsing systems and general-purpose multimodal models. In addition, we release Aurora-ESG, the first large-scale public dataset of ESG re- ports, spanning Mainland China, Hong Kong, and U.S. mar- kets, featuring unified structured representations of multi- modal content, enriched with fine-grained layout and seman- tic annotations to better support ESG integration in financial governance and decision-making.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16374v1" target="_blank"><h2>Unsupervised Graph Neural Network Framework for Balanced Multipatterning in Advanced Electronic Design Automation Layouts <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Abdelrahman Helaly, Nourhan Sakr, Kareem Madkour, Ilhami Torunoglu<br><strong><u>Categories:</u></strong> cs.AR, cs.LG<br><strong><u>Comments:</u></strong> manuscript under review<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (title)<br><p><strong><u>Abstract:</u></strong> Multipatterning is an essential decomposition strategy in electronic design automation (EDA) that overcomes lithographic limitations when printing dense circuit layouts. Although heuristic-based backtracking and SAT solvers can address these challenges, they often struggle to simultaneously handle both complex constraints and secondary objectives. In this study, we present a hybrid workflow that casts multipatterning as a variant of a constrained graph coloring problem with the primary objective of minimizing feature violations and a secondary objective of balancing the number of features on each mask. Our pipeline integrates two main components: (1) A GNN-based agent, trained in an unsupervised manner to generate initial color predictions, which are refined by (2) refinement strategies (a GNN-based heuristic and simulated annealing) that together enhance solution quality and balance. Experimental evaluation in both proprietary data sets and publicly available open source layouts demonstrate complete conflict-free decomposition and consistent color balancing. The proposed framework provides a reproducible, data-efficient and deployable baseline for scalable layout decomposition in EDA workflows.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16353v1" target="_blank"><h2>Learning from Sufficient Rationales: Analysing the Relationship Between Explanation Faithfulness and Token-level Regularisation Strategies <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Jonathan Kamp, Lisa Beinborn, Antske Fokkens<br><strong><u>Categories:</u></strong> cs.CL, cs.AI<br><strong><u>Comments:</u></strong> Long paper accepted to the main conference of AACL 2025. Please cite the conference proceedings when available<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Human explanations of natural language, rationales, form a tool to assess whether models learn a label for the right reasons or rely on dataset-specific shortcuts. Sufficiency is a common metric for estimating the informativeness of rationales, but it provides limited insight into the effects of rationale information on model performance. We address this limitation by relating sufficiency to two modelling paradigms: the ability of models to identify which tokens are part of the rationale (through token classification) and the ability of improving model performance by incorporating rationales in the input (through attention regularisation). We find that highly informative rationales are not likely to help classify the instance correctly. Sufficiency conversely captures the classification impact of the non-rationalised context, which interferes with rationale information in the same input. We also find that incorporating rationale information in model inputs can boost cross-domain classification, but results are inconsistent per task and model type. Finally, sufficiency and token classification appear to be unrelated. These results exemplify the complexity of rationales, showing that metrics capable of systematically capturing this type of information merit further investigation.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16346v1" target="_blank"><h2>VersaPants: A Loose-Fitting Textile Capacitive Sensing System for Lower-Body Motion Capture <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Deniz Kasap, Taraneh Aminosharieh Najafi, Jérôme Paul Rémy Thevenot, Jonathan Dan, Stefano Albini, David Atienza<br><strong><u>Categories:</u></strong> eess.SP, cs.LG, eess.SY<br><strong><u>Comments:</u></strong> 14 pages, 8 figures<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> We present VersaPants, the first loose-fitting, textile-based capacitive sensing system for lower-body motion capture, built on the open-hardware VersaSens platform. By integrating conductive textile patches and a compact acquisition unit into a pair of pants, the system reconstructs lower-body pose without compromising comfort. Unlike IMU-based systems that require user-specific fitting or camera-based methods that compromise privacy, our approach operates without fitting adjustments and preserves user privacy. VersaPants is a custom-designed smart garment featuring 6 capacitive channels per leg. We employ a lightweight Transformer-based deep learning model that maps capacitance signals to joint angles, enabling embedded implementation on edge platforms. To test our system, we collected approximately 3.7 hours of motion data from 11 participants performing 16 daily and exercise-based movements. The model achieves a mean per-joint position error (MPJPE) of 11.96 cm and a mean per-joint angle error (MPJAE) of 12.3 degrees across the hip, knee, and ankle joints, indicating the model's ability to generalize to unseen users and movements. A comparative analysis of existing textile-based deep learning architectures reveals that our model achieves competitive reconstruction performance with up to 22 times fewer parameters and 18 times fewer FLOPs, enabling real-time inference at 42 FPS on a commercial smartwatch without quantization. These results position VersaPants as a promising step toward scalable, comfortable, and embedded motion-capture solutions for fitness, healthcare, and wellbeing applications.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16334v1" target="_blank"><h2>OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Kaichen Zhang, Keming Wu, Zuhao Yang, Kairui Hu, Bin Wang, Ziwei Liu, Xingxuan Li, Lidong Bing<br><strong><u>Categories:</u></strong> cs.AI, cs.CL<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Recent advancements in large reasoning models have fueled growing interest in extending such capabilities to multimodal domains. However, despite notable progress in visual reasoning, the lack of transparent and reproducible data curation and training strategies remains a major barrier to scalable research. In this work, we introduce OpenMMReasoner, a fully transparent two-stage recipe for multimodal reasoning spanning supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we construct an 874K-sample cold-start dataset with rigorous step-by-step validation, providing a strong foundation for reasoning capabilities. The subsequent RL stage leverages a 74K-sample dataset across diverse domains to further sharpen and stabilize these abilities, resulting in a more robust and efficient learning process. Extensive evaluations demonstrate that our training recipe not only surpasses strong baselines but also highlights the critical role of data quality and training design in shaping multimodal reasoning performance. Notably, our method achieves a 11.6% improvement over the Qwen2.5-VL-7B-Instruct baseline across nine multimodal reasoning benchmarks, establishing a solid empirical foundation for future large-scale multimodal reasoning research. We open-sourced all our codes, pipeline, and data at https://github.com/EvolvingLMMs-Lab/OpenMMReasoner.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16333v1" target="_blank"><h2>Beyond Generative AI: World Models for Clinical Prediction, Counterfactuals, and Planning <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Mohammad Areeb Qazi, Maryam Nadeem, Mohammad Yaqub<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 2 Figures, 1 Table<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> VAE (abstract), multimodal (abstract), transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Healthcare requires AI that is predictive, reliable, and data-efficient. However, recent generative models lack physical foundation and temporal reasoning required for clinical decision support. As scaling language models show diminishing returns for grounded clinical reasoning, world models are gaining traction because they learn multimodal, temporally coherent, and action-conditioned representations that reflect the physical and causal structure of care. This paper reviews World Models for healthcare systems that learn predictive dynamics to enable multistep rollouts, counterfactual evaluation and planning. We survey recent work across three domains: (i) medical imaging and diagnostics (e.g., longitudinal tumor simulation, projection-transition modeling, and Joint Embedding Predictive Architecture i.e., JEPA-style predictive representation learning), (ii) disease progression modeling from electronic health records (generative event forecasting at scale), and (iii) robotic surgery and surgical planning (action-conditioned guidance and control). We also introduce a capability rubric: L1 temporal prediction, L2 action-conditioned prediction, L3 counterfactual rollouts for decision support, and L4 planning/control. Most reviewed systems achieve L1--L2, with fewer instances of L3 and rare L4. We identify cross-cutting gaps that limit clinical reliability; under-specified action spaces and safety constraints, weak interventional validation, incomplete multimodal state construction, and limited trajectory-level uncertainty calibration. This review outlines a research agenda for clinically robust prediction-first world models that integrate generative backbones (transformers, diffusion, VAE) with causal/mechanical foundation for safe decision support in healthcare.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16314v1" target="_blank"><h2>Thermal equilibrium curves of accretion disks driven by magnetorotational instability <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Shigenobu Hirose<br><strong><u>Categories:</u></strong> astro-ph.HE, astro-ph.EP, astro-ph.SR<br><strong><u>Comments:</u></strong> This paper is based on an invited talk presented at the 87th Fujihara Seminar: The 50th Anniversary Workshop of the Disk Instability Model in Compact Binary Stars (DIM50TH2025), held on 22--26 September 2025 in Tomakomai, Japan. It has been accepted for publication in PoS (Proceedings of Science), and is scheduled to appear in February 2026 atthis https URL<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> Analogous to the HR diagram for stars, the thermal equilibrium curve encodes the thermodynamics of accretion disks by expressing the local balance between heating -- primarily via viscous dissipation -- and cooling -- typically through radiative transfer. These curves are commonly plotted as surface density versus effective temperature. When an S-shaped locus appears, local annuli become bistable, and limit-cycle oscillations arise when the external mass-transfer rate falls within an unstable band. This behavior underpins the disk instability model for recurring outbursts in cataclysmic variables. This paper reviews first-principles thermal equilibrium curves for accretion disks driven by magnetorotational instability (MRI), with emphasis on dwarf novae. Unlike the parameterized $α$-viscosity approach, the curves are obtained by solving the governing equations with radiation magnetohydrodynamics simulations, thereby reproducing S-shaped loci without prescribing $α$. The disk instability in dwarf-nova systems and the physical origin of angular-momentum transport (shear stresses) are also briefly reviewed. Notes on the stability of radiation-dominated accretion flows are included in the Appendix.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16256v1" target="_blank"><h2>Accelerating Reionization Constraints: An ANN-Emulator Framework for the SCRIPT Semi-numerical Model <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Saptarshi Sarkar, Tirthankar Roy Choudhury<br><strong><u>Categories:</u></strong> astro-ph.IM, astro-ph.CO<br><strong><u>Comments:</u></strong> 22 pages, 5 figures. To be submitted to JCAP<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Constraining the Epoch of Reionization (EoR) with physically motivated simulations is hampered by the high cost of conventional parameter inference. We present an efficient emulator-based framework that dramatically reduces this bottleneck for the photon-conserving semi-numerical code SCRIPT. Our approach combines (i) a reliable coarse-resolution MCMC to locate the high-likelihood region (exploiting the large-scale convergence of SCRIPT) with (ii) an adaptive, targeted sampling strategy to build a compact high-resolution training set for an artificial neural network based emulator of the model likelihood. With only $\approx 10^3$ high-resolution simulations, the trained emulators achieve excellent predictive accuracy ($R^2 \approx 0.97$--$0.99$) and, when embedded within an MCMC framework, reproduce posterior distributions from full high-resolution runs. Compared to conventional MCMC, our pipeline reduces the number of expensive simulations by a factor of $\sim 100$ and lowers total CPU cost by up to a factor of $\sim 70$, while retaining statistical fidelity. This computational speedup makes inference in much higher-dimensional models tractable (e.g., those needed to incorporate JWST and upcoming 21 cm datasets) and provides a general strategy for building efficient emulators for next generation of EoR constraints.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16248v1" target="_blank"><h2>Revisiting Fairness-aware Interactive Recommendation: Item Lifecycle as a Control Knob <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Yun Lu, Xiaoyu Shi, Hong Xie, Chongjun Xia, Zhenhui Gong, Mingsheng Shang<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> 8 pages, 5 figures, conference<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> This paper revisits fairness-aware interactive recommendation (e.g., TikTok, KuaiShou) by introducing a novel control knob, i.e., the lifecycle of items. We make threefold contributions. First, we conduct a comprehensive empirical analysis and uncover that item lifecycles in short-video platforms follow a compressed three-phase pattern, i.e., rapid growth, transient stability, and sharp decay, which significantly deviates from the classical four-stage model (introduction, growth, maturity, decline). Second, we introduce LHRL, a lifecycle-aware hierarchical reinforcement learning framework that dynamically harmonizes fairness and accuracy by leveraging phase-specific exposure dynamics. LHRL consists of two key components: (1) PhaseFormer, a lightweight encoder combining STL decomposition and attention mechanisms for robust phase detection; (2) a two-level HRL agent, where the high-level policy imposes phase-aware fairness constraints, and the low-level policy optimizes immediate user engagement. This decoupled optimization allows for effective reconciliation between long-term equity and short-term utility. Third, experiments on multiple real-world interactive recommendation datasets demonstrate that LHRL significantly improves both fairness and user engagement. Furthermore, the integration of lifecycle-aware rewards into existing RL-based models consistently yields performance gains, highlighting the generalizability and practical value of our approach.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16231v1" target="_blank"><h2>Pass@k Metric for RLVR: A Diagnostic Tool of Exploration, But Not an Objective <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Yang Yu<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> The ability of Large Language Models (LLMs) to perform complex, multi-step reasoning is a central focus of modern AI research. To evaluate and enhance this capability, the pass@k metric, which measures the probability of obtaining at least one correct solution in k independent samples, has received significant attention. Its intuitive appeal has led to its adoption not only as an evaluation standard but also as a direct optimization objective in reinforcement learning. In this paper, we analyze the pass@k objective, derive its gradient, and demonstrate that it is fundamentally a per-example positive reweighting of the simpler pass@1 objective. Our analysis reveals that the pass@k objective provides a vanishing learning signal in regimes where exploration is most critical. We further analyze the dynamics of "exploration collapse", showing that as the policy concentrates probability mass, the gap between pass@k and pass@1 diminishes. We conclude that while pass@k is a useful diagnostic tool, it may be an unsuitable direct objective for optimization. Instead, mechanisms explicitly encouraging efficient exploration could offer a more effective path forward for reinforcement learning in reasoning tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16229v1" target="_blank"><h2>Q-MLLM: Vector Quantization for Robust Multimodal Large Language Model Security <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Wei Zhao, Zhe Li, Yige Li, Jun Sun<br><strong><u>Categories:</u></strong> cs.CR, cs.AI<br><strong><u>Comments:</u></strong> Accepted by NDSS 2026<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities in cross-modal understanding, but remain vulnerable to adversarial attacks through visual inputs despite robust textual safety mechanisms. These vulnerabilities arise from two core weaknesses: the continuous nature of visual representations, which allows for gradient-based attacks, and the inadequate transfer of text-based safety mechanisms to visual content. We introduce Q-MLLM, a novel architecture that integrates two-level vector quantization to create a discrete bottleneck against adversarial attacks while preserving multimodal reasoning capabilities. By discretizing visual representations at both pixel-patch and semantic levels, Q-MLLM blocks attack pathways and bridges the cross-modal safety alignment gap. Our two-stage training methodology ensures robust learning while maintaining model utility. Experiments demonstrate that Q-MLLM achieves significantly better defense success rate against both jailbreak attacks and toxic image attacks than existing approaches. Notably, Q-MLLM achieves perfect defense success rate (100\%) against jailbreak attacks except in one arguable case, while maintaining competitive performance on multiple utility benchmarks with minimal inference overhead. This work establishes vector quantization as an effective defense mechanism for secure multimodal AI systems without requiring expensive safety-specific fine-tuning or detection overhead. Code is available at https://github.com/Amadeuszhao/QMLLM.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16226v1" target="_blank"><h2>Deep SOR Minimax Q-learning for Two-player Zero-sum Game <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Saksham Gautam, Lakshmi Mandal, Shalabh Bhatnagar<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> In this work, we consider the problem of a two-player zero-sum game. In the literature, the successive over-relaxation Q-learning algorithm has been developed and implemented, and it is seen to result in a lower contraction factor for the associated Q-Bellman operator resulting in a faster value iteration-based procedure. However, this has been presented only for the tabular case and not for the setting with function approximation that typically caters to real-world high-dimensional state-action spaces. Furthermore, such settings in the case of two-player zero-sum games have not been considered. We thus propose a deep successive over-relaxation minimax Q-learning algorithm that incorporates deep neural networks as function approximators and is suitable for high-dimensional spaces. We prove the finite-time convergence of the proposed algorithm. Through numerical experiments, we show the effectiveness of the proposed method over the existing Q-learning algorithm. Our ablation studies demonstrate the effect of different values of the crucial successive over-relaxation parameter.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16225v1" target="_blank"><h2>Real-Time Inference for Distributed Multimodal Systems under Communication Delay Uncertainty <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Victor Croisfelt, João Henrique Inacio de Souza, Shashi Raj Pandey, Beatriz Soret, Petar Popovski<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 6 pages, 3 figures, submitted to IEEE ICC 2026<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (title)<br><p><strong><u>Abstract:</u></strong> Connected cyber-physical systems perform inference based on real-time inputs from multiple data streams. Uncertain communication delays across data streams challenge the temporal flow of the inference process. State-of-the-art (SotA) non-blocking inference methods rely on a reference-modality paradigm, requiring one modality input to be fully received before processing, while depending on costly offline profiling. We propose a novel, neuro-inspired non-blocking inference paradigm that primarily employs adaptive temporal windows of integration (TWIs) to dynamically adjust to stochastic delay patterns across heterogeneous streams while relaxing the reference-modality requirement. Our communication-delay-aware framework achieves robust real-time inference with finer-grained control over the accuracy-latency tradeoff. Experiments on the audio-visual event localization (AVEL) task demonstrate superior adaptability to network dynamics compared to SotA approaches.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16205v1" target="_blank"><h2>ChemLabs on ChemO: A Multi-Agent System for Multimodal Reasoning on IChO 2025 <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Xu Qiang, Shengyuan Bai, Leqing Chen, Zijing Liu, Yu Li<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> 13 pages, 1 figures<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Olympiad-level benchmarks in mathematics and physics are crucial testbeds for advanced AI reasoning, but chemistry, with its unique multimodal symbolic language, has remained an open challenge. We introduce ChemO, a new benchmark built from the International Chemistry Olympiad (IChO) 2025. ChemO features two key innovations for automated assessment: Assessment-Equivalent Reformulation (AER), which converts problems requiring visual outputs (e.g., drawing molecules) into computationally tractable formats, and Structured Visual Enhancement (SVE), a diagnostic mechanism to disentangle a model's visual perception capabilities from its core chemical reasoning. To tackle this benchmark, we propose ChemLabs, a hierarchical multi-agent framework that mimics human expert collaboration through specialized agents for problem decomposition, perception, reasoning, and auditing. Experiments on state-of-the-art multimodal models demonstrate that combining SVE with our multi-agent system yields dramatic performance gains. Our top configuration achieves a score of 93.6 out of 100, surpassing an estimated human gold medal threshold and establishing a new state-of-the-art in automated chemical problem-solving. ChemO Dataset: https://huggingface.co/datasets/IDEA-AI4SCI/ChemO</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16203v1" target="_blank"><h2>When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Yuping Yan, Yuhan Xie, Yinxin Zhang, Lingjuan Lyu, Yaochu Jin<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Vision-Language-Action models (VLAs) have recently demonstrated remarkable progress in embodied environments, enabling robots to perceive, reason, and act through unified multimodal understanding. Despite their impressive capabilities, the adversarial robustness of these systems remains largely unexplored, especially under realistic multimodal and black-box conditions. Existing studies mainly focus on single-modality perturbations and overlook the cross-modal misalignment that fundamentally affects embodied reasoning and decision-making. In this paper, we introduce VLA-Fool, a comprehensive study of multimodal adversarial robustness in embodied VLA models under both white-box and black-box settings. VLA-Fool unifies three levels of multimodal adversarial attacks: (1) textual perturbations through gradient-based and prompt-based manipulations, (2) visual perturbations via patch and noise distortions, and (3) cross-modal misalignment attacks that intentionally disrupt the semantic correspondence between perception and instruction. We further incorporate a VLA-aware semantic space into linguistic prompts, developing the first automatically crafted and semantically guided prompting framework. Experiments on the LIBERO benchmark using a fine-tuned OpenVLA model reveal that even minor multimodal perturbations can cause significant behavioral deviations, demonstrating the fragility of embodied multimodal alignment.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16201v1" target="_blank"><h2>From Performance to Understanding: A Vision for Explainable Automated Algorithm Design <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Niki van Stein, Anna V. Kononova, Thomas Bäck<br><strong><u>Categories:</u></strong> cs.AI, cs.NE<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> explainable (title, abstract)<br><p><strong><u>Abstract:</u></strong> Automated algorithm design is entering a new phase: Large Language Models can now generate full optimisation (meta)heuristics, explore vast design spaces and adapt through iterative feedback. Yet this rapid progress is largely performance-driven and opaque. Current LLM-based approaches rarely reveal why a generated algorithm works, which components matter or how design choices relate to underlying problem structures. This paper argues that the next breakthrough will come not from more automation, but from coupling automation with understanding from systematic benchmarking. We outline a vision for explainable automated algorithm design, built on three pillars: (i) LLM-driven discovery of algorithmic variants, (ii) explainable benchmarking that attributes performance to components and hyperparameters and (iii) problem-class descriptors that connect algorithm behaviour to landscape structure. Together, these elements form a closed knowledge loop in which discovery, explanation and generalisation reinforce each other. We argue that this integration will shift the field from blind search to interpretable, class-specific algorithm design, accelerating progress while producing reusable scientific insight into when and why optimisation strategies succeed.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16191v1" target="_blank"><h2>CausalMamba: Interpretable State Space Modeling for Temporal Rumor Causality <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Xiaotong Zhan, Xi Cheng<br><strong><u>Categories:</u></strong> cs.LG, cs.SI<br><strong><u>Comments:</u></strong> Preprint. 9 pages, 3 figures, 2 tables. Code and implementation details available at:this https URL<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), explainable (abstract), causality (title)<br><p><strong><u>Abstract:</u></strong> Rumor detection on social media remains a challenging task due to the complex propagation dynamics and the limited interpretability of existing models. While recent neural architectures capture content and structural features, they often fail to reveal the underlying causal mechanisms of misinformation spread. We propose CausalMamba, a novel framework that integrates Mamba-based sequence modeling, graph convolutional networks (GCNs), and differentiable causal discovery via NOTEARS. CausalMamba learns joint representations of temporal tweet sequences and reply structures, while uncovering latent causal graphs to identify influential nodes within each propagation chain. Experiments on the Twitter15 dataset show that our model achieves competitive classification performance compared to strong baselines, and uniquely enables counterfactual intervention analysis. Qualitative results demonstrate that removing top-ranked causal nodes significantly alters graph connectivity, offering interpretable insights into rumor dynamics. Our framework provides a unified approach for rumor classification and influence analysis, paving the way for more explainable and actionable misinformation detection systems.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16183v1" target="_blank"><h2>FOOTPASS: A Multi-Modal Multi-Agent Tactical Context Dataset for Play-by-Play Action Spotting in Soccer Broadcast Videos <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Jeremie Ochin, Raphael Chekroun, Bogdan Stanciulescu, Sotiris Manitsaris<br><strong><u>Categories:</u></strong> cs.AI, cs.CV<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), multi-modal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Soccer video understanding has motivated the creation of datasets for tasks such as temporal action localization, spatiotemporal action detection (STAD), or multiobject tracking (MOT). The annotation of structured sequences of events (who does what, when, and where) used for soccer analytics requires a holistic approach that integrates both STAD and MOT. However, current action recognition methods remain insufficient for constructing reliable play-by-play data and are typically used to assist rather than fully automate annotation. Parallel research has advanced tactical modeling, trajectory forecasting, and performance analysis, all grounded in game-state and play-by-play data. This motivates leveraging tactical knowledge as a prior to support computer-vision-based predictions, enabling more automated and reliable extraction of play-by-play data. We introduce Footovision Play-by-Play Action Spotting in Soccer Dataset (FOOTPASS), the first benchmark for play-by-play action spotting over entire soccer matches in a multi-modal, multi-agent tactical context. It enables the development of methods for player-centric action spotting that exploit both outputs from computer-vision tasks (e.g., tracking, identification) and prior knowledge of soccer, including its tactical regularities over long time horizons, to generate reliable play-by-play data streams. These streams form an essential input for data-driven sports analytics.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16175v1" target="_blank"><h2>Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Yi Yang, Xueqi Li, Yiyang Chen, Jin Song, Yihan Wang, Zipeng Xiao, Jiadi Su, You Qiaoben, Pengfei Liu, Zhijie Deng<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Recent advances in Vision-Language-Action (VLA) models demonstrate that visual signals can effectively complement sparse action supervisions. However, letting VLA directly predict high-dimensional visual states can distribute model capacity and incur prohibitive training cost, while compressing visual states into more compact supervisory signals inevitably incurs information bottlenecks. Moreover, existing methods often suffer from poor comprehension and reasoning capabilities due to the neglect of language supervision. This paper introduces Mantis, a novel framework featuring a Disentangled Visual Foresight (DVF) to tackle these issues. Specifically, Mantis decouples visual foresight prediction from the backbone with the combination of meta queries and a diffusion Transformer (DiT) head. With the current visual state provided to the DiT via a residual connection, a simple next-state prediction objective enables the meta queries to automatically capture the latent actions that delineate the visual trajectory, and hence boost the learning of explicit actions. The disentanglement reduces the burden of the VLA backbone, enabling it to maintain comprehension and reasoning capabilities through language supervision. Empirically, pretrained on human manipulation videos, robot demonstrations, and image-text pairs, Mantis achieves a 96.7% success rate on LIBERO benchmark after fine-tuning, surpassing powerful baselines while exhibiting high convergence speed. Real-world evaluations show that Mantis outperforms $π_{0.5}$, a leading open-source VLA model, particularly in instruction-following capability, generalization to unseen instructions, and reasoning ability. Code and weights are released to support the open-source community.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16149v1" target="_blank"><h2>Approximation rates of quantum neural networks for periodic functions via Jackson's inequality <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Ariel Neufeld, Philipp Schmocker, Viet Khoa Tran<br><strong><u>Categories:</u></strong> quant-ph, cs.LG, math.NA, stat.ML<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> Quantum neural networks (QNNs) are an analog of classical neural networks in the world of quantum computing, which are represented by a unitary matrix with trainable parameters. Inspired by the universal approximation property of classical neural networks, ensuring that every continuous function can be arbitrarily well approximated uniformly on a compact set of a Euclidean space, some recent works have established analogous results for QNNs, ranging from single-qubit to multi-qubit QNNs, and even hybrid classical-quantum models. In this paper, we study the approximation capabilities of QNNs for periodic functions with respect to the supremum norm. We use the Jackson inequality to approximate a given function by implementing its approximating trigonometric polynomial via a suitable QNN. In particular, we see that by restricting to the class of periodic functions, one can achieve a quadratic reduction of the number of parameters, producing better approximation results than in the literature. Moreover, the smoother the function, the fewer parameters are needed to construct a QNN to approximate the function.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16148v1" target="_blank"><h2>Enhancing Nuclear Reactor Core Simulation through Data-Based Surrogate Models <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Perceval Beja-Battais, Alain Grossetête, Nicolas Vayatis<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> In recent years, there has been an increasing need for Nuclear Power Plants (NPPs) to improve flexibility in order to match the rapid growth of renewable energies. The Operator Assistance Predictive System (OAPS) developed by Framatome addresses this problem through Model Predictive Control (MPC). In this work, we aim to improve MPC methods through data-driven simulation schemes. Thus, from a set of nonlinear stiff ordinary differential equations (ODEs), this paper introduces two surrogate models acting as alternative simulation schemes to enhance nuclear reactor core simulation. We show that both data-driven and physics-informed models can rapidly integrate complex dynamics, with a very low computational time (up to 1000x time reduction).</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16145v1" target="_blank"><h2>Labels Matter More Than Models: Quantifying the Benefit of Supervised Time Series Anomaly Detection <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Zhijie Zhong, Zhiwen Yu, Kaixiang Yang, C. L. Philip Chen<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 16 pages, 14 figures, 7 tables. Under review<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> anomaly detection (title, abstract)<br><p><strong><u>Abstract:</u></strong> Time series anomaly detection (TSAD) is a critical data mining task often constrained by label scarcity. Consequently, current research predominantly focuses on Unsupervised Time-series Anomaly Detection (UTAD), relying on complex architectures to model normal data distributions. However, this approach often overlooks the significant performance gains available from limited anomaly labels achievable in practical scenarios. This paper challenges the premise that architectural complexity is the optimal path for TSAD. We conduct the first methodical comparison between supervised and unsupervised paradigms and introduce STAND, a streamlined supervised baseline. Extensive experiments on five public datasets demonstrate that: (1) Labels matter more than models: under a limited labeling budget, simple supervised models significantly outperform complex state-of-the-art unsupervised methods; (2) Supervision yields higher returns: the performance gain from minimal supervision far exceeds that from architectural innovations; and (3) Practicality: STAND exhibits superior prediction consistency and anomaly localization compared to unsupervised counterparts. These findings advocate for a data-centric shift in TSAD research, emphasizing label utilization over purely algorithmic complexity. The code is publicly available at https://github.com/EmorZz1G/STAND.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16139v1" target="_blank"><h2>Multidimensional Rubric-oriented Reward Model Learning via Geometric Projection Reference Constraints <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Yongnan Jin, Xurui Li, Feng Cao, Liucun Gao, Juanjuan Yao<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> The integration of large language models (LLMs) into medical practice holds transformative potential, yet their real-world clinical utility remains limited by critical alignment challenges: (1) a disconnect between static evaluation benchmarks and dynamic clinical cognitive needs, (2) difficulties in adapting to evolving, multi-source medical standards, and (3) the inability of conventional reward models to capture nuanced, multi-dimensional medical quality criteria. To address these gaps, we propose MR-RML (Multidimensional Rubric-oriented Reward Model Learning) via GPRC (Geometric Projection Reference Constraints), a novel alignment framework that integrates medical standards into a structured "Dimensions-Scenarios-Disciplines" matrix to guide data generation and model optimization. MR-RML introduces three core innovations: (1) a "Dimensions-Scenarios-Disciplines" medical standard system that embeds domain standards into the full training pipeline; (2) an independent multi-dimensional reward model that decomposes evaluation criteria, shifting from real-time rubric-based scoring to internalized reward modeling for improved consistency and cost-efficiency; (3) geometric projection reference constraints that transform medical cognitive logic into mathematical regularization, aligning scoring gradients with clinical reasoning and enabling synthetic data-driven training. Through extensive evaluations on the authoritative medical benchmark Healthbench, our method yields substantial performance gains over the base LLM Qwen-32B (45% on the full subset and 85% on Hard subset, respectively). It achieves a SOTA among open-source LLMs with scores of 62.7 (full subset) and 44.7 (hard subset), while also outperforming the majority of closed-source models.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16130v1" target="_blank"><h2>Creation of Viscous Dark Energy by the Hubble Flow: Comparison with SNe Ia Master Sample Binned Data <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Iolanda Navonea, Maria Giovanna Dainotti, Elisa Fazzari, Giovanni Montani, Naoto Maki<br><strong><u>Categories:</u></strong> astro-ph.CO<br><strong><u>Comments:</u></strong> 14 pages, 10 figures<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> We study a cosmological model featuring evolutionary dark energy, according to the idea that the creation of its constituents arises from the gravitational field of the expanding universe, whose non-equilibrium physics is described by a non-zero bulk viscosity coefficient. This physical scenario calls for two additional parameters with respect to the ΛCDM model, one of which is the equation of state parameter of the created dark energy. The model is constrained by the requirement that its deceleration parameter coincides with the one predicted by the ΛCDM model. Then, we construct the effective running Hubble constant, a theoretical function that corresponds to the ratio of the Hubble parameter in our model to the ΛCDM expansion rate. The model's theoretical predictions for the effective running Hubble constant are compared with the binned data of the Supernovae Ia Master Sample. The comparison is performed by a MCMC procedure for each bin, with three parameters left free to vary, while the particle creation rate is taken from a grid of values, each of which is fixed in the given MCMC run. The most important result emerging from this analysis is that the created dark energy constituent corresponds to an equation of state parameter with phantom character. Only if particle creation is removed do the dark energy constituents acquire a quintessence character. No matter the intrinsic nature of the constituents, their effective z-dependent equation of state parameter is, both with and without considering particle creation, entirely of phantom nature across the considered redshift range.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16105v1" target="_blank"><h2>Pathlet Variational Auto-Encoder for Robust Trajectory Generation <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Yuanbo Tang, Yan Tang, Zixuan Zhang, Zihui Zhao, Yang Li<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> variational autoencoder (abstract), VAE (abstract)<br><p><strong><u>Abstract:</u></strong> Trajectory generation has recently drawn growing interest in privacy-preserving urban mobility studies and location-based service applications. Although many studies have used deep learning or generative AI methods to model trajectories and have achieved promising results, the robustness and interpretability of such models are largely unexplored. This limits the application of trajectory generation algorithms on noisy real-world data and their trustworthiness in downstream tasks. To address this issue, we exploit the regular structure in urban trajectories and propose a deep generative model based on the pathlet representation, which encode trajectories with binary vectors associated with a learned dictionary of trajectory segments. Specifically, we introduce a probabilistic graphical model to describe the trajectory generation process, which includes a Variational Autoencoder (VAE) component and a linear decoder component. During training, the model can simultaneously learn the latent embedding of pathlet representations and the pathlet dictionary that captures mobility patterns in the trajectory dataset. The conditional version of our model can also be used to generate customized trajectories based on temporal and spatial constraints.
  Our model can effectively learn data distribution even using noisy data, achieving relative improvements of $35.4\%$ and $26.3\%$ over strong baselines on two real-world trajectory datasets. Moreover, the generated trajectories can be conveniently utilized for multiple downstream tasks, including trajectory prediction and data denoising. Lastly, the framework design offers a significant efficiency advantage, saving $64.8\%$ of the time and $56.5\%$ of GPU memory compared to previous approaches.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16101v1" target="_blank"><h2>HybSpecNet: A Critical Analysis of Architectural Instability in Hybrid-Domain Spectral GNNs <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Huseyin Goksu<br><strong><u>Categories:</u></strong> cs.LG, eess.SP<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Spectral Graph Neural Networks offer a principled approach to graph filtering but face a fundamental "Stability-vs-Adaptivity" trade-off. This trade-off is dictated by the choice of spectral domain. Filters in the finite [-1, 1] domain (e.g., ChebyNet) are numerically stable at high polynomial degrees (K) but are static and low-pass, causing them to fail on heterophilic graphs. Conversely, filters in the semi-infinite [0, infty) domain (e.g., KrawtchoukNet) are highly adaptive and achieve SOTA results on heterophily by learning non-low-pass responses. However, as we demonstrate, these adaptive filters can also suffer from numerical instability, leading to catastrophic performance collapse at high K. In this paper, we propose to resolve this trade-off by designing a hybrid-domain GNN, HybSpecNet, which combines a stable `ChebyNet` branch with an adaptive `KrawtchoukNet` branch. We first demonstrate that a "naive" hybrid architecture, which fuses the branches via concatenation, successfully unifies performance at low K, achieving strong results on both homophilic and heterophilic benchmarks. However, we then prove that this naive architecture fails the stability test. Our K-ablation experiments show that this architecture catastrophically collapses at K=25, exactly mirroring the collapse of its unstable `KrawtchoukNet` branch. We identify this critical finding as "Instability Poisoning," where `NaN`/`Inf` gradients from the adaptive branch destroy the training of the model. Finally, we propose and validate an advanced architecture that uses "Late Fusion" to completely isolate the gradient pathways. We demonstrate that this successfully solves the instability problem, remaining perfectly stable up to K=30 while retaining its SOTA performance across all graph types. This work identifies a critical architectural pitfall in hybrid GNN design and provides the robust architectural solution.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16087v1" target="_blank"><h2>AssayMatch: Learning to Select Data for Molecular Activity Models <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Vincent Fan, Regina Barzilay<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> The performance of machine learning models in drug discovery is highly dependent on the quality and consistency of the underlying training data. Due to limitations in dataset sizes, many models are trained by aggregating bioactivity data from diverse sources, including public databases such as ChEMBL. However, this approach often introduces significant noise due to variability in experimental protocols. We introduce AssayMatch, a framework for data selection that builds smaller, more homogenous training sets attuned to the test set of interest. AssayMatch leverages data attribution methods to quantify the contribution of each training assay to model performance. These attribution scores are used to finetune language embeddings of text-based assay descriptions to capture not just semantic similarity, but also the compatibility between assays. Unlike existing data attribution methods, our approach enables data selection for a test set with unknown labels, mirroring real-world drug discovery campaigns where the activities of candidate molecules are not known in advance. At test time, embeddings finetuned with AssayMatch are used to rank all available training data. We demonstrate that models trained on data selected by AssayMatch are able to surpass the performance of the model trained on the complete dataset, highlighting its ability to effectively filter out harmful or noisy experiments. We perform experiments on two common machine learning architectures and see increased prediction capability over a strong language-only baseline for 9/12 model-target pairs. AssayMatch provides a data-driven mechanism to curate higher-quality datasets, reducing noise from incompatible experiments and improving the predictive power and data efficiency of models for drug discovery. AssayMatch is available at https://github.com/Ozymandias314/AssayMatch.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16062v1" target="_blank"><h2>Gauge-Equivariant Graph Networks via Self-Interference Cancellation <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Yoonhyuk Choi, Chong-Kwon Kim<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Graph Neural Networks (GNNs) excel on homophilous graphs but often fail under heterophily due to self-reinforcing and phase-inconsistent signals. We propose a Gauge-Equivariant Graph Network with Self-Interference Cancellation (GESC), which replaces additive aggregation with a projection-based interference mechanism. Unlike prior magnetic or gauge-equivariant GNNs that typically focus on phase handling in spectral filtering while largely relying on scalar weighting, GESC introduces a $\mathrm{U}(1)$ phase connection followed by a rank-1 projection that attenuates self-parallel components before attention. A sign- and phase-aware gate further regulates neighbor influence, attenuating components aligned with current node states and acting as a local notch on low-frequency modes. Across diverse graph benchmarks, our method consistently outperforms recent state-of-the-art models while offering a unified, interference-aware view of message passing. Our code is available at \href{here}{https://anonymous.4open.science/r/GESC-1B22}.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16060v1" target="_blank"><h2>Neuromorphic Astronomy: An End-to-End SNN Pipeline for RFI Detection Hardware <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Nicholas J. Pritchard, Andreas Wicenec, Richard Dodson, Mohammed Bennamoun, Dylan R. Muir<br><strong><u>Categories:</u></strong> cs.NE, astro-ph.IM<br><strong><u>Comments:</u></strong> 21 pages, 4 figures, 12 tables<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Imminent radio telescope observatories provide massive data rates making deep learning based processing appealing while simultaneously demanding real-time performance at low-energy; prohibiting the use of many artificial neural network based approaches. We begin tackling the scientifically existential challenge of Radio Frequency Interference (RFI) detection by deploying deep Spiking Neural Networks (SNNs) on resource-constrained neuromorphic hardware. Our approach partitions large, pre-trained networks onto SynSense Xylo hardware using maximal splitting, a novel greedy algorithm. We validate this pipeline with on-chip power measurements, achieving instrument-scaled inference at 100mW. While our full-scale SNN achieves state-of-the-art accuracy among SNN baselines, our experiments reveal a more important insight that a smaller un-partitioned model significantly outperforms larger, split models. This finding highlights that hardware co-design is paramount for optimal performance. Our work thus provides a practical deployment blueprint, a key insight into the challenges of model scaling, and reinforces radio astronomy as a demanding yet ideal domain for advancing applied neuromorphic computing.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16048v1" target="_blank"><h2>Semantic Glitch: Agency and Artistry in an Autonomous Pixel Cloud <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Qing Zhang, Jing Huang, Mingyang Xu, Jun Rekimoto<br><strong><u>Categories:</u></strong> cs.RO, cs.AI, cs.HC<br><strong><u>Comments:</u></strong> NeurIPS 2025 Creative AI Track, The Thirty-Ninth Annual Conference on Neural Information Processing Systems<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> While mainstream robotics pursues metric precision and flawless performance, this paper explores the creative potential of a deliberately "lo-fi" approach. We present the "Semantic Glitch," a soft flying robotic art installation whose physical form, a 3D pixel style cloud, is a "physical glitch" derived from digital archaeology. We detail a novel autonomous pipeline that rejects conventional sensors like LiDAR and SLAM, relying solely on the qualitative, semantic understanding of a Multimodal Large Language Model to navigate. By authoring a bio-inspired personality for the robot through a natural language prompt, we create a "narrative mind" that complements the "weak," historically, loaded body. Our analysis begins with a 13-minute autonomous flight log, and a follow-up study statistically validates the framework's robustness for authoring quantifiably distinct personas. The combined analysis reveals emergent behaviors, from landmark-based navigation to a compelling "plan to execution" gap, and a character whose unpredictable, plausible behavior stems from a lack of precise proprioception. This demonstrates a lo-fi framework for creating imperfect companions whose success is measured in character over efficiency.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16027v1" target="_blank"><h2>HGCN2SP: Hierarchical Graph Convolutional Network for Two-Stage Stochastic Programming <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Yang Wu, Yifan Zhang, Zhenxing Liang, Jian Cheng<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 17 pages, 4 figures<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> convolutional (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Two-stage Stochastic Programming (2SP) is a standard framework for modeling decision-making problems under uncertainty. While numerous methods exist, solving such problems with many scenarios remains challenging. Selecting representative scenarios is a practical method for accelerating solutions. However, current approaches typically rely on clustering or Monte Carlo sampling, failing to integrate scenario information deeply and overlooking the significant impact of the scenario order on solving time. To address these issues, we develop HGCN2SP, a novel model with a hierarchical graph designed for 2SP problems, encoding each scenario and modeling their relationships hierarchically. The model is trained in a reinforcement learning paradigm to utilize the feedback of the solver. The policy network is equipped with a hierarchical graph convolutional network for feature encoding and an attention-based decoder for scenario selection in proper order. Evaluation of two classic 2SP problems demonstrates that HGCN2SP provides high-quality decisions in a short computational time. Furthermore, HGCN2SP exhibits remarkable generalization capabilities in handling large-scale instances, even with a substantial number of variables or scenarios that were unseen during the training phase.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16026v1" target="_blank"><h2>Towards a Safer and Sustainable Manufacturing Process: Material classification in Laser Cutting Using Deep Learning <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Mohamed Abdallah Salem, Hamdy Ahmed Ashur, Ahmed Elshinnawy<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.LG, cs.RO<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Laser cutting is a widely adopted technology in material processing across various industries, but it generates a significant amount of dust, smoke, and aerosols during operation, posing a risk to both the environment and workers' health. Speckle sensing has emerged as a promising method to monitor the cutting process and identify material types in real-time. This paper proposes a material classification technique using a speckle pattern of the material's surface based on deep learning to monitor and control the laser cutting process. The proposed method involves training a convolutional neural network (CNN) on a dataset of laser speckle patterns to recognize distinct material types for safe and efficient cutting. Previous methods for material classification using speckle sensing may face issues when the color of the laser used to produce the speckle pattern is changed. Experiments conducted in this study demonstrate that the proposed method achieves high accuracy in material classification, even when the laser color is changed. The model achieved an accuracy of 98.30 % on the training set and 96.88% on the validation set. Furthermore, the model was evaluated on a set of 3000 new images for 30 different materials, achieving an F1-score of 0.9643. The proposed method provides a robust and accurate solution for material-aware laser cutting using speckle sensing.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16020v1" target="_blank"><h2>Physically Realistic Sequence-Level Adversarial Clothing for Robust Human-Detection Evasion <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Dingkun Zhou, Patrick P. K. Chan, Hengxu Wu, Shikang Zheng, Ruiqi Huang, Yuanjie Zhao<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Deep neural networks used for human detection are highly vulnerable to adversarial manipulation, creating safety and privacy risks in real surveillance environments. Wearable attacks offer a realistic threat model, yet existing approaches usually optimize textures frame by frame and therefore fail to maintain concealment across long video sequences with motion, pose changes, and garment deformation. In this work, a sequence-level optimization framework is introduced to generate natural, printable adversarial textures for shirts, trousers, and hats that remain effective throughout entire walking videos in both digital and physical settings. Product images are first mapped to UV space and converted into a compact palette and control-point parameterization, with ICC locking to keep all colors printable. A physically based human-garment pipeline is then employed to simulate motion, multi-angle camera viewpoints, cloth dynamics, and illumination variation. An expectation-over-transformation objective with temporal weighting is used to optimize the control points so that detection confidence is minimized across whole sequences. Extensive experiments demonstrate strong and stable concealment, high robustness to viewpoint changes, and superior cross-model transferability. Physical garments produced with sublimation printing achieve reliable suppression under indoor and outdoor recordings, confirming real-world feasibility.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16014v1" target="_blank"><h2>MUSEKG: A Knowledge Graph Over Museum Collections <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Jinhao Li, Jianzhong Qi, Soyeon Caren Han, Eun-Jung Holden<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Digital transformation in the cultural heritage sector has produced vast yet fragmented collections of artefact data. Existing frameworks for museum information systems struggle to integrate heterogeneous metadata, unstructured documents, and multimodal artefacts into a coherent and queryable form. We present MuseKG, an end-to-end knowledge-graph framework that unifies structured and unstructured museum data through symbolic-neural integration. MuseKG constructs a typed property graph linking objects, people, organisations, and visual or textual labels, and supports natural language queries. Evaluations on real museum collections demonstrate robust performance across queries over attributes, relations, and related entities, surpassing large-language-model zero-shot, few-shot and SPARQL prompt baselines. The results highlight the importance of symbolic grounding for interpretable and scalable cultural heritage reasoning, and pave the way for web-scale integration of digital heritage knowledge.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16013v1" target="_blank"><h2>Physics-Guided Inductive Spatiotemporal Kriging for PM2.5 with Satellite Gradient Constraints <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Shuo Wang, Mengfan Teng, Yun Cheng, Lothar Thiele, Olga Saukh, Shuangshuang He, Yuanting Zhang, Jiang Zhang, Gangfeng Zhang, Xingyuan Yuan, Jingfang Fan<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> High-resolution mapping of fine particulate matter (PM2.5) is a cornerstone of sustainable urbanism but remains critically hindered by the spatial sparsity of ground monitoring networks. While traditional data-driven methods attempt to bridge this gap using satellite Aerosol Optical Depth (AOD), they often suffer from severe, non-random data missingness (e.g., due to cloud cover or nighttime) and inversion biases. To overcome these limitations, this study proposes the Spatiotemporal Physics-Guided Inference Network (SPIN), a novel framework designed for inductive spatiotemporal kriging. Unlike conventional approaches, SPIN synergistically integrates domain knowledge into deep learning by explicitly modeling physical advection and diffusion processes via parallel graph kernels. Crucially, we introduce a paradigm-shifting training strategy: rather than using error-prone AOD as a direct input, we repurpose it as a spatial gradient constraint within the loss function. This allows the model to learn structural pollution patterns from satellite data while remaining robust to data voids. Validated in the highly polluted Beijing-Tianjin-Hebei and Surrounding Areas (BTHSA), SPIN achieves a new state-of-the-art with a Mean Absolute Error (MAE) of 9.52 ug/m^3, effectively generating continuous, physically plausible pollution fields even in unmonitored areas. This work provides a robust, low-cost, and all-weather solution for fine-grained environmental management.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16006v1" target="_blank"><h2>Synergizing Deconfounding and Temporal Generalization For Time-series Counterfactual Outcome Estimation <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Yiling Liu, Juncheng Dong, Chen Fu, Wei Shi, Ziyang Jiang, Zhigang Hua, David Carlson<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> latent space (abstract)<br><p><strong><u>Abstract:</u></strong> Estimating counterfactual outcomes from time-series observations is crucial for effective decision-making, e.g. when to administer a life-saving treatment, yet remains significantly challenging because (i) the counterfactual trajectory is never observed and (ii) confounders evolve with time and distort estimation at every step. To address these challenges, we propose a novel framework that synergistically integrates two complementary approaches: Sub-treatment Group Alignment (SGA) and Random Temporal Masking (RTM). Instead of the coarse practice of aligning marginal distributions of the treatments in latent space, SGA uses iterative treatment-agnostic clustering to identify fine-grained sub-treatment groups. Aligning these fine-grained groups achieves improved distributional matching, thus leading to more effective deconfounding. We theoretically demonstrate that SGA optimizes a tighter upper bound on counterfactual risk and empirically verify its deconfounding efficacy. RTM promotes temporal generalization by randomly replacing input covariates with Gaussian noises during training. This encourages the model to rely less on potentially noisy or spuriously correlated covariates at the current step and more on stable historical patterns, thereby improving its ability to generalize across time and better preserve underlying causal relationships. Our experiments demonstrate that while applying SGA and RTM individually improves counterfactual outcome estimation, their synergistic combination consistently achieves state-of-the-art performance. This success comes from their distinct yet complementary roles: RTM enhances temporal generalization and robustness across time steps, while SGA improves deconfounding at each specific time point.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15997v1" target="_blank"><h2>Sensorium Arc: AI Agent System for Oceanic Data Exploration and Interactive Eco-Art <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Noah Bissell, Ethan Paley, Joshua Harrison, Juliano Calil, Myungin Lee<br><strong><u>Categories:</u></strong> cs.AI, cs.MM<br><strong><u>Comments:</u></strong> (to appear) NeurIPS 2025 Creative AI Track<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Sensorium Arc (AI reflects on climate) is a real-time multimodal interactive AI agent system that personifies the ocean as a poetic speaker and guides users through immersive explorations of complex marine data. Built on a modular multi-agent system and retrieval-augmented large language model (LLM) framework, Sensorium enables natural spoken conversations with AI agents that embodies the ocean's perspective, generating responses that blend scientific insight with ecological poetics. Through keyword detection and semantic parsing, the system dynamically triggers data visualizations and audiovisual playback based on time, location, and thematic cues drawn from the dialogue. Developed in collaboration with the Center for the Study of the Force Majeure and inspired by the eco-aesthetic philosophy of Newton Harrison, Sensorium Arc reimagines ocean data not as an abstract dataset but as a living narrative. The project demonstrates the potential of conversational AI agents to mediate affective, intuitive access to high-dimensional environmental data and proposes a new paradigm for human-machine-ecosystem.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15986v1" target="_blank"><h2>Fairness in Multi-modal Medical Diagnosis with Demonstration Selection <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Dawei Li, Zijian Gu, Peng Wang, Chuhan Song, Zhen Tan, Mohan Zhang, Tianlong Chen, Yu Tian, Song Wang<br><strong><u>Categories:</u></strong> cs.CV, cs.CY, cs.LG<br><strong><u>Comments:</u></strong> 10 pages (including 2 pages of references), 4 figures. This work explores fairness in multi-modal medical image reasoning using in-context learning<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract), multi-modal (title)<br><p><strong><u>Abstract:</u></strong> Multimodal large language models (MLLMs) have shown strong potential for medical image reasoning, yet fairness across demographic groups remains a major concern. Existing debiasing methods often rely on large labeled datasets or fine-tuning, which are impractical for foundation-scale models. We explore In-Context Learning (ICL) as a lightweight, tuning-free alternative for improving fairness. Through systematic analysis, we find that conventional demonstration selection (DS) strategies fail to ensure fairness due to demographic imbalance in selected exemplars. To address this, we propose Fairness-Aware Demonstration Selection (FADS), which builds demographically balanced and semantically relevant demonstrations via clustering-based sampling. Experiments on multiple medical imaging benchmarks show that FADS consistently reduces gender-, race-, and ethnicity-related disparities while maintaining strong accuracy, offering an efficient and scalable path toward fair medical image reasoning. These results highlight the potential of fairness-aware in-context learning as a scalable and data-efficient solution for equitable medical image reasoning.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15975v1" target="_blank"><h2>SN 2019vxm: A Shocking Coincidence between Fermi and TESS <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Zachary G. Lane, Ryan Ridden-Harper, Sofia Rest, Armin Rest, Conor L. Ransome, Qinan Wang, Clarinda Montilla, Micaela Steed, Igor Andreoni, Patrick Armstrong, Peter J. Brown, Jeffrey Cooke, David A. Coulter, Ori Fox, James Freeburn, Marco Galoppo, Avishay Gal-Yam, Jared A. Goldberg, Christopher Harvey-Hawes, Rebekah Hounsell, Brayden Leicester, Itai Linial, Thomas Moore, Pierre Mourier, Anya E. Nugent, David O'Neill, Hugh Roxburgh, Koji Shukawa, Stephen J. Smartt, Nathan Smith, Ken W. Smith, Sebastian Vergara Carrasco, V. Ashley Villar, Tal Wasserman, Zenati Yossef, Erez Zimmerman<br><strong><u>Categories:</u></strong> astro-ph.HE<br><strong><u>Comments:</u></strong> 25 pages, 9 figures. Submitted to The Astrophysical Journal<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> Shock breakout and, in some cases, jet-driven high-energy emission are increasingly recognized as key signatures of the earliest phases of core-collapse supernovae, especially in Type IIn systems due to their dense, interaction-dominated circumstellar environments. We present a comprehensive photometric analysis of SN 2019vxm, a long-duration, luminous Type IIn supernova, $M_V^{}=-21.41\pm0.05\;{\rm mag}$, observed from X-ray to near-infrared. SN 2019vxm is the first superluminous supernovae Type IIn to be caught with well-sampled TESS photometric data on the rise and has a convincing coincident X-ray source at the time of first light. The high-cadence TESS light curve captures the early-time rise, which is well described by a broken power law with an index of $n=1.41\pm0.04$, significantly shallower than the canonical $n=2$ behavior. From this, we constrain the time of first light to within 7.2 hours. We identify a spatial and temporal coincidence between SN 2019vxm and the X-ray transient GRB191117A, corresponding to a $3.3σ$ association confidence. Both the short-duration X-ray event and the lightcurve modeling are consistent with shock breakout into a dense, asymmetric circumstellar medium, indicative of a massive, compact progenitor such as a luminous blue variable transitioning to Wolf-Rayet phase embedded in a clumpy, asymmetric environment.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15974v2" target="_blank"><h2>KRAL: Knowledge and Reasoning Augmented Learning for LLM-assisted Clinical Antimicrobial Therapy <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Zhe Li, Yehan Qiu, Yujie Chen, Xiang Zhou<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> data augmentation (abstract)<br><p><strong><u>Abstract:</u></strong> Clinical antimicrobial therapy requires the dynamic integration of pathogen profiles,host factors, pharmacological properties of antimicrobials,and the severity of infection. This complexity imposes fundamental limitations on the applicability of Large Language Models (LLMs) in high-stakes clinical decision-making including knowledge gaps, data privacy concerns, high deployment costs, and limited reasoning capabilities. To address these challenges, we propose KRAL (Knowledge and Reasoning Augmented Learning), a low-cost, scalable, privacy-preserving paradigm that leverages teacher-model reasoning to automatically distill knowledge and reasoning trajectories via answer-to-question reverse generation, employs heuristic learning for semi-supervised data augmentation (reducing manual annotation requirements by approximately 80%), and utilizes agentic reinforcement learning to jointly enhance medical knowledge and reasoning while optimizing computational and memory efficiency. A hierarchical evaluation employing diverse teacher-model proxies reduces assessment costs, while modular interface design facilitates seamless system updates. Experimental results demonstrate that KRAL significantly outperforms traditional Retrieval-Augmented Generation (RAG) and Supervised Fine-Tuning (SFT) methods. It improves knowledge question-answering capability (Accuracy@1 on the external open-source benchmark MEDQA increased by 1.8% vs. SFT and 3.6% vs. RAG) and reasoning capability (Pass@1 on the external benchmark PUMCH Antimicrobial increased by 27% vs. SFT and 27.2% vs. RAG), achieved at about 20% of SFT's long-term training costs. This establishes KRAL as an effective solution for enhancing local LLMs' clinical diagnostic capabilities, enabling low-cost, high-safety deployment in complex medical decision support.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15965v1" target="_blank"><h2>Self-supervised and Multi-fidelity Learning for Extended Predictive Soil Spectroscopy <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Luning Sun, José L. Safanelli, Jonathan Sanderman, Katerina Georgiou, Colby Brungard, Kanchan Grover, Bryan G. Hopkins, Shusen Liu, Timo Bremer<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 49 pages, 9 figures, submitted to Geoderma<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> variational autoencoder (abstract), latent space (abstract)<br><p><strong><u>Abstract:</u></strong> We propose a self-supervised machine learning (SSML) framework for multi-fidelity learning and extended predictive soil spectroscopy based on latent space embeddings. A self-supervised representation was pretrained with the large MIR spectral library and the Variational Autoencoder algorithm to obtain a compressed latent space for generating spectral embeddings. At this stage, only unlabeled spectral data were used, allowing us to leverage the full spectral database and the availability of scan repeats for augmented training. We also leveraged and froze the trained MIR decoder for a spectrum conversion task by plugging it into a NIR encoder to learn the mapping between NIR and MIR spectra in an attempt to leverage the predictive capabilities contained in the large MIR library with a low cost portable NIR scanner. This was achieved by using a smaller subset of the KSSL library with paired NIR and MIR spectra. Downstream machine learning models were then trained to map between original spectra, predicted spectra, and latent space embeddings for nine soil properties. The performance of was evaluated independently of the KSSL training data using a gold-standard test set, along with regression goodness-of-fit metrics. Compared to baseline models, the proposed SSML and its embeddings yielded similar or better accuracy in all soil properties prediction tasks. Predictions derived from the spectrum conversion (NIR to MIR) task did not match the performance of the original MIR spectra but were similar or superior to predictive performance of NIR-only models, suggesting the unified spectral latent space can effectively leverage the larger and more diverse MIR dataset for prediction of soil properties not well represented in current NIR libraries.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15960v1" target="_blank"><h2>Machine Learning vs. Randomness: Challenges in Predicting Binary Options Movements <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Gabriel M. Arantes, Richard F. Pinto, Bruno L. Dalmazo, Eduardo N. Borges, Giancarlo Lucca, Viviane L. D. de Mattos, Fabian C. Cardoso, Rafael A. Berri<br><strong><u>Categories:</u></strong> q-fin.CP, cs.LG<br><strong><u>Comments:</u></strong> Accepted for publication at the 26th International Conference on Intelligent Data Engineering and Automated Learning (IDEAL 2025)<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Binary options trading is often marketed as a field where predictive models can generate consistent profits. However, the inherent randomness and stochastic nature of binary options make price movements highly unpredictable, posing significant challenges for any forecasting approach. This study demonstrates that machine learning algorithms struggle to outperform a simple baseline in predicting binary options movements. Using a dataset of EUR/USD currency pairs from 2021 to 2023, we tested multiple models, including Random Forest, Logistic Regression, Gradient Boosting, and k-Nearest Neighbors (kNN), both before and after hyperparameter optimization. Furthermore, several neural network architectures, including Multi-Layer Perceptrons (MLP) and a Long Short-Term Memory (LSTM) network, were evaluated under different training conditions. Despite these exhaustive efforts, none of the models surpassed the ZeroR baseline accuracy, highlighting the inherent randomness of binary options. These findings reinforce the notion that binary options lack predictable patterns, making them unsuitable for machine learning-based forecasting.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15927v1" target="_blank"><h2>Breaking the Bottleneck with DiffuApriel: High-Throughput Diffusion LMs with Mamba Backbone <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Vaibhav Singh, Oleksiy Ostapenko, Pierre-André Noël, Torsten Scholak<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 9 pages, 4 figures<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> time sequence (abstract), transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Diffusion-based language models have recently emerged as a promising alternative to autoregressive generation, yet their reliance on Transformer backbones limits inference efficiency due to quadratic attention and KV-cache overhead. In this work, we introduce DiffuApriel, a masked diffusion language model built on a bidirectional Mamba backbone that combines the diffusion objective with linear-time sequence modeling. DiffuApriel matches the performance of Transformer-based diffusion models while achieving up to 4.4x higher inference throughput for long sequences with a 1.3B model. We further propose DiffuApriel-H, a hybrid variant that interleaves attention and mamba layers, offering up to 2.6x throughput improvement with balanced global and local context modeling. Our results demonstrate that bidirectional state-space architectures serve as strong denoisers in masked diffusion LMs, providing a practical and scalable foundation for faster, memory-efficient text generation.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15902v1" target="_blank"><h2>EEG Emotion Recognition Through Deep Learning <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Roman Dolgopolyi, Antonis Chatzipanagiotou<br><strong><u>Categories:</u></strong> eess.SP, cs.LG<br><strong><u>Comments:</u></strong> This version corresponds to the original manuscript submitted to the 22nd EMCIS conference prior to peer review. The peer-reviewed and accepted version will appear in the Springer conference proceedings<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> An advanced emotion classification model was developed using a CNN-Transformer architecture for emotion recognition from EEG brain wave signals, effectively distinguishing among three emotional states, positive, neutral and negative. The model achieved a testing accuracy of 91%, outperforming traditional models such as SVM, DNN, and Logistic Regression. Training was conducted on a custom dataset created by merging data from SEED, SEED-FRA, and SEED-GER repositories, comprising 1,455 samples with EEG recordings labeled according to emotional states. The combined dataset represents one of the largest and most culturally diverse collections available. Additionally, the model allows for the reduction of the requirements of the EEG apparatus, by leveraging only 5 electrodes of the 62. This reduction demonstrates the feasibility of deploying a more affordable consumer-grade EEG headset, thereby enabling accessible, at-home use, while also requiring less computational power. This advancement sets the groundwork for future exploration into mood changes induced by media content consumption, an area that remains underresearched. Integration into medical, wellness, and home-health platforms could enable continuous, passive emotional monitoring, particularly beneficial in clinical or caregiving settings where traditional behavioral cues, such as facial expressions or vocal tone, are diminished, restricted, or difficult to interpret, thus potentially transforming mental health diagnostics and interventions...</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15870v1" target="_blank"><h2>AquaSentinel: Next-Generation AI System Integrating Sensor Networks for Urban Underground Water Pipeline Anomaly Detection via Collaborative MoE-LLM Agent Architecture <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Qiming Guo, Bishal Khatri, Wenbo Sun, Jinwen Tang, Hua Zhang, Wenlu Wang<br><strong><u>Categories:</u></strong> cs.CE, cs.AI<br><strong><u>Comments:</u></strong> 7 pages, 1 figure, 2 tables, Accepted to the 40th AAAI Conference on Artificial Intelligence (AAAI 2026), IAAI Deployed Applications Track<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> anomaly detection (title, abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Underground pipeline leaks and infiltrations pose significant threats to water security and environmental safety. Traditional manual inspection methods provide limited coverage and delayed response, often missing critical anomalies. This paper proposes AquaSentinel, a novel physics-informed AI system for real-time anomaly detection in urban underground water pipeline networks. We introduce four key innovations: (1) strategic sparse sensor deployment at high-centrality nodes combined with physics-based state augmentation to achieve network-wide observability from minimal infrastructure; (2) the RTCA (Real-Time Cumulative Anomaly) detection algorithm, which employs dual-threshold monitoring with adaptive statistics to distinguish transient fluctuations from genuine anomalies; (3) a Mixture of Experts (MoE) ensemble of spatiotemporal graph neural networks that provides robust predictions by dynamically weighting model contributions; (4) causal flow-based leak localization that traces anomalies upstream to identify source nodes and affected pipe segments. Our system strategically deploys sensors at critical network junctions and leverages physics-based modeling to propagate measurements to unmonitored nodes, creating virtual sensors that enhance data availability across the entire network. Experimental evaluation using 110 leak scenarios demonstrates that AquaSentinel achieves 100% detection accuracy. This work advances pipeline monitoring by demonstrating that physics-informed sparse sensing can match the performance of dense deployments at a fraction of the cost, providing a practical solution for aging urban infrastructure.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15853v1" target="_blank"><h2>The Ensemble Kalman Inversion Race <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Rebecca Gjini, Matthias Morzfeld, Oliver R. A. Dunbar, Tapio Schneider<br><strong><u>Categories:</u></strong> physics.data-an, stat.ML<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Ensemble Kalman methods were initially developed to solve nonlinear data assimilation problems in oceanography, but are now popular in applications far beyond their original use cases. Of particular interest is climate model calibration. As hybrid physics and machine-learning models evolve, the number of parameters and complexity of parameterizations in climate models will continue to grow. Thus, robust calibration of these parameters plays an increasingly important role. We focus on learning climate model parameters from minimizing the misfit between modeled and observed climate statistics in an idealized setting. Ensemble Kalman methods are a natural choice for this problem because they are derivative-free, scalable to high dimensions, and robust to noise caused by statistical observations. Given the many variants of ensemble methods proposed, an important question is: Which ensemble Kalman method should be used for climate model calibration? To answer this question, we perform systematic numerical experiments to explore the relative computational efficiencies of several ensemble Kalman methods. The numerical experiments involve statistical observations of Lorenz-type models of increasing complexity, frequently used to represent simplified atmospheric systems, and some feature neural network parameterizations. For each test problem, several ensemble Kalman methods and a derivative-based method "race" to reach a specified accuracy, and we measure the computational cost required to achieve the desired accuracy. We investigate how prior information and the parameter or data dimensions play a role in choosing the ensemble method variant. The derivative-based method consistently fails to complete the race because it does not adaptively handle the noisy loss landscape.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15848v1" target="_blank"><h2>Step-Audio-R1 Technical Report <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Fei Tian, Xiangyu Tony Zhang, Yuxin Zhang, Haoyang Zhang, Yuxin Li, Daijiao Liu, Yayue Deng, Donghang Wu, Jun Chen, Liang Zhao, Chengyuan Yao, Hexin Liu, Eng Siong Chng, Xuerui Yang, Xiangyu Zhang, Daxin Jiang, Gang Yu<br><strong><u>Categories:</u></strong> cs.AI, cs.CL, cs.SD<br><strong><u>Comments:</u></strong> 15 pages, 5 figures. Technical Report<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Recent advances in reasoning models have demonstrated remarkable success in text and vision domains through extended chain-of-thought deliberation. However, a perplexing phenomenon persists in audio language models: they consistently perform better with minimal or no reasoning, raising a fundamental question - can audio intelligence truly benefit from deliberate thinking? We introduce Step-Audio-R1, the first audio reasoning model that successfully unlocks reasoning capabilities in the audio domain. Through our proposed Modality-Grounded Reasoning Distillation (MGRD) framework, Step-Audio-R1 learns to generate audio-relevant reasoning chains that genuinely ground themselves in acoustic features rather than hallucinating disconnected deliberations. Our model exhibits strong audio reasoning capabilities, surpassing Gemini 2.5 Pro and achieving performance comparable to the state-of-the-art Gemini 3 Pro across comprehensive audio understanding and reasoning benchmarks spanning speech, environmental sounds, and music. These results demonstrate that reasoning is a transferable capability across modalities when appropriately anchored, transforming extended deliberation from a liability into a powerful asset for audio intelligence. By establishing the first successful audio reasoning model, Step-Audio-R1 opens new pathways toward building truly multimodal reasoning systems that think deeply across all sensory modalities.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15847v1" target="_blank"><h2>Transparent Early ICU Mortality Prediction with Clinical Transformer and Per-Case Modality Attribution <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Alexander Bakumenko, Janine Hoelscher, Hudson Smith<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract), transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> Early identification of intensive care patients at risk of in-hospital mortality enables timely intervention and efficient resource allocation. Despite high predictive performance, existing machine learning approaches lack transparency and robustness, limiting clinical adoption. We present a lightweight, transparent multimodal ensemble that fuses physiological time-series measurements with unstructured clinical notes from the first 48 hours of an ICU stay. A logistic regression model combines predictions from two modality-specific models: a bidirectional LSTM for vitals and a finetuned ClinicalModernBERT transformer for notes. This traceable architecture allows for multilevel interpretability: feature attributions within each modality and direct per-case modality attributions quantifying how vitals and notes influence each decision. On the MIMIC-III benchmark, our late-fusion ensemble improves discrimination over the best single model (AUPRC 0.565 vs. 0.526; AUROC 0.891 vs. 0.876) while maintaining well-calibrated predictions. The system remains robust through a calibrated fallback when a modality is missing. These results demonstrate competitive performance with reliable, auditable risk estimates and transparent, predictable operation, which together are crucial for clinical use.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15846v2" target="_blank"><h2>The Loss of Control Playbook: Degrees, Dynamics, and Preparedness <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Charlotte Stix, Annika Hallensleben, Alejandro Ortega, Matteo Pistillo<br><strong><u>Categories:</u></strong> cs.CY, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> attention (abstract), literature review (abstract)<br><p><strong><u>Abstract:</u></strong> This research report addresses the absence of an actionable definition for Loss of Control (LoC) in AI systems by developing a novel taxonomy and preparedness framework. Despite increasing policy and research attention, existing LoC definitions vary significantly in scope and timeline, hindering effective LoC assessment and mitigation. To address this issue, we draw from an extensive literature review and propose a graded LoC taxonomy, based on the metrics of severity and persistence, that distinguishes between Deviation, Bounded LoC, and Strict LoC. We model pathways toward a societal state of vulnerability in which sufficiently advanced AI systems have acquired or could acquire the means to cause Bounded or Strict LoC once a catalyst, either misalignment or pure malfunction, materializes. We argue that this state becomes increasingly likely over time, absent strategic intervention, and propose a strategy to avoid reaching a state of vulnerability. Rather than focusing solely on intervening on AI capabilities and propensities potentially relevant for LoC or on preventing potential catalysts, we introduce a complementary framework that emphasizes three extrinsic factors: Deployment context, Affordances, and Permissions (the DAP framework). Compared to work on intrinsic factors and catalysts, this framework has the unfair advantage of being actionable today. Finally, we put forward a plan to maintain preparedness and prevent the occurrence of LoC outcomes should a state of societal vulnerability be reached, focusing on governance measures (threat modeling, deployment policies, emergency response) and technical controls (pre-deployment testing, control measures, monitoring) that could maintain a condition of perennial suspension.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15838v1" target="_blank"><h2>Attention-Based Feature Online Conformal Prediction for Time Series <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Meiyi Zhu, Caili Guo, Chunyan Feng, Osvaldo Simeone<br><strong><u>Categories:</u></strong> cs.LG, cs.IT, eess.SP<br><strong><u>Comments:</u></strong> 25 pages, 24 figures<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Online conformal prediction (OCP) wraps around any pre-trained predictor to produce prediction sets with coverage guarantees that hold irrespective of temporal dependencies or distribution shifts. However, standard OCP faces two key limitations: it operates in the output space using simple nonconformity (NC) scores, and it treats all historical observations uniformly when estimating quantiles. This paper introduces attention-based feature OCP (AFOCP), which addresses both limitations through two key innovations. First, AFOCP operates in the feature space of pre-trained neural networks, leveraging learned representations to construct more compact prediction sets by concentrating on task-relevant information while suppressing nuisance variation. Second, AFOCP incorporates an attention mechanism that adaptively weights historical observations based on their relevance to the current test point, effectively handling non-stationarity and distribution shifts. We provide theoretical guarantees showing that AFOCP maintains long-term coverage while provably achieving smaller prediction intervals than standard OCP under mild regularity conditions. Extensive experiments on synthetic and real-world time series datasets demonstrate that AFOCP consistently reduces the size of prediction intervals by as much as $88\%$ as compared to OCP, while maintaining target coverage levels, validating the benefits of both feature-space calibration and attention-based adaptive weighting.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15828v1" target="_blank"><h2>Observational constraints on the product of dark energy chemical potential and number density in out-of-equilibrium models <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> J. M. Costa Netto, Javier E. Gonzalez, H. H. B. Silva<br><strong><u>Categories:</u></strong> astro-ph.CO<br><strong><u>Comments:</u></strong> 11 pages, 6 figures, 3 tables<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> In this work, we impose observational limits on the product of dark energy chemical potential, $μ$, and number density, $n$, at the present time in out-of-equilibrium models, considering that particles can be created or destroyed in the fluid at a rate $Γ=3αH(a)$, where $α$ is a constant and $H(a)\equiv\dot{a}/a$ is the Hubble parameter. We combine the bounds derived from the positivity of entropy and the second law of thermodynamics with observational constraints on the Chevallier-Polarski-Linder (CPL) and Barboza-Alcaniz (BA) parameterizations of the equation of state (EoS) of the component. We use Type Ia supernovae (SN Ia) data from Pantheon+; baryon acoustic oscillation (BAO) data from DESI DR2; and cosmic microwave background (CMB) measurements from Planck. For $α>0$ (particle creation), the thermodynamic restrictions yield only upper limits for the $μ_{0}n_{0}$ product, while in the case of $α<0$ (particle destruction) they establish both upper and lower limits, allowing for a range of values to be obtained. In both scenarios, however, we find that the chemical potential of dark energy must be negative, $μ<0$, which indicates a preference for the phantom regime. In particular, when $α<0$, it is noted that the thermodynamic bounds are simultaneously compatible only for very small absolute values of $α$, with $α=-0.0002$ being the limiting case and resulting in $μ_{0}n_{0}(α=-0.0002)=-2.2_{-0.7}^{+1.0}\,\,GeV/m^{3}$.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15822v1" target="_blank"><h2>Atlas Gaussian processes on restricted domains and point clouds <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Mu Niu, Yue Zhang, Ke Ye, Pokman Cheung, Yizhu Wang, Xiaochen Yang<br><strong><u>Categories:</u></strong> stat.ML, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> latent space (abstract)<br><p><strong><u>Abstract:</u></strong> In real-world applications, data often reside in restricted domains with unknown boundaries, or as high-dimensional point clouds lying on a lower-dimensional, nontrivial, unknown manifold. Traditional Gaussian Processes (GPs) struggle to capture the underlying geometry in such settings. Some existing methods assume a flat space embedded in a point cloud, which can be represented by a single latent chart (latent space), while others exhibit weak performance when the point cloud is sparse or irregularly sampled. The goal of this work is to address these challenges. The main contributions are twofold: (1) We establish the Atlas Brownian Motion (BM) framework for estimating the heat kernel on point clouds with unknown geometries and nontrivial topological structures; (2) Instead of directly using the heat kernel estimates, we construct a Riemannian corrected kernel by combining the global heat kernel with local RBF kernel and leading to the formulation of Riemannian-corrected Atlas Gaussian Processes (RC-AGPs). The resulting RC-AGPs are applied to regression tasks across synthetic and real-world datasets. These examples demonstrate that our method outperforms existing approaches in both heat kernel estimation and regression accuracy. It improves statistical inference by effectively bridging the gap between complex, high-dimensional observations and manifold-based inferences.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15809v1" target="_blank"><h2>Unveiling Chemical Enrichment in the Abell 2029 Core with XRISM, XMM-Newton, and Chandra <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Arnab Sarkar, Eric D. Miller, Brian McNamara, Ming Sun, Richard Mushotzky, Stefano Ettori, Lorenzo Lovisari, Irina Zhuravleva, Naomi Ota<br><strong><u>Categories:</u></strong> astro-ph.GA, astro-ph.CO, astro-ph.HE<br><strong><u>Comments:</u></strong> 5 figures, two tables; Accepted for publication in The Astrophysical Journal Letters<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> We present new measurements of the chemical abundance pattern in the core of the nearby galaxy cluster Abell~2029, based on XRISM observations with Resolve (37 ks) and Xtend (500 ks), combined with archival data from XMM-Newton (EPIC, RGS) and Chandra. Fe abundances derived from Resolve, Xtend, and EPIC are broadly consistent, while RGS gives systematically lower values. Because the XRISM gate valve remained closed during these observations, Resolve spectral fitting is restricted to the 2--10 keV band, providing reliable constraints only for elements with strong lines in this band (S, Ar, Ca, Fe, Ni). Abundances of the $α$-elements are therefore derived using complementary observations from Xtend, EPIC, RGS, and Chandra. We construct an average X/Fe pattern in the cluster core by using Resolve exclusively for S/Fe, Ar/Fe, Ca/Fe, and Ni/Fe, and RGS + Xtend for O/Fe. The Ne/Fe ratio is averaged from Xtend, EPIC, RGS, and Chandra measurements; Mg/Fe from EPIC and Chandra measurements; and Si/Fe from Xtend, EPIC, and Chandra. Comparison with the supernovae yield models indicates that the observed abundance pattern in A2029 core is best reproduced by a combination of core-collapsed yields from low-metallicity progenitors ($Z_{\rm init}=0.001$) and a sub-Chandrasekhar-mass, double-degenerate Type Ia model. Additionally, we find an excess in Ca abundance in the core of A2029 that cannot be reproduced by the standard supernovae yield models.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15807v1" target="_blank"><h2>TopoReformer: Mitigating Adversarial Attacks Using Topological Purification in OCR Models <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Bhagyesh Kumar, A S Aravinthakashan, Akshat Satyanarayan, Ishaan Gakhar, Ujjwal Verma<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> Accepted at AAAI 2026 AI for CyberSecurity (AICS) Workshop<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> latent space (abstract)<br><p><strong><u>Abstract:</u></strong> Adversarially perturbed images of text can cause sophisticated OCR systems to produce misleading or incorrect transcriptions from seemingly invisible changes to humans. Some of these perturbations even survive physical capture, posing security risks to high-stakes applications such as document processing, license plate recognition, and automated compliance systems. Existing defenses, such as adversarial training, input preprocessing, or post-recognition correction, are often model-specific, computationally expensive, and affect performance on unperturbed inputs while remaining vulnerable to unseen or adaptive attacks. To address these challenges, TopoReformer is introduced, a model-agnostic reformation pipeline that mitigates adversarial perturbations while preserving the structural integrity of text images. Topology studies properties of shapes and spaces that remain unchanged under continuous deformations, focusing on global structures such as connectivity, holes, and loops rather than exact distance. Leveraging these topological features, TopoReformer employs a topological autoencoder to enforce manifold-level consistency in latent space and improve robustness without explicit gradient regularization. The proposed method is benchmarked on EMNIST, MNIST, against standard adversarial attacks (FGSM, PGD, Carlini-Wagner), adaptive attacks (EOT, BDPA), and an OCR-specific watermark attack (FAWA).</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15796v1" target="_blank"><h2>Teukolsky by Design: A Hybrid Spectral-PINN solver for Kerr Quasinormal Modes <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Alexandre M. Pombo, Lorenzo Pizzuti<br><strong><u>Categories:</u></strong> gr-qc, astro-ph.HE<br><strong><u>Comments:</u></strong> 23 pages, 5 figures, 3 tables<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> We introduce SpectralPINN, a hybrid pseudo-spectral/physics-informed neural network (PINN) solver for Kerr quasinormal modes that targets the Teukolsky equation in both the separated (radial/angular) and joint two-dimensional formulations. The solver replaces standard neural activation functions with Chebyshev polynomials of the first kind and supports both soft -- via loss penalties -- and hard -- enforced by analytic masks -- implementations of Leaver's normalization. Benchmarking against Leaver's continued-fraction method shows cumulative (real+imaginary part) relative frequency errors of $\sim 0.001\%$ for the separated formulation with hard normalization, $\sim 0.1\%$ for both the soft separated and soft joint formulations, and $\sim 0.01\%$ for the hard joint case. Exploiting our ability to solve the joint equation, we add a small quadrupolar perturbation to the Teukolsky operator, effectively rendering the problem non-separable. The resulting perturbed quasinormal modes are compared against the expected precision of the Einstein Telescope, allowing us to constrain the magnitude of the perturbation. These proof-of-concept results demonstrate that hybrid spectral-PINN solvers can provide a flexible pathway to quasinormal spectra in settings where separability, asymptotics, or field content become more intricate and high accuracy is required.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15704v1" target="_blank"><h2>In-N-On: Scaling Egocentric Manipulation with in-the-wild and on-task Data <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Xiongyi Cai, Ri-Zhao Qiu, Geng Chen, Lai Wei, Isabella Liu, Tianshu Huang, Xuxin Cheng, Xiaolong Wang<br><strong><u>Categories:</u></strong> cs.RO, cs.AI, cs.CV<br><strong><u>Comments:</u></strong> Project webpage:this https URL<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> domain adaptation (abstract)<br><p><strong><u>Abstract:</u></strong> Egocentric videos are a valuable and scalable data source to learn manipulation policies. However, due to significant data heterogeneity, most existing approaches utilize human data for simple pre-training, which does not unlock its full potential. This paper first provides a scalable recipe for collecting and using egocentric data by categorizing human data into two categories: in-the-wild and on-task alongside with systematic analysis on how to use the data. We first curate a dataset, PHSD, which contains over 1,000 hours of diverse in-the-wild egocentric data and over 20 hours of on-task data directly aligned to the target manipulation tasks. This enables learning a large egocentric language-conditioned flow matching policy, Human0. With domain adaptation techniques, Human0 minimizes the gap between humans and humanoids. Empirically, we show Human0 achieves several novel properties from scaling human data, including language following of instructions from only human data, few-shot learning, and improved robustness using on-task data. Project website: https://xiongyicai.github.io/In-N-On/</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15699v1" target="_blank"><h2>Joint Semantic-Channel Coding and Modulation for Token Communications <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Jingkai Ying, Zhijin Qin, Yulong Feng, Liejun Wang, Xiaoming Tao<br><strong><u>Categories:</u></strong> eess.SP, cs.AI<br><strong><u>Comments:</u></strong> 14 pages, 14 figures, 2 tables<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> In recent years, the Transformer architecture has achieved outstanding performance across a wide range of tasks and modalities. Token is the unified input and output representation in Transformer-based models, which has become a fundamental information unit. In this work, we consider the problem of token communication, studying how to transmit tokens efficiently and reliably. Point cloud, a prevailing three-dimensional format which exhibits a more complex spatial structure compared to image or video, is chosen to be the information source. We utilize the set abstraction method to obtain point tokens. Subsequently, to get a more informative and transmission-friendly representation based on tokens, we propose a joint semantic-channel and modulation (JSCCM) scheme for the token encoder, mapping point tokens to standard digital constellation points (modulated tokens). Specifically, the JSCCM consists of two parallel Point Transformer-based encoders and a differential modulator which combines the Gumel-softmax and soft quantization methods. Besides, the rate allocator and channel adapter are developed, facilitating adaptive generation of high-quality modulated tokens conditioned on both semantic information and channel conditions. Extensive simulations demonstrate that the proposed method outperforms both joint semantic-channel coding and traditional separate coding, achieving over 1dB gain in reconstruction and more than 6x compression ratio in modulated symbols.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15687v1" target="_blank"><h2>Impact of cosmic expansion on gravitational wave spectra from strongly supercooled first-order phase transitions <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Marek Lewicki, Ville Vaskonen<br><strong><u>Categories:</u></strong> astro-ph.CO, hep-ph<br><strong><u>Comments:</u></strong> 6 pages, 4 figures<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> causality (abstract)<br><p><strong><u>Abstract:</u></strong> We compute the gravitational wave spectra from strongly supercooled first-order phase transitions, explicitly incorporating the evolution of the background metric across the transition from thermal inflation to radiation domination. We find that the spectral shape remains largely unchanged apart from a causality-induced super-horizon tail. However, in contrast to standard expectations, for slow transitions we show that the peak amplitude and frequency exhibit a weaker dependence on the transition rate $β$ than the usual scaling of $\propto β^{-2}$ and $\proptoβ$, respectively.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15684v1" target="_blank"><h2>Walrus: A Cross-Domain Foundation Model for Continuum Dynamics <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Michael McCabe, Payel Mukhopadhyay, Tanya Marwah, Bruno Regaldo-Saint Blancard, Francois Rozet, Cristiana Diaconu, Lucas Meyer, Kaze W. K. Wong, Hadi Sotoudeh, Alberto Bietti, Irina Espejo, Rio Fear, Siavash Golkar, Tom Hehir, Keiya Hirashima, Geraud Krawezik, Francois Lanusse, Rudy Morel, Ruben Ohana, Liam Parker, Mariel Pettee, Jeff Shen, Kyunghyun Cho, Miles Cranmer, Shirley Ho<br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.CE<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Foundation models have transformed machine learning for language and vision, but achieving comparable impact in physical simulation remains a challenge. Data heterogeneity and unstable long-term dynamics inhibit learning from sufficiently diverse dynamics, while varying resolutions and dimensionalities challenge efficient training on modern hardware. Through empirical and theoretical analysis, we incorporate new approaches to mitigate these obstacles, including a harmonic-analysis-based stabilization method, load-balanced distributed 2D and 3D training strategies, and compute-adaptive tokenization. Using these tools, we develop Walrus, a transformer-based foundation model developed primarily for fluid-like continuum dynamics. Walrus is pretrained on nineteen diverse scenarios spanning astrophysics, geoscience, rheology, plasma physics, acoustics, and classical fluids. Experiments show that Walrus outperforms prior foundation models on both short and long term prediction horizons on downstream tasks and across the breadth of pretraining data, while ablation studies confirm the value of our contributions to forecast stability, training throughput, and transfer performance over conventional approaches. Code and weights are released for community use.</p><br><hr><br><hr><p><em>Summary: Showing 156 papers (0 new, 156 seen before)</em></p></body></html>