<!DOCTYPE html><html><head><meta charset='utf-8'><link href='https://fonts.googleapis.com/css?family=Montserrat' rel='stylesheet'>
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [['$','$']],
            processEscapes: true
        },
        "HTML-CSS": {
            availableFonts: ["TeX"]
        }
    });
    </script>
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>
    <style>
    body {font-family: 'Montserrat', sans-serif; background: #F3F3F3; width: 740px; margin: 0 auto; line-height: 150%; margin-top: 50px; font-size: 15px}
    h1 {font-size: 70px}
    a {color: #45ABC2}
    em {font-size: 120%}
    </style>
    </head><body><h1><center>ArXiv Alert</center></h1><font color='#DDAD5C'><em>Update: from 16 Nov 2025 to 23 Nov 2025</em></font><br><br><a href="https://arxiv.org/pdf/2511.17489v1" target="_blank"><h2>Harnessing Data from Clustered LQR Systems: Personalized and Collaborative Policy Optimization <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Vinay Kanakeri, Shivam Bajaj, Ashwin Verma, Vijay Gupta, Aritra Mitra<br><strong><u>Categories:</u></strong> cs.LG, eess.SY, math.OC<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> It is known that reinforcement learning (RL) is data-hungry. To improve sample-efficiency of RL, it has been proposed that the learning algorithm utilize data from 'approximately similar' processes. However, since the process models are unknown, identifying which other processes are similar poses a challenge. In this work, we study this problem in the context of the benchmark Linear Quadratic Regulator (LQR) setting. Specifically, we consider a setting with multiple agents, each corresponding to a copy of a linear process to be controlled. The agents' local processes can be partitioned into clusters based on similarities in dynamics and tasks. Combining ideas from sequential elimination and zeroth-order policy optimization, we propose a new algorithm that performs simultaneous clustering and learning to output a personalized policy (controller) for each cluster. Under a suitable notion of cluster separation that captures differences in closed-loop performance across systems, we prove that our approach guarantees correct clustering with high probability. Furthermore, we show that the sub-optimality gap of the policy learned for each cluster scales inversely with the size of the cluster, with no additional bias, unlike in prior works on collaborative learning-based control. Our work is the first to reveal how clustering can be used in data-driven control to learn personalized policies that enjoy statistical gains from collaboration but do not suffer sub-optimality due to inclusion of data from dissimilar processes. From a distributed implementation perspective, our method is attractive as it incurs only a mild logarithmic communication overhead.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17477v1" target="_blank"><h2>Enhancing Quranic Learning: A Multimodal Deep Learning Approach for Arabic Phoneme Recognition <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Ayhan Kucukmanisa, Derya Gelmez, Sukru Selim Calik, Zeynep Hilal Kilimci<br><strong><u>Categories:</u></strong> cs.SD, cs.AI<br><strong><u>Comments:</u></strong> 11 pages, 2 figures, 3 tables<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract), transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Recent advances in multimodal deep learning have greatly enhanced the capability of systems for speech analysis and pronunciation assessment. Accurate pronunciation detection remains a key challenge in Arabic, particularly in the context of Quranic recitation, where subtle phonetic differences can alter meaning. Addressing this challenge, the present study proposes a transformer-based multimodal framework for Arabic phoneme mispronunciation detection that combines acoustic and textual representations to achieve higher precision and robustness. The framework integrates UniSpeech-derived acoustic embeddings with BERT-based textual embeddings extracted from Whisper transcriptions, creating a unified representation that captures both phonetic detail and linguistic context. To determine the most effective integration strategy, early, intermediate, and late fusion methods were implemented and evaluated on two datasets containing 29 Arabic phonemes, including eight hafiz sounds, articulated by 11 native speakers. Additional speech samples collected from publicly available YouTube recordings were incorporated to enhance data diversity and generalization. Model performance was assessed using standard evaluation metrics: accuracy, precision, recall, and F1-score, allowing a detailed comparison of the fusion strategies. Experimental findings show that the UniSpeech-BERT multimodal configuration provides strong results and that fusion-based transformer architectures are effective for phoneme-level mispronunciation detection. The study contributes to the development of intelligent, speaker-independent, and multimodal Computer-Aided Language Learning (CALL) systems, offering a practical step toward technology-supported Quranic pronunciation training and broader speech-based educational applications.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17475v1" target="_blank"><h2>Addressing A Posteriori Performance Degradation in Neural Network Subgrid Stress Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Andy Wu, Sanjiva K. Lele<br><strong><u>Categories:</u></strong> physics.flu-dyn, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract), data augmentation (abstract)<br><p><strong><u>Abstract:</u></strong> Neural network subgrid stress models often have a priori performance that is far better than the a posteriori performance, leading to neural network models that look very promising a priori completely failing in a posteriori Large Eddy Simulations (LES). This performance gap can be decreased by combining two different methods, training data augmentation and reducing input complexity to the neural network. Augmenting the training data with two different filters before training the neural networks has no performance degradation a priori as compared to a neural network trained with one filter. A posteriori, neural networks trained with two different filters are far more robust across two different LES codes with different numerical schemes. In addition, by ablating away the higher order terms input into the neural network, the a priori versus a posteriori performance changes become less apparent. When combined, neural networks that use both training data augmentation and a less complex set of inputs have a posteriori performance far more reflective of their a priori evaluation.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17446v1" target="_blank"><h2>Unmasking Airborne Threats: Guided-Transformers for Portable Aerosol Mass Spectrometry <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Kyle M. Regan, Michael McLoughlin, Wayne A. Bryden, Gonzalo R. Arce<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 13 pages, 9 figures. Preprint. Submitted to Computers in Biology and Medicine<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> Matrix Assisted Laser Desorption/Ionization Mass Spectrometry (MALDI-MS) is a cornerstone in biomolecular analysis, offering precise identification of pathogens through unique mass spectral signatures. Yet, its reliance on labor-intensive sample preparation and multi-shot spectral averaging restricts its use to laboratory settings, rendering it impractical for real-time environmental monitoring. These limitations are especially pronounced in emerging aerosol MALDI-MS systems, where autonomous sampling generates noisy spectra for unknown aerosol analytes, requiring single-shot detection for effective analysis. Addressing these challenges, we propose the Mass Spectral Dictionary-Guided Transformer (MS-DGFormer): a data-driven framework that redefines spectral analysis by directly processing raw, minimally prepared mass spectral data. MS-DGFormer leverages a transformer architecture, designed to capture the long-range dependencies inherent in these time-series spectra. To enhance feature extraction, we introduce a novel dictionary encoder that integrates denoised spectral information derived from Singular Value Decomposition (SVD), enabling the model to discern critical biomolecular patterns from single-shot spectra with robust performance. This innovation provides a system to achieve superior pathogen identification from aerosol samples, facilitating autonomous, real-time analysis in field conditions. By eliminating the need for extensive preprocessing, our method unlocks the potential for portable, deployable MALDI-MS platforms, revolutionizing environmental pathogen detection and rapid response to biological threats.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17443v1" target="_blank"><h2>GRAPHIC--Guidelines for Reviewing Algorithmic Practices in Human-centred Design and Interaction for Creativity <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Joana Rovira Martins, Pedro Martins, Ana Boavida<br><strong><u>Categories:</u></strong> cs.HC, cs.AI, cs.GR<br><strong><u>Comments:</u></strong> 20 pages, 16 figures<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> explainable (abstract)<br><p><strong><u>Abstract:</u></strong> Artificial Intelligence (AI) has been increasingly applied to creative domains, leading to the development of systems that collaborate with humans in design processes. In Graphic Design, integrating computational systems into co-creative workflows presents specific challenges, as it requires balancing scientific rigour with the subjective and visual nature of design practice. Following the PRISMA methodology, we identified 872 articles, resulting in a final corpus of 71 publications describing 68 unique systems. Based on this review, we introduce GRAPHIC (Guidelines for Reviewing Algorithmic Practices in Human-centred Design and Interaction for Creativity), a framework for analysing AI-based systems applied to Graphic Design. Its goal is to understand how current systems support human-AI collaboration in the Graphic Design discipline. The framework comprises main dimensions, which our analysis revealed to be essential across diverse system types: (1) Collaborative Panorama, (2) Processes and Modalities, and (3) Graphic Design Principles. Its application revealed research gaps, including the need to balance initiative and control between agents, improve communication through explainable interaction models, and promote systems that support transformational creativity grounded in core design principles.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17442v1" target="_blank"><h2>REMSA: An LLM Agent for Foundation Model Selection in Remote Sensing <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Binger Chen, Tacettin Emre Bök, Behnood Rasti, Volker Markl, Begüm Demir<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> Code and data available atthis https URL<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Foundation Models (FMs) are increasingly used in remote sensing (RS) for tasks such as environmental monitoring, disaster assessment, and land-use mapping. These models include unimodal vision encoders trained on a single data modality and multimodal architectures trained on combinations of SAR, multispectral, hyperspectral, and image-text data. They support diverse RS tasks including semantic segmentation, image classification, change detection, and visual question answering. However, selecting an appropriate remote sensing foundation model (RSFM) remains difficult due to scattered documentation, heterogeneous formats, and varied deployment constraints. We introduce the RSFM Database (RS-FMD), a structured resource covering over 150 RSFMs spanning multiple data modalities, resolutions, and learning paradigms. Built on RS-FMD, we present REMSA, the first LLM-based agent for automated RSFM selection from natural language queries. REMSA interprets user requirements, resolves missing constraints, ranks candidate models using in-context learning, and provides transparent justifications. We also propose a benchmark of 75 expert-verified RS query scenarios, producing 900 configurations under an expert-centered evaluation protocol. REMSA outperforms several baselines, including naive agents, dense retrieval, and unstructured RAG-based LLMs. It operates entirely on publicly available metadata and does not access private or sensitive data.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17439v1" target="_blank"><h2>InTAct: Interval-based Task Activation Consolidation for Continual Learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Patryk Krukowski, Jan Miksa, Piotr Helm, Jacek Tabor, Paweł Wawrzyński, Przemysław Spurek<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Continual learning aims to enable neural networks to acquire new knowledge without forgetting previously learned information. While recent prompt-based methods perform strongly in class-incremental settings, they remain vulnerable under domain shifts, where the input distribution changes but the label space remains fixed. This exposes a persistent problem known as representation drift. Shared representations evolve in ways that overwrite previously useful features and cause forgetting even when prompts isolate task-specific parameters. To address this issue, we introduce InTAct, a method that preserves functional behavior in shared layers without freezing parameters or storing past data. InTAct captures the characteristic activation ranges associated with previously learned tasks and constrains updates to ensure the network remains consistent within these regions, while still allowing for flexible adaptation elsewhere. In doing so, InTAct stabilizes the functional role of important neurons rather than directly restricting parameter values. The approach is architecture-agnostic and integrates seamlessly into existing prompt-based continual learning frameworks. By regulating representation changes where past knowledge is encoded, InTAct achieves a principled balance between stability and plasticity. Across diverse domain-incremental benchmarks, including DomainNet and ImageNet-R, InTAct consistently reduces representation drift and improves performance, increasing Average Accuracy by up to 8 percentage points over state-of-the-art baselines.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17435v1" target="_blank"><h2>Multi-Agent Pointer Transformer: Seq-to-Seq Reinforcement Learning for Multi-Vehicle Dynamic Pickup-Delivery Problems <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zengyu Zou, Jingyuan Wang, Yixuan Huang, Junjie Wu<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 15 pages<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> This paper addresses the cooperative Multi-Vehicle Dynamic Pickup and Delivery Problem with Stochastic Requests (MVDPDPSR) and proposes an end-to-end centralized decision-making framework based on sequence-to-sequence, named Multi-Agent Pointer Transformer (MAPT). MVDPDPSR is an extension of the vehicle routing problem and a spatio-temporal system optimization problem, widely applied in scenarios such as on-demand delivery. Classical operations research methods face bottlenecks in computational complexity and time efficiency when handling large-scale dynamic problems. Although existing reinforcement learning methods have achieved some progress, they still encounter several challenges: 1) Independent decoding across multiple vehicles fails to model joint action distributions; 2) The feature extraction network struggles to capture inter-entity relationships; 3) The joint action space is exponentially large. To address these issues, we designed the MAPT framework, which employs a Transformer Encoder to extract entity representations, combines a Transformer Decoder with a Pointer Network to generate joint action sequences in an AutoRegressive manner, and introduces a Relation-Aware Attention module to capture inter-entity relationships. Additionally, we guide the model's decision-making using informative priors to facilitate effective exploration. Experiments on 8 datasets demonstrate that MAPT significantly outperforms existing baseline methods in terms of performance and exhibits substantial computational time advantages compared to classical operations research methods.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17408v1" target="_blank"><h2>That's not natural: The Impact of Off-Policy Training Data on Probe Performance <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Nathalie Kirch, Samuel Dower, Adrians Skapars, Ekdeep Singh Lubana, Dmitrii Krasheninnikov<br><strong><u>Categories:</u></strong> cs.AI, cs.LG<br><strong><u>Comments:</u></strong> 10 pages, EurIPS 2025 Workshop on Private AI Governance<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Probing has emerged as a promising method for monitoring Large Language Models (LLMs), enabling inference-time detection of concerning behaviours such as deception and sycophancy. However, natural examples of many behaviours are rare, forcing researchers to rely on synthetic or off-policy LLM responses for training probes. We systematically evaluate how the use of synthetic and off-policy data influences probe generalisation across eight distinct LLM behaviours. Testing linear and attention probes across multiple LLMs, we find that the response generation strategy can significantly affect probe performance, though the magnitude of this effect varies by behaviour. We find that successful generalisation from off-policy data, to test sets where the model is incentivised to produce the target behaviour, is predictive of successful on-policy generalisation. Leveraging this result, we predict that Deception and Sandbagging probes may fail to generalise from off-policy to on-policy data when used in real monitoring scenarios. Notably, shifts in the training data domain still cause even larger performance degradation, with different-domain test scores being consistently lower than the same-domain ones. These results indicate that, in the absence of on-policy data, using same-domain off-policy data yields more reliable probes than using on-policy data from a different domain, emphasizing the need for methods that can better handle distribution shifts in LLM monitoring.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17405v1" target="_blank"><h2>Beyond Multiple Choice: A Hybrid Framework for Unifying Robust Evaluation and Verifiable Reasoning Training <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yesheng Liu, Hao Li, Haiyu Xu, Baoqi Pei, Jiahao Wang, Mingxuan Zhao, Jingshu Zheng, Zheqi He, JG Yao, Bowen Qin, Xi Yang, Jiajun Zhang<br><strong><u>Categories:</u></strong> cs.CL, cs.AI<br><strong><u>Comments:</u></strong> Project url:this https URL<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Multiple-choice question answering (MCQA) has been a popular format for evaluating and reinforcement fine-tuning (RFT) of modern multimodal language models. Its constrained output format allows for simplified, deterministic automatic verification. However, we find that the options may leak exploitable signals, which makes the accuracy metrics unreliable for indicating real capabilities and encourages explicit or implicit answer guessing behaviors during RFT. We propose ReVeL (Rewrite and Verify by LLM), a framework that rewrites multiple-choice questions into open-form questions while keeping answers verifiable whenever possible. The framework categorizes questions according to different answer types, apply different rewriting and verification schemes, respectively. When applied for RFT, we converted 20k MCQA examples and use GRPO to finetune Qwen2.5-VL models. Models trained on ReVeL-OpenQA match MCQA accuracy on multiple-choice benchmarks and improve OpenQA accuracy by about six percentage points, indicating better data efficiency and more robust reward signals than MCQA-based training. When used for evaluation, ReVeL also reveals up to 20 percentage points of score inflation in MCQA benchmarks (relative to OpenQA), improves judging accuracy, and reduces both cost and latency. We will release code and data publicly.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17400v1" target="_blank"><h2>Sparse Mixture-of-Experts for Multi-Channel Imaging: Are All Channel Interactions Required? <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Sukwon Yun, Heming Yao, Burkhard Hoeckendorf, David Richmond, Aviv Regev, Russell Littman<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> This has been accepted at the NeurIPS AI4Science Workshop 2025<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Vision Transformers ($\text{ViTs}$) have become the backbone of vision foundation models, yet their optimization for multi-channel domains - such as cell painting or satellite imagery - remains underexplored. A key challenge in these domains is capturing interactions between channels, as each channel carries different information. While existing works have shown efficacy by treating each channel independently during tokenization, this approach naturally introduces a major computational bottleneck in the attention block - channel-wise comparisons leads to a quadratic growth in attention, resulting in excessive $\text{FLOPs}$ and high training cost. In this work, we shift focus from efficacy to the overlooked efficiency challenge in cross-channel attention and ask: "Is it necessary to model all channel interactions?". Inspired by the philosophy of Sparse Mixture-of-Experts ($\text{MoE}$), we propose MoE-ViT, a Mixture-of-Experts architecture for multi-channel images in $\text{ViTs}$, which treats each channel as an expert and employs a lightweight router to select only the most relevant experts per patch for attention. Proof-of-concept experiments on real-world datasets - JUMP-CP and So2Sat - demonstrate that $\text{MoE-ViT}$ achieves substantial efficiency gains without sacrificing, and in some cases enhancing, performance, making it a practical and attractive backbone for multi-channel imaging.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17388v1" target="_blank"><h2>Selective Rotary Position Embedding <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Sajad Movahedi, Timur Carstensen, Arshia Afzal, Frank Hutter, Antonio Orvieto, Volkan Cevher<br><strong><u>Categories:</u></strong> cs.CL, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Position information is essential for language modeling. In softmax transformers, Rotary Position Embeddings (\textit{RoPE}) encode positions through \textit{fixed-angle} rotations, while in linear transformers, order is handled via input-dependent (selective) gating that decays past key-value associations. Selectivity has generally been shown to improve language-related tasks. Inspired by this, we introduce \textit{Selective RoPE}, an \textit{input-dependent} rotary embedding mechanism, that generalizes \textit{RoPE}, and enables rotation in \textit{arbitrary angles} for both linear and softmax transformers. We show that softmax attention already performs a hidden form of these rotations on query-key pairs, uncovering an implicit positional structure. We further show that in state-space models and gated linear transformers, the real part manages forgetting while the imaginary part encodes positions through rotations. We validate our method by equipping gated transformers with \textit{Selective RoPE}, demonstrating that its input-dependent rotations improve performance in language modeling and on difficult sequence tasks like copying, state tracking, and retrieval.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17367v1" target="_blank"><h2>R2PS: Worst-Case Robust Real-Time Pursuit Strategies under Partial Observability <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Runyu Lu, Ruochuan Shi, Yuanheng Zhu, Dongbin Zhao<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Computing worst-case robust strategies in pursuit-evasion games (PEGs) is time-consuming, especially when real-world factors like partial observability are considered. While important for general security purposes, real-time applicable pursuit strategies for graph-based PEGs are currently missing when the pursuers only have imperfect information about the evader's position. Although state-of-the-art reinforcement learning (RL) methods like Equilibrium Policy Generalization (EPG) and Grasper provide guidelines for learning graph neural network (GNN) policies robust to different game dynamics, they are restricted to the scenario of perfect information and do not take into account the possible case where the evader can predict the pursuers' actions. This paper introduces the first approach to worst-case robust real-time pursuit strategies (R2PS) under partial observability. We first prove that a traditional dynamic programming (DP) algorithm for solving Markov PEGs maintains optimality under the asynchronous moves by the evader. Then, we propose a belief preservation mechanism about the evader's possible positions, extending the DP pursuit strategies to a partially observable setting. Finally, we embed the belief preservation into the state-of-the-art EPG framework to finish our R2PS learning scheme, which leads to a real-time pursuer policy through cross-graph reinforcement learning against the asynchronous-move DP evasion strategies. After reinforcement learning, our policy achieves robust zero-shot generalization to unseen real-world graph structures and consistently outperforms the policy directly trained on the test graphs by the existing game RL approach.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17339v1" target="_blank"><h2>ReBaPL: Repulsive Bayesian Prompt Learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yassir Bendou, Omar Ezzahir, Eduardo Fernandes Montesuma, Gabriel Mahuas, Victoria Shevchenko, Mike Gartrell<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> Under review<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Prompt learning has emerged as an effective technique for fine-tuning large-scale foundation models for downstream tasks. However, conventional prompt tuning methods are prone to overfitting and can struggle with out-of-distribution generalization. To address these limitations, Bayesian prompt learning has been proposed, which frames prompt optimization as a Bayesian inference problem to enhance robustness. This paper introduces Repulsive Bayesian Prompt Learning (ReBaPL), a novel method for Bayesian prompt learning, designed to efficiently explore the complex and often multimodal posterior landscape of prompts. Our method integrates a cyclical step-size schedule with a stochastic gradient Hamiltonian Monte Carlo (SGHMC) algorithm, enabling alternating phases of exploration to discover new modes, and exploitation to refine existing modes. Furthermore, we introduce a repulsive force derived from a potential function over probability metrics (including Maximum Mean Discrepancy and Wasserstein distance) computed on the distributions of representations produced by different prompts. This representation-space repulsion diversifies exploration and prevents premature collapse to a single mode. Our approach allows for a more comprehensive characterization of the prompt posterior distribution, leading to improved generalization. In contrast to prior Bayesian prompt learning methods, our method provides a modular plug-and-play Bayesian extension of any existing prompt learning method based on maximum likelihood estimation. We demonstrate the efficacy of ReBaPL on several benchmark datasets, showing superior performance over state-of-the-art methods for prompt learning.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17332v1" target="_blank"><h2>Agentifying Agentic AI <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Virginia Dignum, Frank Dignum<br><strong><u>Categories:</u></strong> cs.AI, cs.MA<br><strong><u>Comments:</u></strong> 10 pages; 1 figure<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> Agentic AI seeks to endow systems with sustained autonomy, reasoning, and interaction capabilities. To realize this vision, its assumptions about agency must be complemented by explicit models of cognition, cooperation, and governance. This paper argues that the conceptual tools developed within the Autonomous Agents and Multi-Agent Systems (AAMAS) community, such as BDI architectures, communication protocols, mechanism design, and institutional modelling, provide precisely such a foundation. By aligning adaptive, data-driven approaches with structured models of reasoning and coordination, we outline a path toward agentic systems that are not only capable and flexible, but also transparent, cooperative, and accountable. The result is a perspective on agency that bridges formal theory and practical autonomy.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17331v1" target="_blank"><h2>AI Workers, Geopolitics, and Algorithmic Collective Action <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Sydney Reis<br><strong><u>Categories:</u></strong> cs.CY, cs.AI, cs.HC<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> According to the theory of International Political Economy (IPE), states are often incentivized to rely on rather than constrain powerful corporations. For this reason, IPE provides a useful lens to explain why efforts to govern Artificial Intelligence (AI) at the international and national levels have thus far been developed, applied, and enforced unevenly. Building on recent work that explores how AI companies engage in geopolitics, this position paper argues that some AI workers can be considered actors of geopolitics. It makes the timely case that governance alone cannot ensure responsible, ethical, or robust AI development and use, and greater attention should be paid to bottom-up interventions at the site of AI development. AI workers themselves should be situated as individual agents of change, especially when considering their potential to foster Algorithmic Collective Action (ACA). Drawing on methods of Participatory Design (PD), this paper proposes engaging AI workers as sources of knowledge, relative power, and intentionality to encourage more responsible and just AI development and create the conditions that can facilitate ACA.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17323v1" target="_blank"><h2>MusicAIR: A Multimodal AI Music Generation Framework Powered by an Algorithm-Driven Core <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Callie C. Liao, Duoduo Liao, Ellie L. Zhang<br><strong><u>Categories:</u></strong> cs.SD, cs.AI, cs.CL, cs.MM<br><strong><u>Comments:</u></strong> Accepted by IEEE Big Data 2025<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Recent advances in generative AI have made music generation a prominent research focus. However, many neural-based models rely on large datasets, raising concerns about copyright infringement and high-performance costs. In contrast, we propose MusicAIR, an innovative multimodal AI music generation framework powered by a novel algorithm-driven symbolic music core, effectively mitigating copyright infringement risks. The music core algorithms connect critical lyrical and rhythmic information to automatically derive musical features, creating a complete, coherent melodic score solely from the lyrics. The MusicAIR framework facilitates music generation from lyrics, text, and images. The generated score adheres to established principles of music theory, lyrical structure, and rhythmic conventions. We developed Generate AI Music (GenAIM), a web tool using MusicAIR for lyric-to-song, text-to-music, and image-to-music generation. In our experiments, we evaluated AI-generated music scores produced by the system using both standard music metrics and innovative analysis that compares these compositions with original works. The system achieves an average key confidence of 85%, outperforming human composers at 79%, and aligns closely with established music theory standards, demonstrating its ability to generate diverse, human-like compositions. As a co-pilot tool, GenAIM can serve as a reliable music composition assistant and a possible educational composition tutor while simultaneously lowering the entry barrier for all aspiring musicians, which is innovative and significantly contributes to AI for music generation.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17318v1" target="_blank"><h2>FORWARD: Dataset of a forwarder operating in rough terrain <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Mikael Lundbäck, Erik Wallin, Carola Häggström, Mattias Nyström, Andreas Grönlund, Mats Richardson, Petrus Jönsson, William Arnvik, Lucas Hedström, Arvid Fälldin, Martin Servin<br><strong><u>Categories:</u></strong> cs.RO, cs.AI, cs.CE, cs.LG, physics.app-ph<br><strong><u>Comments:</u></strong> 25 pages, 22 figures<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> We present FORWARD, a high-resolution multimodal dataset of a cut-to-length forwarder operating in rough terrain on two harvest sites in the middle part of Sweden. The forwarder is a large Komatsu model equipped with a variety of sensors, including RTK-GNSS, 360-camera, operator vibration sensors, internal CAN-bus signal recording, and multiple IMUs. The data includes event time logs recorded in 5 Hz with e.g., driving speed, fuel consumption, vehicle position with centimeter accuracy, and crane use while the vehicle operates in forest areas laser-scanned with very high-resolution, $\sim$1500 points per square meter. Production log files (StanForD standard) with time-stamped machine events, extensive video material, and terrain data in various formats are included as well. About 18 hours of regular wood extraction work during three days is annotated from 360-video material into individual work elements and included in the dataset. We also include scenario specifications of conducted experiments on forest roads and in terrain. Scenarios include repeatedly driving the same routes with and without steel tracks, different load weight, and different target driving speeds. The dataset is intended for developing models and algorithms for trafficability, perception, and autonomous control of forest machines using artificial intelligence, simulation, and experiments on physical testbeds. In part, we focus on forwarders traversing terrain, avoiding obstacles, and loading or unloading logs, with consideration for efficiency, fuel consumption, safety, and environmental impact. Other benefits of the open dataset include the ability to explore auto-generation and calibration of forestry machine simulators and automation scenario descriptions using the data recorded in the field.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17309v1" target="_blank"><h2>MuM: Multi-View Masked Image Modeling for 3D Vision <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> David Nordström, Johan Edstedt, Fredrik Kahl, Georg Bökman<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Self-supervised learning on images seeks to extract meaningful visual representations from unlabeled data. When scaled to large datasets, this paradigm has achieved state-of-the-art performance and the resulting trained models such as DINOv3 have seen widespread adoption. However, most prior efforts are optimized for semantic understanding rather than geometric reasoning. One important exception is Cross-View Completion, CroCo, which is a form of masked autoencoding (MAE) tailored for 3D understanding. In this work, we continue on the path proposed by CroCo and focus on learning features tailored for 3D vision. In a nutshell, we extend MAE to arbitrarily many views of the same scene. By uniformly masking all views and employing a lightweight decoder with inter-frame attention, our approach is inherently simpler and more scalable than CroCo. We evaluate the resulting model, MuM, extensively on downstream tasks including feedforward reconstruction, dense image matching and relative pose estimation, finding that it outperforms the state-of-the-art visual encoders DINOv3 and CroCo v2.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17298v1" target="_blank"><h2>SAVeD: Semantic Aware Version Discovery <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Artem Frenk, Roee Shraga<br><strong><u>Categories:</u></strong> cs.LG, stat.ML<br><strong><u>Comments:</u></strong> 11 pages, 6 figures<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> latent space (abstract), transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Our work introduces SAVeD (Semantically Aware Version Detection), a contrastive learning-based framework for identifying versions of structured datasets without relying on metadata, labels, or integration-based assumptions. SAVeD addresses a common challenge in data science of repeated labor due to a difficulty of similar work or transformations on datasets. SAVeD employs a modified SimCLR pipeline, generating augmented table views through random transformations (e.g., row deletion, encoding perturbations). These views are embedded via a custom transformer encoder and contrasted in latent space to optimize semantic similarity. Our model learns to minimize distances between augmented views of the same dataset and maximize those between unrelated tables. We evaluate performance using validation accuracy and separation, defined respectively as the proportion of correctly classified version/non-version pairs on a hold-out set, and the difference between average similarities of versioned and non-versioned tables (defined by a benchmark, and not provided to the model). Our experiments span five canonical datasets from the Semantic Versioning in Databases Benchmark, and demonstrate substantial gains post-training. SAVeD achieves significantly higher accuracy on completely unseen tables in, and a significant boost in separation scores, confirming its capability to distinguish semantically altered versions. Compared to untrained baselines and prior state-of-the-art dataset-discovery methods like Starmie, our custom encoder achieves competitive or superior results.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17276v1" target="_blank"><h2>Leveraging CVAE for Joint Configuration Estimation of Multifingered Grippers from Point Cloud Data <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Julien Merand, Boris Meden, Mathieu Grossard<br><strong><u>Categories:</u></strong> cs.RO, cs.AI, cs.CV<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> VAE (title, abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> This paper presents an efficient approach for determining the joint configuration of a multifingered gripper solely from the point cloud data of its poly-articulated chain, as generated by visual sensors, simulations or even generative neural networks. Well-known inverse kinematics (IK) techniques can provide mathematically exact solutions (when they exist) for joint configuration determination based solely on the fingertip pose, but often require post-hoc decision-making by considering the positions of all intermediate phalanges in the gripper's fingers, or rely on algorithms to numerically approximate solutions for more complex kinematics. In contrast, our method leverages machine learning to implicitly overcome these challenges. This is achieved through a Conditional Variational Auto-Encoder (CVAE), which takes point cloud data of key structural elements as input and reconstructs the corresponding joint configurations. We validate our approach on the MultiDex grasping dataset using the Allegro Hand, operating within 0.05 milliseconds and achieving accuracy comparable to state-of-the-art methods. This highlights the effectiveness of our pipeline for joint configuration estimation within the broader context of AI-driven techniques for grasp planning.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17254v1" target="_blank"><h2>Intervene-All-Paths: Unified Mitigation of LVLM Hallucinations across Alignment Formats <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jiaye Qian, Ge Zheng, Yuchen Zhu, Sibei Yang<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> Accepted to NeurIPS 2025, Project Page:this https URL<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Despite their impressive performance across a wide range of tasks, Large Vision-Language Models (LVLMs) remain prone to hallucination. In this study, we propose a comprehensive intervention framework aligned with the transformer's causal architecture in LVLMs, integrating the effects of different intervention paths on hallucination. We find that hallucinations in LVLMs do not arise from a single causal path, but rather from the interplay among image-to-input-text, image-to-output-text, and text-to-text pathways. For the first time, we also find that LVLMs rely on different pathways depending on the question-answer alignment format. Building on these insights, we propose simple yet effective methods to identify and intervene on critical hallucination heads within each pathway, tailored to discriminative and generative formats. Experiments across multiple benchmarks demonstrate that our approach consistently reduces hallucinations across diverse alignment types.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17242v1" target="_blank"><h2>Equivariant-Aware Structured Pruning for Efficient Edge Deployment: A Comprehensive Framework with Adaptive Fine-Tuning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Mohammed Alnemari<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> 8 pages, 5 tables, 1 figure. Accepted at IEEE EdgeCom 2025 (11th IEEE International Conference on Edge Computing and Scalable Cloud)<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> This paper presents a novel framework combining group equivariant convolutional neural networks (G-CNNs) with equivariant-aware structured pruning to produce compact, transformation-invariant models for resource-constrained environments. Equivariance to rotations is achieved through the C4 cyclic group via the e2cnn library,enabling consistent performance under geometric transformations while reducing computational overhead.
  Our approach introduces structured pruning that preserves equivariant properties by analyzing e2cnn layer structure and applying neuron-level pruning to fully connected components. To mitigate accuracy degradation, we implement adaptive fine-tuning that automatically triggers when accuracy drop exceeds 2%, using early stopping and learning rate scheduling for efficient recovery. The framework includes dynamic INT8 quantization and a comprehensive pipeline encompassing training, knowledge distillation, structured pruning, fine-tuning, and quantization.
  We evaluate our method on satellite imagery (EuroSAT) and standard benchmarks (CIFAR-10, Rotated MNIST) demonstrating effectiveness across diverse domains. Experimental results show 29.3% parameter reduction with significant accuracy recovery, demonstrating that structured pruning of equivariant networks achieves substantial compression while maintaining geometric robustness. Our pipeline provides a reproducible framework for optimizing equivariant models, bridging the gap between group-theoretic network design and practical deployment constraints, with particular relevance to satellite imagery analysis and geometric vision tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17233v1" target="_blank"><h2>Algorithmic design and implementation considerations of deep MPC <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Prabhat K. Mishra, Mateus V. Gasparino, Girish Chowdhary<br><strong><u>Categories:</u></strong> eess.SY, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Deep Model Predictive Control (Deep MPC) is an evolving field that integrates model predictive control and deep learning. This manuscript is focused on a particular approach, which employs deep neural network in the loop with MPC. This class of approaches distributes control authority between a neural network and an MPC controller, in such a way that the neural network learns the model uncertainties while the MPC handles constraints. The approach is appealing because training data collected while the system is in operation can be used to fine-tune the neural network, and MPC prevents unsafe behavior during those learning transients. This manuscript explains implementation challenges of Deep MPC, algorithmic way to distribute control authority and argues that a poor choice in distributing control authority may lead to poor performance. A reason of poor performance is explained through a numerical experiment on a four-wheeled skid-steer dynamics.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17228v1" target="_blank"><h2>Intrinsic preservation of plasticity in continual quantum learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yu-Qin Chen, Shi-Xin Zhang<br><strong><u>Categories:</u></strong> quant-ph, cs.LG<br><strong><u>Comments:</u></strong> 11 pages, 5 figures and supplementary information<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Artificial intelligence in dynamic, real-world environments requires the capacity for continual learning. However, standard deep learning suffers from a fundamental issue: loss of plasticity, in which networks gradually lose their ability to learn from new data. Here we show that quantum learning models naturally overcome this limitation, preserving plasticity over long timescales. We demonstrate this advantage systematically across a broad spectrum of tasks from multiple learning paradigms, including supervised learning and reinforcement learning, and diverse data modalities, from classical high-dimensional images to quantum-native datasets. Although classical models exhibit performance degradation correlated with unbounded weight and gradient growth, quantum neural networks maintain consistent learning capabilities regardless of the data or task. We identify the origin of the advantage as the intrinsic physical constraints of quantum models. Unlike classical networks where unbounded weight growth leads to landscape ruggedness or saturation, the unitary constraints confine the optimization to a compact manifold. Our results suggest that the utility of quantum computing in machine learning extends beyond potential speedups, offering a robust pathway for building adaptive artificial intelligence and lifelong learners.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17219v1" target="_blank"><h2>DelTriC: A Novel Clustering Method with Accurate Outlier <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Tomas Javurek, Michal Gregor, Sebastian Kula, Marian Simko<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 10 pages, submitted to AISTATS<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> anomaly detection (abstract)<br><p><strong><u>Abstract:</u></strong> The paper introduces DelTriC (Delaunay Triangulation Clustering), a clustering algorithm which integrates PCA/UMAP-based projection, Delaunay triangulation, and a novel back-projection mechanism to form clusters in the original high-dimensional space. DelTriC decouples neighborhood construction from decision-making by first triangulating in a low-dimensional proxy to index local adjacency, and then back-projecting to the original space to perform robust edge pruning, merging, and anomaly detection. DelTriC can outperform traditional methods such as k-means, DBSCAN, and HDBSCAN in many scenarios; it is both scalable and accurate, and it also significantly improves outlier detection.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17184v1" target="_blank"><h2>Attention-Guided Feature Fusion (AGFF) Model for Integrating Statistical and Semantic Features in News Text Classification <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Mohammad Zare<br><strong><u>Categories:</u></strong> cs.CL, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> News text classification is a crucial task in natural language processing, essential for organizing and filtering the massive volume of digital content. Traditional methods typically rely on statistical features like term frequencies or TF-IDF values, which are effective at capturing word-level importance but often fail to reflect contextual meaning. In contrast, modern deep learning approaches utilize semantic features to understand word usage within context, yet they may overlook simple, high-impact statistical indicators. This paper introduces an Attention-Guided Feature Fusion (AGFF) model that combines statistical and semantic features in a unified framework. The model applies an attention-based mechanism to dynamically determine the relative importance of each feature type, enabling more informed classification decisions. Through evaluation on benchmark news datasets, the AGFF model demonstrates superior performance compared to both traditional statistical models and purely semantic deep learning models. The results confirm that strategic integration of diverse feature types can significantly enhance classification accuracy. Additionally, ablation studies validate the contribution of each component in the fusion process. The findings highlight the model's ability to balance and exploit the complementary strengths of statistical and semantic representations, making it a practical and effective solution for real-world news classification tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17181v1" target="_blank"><h2>Investigating self-supervised representations for audio-visual deepfake detection <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Dragos-Alexandru Boldisor, Stefan Smeu, Dan Oneata, Elisabeta Oneata<br><strong><u>Categories:</u></strong> cs.CV, cs.LG, cs.SD<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Self-supervised representations excel at many vision and speech tasks, but their potential for audio-visual deepfake detection remains underexplored. Unlike prior work that uses these features in isolation or buried within complex architectures, we systematically evaluate them across modalities (audio, video, multimodal) and domains (lip movements, generic visual content). We assess three key dimensions: detection effectiveness, interpretability of encoded information, and cross-modal complementarity. We find that most self-supervised features capture deepfake-relevant information, and that this information is complementary. Moreover, models primarily attend to semantically meaningful regions rather than spurious artifacts. Yet none generalize reliably across datasets. This generalization failure likely stems from dataset characteristics, not from the features themselves latching onto superficial patterns. These results expose both the promise and fundamental challenges of self-supervised representations for deepfake detection: while they learn meaningful patterns, achieving robust cross-domain performance remains elusive.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17171v1" target="_blank"><h2>FireScope: Wildfire Risk Prediction with a Chain-of-Thought Oracle <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Mario Markov, Stefan Maria Ailuro, Luc Van Gool, Konrad Schindler, Danda Pani Paudel<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Predicting wildfire risk is a reasoning-intensive spatial problem that requires the integration of visual, climatic, and geographic factors to infer continuous risk maps. Existing methods lack the causal reasoning and multimodal understanding required for reliable generalization. We introduce $\textbf{FireScope-Bench}$, a large-scale dataset and benchmark that couples Sentinel-2 imagery and climate data with expert-defined risk rasters across the USA, and real wildfire events in Europe for cross-continental evaluation. Building on this dataset, we propose $\textbf{FireScope}$, a VLM-based reasoning-to-generation framework that learns from both reinforcement learning and visual supervision to predict risk rasters with complementary reasoning traces. When trained in the USA and tested in Europe, $\textbf{FireScope}$ achieves substantial performance gains, while expert feedback and automated analysis confirm that its reasoning traces are faithful and semantically meaningful. Our findings demonstrate that reasoning can ground raster prediction models, improving both generalization and interpretability. To our knowledge, this is the first framework to (1) demonstrate that language-based reasoning can improve generalization in visual generation, (2) propose a high-resolution wildfire risk model that can be applied across continents, and (3) enable systematic studies of robust cross-continental generalization for multimodal fire risk models. We believe that $\textbf{FireScope-Bench}$ has the potential to serve as a foundation for advancing reasoning-driven, interpretable and generalizable spatial modeling. Data and source code will be made publicly available.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17162v1" target="_blank"><h2>The Belief-Desire-Intention Ontology for modelling mental reality and agency <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Sara Zuppiroli, Carmelo Fabio Longo, Anna Sofia Lippolis, Rocco Paolillo, Lorenzo Giammei, Miguel Ceriani, Francesco Poggi, Antonio Zinilli, Andrea Giovanni Nuzzolese<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> explainable (abstract)<br><p><strong><u>Abstract:</u></strong> The Belief-Desire-Intention (BDI) model is a cornerstone for representing rational agency in artificial intelligence and cognitive sciences. Yet, its integration into structured, semantically interoperable knowledge representations remains limited. This paper presents a formal BDI Ontology, conceived as a modular Ontology Design Pattern (ODP) that captures the cognitive architecture of agents through beliefs, desires, intentions, and their dynamic interrelations. The ontology ensures semantic precision and reusability by aligning with foundational ontologies and best practices in modular design. Two complementary lines of experimentation demonstrate its applicability: (i) coupling the ontology with Large Language Models (LLMs) via Logic Augmented Generation (LAG) to assess the contribution of ontological grounding to inferential coherence and consistency; and (ii) integrating the ontology within the Semas reasoning platform, which implements the Triples-to-Beliefs-to-Triples (T2B2T) paradigm, enabling a bidirectional flow between RDF triples and agent mental states. Together, these experiments illustrate how the BDI Ontology acts as both a conceptual and operational bridge between declarative and procedural intelligence, paving the way for cognitively grounded, explainable, and semantically interoperable multi-agent and neuro-symbolic systems operating within the Web of Data.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17161v1" target="_blank"><h2>The PLLuM Instruction Corpus <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Piotr Pęzik, Filip Żarnecki, Konrad Kaczyński, Anna Cichosz, Zuzanna Deckert, Monika Garnys, Izabela Grabarczyk, Wojciech Janowski, Sylwia Karasińska, Aleksandra Kujawiak, Piotr Misztela, Maria Szymańska, Karolina Walkusz, Igor Siek, Maciej Chrabąszcz, Anna Kołos, Agnieszka Karlińska, Karolina Seweryn, Aleksandra Krasnodębska, Paula Betscher, Zofia Cieślińska, Katarzyna Kowol, Artur Wilczek, Maciej Trzciński, Katarzyna Dziewulska, Roman Roszko, Tomasz Bernaś, Jurgita Vaičenonienė, Danuta Roszko, Paweł Levchuk, Paweł Kowalski, Irena Prawdzic-Jankowska, Marek Kozłowski, Sławomir Dadas, Rafał Poświata, Alina Wróblewska, Katarzyna Krasnowska-Kieraś, Maciej Ogrodniczuk, Michał Rudolf, Piotr Rybak, Karolina Saputa, Joanna Wołoszyn, Marcin Oleksy, Bartłomiej Koptyra, Teddy Ferdinan, Stanisław Woźniak, Maciej Piasecki, Paweł Walkowiak, Konrad Wojtasik, Arkadiusz Janz, Przemysław Kazienko, Julia Moska, Jan Kocoń<br><strong><u>Categories:</u></strong> cs.CL, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> This paper describes the instruction dataset used to fine-tune a set of transformer-based large language models (LLMs) developed in the PLLuM (Polish Large Language Model) project. We present a functional typology of the organic, converted, and synthetic instructions used in PLLuM and share some observations about the implications of using human-authored versus synthetic instruction datasets in the linguistic adaptation of base LLMs. Additionally, we release the first representative subset of the PLLuM instruction corpus (PLLuMIC), which we believe to be useful in guiding and planning the development of similar datasets for other LLMs.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17147v1" target="_blank"><h2>A lightweight detector for real-time detection of remote sensing images <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Qianyi Wang, Guoqiang Ren<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> none<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Remote sensing imagery is widely used across various fields, yet real-time detection remains challenging due to the prevalence of small objects and the need to balance accuracy with efficiency. To address this, we propose DMG-YOLO, a lightweight real-time detector tailored for small object detection in remote sensing images. Specifically, we design a Dual-branch Feature Extraction (DFE) module in the backbone, which partitions feature maps into two parallel branches: one extracts local features via depthwise separable convolutions, and the other captures global context using a vision transformer with a gating mechanism. Additionally, a Multi-scale Feature Fusion (MFF) module with dilated convolutions enhances multi-scale integration while preserving fine details. In the neck, we introduce the Global and Local Aggregate Feature Pyramid Network (GLAFPN) to further boost small object detection through global-local feature fusion. Extensive experiments on the VisDrone2019 and NWPU VHR-10 datasets show that DMG-YOLO achieves competitive performance in terms of mAP, model size, and other key metrics.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17136v1" target="_blank"><h2>Device-Guided Music Transfer <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Manh Pham Hung, Changshuo Hu, Ting Dang, Dong Ma<br><strong><u>Categories:</u></strong> cs.SD, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Device-guided music transfer adapts playback across unseen devices for users who lack them. Existing methods mainly focus on modifying the timbre, rhythm, harmony, or instrumentation to mimic genres or artists, overlooking the diverse hardware properties of the playback device (i.e., speaker). Therefore, we propose DeMT, which processes a speaker's frequency response curve as a line graph using a vision-language model to extract device embeddings. These embeddings then condition a hybrid transformer via feature-wise linear modulation. Fine-tuned on a self-collected dataset, DeMT enables effective speaker-style transfer and robust few-shot adaptation for unseen devices, supporting applications like device-style augmentation and quality enhancement.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17127v1" target="_blank"><h2>Training Foundation Models on a Full-Stack AMD Platform: Compute, Networking, and System Design <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Quentin Anthony, Yury Tokpanov, Skyler Szot, Srivatsan Rajagopal, Praneeth Medepalli, Rishi Iyer, Vasu Shyam, Anna Golubeva, Ansh Chaurasia, Xiao Yang, Tomas Figliolia, Robert Washbourne, Drew Thorstensen, Amartey Pearson, Zack Grossbart, Jason van Patten, Emad Barsoum, Zhenyu Gu, Yao Fu, Beren Millidge<br><strong><u>Categories:</u></strong> cs.CL, cs.AI, cs.DC<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> We report on the first large-scale mixture-of-experts (MoE) pretraining study on pure AMD hardware, utilizing both MI300X GPUs with Pollara interconnect. We distill practical guidance for both systems and model design. On the systems side, we deliver a comprehensive cluster and networking characterization: microbenchmarks for all core collectives (all-reduce, reduce-scatter, all-gather, broadcast) across message sizes and GPU counts on Pollara. To our knowledge, this is the first at this scale. We further provide MI300X microbenchmarks on kernel sizing and memory bandwidth to inform model design. On the modeling side, we introduce and apply MI300X-aware transformer sizing rules for attention and MLP blocks and justify MoE widths that jointly optimize training throughput and inference latency. We describe our training stack in depth, including often-ignored utilities such as fault-tolerance and checkpoint-reshaping, as well as detailed information on our training recipe. We also provide a preview of our model architecture and base model - ZAYA1 (760M active, 8.3B total parameters MoE) - which will be further improved upon in forthcoming papers. ZAYA1-base achieves performance comparable to leading base models such as Qwen3-4B and Gemma3-12B at its scale and larger, and outperforms models including Llama-3-8B and OLMoE across reasoning, mathematics, and coding benchmarks. Together, these results demonstrate that the AMD hardware, network, and software stack are mature and optimized enough for competitive large-scale pretraining.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17126v1" target="_blank"><h2>OmniLens++: Blind Lens Aberration Correction via Large LensLib Pre-Training and Latent PSF Representation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Qi Jiang, Xiaolong Qian, Yao Gao, Lei Sun, Kailun Yang, Zhonghua Yi, Wenyong Li, Ming-Hsuan Yang, Luc Van Gool, Kaiwei Wang<br><strong><u>Categories:</u></strong> eess.IV, cs.CV, cs.LG, physics.optics<br><strong><u>Comments:</u></strong> The source code and datasets will be made publicly available atthis https URL<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> VAE (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Emerging deep-learning-based lens library pre-training (LensLib-PT) pipeline offers a new avenue for blind lens aberration correction by training a universal neural network, demonstrating strong capability in handling diverse unknown optical degradations. This work proposes the OmniLens++ framework, which resolves two challenges that hinder the generalization ability of existing pipelines: the difficulty of scaling data and the absence of prior guidance characterizing optical degradation. To improve data scalability, we expand the design specifications to increase the degradation diversity of the lens source, and we sample a more uniform distribution by quantifying the spatial-variation patterns and severity of optical degradation. In terms of model design, to leverage the Point Spread Functions (PSFs), which intuitively describe optical degradation, as guidance in a blind paradigm, we propose the Latent PSF Representation (LPR). The VQVAE framework is introduced to learn latent features of LensLib's PSFs, which is assisted by modeling the optical degradation process to constrain the learning of degradation priors. Experiments on diverse aberrations of real-world lenses and synthetic LensLib show that OmniLens++ exhibits state-of-the-art generalization capacity in blind aberration correction. Beyond performance, the AODLibpro is verified as a scalable foundation for more effective training across diverse aberrations, and LPR can further tap the potential of large-scale LensLib. The source code and datasets will be made publicly available at https://github.com/zju-jiangqi/OmniLens2.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17123v1" target="_blank"><h2>Layer-wise Weight Selection for Power-Efficient Neural Network Acceleration <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jiaxun Fang, Li Zhang, Shaoyi Huang<br><strong><u>Categories:</u></strong> cs.AR, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> neural network (title)<br><p><strong><u>Abstract:</u></strong> Systolic array accelerators execute CNNs with energy dominated by the switching activity of multiply accumulate (MAC) units. Although prior work exploits weight dependent MAC power for compression, existing methods often use global activation models, coarse energy proxies, or layer-agnostic policies, which limits their effectiveness on real hardware. We propose an energy aware, layer-wise compression framework that explicitly leverages MAC and layer level energy characteristics. First, we build a layer-aware MAC energy model that combines per-layer activation statistics with an MSB-Hamming distance grouping of 22-bit partial sum transitions, and integrate it with a tile-level systolic mapping to estimate convolution-layer energy. On top of this model, we introduce an energy accuracy co-optimized weight selection algorithm within quantization aware training and an energy-prioritized layer-wise schedule that compresses high energy layers more aggressively under a global accuracy constraint. Experiments on different CNN models demonstrate up to 58.6\% energy reduction with 2-3\% accuracy drop, outperforming a state-of-the-art power-aware baseline.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17113v1" target="_blank"><h2>AutoGraphAD: A novel approach using Variational Graph Autoencoders for anomalous network flow detection <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Georgios Anyfantis, Pere Barlet-Ros<br><strong><u>Categories:</u></strong> cs.CR, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> 11 pages, 9 figures<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> anomaly detection (abstract)<br><p><strong><u>Abstract:</u></strong> Network Intrusion Detection Systems (NIDS) are essential tools for detecting network attacks and intrusions. While extensive research has explored the use of supervised Machine Learning for attack detection and characterisation, these methods require accurately labelled datasets, which are very costly to obtain. Moreover, existing public datasets have limited and/or outdated attacks, and many of them suffer from mislabelled data. To reduce the reliance on labelled data, we propose AutoGraphAD, a novel unsupervised anomaly detection approach based on a Heterogeneous Variational Graph Autoencoder. AutoGraphAD operates on heterogeneous graphs, made from connection and IP nodes that capture network activity within a time window. The model is trained using unsupervised and contrastive learning, without relying on any labelled data. The reconstruction, structural loss, and KL divergence are then weighted and combined in an anomaly score that is then used for anomaly detection. Overall, AutoGraphAD yields the same, and in some cases better, results than previous unsupervised approaches, such as Anomal-E, but without requiring costly downstream anomaly detectors. As a result, AutoGraphAD achieves around 1.18 orders of magnitude faster training and 1.03 orders of magnitude faster inference, which represents a significant advantage for operational deployment.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17084v1" target="_blank"><h2>Spectral synthesis techniques for supernovae and kilonovae <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Anders Jerkstrand<br><strong><u>Categories:</u></strong> astro-ph.HE, astro-ph.SR<br><strong><u>Comments:</u></strong> Invited review article for Living Reviews in Computational Astrophysics, August 2025<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> VAE (title, abstract)<br><p><strong><u>Abstract:</u></strong> Supernovae (SNe) and kilonovae (KNe) are the most violent explosions in cosmos, signalling the destruction of a massive star (core-collapse SN), a white dwarf (thermonuclear SN) and a neutron star (KN), respectively. The ejected debris in these explosions is believed to be the main cosmic source of most elements in the periodic table. However, decoding the spectra of these transients is a challenging task requiring sophisticated spectral synthesis modelling. Here, the techniques for such modelling is reviewed, with particular focus on the computational aspects. We build from a historical review of how methodologies evolved from modelling of stellar winds, to supernovae, to kilonovae, studying various approximations in use for the central physical processes. Similarities and differences in the numeric schemes employed by current codes are discussed, and the path towards improved models is laid out.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17061v1" target="_blank"><h2>Morphological Image Similarity Search on the ALMA Science Archive Query Interface Using Deep Unsupervised Contrastive Representation Learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Felix Stoehr, Andrea Farago, Stefan Curiban, Alisdair Manning, Jorge Garcia, Pei-Ying Hsieh, Andrew Lipnicky, Adele Plunkett<br><strong><u>Categories:</u></strong> astro-ph.IM<br><strong><u>Comments:</u></strong> 3 pages, 2 figures<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> With the exponential growth of astronomical data over time, finding the needles in the haystack is becoming increasingly difficult. The next frontier for science archives is to enable searches not only on observational metadata, but also on the content of the observations themselves. As a step in this direction, we have implemented morphological image similarity search into the ALMA Science Archive (ASA). To achieve this we use self-supervised contrastive affine-transformation-independent representation learning of source morphologies with a deep neural network. For a given image on the ASA web interface, astronomers are presented with a summary view of the morphologically most similar images. Each time an astronomer selects an additional image from that view, the display is instantly updated to show the images most similar to the combination of the selected images. Each selection thus refines the similarity display according to the scientific needs of the astronomer. This is the first time image similarity search has been offered in an astronomical science archive.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17056v1" target="_blank"><h2>Patient-level Information Extraction by Consistent Integration of Textual and Tabular Evidence with Bayesian Networks <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Paloma Rabaey, Adrick Tench, Stefan Heytens, Thomas Demeester<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> multi-modal (abstract)<br><p><strong><u>Abstract:</u></strong> Electronic health records (EHRs) form an invaluable resource for training clinical decision support systems. To leverage the potential of such systems in high-risk applications, we need large, structured tabular datasets on which we can build transparent feature-based models. While part of the EHR already contains structured information (e.g. diagnosis codes, medications, and lab results), much of the information is contained within unstructured text (e.g. discharge summaries and nursing notes). In this work, we propose a method for multi-modal patient-level information extraction that leverages both the tabular features available in the patient's EHR (using an expert-informed Bayesian network) as well as clinical notes describing the patient's symptoms (using neural text classifiers). We propose the use of virtual evidence augmented with a consistency node to provide an interpretable, probabilistic fusion of the models' predictions. The consistency node improves the calibration of the final predictions compared to virtual evidence alone, allowing the Bayesian network to better adjust the neural classifier's output to handle missing information and resolve contradictions between the tabular and text data. We show the potential of our method on the SimSUM dataset, a simulated benchmark linking tabular EHRs with clinical notes through expert knowledge.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17045v1" target="_blank"><h2>RacketVision: A Multiple Racket Sports Benchmark for Unified Ball and Racket Analysis <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Linfeng Dong, Yuchen Yang, Hao Wu, Wei Wang, Yuenan HouZhihang Zhong, Xiao Sun<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.MM<br><strong><u>Comments:</u></strong> Accepted to AAAI 2026 (Oral)<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract), multi-modal (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> We introduce RacketVision, a novel dataset and benchmark for advancing computer vision in sports analytics, covering table tennis, tennis, and badminton. The dataset is the first to provide large-scale, fine-grained annotations for racket pose alongside traditional ball positions, enabling research into complex human-object interactions. It is designed to tackle three interconnected tasks: fine-grained ball tracking, articulated racket pose estimation, and predictive ball trajectory forecasting. Our evaluation of established baselines reveals a critical insight for multi-modal fusion: while naively concatenating racket pose features degrades performance, a CrossAttention mechanism is essential to unlock their value, leading to trajectory prediction results that surpass strong unimodal baselines. RacketVision provides a versatile resource and a strong starting point for future research in dynamic object tracking, conditional motion forecasting, and multimodal analysis in sports. Project page at https://github.com/OrcustD/RacketVision</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17043v1" target="_blank"><h2>MedImageInsight for Thoracic Cavity Health Classification from Chest X-rays <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Rama Krishna Boya, Mohan Kireeti Magalanadu, Azaruddin Palavalli, Rupa Ganesh Tekuri, Amrit Pattanayak, Prasanthi Enuga, Vignesh Esakki Muthu, Vivek Aditya Boya<br><strong><u>Categories:</u></strong> eess.IV, cs.AI, cs.CV<br><strong><u>Comments:</u></strong> 9 pages, 5 figures and 3 tables<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> transfer learning (abstract)<br><p><strong><u>Abstract:</u></strong> Chest radiography remains one of the most widely used imaging modalities for thoracic diagnosis, yet increasing imaging volumes and radiologist workload continue to challenge timely interpretation. In this work, we investigate the use of MedImageInsight, a medical imaging foundational model, for automated binary classification of chest X-rays into Normal and Abnormal categories. Two approaches were evaluated: (1) fine-tuning MedImageInsight for end-to-end classification, and (2) employing the model as a feature extractor for a transfer learning pipeline using traditional machine learning classifiers. Experiments were conducted using a combination of the ChestX-ray14 dataset and real-world clinical data sourced from partner hospitals. The fine-tuned classifier achieved the highest performance, with an ROC-AUC of 0.888 and superior calibration compared to the transfer learning models, demonstrating performance comparable to established architectures such as CheXNet. These results highlight the effectiveness of foundational medical imaging models in reducing task-specific training requirements while maintaining diagnostic reliability. The system is designed for integration into web-based and hospital PACS workflows to support triage and reduce radiologist burden. Future work will extend the model to multi-label pathology classification to provide preliminary diagnostic interpretation in clinical environments.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17040v1" target="_blank"><h2>Step-E: A Differentiable Data Cleaning Framework for Robust Learning with Noisy Labels <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Wenzhang Du<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 12 pages, 4 figures<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Training data collected in the wild often contain noisy labels and outliers that substantially degrade the performance and reliability of deep neural networks. While data cleaning is commonly applied as a separate preprocessing stage, such two-stage pipelines neither fully exploit feedback from the downstream model nor adapt to unknown noise patterns. We propose Step-E, a simple framework that integrates sample selection and model learning into a single optimization process. At each epoch, Step-E ranks samples by loss and gradually increases the fraction of high-loss examples that are excluded from gradient updates after a brief warm-up stage, yielding an online curriculum that focuses on easy and consistent examples and eventually ignores persistent outliers. On CIFAR-100N, Step-E improves the test accuracy of a ResNet-18 model from 43.3% (+/- 0.7%) to 50.4% (+/- 0.9%), clearly outperforming loss truncation, self-paced learning, and one-shot filtering while approaching the clean-label oracle at 60.5% (+/- 0.2%). On CIFAR-10N (aggre), Step-E also improves over the noisy baseline (85.3% vs. 83.9%) and nearly matches the clean-label oracle (85.9%), with only moderate training-time overhead.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17038v1" target="_blank"><h2>DAPS++: Rethinking Diffusion Inverse Problems with Decoupled Posterior Annealing <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Hao Chen, Renzheng Zhang, Scott S. Howard<br><strong><u>Categories:</u></strong> cs.AI, stat.ML<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> From a Bayesian perspective, score-based diffusion solves inverse problems through joint inference, embedding the likelihood with the prior to guide the sampling process. However, this formulation fails to explain its practical behavior: the prior offers limited guidance, while reconstruction is largely driven by the measurement-consistency term, leading to an inference process that is effectively decoupled from the diffusion dynamics. To clarify this structure, we reinterpret the role of diffusion in inverse problem solving as an initialization stage within an expectation--maximization (EM)--style framework, where the diffusion stage and the data-driven refinement are fully decoupled. We introduce \textbf{DAPS++}, which allows the likelihood term to guide inference more directly while maintaining numerical stability and providing insight into why unified diffusion trajectories remain effective in practice. By requiring fewer function evaluations (NFEs) and measurement-optimization steps, \textbf{DAPS++} achieves high computational efficiency and robust reconstruction performance across diverse image restoration tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17008v1" target="_blank"><h2>Mask the Redundancy: Evolving Masking Representation Learning for Multivariate Time-Series Clustering <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zexi Tan, Xiaopeng Luo, Yunlin Liu, Yiqun Zhang<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> Accepted to AAAI 2026<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Multivariate Time-Series (MTS) clustering discovers intrinsic grouping patterns of temporal data samples. Although time-series provide rich discriminative information, they also contain substantial redundancy, such as steady-state machine operation records and zero-output periods of solar power generation. Such redundancy diminishes the attention given to discriminative timestamps in representation learning, thus leading to performance bottlenecks in MTS clustering. Masking has been widely adopted to enhance the MTS representation, where temporal reconstruction tasks are designed to capture critical information from MTS. However, most existing masking strategies appear to be standalone preprocessing steps, isolated from the learning process, which hinders dynamic adaptation to the importance of clustering-critical timestamps. Accordingly, this paper proposes the Evolving-masked MTS Clustering (EMTC) method, with its model architecture composed of Importance-aware Variate-wise Masking (IVM) and Multi-Endogenous Views (MEV) representation learning modules. IVM adaptively guides the model in learning more discriminative representations for clustering, while the MEV-based reconstruction and contrastive learning pathways enhance the generalization. That is, the MEV reconstruction facilitates multi-perspective complementary to prevent the masking from premature convergence, and the clustering-guided contrastive learning facilitates the joint optimization of representation and clustering. Extensive experiments on 15 real benchmark datasets demonstrate the superiority of EMTC in comparison with eight SOTA methods, where the EMTC achieves an average improvement of 4.85% over the strongest baselines.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17007v1" target="_blank"><h2>Generative MIMO Beam Map Construction for Location Recovery and Beam Tracking <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Wangqian Chen, Junting Chen, Shuguang Cui<br><strong><u>Categories:</u></strong> eess.SP, cs.LG, eess.SY<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), convolutional (abstract), latent space (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Machine learning (ML) has greatly advanced data-driven channel modeling and resource optimization in wireless communication systems. However, most existing ML-based methods rely on large, accurately labeled datasets with location information, which are often difficult and costly to obtain. This paper proposes a generative framework to recover location labels directly from sequences of sparse channel state information (CSI) measurements, without explicit location labels for radio map construction. Instead of directly storing raw CSI, we learn a compact low-dimensional radio map embedding and leverage a generative model to reconstruct the high-dimensional CSI. Specifically, to address the uncertainty of sparse CSI, a dual-scale feature extraction scheme is designed to enhance feature representation by jointly exploiting correlations from angular space and across neighboring samples. We develop a hybrid recurrent-convolutional encoder to learn mobility patterns, which combines a truncation strategy and multi-scale convolutions in the recurrent neural network (RNN) to ensure feature robustness against short-term fluctuations. Unlike conventional Gaussian priors in latent space, we embed a learnable radio map to capture the location information by encoding high-level positional features from CSI measurements. Finally, a diffusion-based generative decoder reconstructs the full CSI with high fidelity by conditioning on the positional features in the radio map. Numerical experiments demonstrate that the proposed model can improve localization accuracy by over 30% and achieve a 20% capacity gain in non-line-of-sight (NLOS) scenarios compared with model-based Kalman filter approaches.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.17005v1" target="_blank"><h2>FLUID: Training-Free Face De-identification via Latent Identity Substitution <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jinhyeong Park, Shaheryar Muhammad, Seangmin Lee, Jong Taek Lee, Soon Ki Jung<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> latent space (abstract)<br><p><strong><u>Abstract:</u></strong> We present FLUID (Face de-identification in the Latent space via Utility-preserving Identity Displacement), a training-free framework that directly substitutes identity in the latent space of pretrained diffusion models. Inspired by substitution mechanisms in chemistry, we reinterpret identity editing as semantic displacement in the latent h-space of a pretrained unconditional diffusion model. Our framework discovers identity-editing directions through optimization guided by novel reagent losses, which supervise for attribute preservation and identity suppression. We further propose both linear and geodesic (tangent-based) editing schemes to effectively navigate the latent manifold. Experimental results on CelebA-HQ and FFHQ demonstrate that FLUID achieves a superior trade-off between identity suppression and attribute preservation, outperforming state-of-the-art de-identification methods in both qualitative and quantitative metrics.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16982v1" target="_blank"><h2>A Diversity-optimized Deep Ensemble Approach for Accurate Plant Leaf Disease Detection <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Sai Nath Chowdary Medikonduru, Hongpeng Jin, Yanzhao Wu<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Plant diseases pose a significant threat to global agriculture, causing over $220 billion in annual economic losses and jeopardizing food security. The timely and accurate detection of these diseases from plant leaf images is critical to mitigating their adverse effects. Deep neural network Ensembles (Deep Ensembles) have emerged as a powerful approach to enhancing prediction accuracy by leveraging the strengths of diverse Deep Neural Networks (DNNs). However, selecting high-performing ensemble member models is challenging due to the inherent difficulty in measuring ensemble diversity. In this paper, we introduce the Synergistic Diversity (SQ) framework to enhance plant disease detection accuracy. First, we conduct a comprehensive analysis of the limitations of existing ensemble diversity metrics (denoted as Q metrics), which often fail to identify optimal ensemble teams. Second, we present the SQ metric, a novel measure that captures the synergy between ensemble members and consistently aligns with ensemble accuracy. Third, we validate our SQ approach through extensive experiments on a plant leaf image dataset, which demonstrates that our SQ metric substantially improves ensemble selection and enhances detection accuracy. Our findings pave the way for a more reliable and efficient image-based plant disease detection.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16976v1" target="_blank"><h2>Gradient flow for deep equilibrium single-index models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Sanjit Dandapanthula, Aaditya Ramdas<br><strong><u>Categories:</u></strong> cs.LG, math.ST, stat.ML<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Deep equilibrium models (DEQs) have recently emerged as a powerful paradigm for training infinitely deep weight-tied neural networks that achieve state of the art performance across many modern machine learning tasks. Despite their practical success, theoretically understanding the gradient descent dynamics for training DEQs remains an area of active research. In this work, we rigorously study the gradient descent dynamics for DEQs in the simple setting of linear models and single-index models, filling several gaps in the literature. We prove a conservation law for linear DEQs which implies that the parameters remain trapped on spheres during training and use this property to show that gradient flow remains well-conditioned for all time. We then prove linear convergence of gradient descent to a global minimizer for linear DEQs and deep equilibrium single-index models under appropriate initialization and with a sufficiently small step size. Finally, we validate our theoretical findings through experiments.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16943v1" target="_blank"><h2>RASTP: Representation-Aware Semantic Token Pruning for Generative Recommendation with Semantic Identifiers <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Tianyu Zhan, Kairui Fu, Zheqi Lv, Shengyu Zhang<br><strong><u>Categories:</u></strong> cs.IR, cs.AI<br><strong><u>Comments:</u></strong> 4 pages<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Generative recommendation systems typically leverage Semantic Identifiers (SIDs), which represent each item as a sequence of tokens that encode semantic information. However, representing item ID with multiple SIDs significantly increases input sequence length, which is a major determinant of computational complexity and memory consumption. While existing efforts primarily focus on optimizing attention computation and KV cache, we propose RASTP (Representation-Aware Semantic Token Pruning), which directly prunes less informative tokens in the input sequence. Specifically, RASTP evaluates token importance by combining semantic saliency, measured via representation magnitude, and attention centrality, derived from cumulative attention weights. Since RASTP dynamically prunes low-information or irrelevant semantic tokens, experiments on three real-world Amazon datasets show that RASTP reduces training time by 26.7\%, while maintaining or slightly improving recommendation performance. The code has been open-sourced at https://github.com/Yuzt-zju/RASTP.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16937v1" target="_blank"><h2>OmniGround: A Comprehensive Spatio-Temporal Grounding Benchmark for Real-World Complex Scenarios <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Hong Gao, Jingyu Wu, Xiangkai Xu, Kangni Xie, Yunchen Zhang, Bin Zhong, Xurui Gao, Min-Ling Zhang<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> 20 pages<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Spatio-Temporal Video Grounding (STVG) aims to localize target objects in videos based on natural language descriptions. Despite recent advances in Multimodal Large Language Models, a significant gap remains between current models and real-world demands involving diverse objects and complex queries. We attribute this to limited benchmark scope, causing models to exhibit category bias, oversimplified reasoning, and poor linguistic robustness. To address these limitations, we introduce OmniGround, a comprehensive benchmark with 3,475 videos spanning 81 categories and complex real-world queries. We propose the Forward-Backward-Refinement annotation pipeline that combines multi-directional tracking with intelligent error correction for high-quality labels. We further introduce DeepSTG, a systematic evaluation framework quantifying dataset quality across four complementary dimensions beyond superficial statistics. Evaluations reveal performance average drop of 10.4% on complex real-world scenes, particularly with small/occluded objects and intricate spatial relations. Motivated by these, we propose PG-TAF, a training-free two-stage framework decomposing STVG into high-level temporal grounding and fine-grained spatio-temporal propagation. Experiments demonstrate PG-TAF achieves 25.6% and 35.6% improvements in m\_tIoU and m\_vIoU on OmniGround with consistent gains across four benchmarks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16929v1" target="_blank"><h2>CroTad: A Contrastive Reinforcement Learning Framework for Online Trajectory Anomaly Detection <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Rui Xue, Dan He, Fengmei Jin, Chen Zhang, Xiaofang Zhou<br><strong><u>Categories:</u></strong> cs.LG, cs.DB<br><strong><u>Comments:</u></strong> 18 pages, 4 figures, will be submitted to VLDBJ<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> anomaly detection (title, abstract)<br><p><strong><u>Abstract:</u></strong> Detecting trajectory anomalies is a vital task in modern Intelligent Transportation Systems (ITS), enabling the identification of unsafe, inefficient, or irregular travel behaviours. While deep learning has emerged as the dominant approach, several key challenges remain unresolved. First, sub-trajectory anomaly detection, capable of pinpointing the precise segments where anomalies occur, remains underexplored compared to whole-trajectory analysis. Second, many existing methods depend on carefully tuned thresholds, limiting their adaptability in real-world applications. Moreover, the irregular sampling of trajectory data and the presence of noise in training sets further degrade model performance, making it difficult to learn reliable representations of normal routes. To address these challenges, we propose a contrastive reinforcement learning framework for online trajectory anomaly detection, CroTad. Our method is threshold-free and robust to noisy, irregularly sampled data. By incorporating contrastive learning, CroTad learns to extract diverse normal travel patterns for different itineraries and effectively distinguish anomalous behaviours at both sub-trajectory and point levels. The detection module leverages deep reinforcement learning to perform online, real-time anomaly scoring, enabling timely and fine-grained identification of abnormal segments. Extensive experiments on two real-world datasets demonstrate the effectiveness and robustness of our framework across various evaluation scenarios.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16905v1" target="_blank"><h2>Predicting Talent Breakout Rate using Twitter and TV data <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Bilguun Batsaikhan, Hiroyuki Fukuda<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 4 pages. Presented at the 34th Annual Conference of the Japanese Society for Artificial Intelligence (JSAI 2020), paper ID 1K3-ES-2-02<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Early detection of rising talents is of paramount importance in the field of advertising. In this paper, we define a concept of talent breakout and propose a method to detect Japanese talents before their rise to stardom. The main focus of the study is to determine the effectiveness of combining Twitter and TV data on predicting time-dependent changes in social data. Although traditional time-series models are known to be robust in many applications, the success of neural network models in various fields (e.g.\ Natural Language Processing, Computer Vision, Reinforcement Learning) continues to spark an interest in the time-series community to apply new techniques in practice. Therefore, in order to find the best modeling approach, we have experimented with traditional, neural network and ensemble learning methods. We observe that ensemble learning methods outperform traditional and neural network models based on standard regression metrics. However, by utilizing the concept of talent breakout, we are able to assess the true forecasting ability of the models, where neural networks outperform traditional and ensemble learning methods in terms of precision and recall.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16884v1" target="_blank"><h2>Generative AI in Sociological Research: State of the Discipline <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> AJ Alvero, Dustin S. Stoltz, Oscar Stuhler, Marshall Taylor<br><strong><u>Categories:</u></strong> cs.CY, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Generative artificial intelligence (GenAI) has garnered considerable attention for its potential utility in research and scholarship, even among those who typically do not rely on computational tools. Early commentators, however, have also articulated concerns about how GenAI usage comes with enormous environmental costs, serious social risks, and a tendency to produce low-quality content. In the midst of both excitement and skepticism, it is crucial to take stock of how GenAI is actually being used. Our study focuses on sociological research as our site, and here we present findings from a survey of 433 authors of articles published in 50 sociology journals in the last five years. The survey provides an overview of the state of the discipline with regard to the use of GenAI by providing answers to fundamental questions: how (much) do scholars use the technology for their research; what are their reasons for using it; and how concerned, trustful, and optimistic are they about the technology? Of the approximately one third ofrespondents who self-report using GenAI at least weekly, the primary uses are for writing assistance and comparatively less so in planning, data collection, or data analysis. In both use and attitudes, there are surprisingly few differences between self-identified computational and non-computational researchers. Generally, respondents are very concerned about the social and environmental consequences of GenAI. Trust in GenAI outputs is low, regardless of expertise or frequency of use. While optimism that GenAI will improve is high, scholars are divided on whether GenAI will have a positive impact on the field.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16871v1" target="_blank"><h2>Topologic Attention Networks: Attending to Direct and Indirect Neighbors through Gaussian Belief Propagation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Marshall Rosenhoover, Huaming Zhang<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 15 pages, 13 Figures<br><strong><u>Published:</u></strong> 2025-11-21<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Graph Neural Networks rely on local message passing, which limits their ability to model long-range dependencies in graphs. Existing approaches extend this range through continuous-time dynamics or dense self-attention, but both suffer from high computational cost and limited scalability. We propose Topologic Attention Networks, a new framework that applies topologic attention, a probabilistic mechanism that learns how information should flow through both direct and indirect connections in a graph. Unlike conventional attention that depends on explicit pairwise interactions, topologic attention emerges from the learned information propagation of the graph, enabling unified reasoning over local and global relationships. This method achieves provides state-of-the-art performance across all measured baseline models. Our implementation is available at https://github.com/Marshall-Rosenhoover/Topologic-Attention-Networks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16856v1" target="_blank"><h2>The use of vocal biomarkers in the detection of Parkinson's disease: a robust statistical performance comparison of classic machine learning models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Katia Pires Nascimento do Sacramento, Elliot Q. C. Garcia, Nicéias Silva Vilela, Vinicius P. Sacramento, Tiago A. E. Ferreira<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 18 pages, 3 figures<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Parkinson's disease (PD) is a progressive neurodegenerative disorder that, in addition to directly impairing functional mobility, is frequently associated with vocal impairments such as hypophonia and dysarthria, which typically manifest in the early stages. The use of vocal biomarkers to support the early diagnosis of PD presents a non-invasive, low-cost, and accessible alternative in clinical settings. Thus, the objective of this cross-sectional study was to consistently evaluate the effectiveness of a Deep Neural Network (DNN) in distinguishing individuals with Parkinson's disease from healthy controls, in comparison with traditional Machine Learning (ML) methods, using vocal biomarkers. Two publicly available voice datasets were used. Mel-frequency cepstral coefficients (MFCCs) were extracted from the samples, and model robustness was assessed using a validation strategy with 1000 independent random executions. Performance was evaluated using classification statistics. Since normality assumptions were not satisfied, non-parametric tests (Kruskal-Wallis and Bonferroni post-hoc tests) were applied to verify whether the tested classification models were similar or different in the classification of PD. With an average accuracy of $98.65\%$ and $92.11\%$ on the Italian Voice dataset and Parkinson's Telemonitoring dataset, respectively, the DNN demonstrated superior performance and efficiency compared to traditional ML models, while also achieving competitive results when benchmarked against relevant studies. Overall, this study confirms the efficiency of DNNs and emphasizes their potential to provide greater accuracy and reliability for the early detection of neurodegenerative diseases using voice-based biomarkers.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16849v1" target="_blank"><h2>Better audio representations are more brain-like: linking model-brain alignment with performance in downstream auditory tasks <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Leonardo Pepino, Pablo Riera, Juan Kamienkowski, Luciana Ferrer<br><strong><u>Categories:</u></strong> cs.LG, cs.SD<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Artificial neural networks (ANNs) are increasingly powerful models of brain computation, yet it remains unclear whether improving their task performance also makes their internal representations more similar to brain signals. To address this question in the auditory domain, we quantified the alignment between the internal representations of 36 different audio models and brain activity from two independent fMRI datasets. Using voxel-wise and component-wise regression, and representation similarity analysis (RSA), we found that recent self-supervised audio models with strong performance in diverse downstream tasks are better predictors of auditory cortex activity than older and more specialized models. To assess the quality of the audio representations, we evaluated these models in 6 auditory tasks from the HEAREval benchmark, spanning music, speech, and environmental sounds. This revealed strong positive Pearson correlations ($r>0.7$) between a model's overall task performance and its alignment with brain representations. Finally, we analyzed the evolution of the similarity between audio and brain representations during the pretraining of EnCodecMAE. We discovered that brain similarity increases progressively and emerges early during pretraining, despite the model not being explicitly optimized for this objective. This suggests that brain-like representations can be an emergent byproduct of learning to reconstruct missing information from naturalistic audio data.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16839v1" target="_blank"><h2>Analysis of heart failure patient trajectories using sequence modeling <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Falk Dippela, Yinan Yu, Annika Rosengren, Martin Lindgren, Christina E. Lundberg, Erik Aerts, Martin Adiels, Helen Sjöland<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Transformers have defined the state-of-the-art for clinical prediction tasks involving electronic health records (EHRs). The recently introduced Mamba architecture outperformed an advanced Transformer (Transformer++) based on Llama in handling long context lengths, while using fewer model parameters. Despite the impressive performance of these architectures, a systematic approach to empirically analyze model performance and efficiency under various settings is not well established in the medical domain. The performances of six sequence models were investigated across three architecture classes (Transformers, Transformers++, Mambas) in a large Swedish heart failure (HF) cohort (N = 42820), providing a clinically relevant case study. Patient data included diagnoses, vital signs, laboratories, medications and procedures extracted from in-hospital EHRs. The models were evaluated on three one-year prediction tasks: clinical instability (a readmission phenotype) after initial HF hospitalization, mortality after initial HF hospitalization and mortality after latest hospitalization. Ablations account for modifications of the EHR-based input patient sequence, architectural model configurations, and temporal preprocessing techniques for data collection. Llama achieves the highest predictive discrimination, best calibration, and showed robustness across all tasks, followed by Mambas. Both architectures demonstrate efficient representation learning, with tiny configurations surpassing other large-scaled Transformers. At equal model size, Llama and Mambas achieve superior performance using 25% less training data. This paper presents a first ablation study with systematic design choices for input tokenization, model configuration and temporal data preprocessing. Future model development in clinical prediction tasks using EHRs could build upon this study's recommendation as a starting point.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16828v1" target="_blank"><h2>ManifoldFormer: Geometric Deep Learning for Neural Dynamics on Riemannian Manifolds <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yihang Fu, Lifang He, Qingyu Chen<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 5 pages, under review by ICASSP<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> VAE (abstract), transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Existing EEG foundation models mainly treat neural signals as generic time series in Euclidean space, ignoring the intrinsic geometric structure of neural dynamics that constrains brain activity to low-dimensional manifolds. This fundamental mismatch between model assumptions and neural geometry limits representation quality and cross-subject generalization. ManifoldFormer addresses this limitation through a novel geometric deep learning framework that explicitly learns neural manifold representations. The architecture integrates three key innovations: a Riemannian VAE for manifold embedding that preserves geometric structure, a geometric Transformer with geodesic-aware attention mechanisms operating directly on neural manifolds, and a dynamics predictor leveraging neural ODEs for manifold-constrained temporal evolution. Extensive evaluation across four public datasets demonstrates substantial improvements over state-of-the-art methods, with 4.6-4.8% higher accuracy and 6.2-10.2% higher Cohen's Kappa, while maintaining robust cross-subject generalization. The geometric approach reveals meaningful neural patterns consistent with neurophysiological principles, establishing geometric constraints as essential for effective EEG foundation models.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16786v1" target="_blank"><h2>Revisiting Multimodal KV Cache Compression: A Frequency-Domain-Guided Outlier-KV-Aware Approach <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yaoxin Yang, Peng Ye, Xudong Tan, Chongjun Tu, Maosen Zhao, Jia Hao, Tao Chen<br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.CV<br><strong><u>Comments:</u></strong> Under Review<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Multimodal large language models suffer from substantial inference overhead since multimodal KV Cache grows proportionally with the visual input length. Existing multimodal KV Cache compression methods mostly rely on attention score to reduce cache size, which makes them are incompatible with established efficient attention kernels (e.g., FlashAttention) and ignores the contribution of value vectors to the attention output. In this work, we revisit multimodal KV Cache compression from the perspective of the KV matrices' distribution. First, we observe that frequency-domain energy of multimodal KV matrices is predominantly concentrated in low-frequency and extract this principal energy via a low-pass filter. Further, we find that removing KV pairs that deviate substantially from this principal energy leads to a pronounced performance drop, which we define as Outlier KVs. Considering Outlier KVs are more likely to encode features critical for inference, we propose FlashCache, a frequency-domain-guided, Outlier-KV-aware KV Cache compression framework. First, we introduce an Outlier KV Recognition Module that models the principal component of multimodal KV matrices in the frequency domain and preferentially retains KV pairs that significantly deviate from it. Furthermore, Dynamic Budget Allocation Module is designed to adaptively determine the per-layer KV Cache size to retain more Outlier KVs. Experiments on multiple MLLMs and benchmarks demonstrate that FlashCache outperforms state-of-the-art multimoal KV compression methods, achieving up to 1.69 times faster decoding with 80% lower KV memory usage while maintaining task performance.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16778v1" target="_blank"><h2>GCL-OT: Graph Contrastive Learning with Optimal Transport for Heterophilic Text-Attributed Graphs <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yating Ren, Yikun Ban, Huobin Tan<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> AAAI 2026<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Recently, structure-text contrastive learning has shown promising performance on text-attributed graphs by leveraging the complementary strengths of graph neural networks and language models. However, existing methods typically rely on homophily assumptions in similarity estimation and hard optimization objectives, which limit their applicability to heterophilic graphs. Although existing methods can mitigate heterophily through structural adjustments or neighbor aggregation, they usually treat textual embeddings as static targets, leading to suboptimal alignment. In this work, we identify the multi-granular heterophily in text-attributed graphs, including complete heterophily, partial heterophily, and latent homophily, which makes structure-text alignment particularly challenging due to mixed, noisy, and missing semantic correlations. To achieve flexible and bidirectional alignment, we propose GCL-OT, a novel graph contrastive learning framework with optimal transport, equipped with tailored mechanisms for each type of heterophily. Specifically, for partial heterophily, we design a RealSoftMax-based similarity estimator to emphasize key neighbor-word interactions while easing background noise. For complete heterophily, we introduce a prompt-based filter that adaptively excludes irrelevant noise during optimal transport alignment. Furthermore, we incorporate OT-guided soft supervision to uncover potential neighbors with similar semantics, enhancing the learning of latent homophily. Theoretical analysis shows that GCL-OT can improve the mutual information bound and Bayes error guarantees. Extensive experiments on nine benchmarks show that GCL-OT consistently outperforms state-of-the-art methods, verifying its effectiveness and robustness.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16767v1" target="_blank"><h2>When Structure Doesn't Help: LLMs Do Not Read Text-Attributed Graphs as Effectively as We Expected <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Haotian Xu, Yuning You, Tengfei Ma<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Graphs provide a unified representation of semantic content and relational structure, making them a natural fit for domains such as molecular modeling, citation networks, and social graphs. Meanwhile, large language models (LLMs) have excelled at understanding natural language and integrating cross-modal signals, sparking interest in their potential for graph reasoning. Recent work has explored this by either designing template-based graph templates or using graph neural networks (GNNs) to encode structural information. In this study, we investigate how different strategies for encoding graph structure affect LLM performance on text-attributed graphs. Surprisingly, our systematic experiments reveal that: (i) LLMs leveraging only node textual descriptions already achieve strong performance across tasks; and (ii) most structural encoding strategies offer marginal or even negative gains. We show that explicit structural priors are often unnecessary and, in some cases, counterproductive when powerful language models are involved. This represents a significant departure from traditional graph learning paradigms and highlights the need to rethink how structure should be represented and utilized in the LLM era. Our study is to systematically challenge the foundational assumption that structure is inherently beneficial for LLM-based graph reasoning, opening the door to new, semantics-driven approaches for graph learning.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16755v1" target="_blank"><h2>Turbulence in Core-Collapse Supernovae <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> David Calvert, Michael Redle, Bibek Gautam, Charles J. Stapleford, Carla Fröhlich, James P. Kneller, Matthias Liebendorfer<br><strong><u>Categories:</u></strong> astro-ph.HE<br><strong><u>Comments:</u></strong> 27 pages, 16 figures. Accepted to ApJ. Data behind the figures available atthis https URL<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> VAE (title)<br><p><strong><u>Abstract:</u></strong> It is understood in a general sense that turbulent fluid motion below the shock front in a core-collapse supernova stiffens the effective equation of state of the fluid and aids in the revival of the explosion. However, when one wishes to be precise and quantify the amount of turbulence in a supernova simulation, one immediately encounters the problem that turbulence is difficult to define and measure. Using the 3D magnetohydrodynamic code ELEPHANT, we study how different definitions of turbulence change one's conclusions about the amount of turbulence in a supernova and the extent to which it helps the explosion. We find that, while all the definitions of turbulence we use lead to a qualitatively similar growth pattern over time of the turbulent kinetic energy in the gain region, the total amount of turbulent kinetic energy, and especially the ratios of turbulent to total kinetic energy, distinguish them. Some of the definitions appear to indicate turbulence is a necessary contributor to the explosion, and others indicate it is not. The different definitions also produce turbulence maps with different correlations with maps of the enstrophy, a quantity widely regarded as also indicating the presence of turbulence. We also compute the turbulent adiabatic index and observe that in regions of low enstrophy, this quantity is sensitive to the definition used. As a consequence, the effective adiabatic index depends upon the method used to measure the turbulence and thus alter one's conclusions regarding the impact of turbulence within the supernova.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16754v1" target="_blank"><h2>Data-Driven Stellar Spectral Modelling with GSPICE <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Douglas P. Finkbeiner, Joshua S. Speagle, Tanveer Karim<br><strong><u>Categories:</u></strong> astro-ph.IM<br><strong><u>Comments:</u></strong> 18 pages, 12 figures<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> data-driven (title, abstract)<br><p><strong><u>Abstract:</u></strong> Spectral data reduction pipelines deal with a wide variety of challenges including masking cosmic rays, calibrating wavelength solutions, and estimating background noise while trying to remain model-agnostic. Traditional methods rely on hardware-specific code or pre-calculated stellar model templates to solve this problem, making them model-dependent and not suitable for large datasets that may contain new classes of objects. To solve this problem, we present a flexible, data-driven method: the GausSian PIxelwise Conditional Estimator (GSPICE) that models an ensemble of spectra as a multivariate Gaussian and estimates the expected value and expected variance of each pixel in each spectrum conditional on others. GSPICE compares observed fluxes and errors to its own flux and error estimates to reveal outliers, which then can be completely masked or replaced by their estimates. We apply GSPICE to 3.9 million stellar spectra from the LAMOST survey, and show that variations of the method can directly identify and correct both individual pixel-level outliers (e.g., from cosmic ray hits) as well as extended systematic features (e.g., from incorrect wavelength calibrations), while still providing a novel characterization of the true per-pixel measurement uncertainties. We also demonstrate how GSPICE can take advantage of data partitioning with an application to diffuse interstellar bands. Implementations of GSPICE in both Python and IDL can be found here http://github.com/dfink/gspice.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16750v1" target="_blank"><h2>CLAWDIA: A dictionary learning framework for gravitational-wave data analysis <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Miquel Llorens-Monteagudo, Alejandro Torres-Forné, José A. Font<br><strong><u>Categories:</u></strong> astro-ph.IM<br><strong><u>Comments:</u></strong> 20 pages, 6 figures<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> time-domain (abstract)<br><p><strong><u>Abstract:</u></strong> Deep-learning methods are becoming increasingly important in gravitational-wave data analysis, yet their performance often relies on large training datasets and models whose internal representations are difficult to interpret. Sparse dictionary learning (SDL) offers a complementary approach: it performs well in scarce-data regimes and yields physically interpretable representations of gravitational-wave morphology. Here we present CLAWDIA (Comprehensive Library for the Analysis of Waves via Dictionary-based Algorithms), an open-source Python framework that integrates SDL-based denoising and classification under realistic detector noise. We systematise previously isolated SDL workflows into a unified, modular environment with a consistent, user-friendly interface. The current release provides several time-domain denoising strategies based on LASSO-regularised sparse coding and a classifier based on Low-Rank Shared Dictionary Learning. A companion toolbox, GWADAMA, supports dataset construction and realistic conditioning of real and simulated interferometer data. We demonstrate CLAWDIA's performance by denoising the signal from binary neutron star event GW170817 and by classifying families of instrumental glitches from LIGO's third observing run, highlighting robustness at low signal-to-noise ratios. CLAWDIA is intended as a community-driven, interoperable library extensible to additional tasks, including detection and parameter estimation.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16741v1" target="_blank"><h2>Fermions and Supersymmetry in Neural Network Field Theories <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Samuel Frank, James Halverson, Anindita Maiti, Fabian Ruehle<br><strong><u>Categories:</u></strong> hep-th, cs.LG<br><strong><u>Comments:</u></strong> 34 pages + appendices<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> We introduce fermionic neural network field theories via Grassmann-valued neural networks. Free theories are obtained by a generalization of the Central Limit Theorem to Grassmann variables. This enables the realization of the free Dirac spinor at infinite width and a four fermion interaction at finite width. Yukawa couplings are introduced by breaking the statistical independence of the output weights for the fermionic and bosonic fields. A large class of interacting supersymmetric quantum mechanics and field theory models are introduced by super-affine transformations on the input that realize a superspace formalism.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16671v1" target="_blank"><h2>Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Ziyu Guo, Renrui Zhang, Hongyu Li, Manyuan Zhang, Xinyan Chen, Sifan Wang, Yan Feng, Peng Pei, Pheng-Ann Heng<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.CL<br><strong><u>Comments:</u></strong> Project Page:this https URLCode:this https URL<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process. As visual content is progressively generating, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones. This dynamic interplay produces more context-aware and semantically rich visual outputs. To unveil the potential of this framework, we investigate three candidate strategies, zero-shot prompting, supervised fine-tuning (SFT) on our curated TwiG-50K dataset, and reinforcement learning (RL) via a customized TwiG-GRPO strategy, each offering unique insights into the dynamics of interleaved reasoning. We hope this work inspires further research into interleaving textual reasoning for enhanced visual generation. Code will be released at: https://github.com/ZiyuGuo99/Thinking-while-Generating.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16658v1" target="_blank"><h2>Prospects for Neutrino Observation and Mass Measurement from Binary Neutron Star Mergers <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Vedran Brdar, Dibya S. Chattopadhyay, Samiur R. Mir, Tousif Raza, Marc S. Romanowski<br><strong><u>Categories:</u></strong> hep-ph, astro-ph.HE, hep-ex<br><strong><u>Comments:</u></strong> 13 pages, 8 figures<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> VAE (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Over the next decade, $\mathcal{O}(100)$ diffuse supernova neutrino background (DSNB) events are expected in Hyper-Kamiokande. Another neutrino source that has received far less attention is binary neutron star mergers. Including the data from recent simulations, we find that detection in current and near-future neutrino experiments is not feasible, and a megaton-scale detector with $\mathcal{O}(10)$ MeV threshold, such as the proposed Deep-TITAND, MEMPHYS, or MICA, will be required. This is due to the updated binary neutron star merger rate and the time-of-flight delay caused by the nonzero neutrino mass. Regarding the former, recent results from LIGO, Virgo, and KAGRA has significantly lowered the upper limit on the neutron star merger rate. As for the latter, neutrino events from neutron star mergers are expected to be recorded shortly after the gravitational wave signal. Limiting the analysis to such short time windows can significantly reduce background rates. While this approach has been qualitatively discussed in the literature, the effect of the time delay caused by neutrino mass, which can substantially extend the observation windows, has been disregarded. We present a refined analysis employing energy-dependent time windows and luminosity distance cuts for the mergers and provide realistic estimates of the detector runtime required to record neutrinos from binary neutron star mergers with small background contamination. The relative timing between the neutrino and gravitational wave signals can also be employed to probe the scale of neutrino mass. We find that the sensitivity to the lightest neutrino mass exceeds both the most stringent terrestrial bounds from KATRIN and the projections based on galactic supernovae. This level of sensitivity may become particularly relevant in the future if terrestrial and supernova constraints are not significantly improved.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16717v1" target="_blank"><h2>A Machine Learning-Driven Solution for Denoising Inertial Confinement Fusion Images <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Asya Y. Akkus, Bradley T. Wolfe, Pinghan Chu, Chengkun Huang, Chris S. Campbell, Mariana Alvarado Alvarez, Petr Volegov, David Fittinghoff, Robert Reinovsky, Zhehui Wang<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> latent space (abstract)<br><p><strong><u>Abstract:</u></strong> Neutron imaging is important in optimizing analysis of inertial confinement fusion (ICF) events such as those at the National Ignition Facility (NIF) and improving current and future ICF platforms. However, images of neutron sources are often degraded by various types of noise. Most commonly, Gaussian and Poisson noise often coexist within one image, obscuring fine details and blurring edges. These noise types often overlap, making them difficult to distinguish and remove using conventional filtering and thresholding methods. As a result, noise removal techniques that preserve image fidelity are important for analyzing and interpreting images of a neutron source. Current solutions include a combination of filtering and thresholding methodologies. In the past, machine learning approaches were rarely implemented due to a lack of ground truth neutron imaging data for ICF processes. However, recent advances in synthetic data production, particularly in the fusion imaging field, have opened opportunities to investigate new denoising procedures using both supervised and unsupervised machine learning methods. In this study, we implement an unsupervised autoencoder with a Cohen-Daubechies- Feauveau (CDF 97) wavelet transform in the latent space for mixed Gaussian-Poisson denoising. The network successfully denoises neutron imaging data. Additionally, it demonstrates lower reconstruction error and superior edge preservation metrics when benchmarked with data generated by a forward model and compared to non-ML-based filtering mechanisms such as Block-matching and 3D filtering (BM3D). This approach presents a promising advancement in neutron image noise reduction and three-dimensional reconstruction analysis of ICF experiments.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16652v1" target="_blank"><h2>Evolution Strategies at the Hyperscale <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Bidipta Sarkar, Mattie Fellows, Juan Agustin Duque, Alistair Letcher, Antonio León Villares, Anya Sims, Dylan Cope, Jarek Liesen, Lukas Seier, Theo Wolf, Uljad Berdica, Alexander David Goldie, Aaron Courville, Karin Sevegnani, Shimon Whiteson, Jakob Nicolaus Foerster<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 48 pages, 12 figures, Website atthis https URL<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> We introduce Evolution Guided General Optimization via Low-rank Learning (EGGROLL), an evolution strategies (ES) algorithm designed to scale backprop-free optimization to large population sizes for modern large neural network architectures with billions of parameters. ES is a set of powerful blackbox optimisation methods that can handle non-differentiable or noisy objectives with excellent scaling potential through parallelisation. Na{ï}ve ES becomes prohibitively expensive at scale due to the computational and memory costs associated with generating matrix perturbations $E\in\mathbb{R}^{m\times n}$ and the batched matrix multiplications needed to compute per-member forward passes. EGGROLL overcomes these bottlenecks by generating random matrices $A\in \mathbb{R}^{m\times r},\ B\in \mathbb{R}^{n\times r}$ with $r\ll \min(m,n)$ to form a low-rank matrix perturbation $A B^\top$ that are used in place of the full-rank perturbation $E$. As the overall update is an average across a population of $N$ workers, this still results in a high-rank update but with significant memory and computation savings, reducing the auxiliary storage from $mn$ to $r(m+n)$ per layer and the cost of a forward pass from $\mathcal{O}(mn)$ to $\mathcal{O}(r(m+n))$ when compared to full-rank ES. A theoretical analysis reveals our low-rank update converges to the full-rank update at a fast $\mathcal{O}\left(\frac{1}{r}\right)$ rate. Our experiments show that (1) EGGROLL does not compromise the performance of ES in tabula-rasa RL settings, despite being faster, (2) it is competitive with GRPO as a technique for improving LLM reasoning, and (3) EGGROLL enables stable pre-training of nonlinear recurrent language models that operate purely in integer datatypes.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16653v1" target="_blank"><h2>Teacher-Guided One-Shot Pruning via Context-Aware Knowledge Distillation <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Md. Samiul Alim, Sharjil Khan, Amrijit Biswas, Fuad Rahman, Shafin Rahman, Nabeel Mohammed<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> Accepted at 2025 IEEE International Conference on Big Data (IEEE BigData 2025)<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Unstructured pruning remains a powerful strategy for compressing deep neural networks, yet it often demands iterative train-prune-retrain cycles, resulting in significant computational overhead. To address this challenge, we introduce a novel teacher-guided pruning framework that tightly integrates Knowledge Distillation (KD) with importance score estimation. Unlike prior approaches that apply KD as a post-pruning recovery step, our method leverages gradient signals informed by the teacher during importance score calculation to identify and retain parameters most critical for both task performance and knowledge transfer. Our method facilitates a one-shot global pruning strategy that efficiently eliminates redundant weights while preserving essential representations. After pruning, we employ sparsity-aware retraining with and without KD to recover accuracy without reactivating pruned connections. Comprehensive experiments across multiple image classification benchmarks, including CIFAR-10, CIFAR-100, and TinyImageNet, demonstrate that our method consistently achieves high sparsity levels with minimal performance degradation. Notably, our approach outperforms state-of-the-art baselines such as EPG and EPSD at high sparsity levels, while offering a more computationally efficient alternative to iterative pruning schemes like COLT. The proposed framework offers a computation-efficient, performance-preserving solution well suited for deployment in resource-constrained environments.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16631v1" target="_blank"><h2>A Core-Collapse Supernova Neutrino Parameterization with Enhanced Physical Interpretability <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Haihao Shi, Zhenyang Huang, Junda Zhou, Guoliang Lü, Xuefei Chen<br><strong><u>Categories:</u></strong> astro-ph.HE<br><strong><u>Comments:</u></strong> 48 pages, 31 figures, It has been accepted by APJS<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> We introduce a novel parameterization of supernova neutrino energy spectra with a clear physical motivation. Its central parameter, $τ(t)$, quantifies the characteristic thermal-diffusion area during the explosion. When applied to the historic SN1987A data, this parameterization yields statistically significant fits and provides robust constraints on the unobserved low-energy portion of the spectrum. Beyond this specific application, we demonstrate the model's power on a suite of 3D core-collapse supernova simulations, finding that the temporal evolution of $τ(t)$ distinctly separates successful from failed explosions. Furthermore, we constrain the progenitor mass of SN 1987A to approximately 19 solar masses by applying Smoothed Isotonic Regression, while noting the sensitivity of this estimate to observational uncertainties. Moreover, in these simulations, $τ(t)$ and the gravitational-wave strain amplitude display a strong, synergistic co-evolution, directly linking the engine's energetic evolution to its geometric asymmetry. This implies that the thermodynamic state of the explosion is imprinted not only on the escaping neutrino flux, but also recorded in the shape of the energy spectrum. Our framework therefore offers a valuable tool for decoding the detailed core dynamics and multi-messenger processes of future galactic supernovae.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16625v1" target="_blank"><h2>MedBayes-Lite: Bayesian Uncertainty Quantification for Safe Clinical Decision Support <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Elias Hossain, Md Mehedi Hasan Nipu, Maleeha Sheikh, Rajib Rana, Subash Neupane, Niloofar Yousefi<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> We propose MedBayes-Lite, a lightweight Bayesian enhancement for transformer-based clinical language models designed to produce reliable, uncertainty-aware predictions. Although transformers show strong potential for clinical decision support, they remain prone to overconfidence, especially in ambiguous medical cases where calibrated uncertainty is critical. MedBayes-Lite embeds uncertainty quantification directly into existing transformer pipelines without any retraining or architectural rewiring, adding no new trainable layers and keeping parameter overhead under 3 percent. The framework integrates three components: (i) Bayesian Embedding Calibration using Monte Carlo dropout for epistemic uncertainty, (ii) Uncertainty-Weighted Attention that marginalizes over token reliability, and (iii) Confidence-Guided Decision Shaping inspired by clinical risk minimization. Across biomedical QA and clinical prediction benchmarks (MedQA, PubMedQA, MIMIC-III), MedBayes-Lite consistently improves calibration and trustworthiness, reducing overconfidence by 32 to 48 percent. In simulated clinical settings, it can prevent up to 41 percent of diagnostic errors by flagging uncertain predictions for human review. These results demonstrate its effectiveness in enabling reliable uncertainty propagation and improving interpretability in medical AI systems.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16614v1" target="_blank"><h2>Deep Learning Framework for Enhanced Neutrino Reconstruction of Single-line Events in the ANTARES Telescope <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> A. Albert, S. Alves, M. André, M. Ardid, S. Ardid, J. -J. Aubert, J. Aublin, B. Baret, S. Basa, Y. Becherini, B. Belhorma, F. Benfenati, V. Bertin, S. Biagi, J. Boumaaza, M. Bouta, M. C. Bouwhuis, H. Brânzaş, R. Bruijn, J. Brunner, J. Busto, B. Caiffi, D. Calvo, S. Campion, A. Capone, F. Carenini, J. Carr, V. Carretero, T. Cartraud, S. Celli, L. Cerisy, M. Chabab, R. Cherkaoui El Moursli, T. Chiarusi, M. Circella, J. A. B. Coelho, A. Coleiro, R. Coniglione, P. Coyle, A. Creusot, A. F. Díaz, B. De Martino, C. Distefano, I. Di Palma, C. Donzaud, D. Dornic, D. Drouhin, T. Eberl, A. Eddymaoui, T. van Eeden, D. van Eijk, S. El Hedri, N. El Khayati, A. Enzenhöfer, P. Fermani, G. Ferrara, F. Filippini, L. Fusco, S. Gagliardini, J. García-Méndez, C. Gatius Oliver, P. Gay, N. Geißelbrecht, H. Glotin, R. Gozzini, R. Gracia Ruiz, K. Graf, C. Guidi, L. Haegel, H. van Haren, A. J. Heijboer, Y. Hello, L. Hennig, J. J. Hernández-Rey, J. Hößl, F. Huang, G. Illuminati, B. Jisse-Jung, M. de Jong, P. de Jong, M. Kadler, O. Kalekin, U. Katz, A. Kouchner, I. Kreykenbohm, V. Kulikovskiy, R. Lahmann, M. Lamoureux, A. Lazo, D. Lefèvre, E. Leonora, G. Levi, S. Le Stum, S. Loucatos, J. Manczak, M. Marcelin, A. Margiotta, A. Marinelli, J. A. Martínez-Mora, P. Migliozzi, A. Moussa, R. Muller, S. Navas, E. Nezri, B. Ó Fearraigh, E. Oukacha, A. M. Păun, G. E. Păvălaş, S. Peña-Martínez, M. Perrin-Terrin, P. Piattelli, C. Poiré, V. Popa, T. Pradier, N. Randazzo, D. Real, G. Riccobene, A. Romanov, A. Sánchez Losa, A. Saina, F. Salesa Greus, D. F. E. Samtleben, M. Sanguineti, P. Sapienza, F. Schüssler, J. Seneca, M. Spurio, Th. Stolarczyk, M. Taiuti, Y. Tayalati, B. Vallage, G. Vannoye, V. Van Elewyck, S. Viola, D. Vivolo, J. Wilms, S. Zavatarelli, A. Zegarelli, J. D. Zornoza, J. Zúñiga<br><strong><u>Categories:</u></strong> physics.comp-ph, astro-ph.IM<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract), transfer learning (abstract)<br><p><strong><u>Abstract:</u></strong> We present the $N$-fit algorithm designed to improve the reconstruction of neutrino events detected by a single line of the ANTARES underwater telescope, usually associated with low energy neutrino events ($\sim$ 100 GeV). $N$-Fit is a neural network model that relies on deep learning and combines several advanced techniques in machine learning --deep convolutional layers, mixture density output layers, and transfer learning. This framework divides the reconstruction process into two dedicated branches for each neutrino event topology --tracks and showers-- composed of sub-models for spatial estimation --direction and position-- and energy inference, which later on are combined for event classification. Regarding the direction of single-line events, the $N$-Fit algorithm significantly refines the estimation of the zenithal angle, and delivers reliable azimuthal angle predictions that were previously unattainable with traditional $χ^2$-fit methods. Improving on energy estimation of single-line events is a tall order; $N$-Fit benefits from transfer learning to efficiently integrate key characteristics, such as the estimation of the closest distance from the event to the detector. $N$-Fit also takes advantage from transfer learning in event topology classification by freezing convolutional layers of the pretrained branches. Tests on Monte Carlo simulations and data demonstrate a significant reduction in mean and median absolute errors across all reconstructed parameters. The improvements achieved by $N$-Fit highlight its potential for advancing multimessenger astrophysics and enhancing our ability to probe fundamental physics beyond the Standard Model using single-line events from ANTARES data.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16600v2" target="_blank"><h2>You Only Forward Once: An Efficient Compositional Judging Paradigm <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Tianlong Zhang, Hongwei Xue, Shilin Yan, Di Wu, Chen Xu, Yunyun Yang<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Multimodal large language models (MLLMs) show strong potential as judges. However, existing approaches face a fundamental trade-off: adapting MLLMs to output a single score misaligns with the generative nature of MLLMs and limits fine-grained requirement understanding, whereas autoregressively generating judging analyses is prohibitively slow in high-throughput settings. Observing that judgment reduces to verifying whether inputs satisfy a set of structured requirements, we propose YOFO, a template-conditioned method that judges all requirements in a single forward pass. Built on an autoregressive model, YOFO accepts a structured requirement template and, in one inference step, produces a binary yes/no decision for each requirement by reading the logits of the final token associated with that requirement. This design yields orders-of-magnitude speedups while preserving interpretability. Extensive experiments show that YOFO not only achieves state-of-the-art results on standard recommendation datasets, but also supports dependency-aware analysis -- where subsequent judgments are conditioned on previous ones -- and further benefits from post-hoc CoT.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16595v1" target="_blank"><h2>TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Boshen Xu, Zihan Xiao, Jiaze Li, Jianzhong Ju, Zhenbo Luo, Jian Luan, Qin Jin<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.CL<br><strong><u>Comments:</u></strong> Project page:this https URL<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract), transformer (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> We introduce TimeViper, a hybrid vision-language model designed to tackle challenges of long video understanding. Processing long videos demands both an efficient model architecture and an effective mechanism for handling extended temporal contexts. To this end, TimeViper adopts a hybrid Mamba-Transformer backbone that combines the efficiency of state-space models with the expressivity of attention mechanisms. Through this hybrid design, we reveal the vision-to-text information aggregation phenomenon, where information progressively flows from vision tokens to text tokens across increasing LLM depth, resulting in severe vision token redundancy. Motivated by this observation, we propose TransV, a token information transfer module that transfers and compresses vision tokens into instruction tokens while maintaining multimodal understanding capabilities. This design enables TimeViper to process hour-long videos exceeding 10,000 frames. Extensive experiments across multiple benchmarks demonstrate that TimeViper competes with state-of-the-art models while extending frame numbers. We further analyze attention behaviors of both Mamba and Transformer layers, offering new insights into hybrid model interpretability. This work represents an initial step towards developing, interpreting, and compressing hybrid Mamba-Transformer architectures.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16588v1" target="_blank"><h2>Formal Abductive Latent Explanations for Prototype-Based Networks <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jules Soria, Zakaria Chihani, Julien Girard-Satabin, Alban Grastien, Romain Xu-Darme, Daniela Cancila<br><strong><u>Categories:</u></strong> cs.AI, cs.LO<br><strong><u>Comments:</u></strong> Accepted at AAAI-26<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> explainable (abstract)<br><p><strong><u>Abstract:</u></strong> Case-based reasoning networks are machine-learning models that make predictions based on similarity between the input and prototypical parts of training samples, called prototypes. Such models are able to explain each decision by pointing to the prototypes that contributed the most to the final outcome. As the explanation is a core part of the prediction, they are often qualified as ``interpretable by design". While promising, we show that such explanations are sometimes misleading, which hampers their usefulness in safety-critical contexts. In particular, several instances may lead to different predictions and yet have the same explanation. Drawing inspiration from the field of formal eXplainable AI (FXAI), we propose Abductive Latent Explanations (ALEs), a formalism to express sufficient conditions on the intermediate (latent) representation of the instance that imply the prediction. Our approach combines the inherent interpretability of case-based reasoning models and the guarantees provided by formal XAI. We propose a solver-free and scalable algorithm for generating ALEs based on three distinct paradigms, compare them, and present the feasibility of our approach on diverse datasets for both standard and fine-grained image classification. The associated code can be found at https://github.com/julsoria/ale</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16573v1" target="_blank"><h2>An Exterior-Embedding Neural Operator Framework for Preserving Conservation Laws <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Huanshuo Dong, Hong Wang, Hao Wu, Zhiwei Zhuang, Xuanze Yang, Ruiqi Shu, Yuan Gao, Xiaomeng Huang<br><strong><u>Categories:</u></strong> cs.OH, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Neural operators have demonstrated considerable effectiveness in accelerating the solution of time-dependent partial differential equations (PDEs) by directly learning governing physical laws from data. However, for PDEs governed by conservation laws(e.g., conservation of mass, energy, or matter), existing neural operators fail to satisfy conservation properties, which leads to degraded model performance and limited generalizability. Moreover, we observe that distinct PDE problems generally require different optimal neural network architectures. This finding underscores the inherent limitations of specialized models in generalizing across diverse problem domains.
  To address these limitations, we propose Exterior-Embedded Conservation Framework (ECF), a universal conserving framework that can be integrated with various data-driven neural operators to enforce conservation laws strictly in predictions. The framework consists of two key components: a conservation quantity encoder that extracts conserved quantities from input data, and a conservation quantity decoder that adjusts the neural operator's predictions using these quantities to ensure strict conservation compliance in the final output. Since our architecture enforces conservation laws, we theoretically prove that it enhances model performance. To validate the performance of our method, we conduct experiments on multiple conservation-law-constrained PDE scenarios, including adiabatic systems, shallow water equations, and the Allen-Cahn problem. These baselines demonstrate that our method effectively improves model accuracy while strictly enforcing conservation laws in the predictions.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16571v1" target="_blank"><h2>Boosting Predictive Performance on Tabular Data through Data Augmentation with Latent-Space Flow-Based Diffusion <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Md. Tawfique Ihsan, Md. Rakibul Hasan Rafi, Ahmed Shoyeb Raihan, Imtiaz Ahmed, Abdullahil Azeem<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 35 Pages<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> VAE (abstract), latent space (abstract), attention (abstract), data augmentation (title, abstract)<br><p><strong><u>Abstract:</u></strong> Severe class imbalance is common in real-world tabular learning, where rare but important minority classes are essential for reliable prediction. Existing generative oversampling methods such as GANs, VAEs, and diffusion models can improve minority-class performance, but they often struggle with tabular heterogeneity, training stability, and privacy concerns. We propose a family of latent-space, tree-driven diffusion methods for minority oversampling that use conditional flow matching with gradient-boosted trees as the vector-field learner. The models operate in compact latent spaces to preserve tabular structure and reduce computation. We introduce three variants: PCAForest, which uses linear PCA embedding; EmbedForest, which uses a learned nonlinear embedding; and AttentionForest, which uses an attention-augmented embedding. Each method couples a GBT-based flow with a decoder back to the original feature space. Across 11 datasets from healthcare, finance, and manufacturing, AttentionForest achieves the best average minority recall while maintaining competitive precision, calibration, and distributional similarity. PCAForest and EmbedForest reach similar utility with much faster generation, offering favorable accuracy-efficiency trade-offs. Privacy evaluated with nearest-neighbor distance ratio and distance-to-closest-record is comparable to or better than the ForestDiffusion baseline. Ablation studies show that smaller embeddings tend to improve minority recall, while aggressive learning rates harm stability. Overall, latent-space, tree-driven diffusion provides an efficient and privacy-aware approach to high-fidelity tabular data augmentation under severe class imbalance.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16566v1" target="_blank"><h2>NutriScreener: Retrieval-Augmented Multi-Pose Graph Attention Network for Malnourishment Screening <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Misaal Khan, Mayank Vatsa, Kuldeep Singh, Richa Singh<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> Accepted in AAAI 2026 Special Track on AI for Social Impact<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Child malnutrition remains a global crisis, yet existing screening methods are laborious and poorly scalable, hindering early intervention. In this work, we present NutriScreener, a retrieval-augmented, multi-pose graph attention network that combines CLIP-based visual embeddings, class-boosted knowledge retrieval, and context awareness to enable robust malnutrition detection and anthropometric prediction from children's images, simultaneously addressing generalizability and class imbalance. In a clinical study, doctors rated it 4.3/5 for accuracy and 4.6/5 for efficiency, confirming its deployment readiness in low-resource settings. Trained and tested on 2,141 children from AnthroVision and additionally evaluated on diverse cross-continent populations, including ARAN and an in-house collected CampusPose dataset, it achieves 0.79 recall, 0.82 AUC, and significantly lower anthropometric RMSEs, demonstrating reliable measurement in unconstrained pediatric settings. Cross-dataset results show up to 25% recall gain and up to 3.5 cm RMSE reduction using demographically matched knowledge bases. NutriScreener offers a scalable and accurate solution for early malnutrition detection in low-resource environments.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16554v1" target="_blank"><h2>Dark Matter-Dark Radiation Interactions and the Hubble Tension <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Manuel A. Buen-Abad, Zackaria Chacko, Ina Flood, Can Kilic, Gustavo Marques-Tavares, Taewook Youn<br><strong><u>Categories:</u></strong> astro-ph.CO, hep-ph<br><strong><u>Comments:</u></strong> 41 pages, 19 figures<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> Models in which a subcomponent of dark matter interacts with dark radiation have been proposed as a solution to the Hubble tension. In this framework, the interacting subcomponent of dark matter is in thermal equilibrium with the dark radiation in the early universe, but decouples from it around the time of matter-radiation equality. We study this general class of models and evaluate the quality of fit to recent cosmological data on the cosmic microwave background (from Planck 2018 and ACT DR6), baryon acoustic oscillations, large-scale structure, supernovae type Ia, and Cepheid variables. We focus on three benchmark scenarios that differ in the rate at which the dark matter decouples from the dark radiation, resulting in different patterns of dark acoustic oscillations. Fitting without ACT DR6 data, we find that all three scenarios significantly reduce the Hubble tension relative to $Λ$CDM, with an exponentially fast decoupling being the most preferred. The tension is reduced to less than $2 \, σ$ in fits that don't include the SH0ES collaboration results as part of the data and to less than $1 \, σ$ when these are included. When ACT DR6 data is included, the fit is significantly worsened. We find that the largest $H_0$ value at the $95 \%$ confidence region is $70.1$ km/s/Mpc without the SH0ES data, leading to only a mild reduction in the tension. This increases to $72.5$ km/s/Mpc, corresponding to a reduction in the tension to less than $3 \, σ$, if the SH0ES results are included in the fit.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16551v1" target="_blank"><h2>Toward Valid Generative Clinical Trial Data with Survival Endpoints <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Perrine Chassat, Van Tuan Nguyen, Lucas Ducrot, Emilie Lanoy, Agathe Guilloux<br><strong><u>Categories:</u></strong> cs.LG, stat.AP, stat.ME, stat.ML<br><strong><u>Comments:</u></strong> P. Chassat and V.T. Nguyen contributed equally to this work<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> variational autoencoder (abstract), VAE (abstract)<br><p><strong><u>Abstract:</u></strong> Clinical trials face mounting challenges: fragmented patient populations, slow enrollment, and unsustainable costs, particularly for late phase trials in oncology and rare diseases. While external control arms built from real-world data have been explored, a promising alternative is the generation of synthetic control arms using generative AI. A central challenge is the generation of time-to-event outcomes, which constitute primary endpoints in oncology and rare disease trials, but are difficult to model under censoring and small sample sizes. Existing generative approaches, largely GAN-based, are data-hungry, unstable, and rely on strong assumptions such as independent censoring. We introduce a variational autoencoder (VAE) that jointly generates mixed-type covariates and survival outcomes within a unified latent variable framework, without assuming independent censoring. Across synthetic and real trial datasets, we evaluate our model in two realistic scenarios: (i) data sharing under privacy constraints, where synthetic controls substitute for original data, and (ii) control-arm augmentation, where synthetic patients mitigate imbalances between treated and control groups. Our method outperforms GAN baselines on fidelity, utility, and privacy metrics, while revealing systematic miscalibration of type I error and power. We propose a post-generation selection procedure that improves calibration, highlighting both progress and open challenges for generative survival modeling.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16550v1" target="_blank"><h2>Broad stochastic configuration residual learning system for norm-convergent universal approximation <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Han Su, Zhongyan Li, Wanquan Liu<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Universal approximation serves as the foundation of neural network learning algorithms. However, some networks establish their universal approximation property by demonstrating that the iterative errors converge in probability measure rather than the more rigorous norm convergence, which makes the universal approximation property of randomized learning networks highly sensitive to random parameter selection, Broad residual learning system (BRLS), as a member of randomized learning models, also encounters this issue. We theoretically demonstrate the limitation of its universal approximation property, that is, the iterative errors do not satisfy norm convergence if the selection of random parameters is inappropriate and the convergence rate meets certain conditions. To address this issue, we propose the broad stochastic configuration residual learning system (BSCRLS) algorithm, which features a novel supervisory mechanism adaptively constraining the range settings of random parameters on the basis of BRLS framework, Furthermore, we prove the universal approximation theorem of BSCRLS based on the more stringent norm convergence. Three versions of incremental BSCRLS algorithms are presented to satisfy the application requirements of various network updates. Solar panels dust detection experiments are performed on publicly available dataset and compared with 13 deep and broad learning algorithms. Experimental results reveal the effectiveness and superiority of BSCRLS algorithms.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16543v1" target="_blank"><h2>The Oracle and The Prism: A Decoupled and Efficient Framework for Generative Recommendation Explanation <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Jiaheng Zhang, Daqiang Zhang<br><strong><u>Categories:</u></strong> cs.IR, cs.AI, cs.CL, cs.LG<br><strong><u>Comments:</u></strong> 11 pages,3 figures<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> explainable (abstract)<br><p><strong><u>Abstract:</u></strong> The integration of Large Language Models (LLMs) into explainable recommendation systems often leads to a performance-efficiency trade-off in end-to-end architectures, where joint optimization of ranking and explanation can result in suboptimal compromises. To resolve this, we propose Prism, a novel decoupled framework that rigorously separates the recommendation process into a dedicated ranking stage and an explanation generation stage.
  Inspired by knowledge distillation, Prism leverages a powerful teacher LLM (e.g., FLAN-T5-XXL) as an Oracle to produce high-fidelity explanatory knowledge. A compact, fine-tuned student model (e.g., BART-Base), the Prism, then specializes in synthesizing this knowledge into personalized explanations. This decomposition ensures that each component is optimized for its specific objective, eliminating inherent conflicts in coupled models.
  Extensive experiments on benchmark datasets demonstrate that our 140M-parameter Prism model significantly outperforms its 11B-parameter teacher in human evaluations of faithfulness and personalization, while achieving a 24 times speedup and a 10 times reduction in memory consumption during inference. These results validate that decoupling, coupled with targeted distillation, provides an efficient and effective pathway to high-quality explainable recommendation.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16527v1" target="_blank"><h2>Contrastive vision-language learning with paraphrasing and negation <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Kwun Ho Ngan, Saman Sadeghi Afgeh, Joe Townsend, Artur d'Avila Garcez<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> latent space (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Contrastive vision-language models continue to be the dominant approach for image and text retrieval. Contrastive Language-Image Pre-training (CLIP) trains two neural networks in contrastive manner to align their image and text embeddings in a shared latent space. Recent results evaluating CLIP on negated or paraphrased text have shown mixed performance because negation changes meaning radically with minimal lexical changes, while paraphrasing can create very different textual expressions with the same intended meaning. This poses a significant challenge for improving the evaluation results and alignment of vision-language models. To address this challenge, this paper evaluates the combination of paraphrasing and negation, proposes a new CLIP contrastive loss function accounting for both paraphrasing and negation, and applies LLM-generated training triples consisting of original, paraphrased and negated textual captions to CLIP-like training models. The approach, called SemCLIP, is shown to move paraphrased captions towards the original image embeddings while pushing negated captions further away in embedding space. Empirically, SemCLIP is shown to be capable of preserving CLIP's performance while increasing considerably the distances to negated captions. On the CC-Neg benchmark using an original over negation image-retrieval accuracy metric, SemCLIP improves accuracy from 68.1% to 78.1%. Although results are mixed when compared with CLIP on the Sugarcrepe++ benchmark, SemCLIP's performance is generally better than the models trained with negated captions. This robustness to negation extends to downstream zero-shot classification tasks where SemCLIP pre-trained on Sugarcrepe++ performs better than CLIP on all tested downstream tasks. These results indicate that SemCLIP can achieve significant robustness to semantic transformations.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16506v1" target="_blank"><h2>Two-beam Multiparticle Many-body simulations of Inhomogeneous FFI <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Zoha Laraib, Sherwood Richers<br><strong><u>Categories:</u></strong> astro-ph.HE<br><strong><u>Comments:</u></strong> 15 pages, 14 figures<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> Neutrino flavor evolution in dense astrophysical environments is inherently nonlinear and sensitive to many-body (MB) quantum effects beyond the mean-field (MF) approximation. Existing MB studies are constrained by small system sizes, closed boundaries, and highly idealized symmetry assumptions. We present a unified tensor-network framework that enables simulations of inhomogeneous and anisotropic flavor evolution under conditions relevant to core-collapse supernovae and neutron-star mergers. Within this framework, we examine the effects of inhomogeneity, boundary conditions, and convergence with resolution for multiple neutrino distributions, allowing direct comparison of these setups under one consistent formulation. In our simulations, many-body systems equilibrate earlier than their mean-field counterparts while approaching similar final flavor states. Enlarging the interaction region allows open boundaries to reproduce closed-system behavior, but only when the beams begin superimposed and interact continuously. By contrast, initially separated configurations develop entanglement more slowly, interact over longer times, and equilibrate to a flavor content that differs from that obtained from initially superimposed calculations.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16501v1" target="_blank"><h2>ODE-ViT: Plug & Play Attention Layer from the Generalization of the ViT as an Ordinary Differential Equation <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Carlos Boned Riera, David Romero Sanchez, Oriol Ramos Terrades<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), transformer (abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> In recent years, increasingly large models have achieved outstanding performance across CV tasks. However, these models demand substantial computational resources and storage, and their growing complexity limits our understanding of how they make decisions. Most of these architectures rely on the attention mechanism within Transformer-based designs. Building upon the connection between residual neural networks and ordinary differential equations (ODEs), we introduce ODE-ViT, a Vision Transformer reformulated as an ODE system that satisfies the conditions for well-posed and stable dynamics. Experiments on CIFAR-10 and CIFAR-100 demonstrate that ODE-ViT achieves stable, interpretable, and competitive performance with up to one order of magnitude fewer parameters, surpassing prior ODE-based Transformer approaches in classification tasks. We further propose a plug-and-play teacher-student framework in which a discrete ViT guides the continuous trajectory of ODE-ViT by treating the intermediate representations of the teacher as solutions of the ODE. This strategy improves performance by more than 10% compared to training a free ODE-ViT from scratch.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16494v1" target="_blank"><h2>Physics-Informed Machine Learning for Efficient Sim-to-Real Data Augmentation in Micro-Object Pose Estimation <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Zongcai Tan, Lan Wei, Dandan Zhang<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> data augmentation (title, abstract)<br><p><strong><u>Abstract:</u></strong> Precise pose estimation of optical microrobots is essential for enabling high-precision object tracking and autonomous biological studies. However, current methods rely heavily on large, high-quality microscope image datasets, which are difficult and costly to acquire due to the complexity of microrobot fabrication and the labour-intensive labelling. Digital twin systems offer a promising path for sim-to-real data augmentation, yet existing techniques struggle to replicate complex optical microscopy phenomena, such as diffraction artifacts and depth-dependent imaging.This work proposes a novel physics-informed deep generative learning framework that, for the first time, integrates wave optics-based physical rendering and depth alignment into a generative adversarial network (GAN), to synthesise high-fidelity microscope images for microrobot pose estimation efficiently. Our method improves the structural similarity index (SSIM) by 35.6% compared to purely AI-driven methods, while maintaining real-time rendering speeds (0.022 s/frame).The pose estimator (CNN backbone) trained on our synthetic data achieves 93.9%/91.9% (pitch/roll) accuracy, just 5.0%/5.4% (pitch/roll) below that of an estimator trained exclusively on real data. Furthermore, our framework generalises to unseen poses, enabling data augmentation and robust pose estimation for novel microrobot configurations without additional training data.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16482v1" target="_blank"><h2>Correlation-Aware Feature Attribution Based Explainable AI <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Poushali Sengupta, Yan Zhang, Frank Eliassen, Sabita Maharjan<br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ML<br><strong><u>Comments:</u></strong> Accepted, 2026 International Conference on Advances in Artificial Intelligence and Machine Learning (AAIML 2026)<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), explainability (abstract), explainable (title, abstract)<br><p><strong><u>Abstract:</u></strong> Explainable AI (XAI) is increasingly essential as modern models become more complex and high-stakes applications demand transparency, trust, and regulatory compliance. Existing global attribution methods often incur high computational costs, lack stability under correlated inputs, and fail to scale efficiently to large or heterogeneous datasets. We address these gaps with \emph{ExCIR} (Explainability through Correlation Impact Ratio), a correlation-aware attribution score equipped with a lightweight transfer protocol that reproduces full-model rankings using only a fraction of the data. ExCIR quantifies sign-aligned co-movement between features and model outputs after \emph{robust centering} (subtracting a robust location estimate, e.g., median or mid-mean, from features and outputs). We further introduce \textsc{BlockCIR}, a \emph{groupwise} extension of ExCIR that scores \emph{sets} of correlated features as a single unit. By aggregating the same signed-co-movement numerators and magnitudes over predefined or data-driven groups, \textsc{BlockCIR} mitigates double-counting in collinear clusters (e.g., synonyms or duplicated sensors) and yields smoother, more stable rankings when strong dependencies are present. Across diverse text, tabular, signal, and image datasets, ExCIR shows trustworthy agreement with established global baselines and the full model, delivers consistent top-$k$ rankings across settings, and reduces runtime via lightweight evaluation on a subset of rows. Overall, ExCIR provides \emph{computationally efficient}, \emph{consistent}, and \emph{scalable} explainability for real-world deployment.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16475v1" target="_blank"><h2>A Comparison Between Decision Transformers and Traditional Offline Reinforcement Learning Algorithms <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Ali Murtaza Caunhye, Asad Jeewa<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 15 pages, 4 figures, published in the Proceedings of the 46th Annual conference of the South African Institute of Computer Scientists and Information Technologists (SIACSIT 2025)<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> The field of Offline Reinforcement Learning (RL) aims to derive effective policies from pre-collected datasets without active environment interaction. While traditional offline RL algorithms like Conservative Q-Learning (CQL) and Implicit Q-Learning (IQL) have shown promise, they often face challenges in balancing exploration and exploitation, especially in environments with varying reward densities. The recently proposed Decision Transformer (DT) approach, which reframes offline RL as a sequence modelling problem, has demonstrated impressive results across various benchmarks. This paper presents a comparative study evaluating the performance of DT against traditional offline RL algorithms in dense and sparse reward settings for the ANT continous control environment. Our research investigates how these algorithms perform when faced with different reward structures, examining their ability to learn effective policies and generalize across varying levels of feedback. Through empirical analysis in the ANT environment, we found that DTs showed less sensitivity to varying reward density compared to other methods and particularly excelled with medium-expert datasets in sparse reward scenarios. In contrast, traditional value-based methods like IQL showed improved performance in dense reward settings with high-quality data, while CQL offered balanced performance across different data qualities. Additionally, DTs exhibited lower variance in performance but required significantly more computational resources compared to traditional approaches. These findings suggest that sequence modelling approaches may be more suitable for scenarios with uncertain reward structures or mixed-quality data, while value-based methods remain competitive in settings with dense rewards and high-quality demonstrations.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16468v1" target="_blank"><h2>Optimizing Quantum Key Distribution Network Performance using Graph Neural Networks <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Akshit Pramod Anchan, Ameiy Acharya, Leki Chom Thungon<br><strong><u>Categories:</u></strong> quant-ph, cs.CR, cs.LG, cs.NI<br><strong><u>Comments:</u></strong> 11 pages, 4 figures, and 2 tables<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> This paper proposes an optimization of Quantum Key Distribution (QKD) Networks using Graph Neural Networks (GNN) framework. Today, the development of quantum computers threatens the security systems of classical cryptography. Moreover, as QKD networks are designed for protecting secret communication, they suffer from multiple operational difficulties: adaptive to dynamic conditions, optimization for multiple parameters and effective resource utilization. In order to overcome these obstacles, we propose a GNN-based framework which can model QKD networks as dynamic graphs and extracts exploitable characteristics from these networks' structure. The graph contains not only topological information but also specific characteristics associated with quantum communication (the number of edges between nodes, etc). Experimental results demonstrate that the GNN-optimized QKD network achieves a substantial increase in total key rate (from 27.1 Kbits/s to 470 Kbits/s), a reduced average QBER (from 6.6% to 6.0%), and maintains path integrity with a slight reduction in average transmission distance (from 7.13 km to 6.42 km). Furthermore, we analyze network performance across varying scales (10 to 250 nodes), showing improved link prediction accuracy and enhanced key generation rate in medium-sized networks. This work introduces a novel operation mode for QKD networks, shifting the paradigm of network optimization through adaptive and scalable quantum communication systems that enhance security and performance.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16467v1" target="_blank"><h2>Anatomy of an Idiom: Tracing Non-Compositionality in Language Models <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Andrew Gomes<br><strong><u>Categories:</u></strong> cs.CL, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> We investigate the processing of idiomatic expressions in transformer-based language models using a novel set of techniques for circuit discovery and analysis. First discovering circuits via a modified path patching algorithm, we find that idiom processing exhibits distinct computational patterns. We identify and investigate ``Idiom Heads,'' attention heads that frequently activate across different idioms, as well as enhanced attention between idiom tokens due to earlier processing, which we term ``augmented reception.'' We analyze these phenomena and the general features of the discovered circuits as mechanisms by which transformers balance computational efficiency and robustness. Finally, these findings provide insights into how transformers handle non-compositional language and suggest pathways for understanding the processing of more complex grammatical constructions.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16449v2" target="_blank"><h2>VLA-Pruner: Temporal-Aware Dual-Level Visual Token Pruning for Efficient Vision-Language-Action Inference <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Ziyan Liu, Yeqiu Chen, Hongyi Cai, Tao Lin, Shuo Yang, Zheng Liu, Bo Zhao<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Vision-Language-Action (VLA) models have shown great promise for embodied AI, yet the heavy computational cost of processing continuous visual streams severely limits their real-time deployment. Token pruning (keeping salient visual tokens and dropping redundant ones) has emerged as an effective approach for accelerating Vision-Language Models (VLMs), offering a solution for efficient VLA. However, these VLM-specific token pruning methods select tokens based solely on semantic salience metrics (e.g., prefill attention), while overlooking the VLA's intrinsic dual-system nature of high-level semantic understanding and low-level action execution. Consequently, these methods bias token retention toward semantic cues, discard critical information for action generation, and significantly degrade VLA performance. To bridge this gap, we propose VLA-Pruner, a versatile plug-and-play VLA-specific token prune method that aligns with the dual-system nature of VLA models and exploits the temporal continuity in robot manipulation. Specifically, VLA-Pruner adopts a dual-level importance criterion for visual token retention: vision-language prefill attention for semantic-level relevance and action decode attention, estimated via temporal smoothing, for action-level importance. Based on this criterion, VLA-Pruner proposes a novel dual-level token selection strategy that adaptively preserves a compact, informative set of visual tokens for both semantic understanding and action execution under given compute budget. Experiments show that VLA-Pruner achieves state-of-the-art performance across multiple VLA architectures and diverse robotic tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16445v1" target="_blank"><h2>PersonaDrift: A Benchmark for Temporal Anomaly Detection in Language-Based Dementia Monitoring <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Joy Lai, Alex Mihailidis<br><strong><u>Categories:</u></strong> cs.AI, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> anomaly detection (title, abstract)<br><p><strong><u>Abstract:</u></strong> People living with dementia (PLwD) often show gradual shifts in how they communicate, becoming less expressive, more repetitive, or drifting off-topic in subtle ways. While caregivers may notice these changes informally, most computational tools are not designed to track such behavioral drift over time. This paper introduces PersonaDrift, a synthetic benchmark designed to evaluate machine learning and statistical methods for detecting progressive changes in daily communication, focusing on user responses to a digital reminder system. PersonaDrift simulates 60-day interaction logs for synthetic users modeled after real PLwD, based on interviews with caregivers. These caregiver-informed personas vary in tone, modality, and communication habits, enabling realistic diversity in behavior. The benchmark focuses on two forms of longitudinal change that caregivers highlighted as particularly salient: flattened sentiment (reduced emotional tone and verbosity) and off-topic replies (semantic drift). These changes are injected progressively at different rates to emulate naturalistic cognitive trajectories, and the framework is designed to be extensible to additional behaviors in future use cases. To explore this novel application space, we evaluate several anomaly detection approaches, unsupervised statistical methods (CUSUM, EWMA, One-Class SVM), sequence models using contextual embeddings (GRU + BERT), and supervised classifiers in both generalized and personalized settings. Preliminary results show that flattened sentiment can often be detected with simple statistical models in users with low baseline variability, while detecting semantic drift requires temporal modeling and personalized baselines. Across both tasks, personalized classifiers consistently outperform generalized ones, highlighting the importance of individual behavioral context.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16432v1" target="_blank"><h2>From generative AI to the brain: five takeaways <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Claudius Gros<br><strong><u>Categories:</u></strong> cs.AI, q-bio.NC<br><strong><u>Comments:</u></strong> Frontiers in Computational Neuroscience, in press<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> The big strides seen in generative AI are not based on somewhat obscure algorithms, but due to clearly defined generative principles. The resulting concrete implementations have proven themselves in large numbers of applications. We suggest that it is imperative to thoroughly investigate which of these generative principles may be operative also in the brain, and hence relevant for cognitive neuroscience. In addition, ML research led to a range of interesting characterizations of neural information processing systems. We discuss five examples, the shortcomings of world modelling, the generation of thought processes, attention, neural scaling laws, and quantization, that illustrate how much neuroscience could potentially learn from ML research.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16430v1" target="_blank"><h2>Graph Neural Networks for Surgical Scene Segmentation <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Yihan Li, Nikhil Churamani, Maria Robu, Imanol Luengo, Danail Stoyanov<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> 12 pages, 4 figures, 3 tables<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (title, abstract), transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Purpose: Accurate identification of hepatocystic anatomy is critical to preventing surgical complications during laparoscopic cholecystectomy. Deep learning models often struggle with occlusions, long-range dependencies, and capturing the fine-scale geometry of rare structures. This work addresses these challenges by introducing graph-based segmentation approaches that enhance spatial and semantic understanding in surgical scene analyses.
  Methods: We propose two segmentation models integrating Vision Transformer (ViT) feature encoders with Graph Neural Networks (GNNs) to explicitly model spatial relationships between anatomical regions. (1) A static k Nearest Neighbours (k-NN) graph with a Graph Convolutional Network with Initial Residual and Identity Mapping (GCNII) enables stable long-range information propagation. (2) A dynamic Differentiable Graph Generator (DGG) with a Graph Attention Network (GAT) supports adaptive topology learning. Both models are evaluated on the Endoscapes-Seg50 and CholecSeg8k benchmarks.
  Results: The proposed approaches achieve up to 7-8% improvement in Mean Intersection over Union (mIoU) and 6% improvement in Mean Dice (mDice) scores over state-of-the-art baselines. It produces anatomically coherent predictions, particularly on thin, rare and safety-critical structures.
  Conclusion: The proposed graph-based segmentation methods enhance both performance and anatomical consistency in surgical scene segmentation. By combining ViT-based global context with graph-based relational reasoning, the models improve interpretability and reliability, paving the way for safer laparoscopic and robot-assisted surgery through a precise identification of critical anatomical features.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16423v1" target="_blank"><h2>TOFA: Training-Free One-Shot Federated Adaptation for Vision-Language Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Li Zhang, Zhongxuan Han, XiaoHua Feng, Jiaming Zhang, Yuyuan Li, Linbo Jiang, Jianan Lin, Chaochao Chen<br><strong><u>Categories:</u></strong> cs.AI, cs.CL<br><strong><u>Comments:</u></strong> Accepted by AAAI 2026<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Efficient and lightweight adaptation of pre-trained Vision-Language Models (VLMs) to downstream tasks through collaborative interactions between local clients and a central server is a rapidly emerging research topic in federated learning. Existing adaptation algorithms are typically trained iteratively, which incur significant communication costs and increase the susceptibility to potential attacks. Motivated by the one-shot federated training techniques that reduce client-server exchanges to a single round, developing a lightweight one-shot federated VLM adaptation method to alleviate these issues is particularly attractive. However, current one-shot approaches face certain challenges in adapting VLMs within federated settings: (1) insufficient exploitation of the rich multimodal information inherent in VLMs; (2) lack of specialized adaptation strategies to systematically handle the severe data heterogeneity; and (3) requiring additional training resource of clients or server. To bridge these gaps, we propose a novel Training-free One-shot Federated Adaptation framework for VLMs, named TOFA. To fully leverage the generalizable multimodal features in pre-trained VLMs, TOFA employs both visual and textual pipelines to extract task-relevant representations. In the visual pipeline, a hierarchical Bayesian model learns personalized, class-specific prototype distributions. For the textual pipeline, TOFA evaluates and globally aligns the generated local text prompts for robustness. An adaptive weight calibration mechanism is also introduced to combine predictions from both modalities, balancing personalization and robustness to handle data heterogeneity. Our method is training-free, not relying on additional training resources on either the client or server side. Extensive experiments across 9 datasets in various federated settings demonstrate the effectiveness of the proposed TOFA method.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16417v1" target="_blank"><h2>Pharos-ESG: A Framework for Multimodal Parsing, Contextual Narration, and Hierarchical Labeling of ESG Report <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yan Chen, Yu Zou, Jialei Zeng, Haoran You, Xiaorui Zhou, Aixi Zhong<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> Accepted to AAAI 26:main technical track Oral<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Environmental, Social, and Governance (ESG) principles are reshaping the foundations of global financial gover- nance, transforming capital allocation architectures, regu- latory frameworks, and systemic risk coordination mecha- nisms. However, as the core medium for assessing corpo- rate ESG performance, the ESG reports present significant challenges for large-scale understanding, due to chaotic read- ing order from slide-like irregular layouts and implicit hier- archies arising from lengthy, weakly structured content. To address these challenges, we propose Pharos-ESG, a uni- fied framework that transforms ESG reports into structured representations through multimodal parsing, contextual nar- ration, and hierarchical labeling. It integrates a reading-order modeling module based on layout flow, hierarchy-aware seg- mentation guided by table-of-contents anchors, and a multi- modal aggregation pipeline that contextually transforms vi- sual elements into coherent natural language. The framework further enriches its outputs with ESG, GRI, and sentiment labels, yielding annotations aligned with the analytical de- mands of financial research. Extensive experiments on anno- tated benchmarks demonstrate that Pharos-ESG consistently outperforms both dedicated document parsing systems and general-purpose multimodal models. In addition, we release Aurora-ESG, the first large-scale public dataset of ESG re- ports, spanning Mainland China, Hong Kong, and U.S. mar- kets, featuring unified structured representations of multi- modal content, enriched with fine-grained layout and seman- tic annotations to better support ESG integration in financial governance and decision-making.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16374v1" target="_blank"><h2>Unsupervised Graph Neural Network Framework for Balanced Multipatterning in Advanced Electronic Design Automation Layouts <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Abdelrahman Helaly, Nourhan Sakr, Kareem Madkour, Ilhami Torunoglu<br><strong><u>Categories:</u></strong> cs.AR, cs.LG<br><strong><u>Comments:</u></strong> manuscript under review<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (title)<br><p><strong><u>Abstract:</u></strong> Multipatterning is an essential decomposition strategy in electronic design automation (EDA) that overcomes lithographic limitations when printing dense circuit layouts. Although heuristic-based backtracking and SAT solvers can address these challenges, they often struggle to simultaneously handle both complex constraints and secondary objectives. In this study, we present a hybrid workflow that casts multipatterning as a variant of a constrained graph coloring problem with the primary objective of minimizing feature violations and a secondary objective of balancing the number of features on each mask. Our pipeline integrates two main components: (1) A GNN-based agent, trained in an unsupervised manner to generate initial color predictions, which are refined by (2) refinement strategies (a GNN-based heuristic and simulated annealing) that together enhance solution quality and balance. Experimental evaluation in both proprietary data sets and publicly available open source layouts demonstrate complete conflict-free decomposition and consistent color balancing. The proposed framework provides a reproducible, data-efficient and deployable baseline for scalable layout decomposition in EDA workflows.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16353v1" target="_blank"><h2>Learning from Sufficient Rationales: Analysing the Relationship Between Explanation Faithfulness and Token-level Regularisation Strategies <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jonathan Kamp, Lisa Beinborn, Antske Fokkens<br><strong><u>Categories:</u></strong> cs.CL, cs.AI<br><strong><u>Comments:</u></strong> Long paper accepted to the main conference of AACL 2025. Please cite the conference proceedings when available<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Human explanations of natural language, rationales, form a tool to assess whether models learn a label for the right reasons or rely on dataset-specific shortcuts. Sufficiency is a common metric for estimating the informativeness of rationales, but it provides limited insight into the effects of rationale information on model performance. We address this limitation by relating sufficiency to two modelling paradigms: the ability of models to identify which tokens are part of the rationale (through token classification) and the ability of improving model performance by incorporating rationales in the input (through attention regularisation). We find that highly informative rationales are not likely to help classify the instance correctly. Sufficiency conversely captures the classification impact of the non-rationalised context, which interferes with rationale information in the same input. We also find that incorporating rationale information in model inputs can boost cross-domain classification, but results are inconsistent per task and model type. Finally, sufficiency and token classification appear to be unrelated. These results exemplify the complexity of rationales, showing that metrics capable of systematically capturing this type of information merit further investigation.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16346v1" target="_blank"><h2>VersaPants: A Loose-Fitting Textile Capacitive Sensing System for Lower-Body Motion Capture <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Deniz Kasap, Taraneh Aminosharieh Najafi, Jérôme Paul Rémy Thevenot, Jonathan Dan, Stefano Albini, David Atienza<br><strong><u>Categories:</u></strong> eess.SP, cs.LG, eess.SY<br><strong><u>Comments:</u></strong> 14 pages, 8 figures<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> We present VersaPants, the first loose-fitting, textile-based capacitive sensing system for lower-body motion capture, built on the open-hardware VersaSens platform. By integrating conductive textile patches and a compact acquisition unit into a pair of pants, the system reconstructs lower-body pose without compromising comfort. Unlike IMU-based systems that require user-specific fitting or camera-based methods that compromise privacy, our approach operates without fitting adjustments and preserves user privacy. VersaPants is a custom-designed smart garment featuring 6 capacitive channels per leg. We employ a lightweight Transformer-based deep learning model that maps capacitance signals to joint angles, enabling embedded implementation on edge platforms. To test our system, we collected approximately 3.7 hours of motion data from 11 participants performing 16 daily and exercise-based movements. The model achieves a mean per-joint position error (MPJPE) of 11.96 cm and a mean per-joint angle error (MPJAE) of 12.3 degrees across the hip, knee, and ankle joints, indicating the model's ability to generalize to unseen users and movements. A comparative analysis of existing textile-based deep learning architectures reveals that our model achieves competitive reconstruction performance with up to 22 times fewer parameters and 18 times fewer FLOPs, enabling real-time inference at 42 FPS on a commercial smartwatch without quantization. These results position VersaPants as a promising step toward scalable, comfortable, and embedded motion-capture solutions for fitness, healthcare, and wellbeing applications.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16334v1" target="_blank"><h2>OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Kaichen Zhang, Keming Wu, Zuhao Yang, Kairui Hu, Bin Wang, Ziwei Liu, Xingxuan Li, Lidong Bing<br><strong><u>Categories:</u></strong> cs.AI, cs.CL<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Recent advancements in large reasoning models have fueled growing interest in extending such capabilities to multimodal domains. However, despite notable progress in visual reasoning, the lack of transparent and reproducible data curation and training strategies remains a major barrier to scalable research. In this work, we introduce OpenMMReasoner, a fully transparent two-stage recipe for multimodal reasoning spanning supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we construct an 874K-sample cold-start dataset with rigorous step-by-step validation, providing a strong foundation for reasoning capabilities. The subsequent RL stage leverages a 74K-sample dataset across diverse domains to further sharpen and stabilize these abilities, resulting in a more robust and efficient learning process. Extensive evaluations demonstrate that our training recipe not only surpasses strong baselines but also highlights the critical role of data quality and training design in shaping multimodal reasoning performance. Notably, our method achieves a 11.6% improvement over the Qwen2.5-VL-7B-Instruct baseline across nine multimodal reasoning benchmarks, establishing a solid empirical foundation for future large-scale multimodal reasoning research. We open-sourced all our codes, pipeline, and data at https://github.com/EvolvingLMMs-Lab/OpenMMReasoner.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16333v1" target="_blank"><h2>Beyond Generative AI: World Models for Clinical Prediction, Counterfactuals, and Planning <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Mohammad Areeb Qazi, Maryam Nadeem, Mohammad Yaqub<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 2 Figures, 1 Table<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> VAE (abstract), multimodal (abstract), transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Healthcare requires AI that is predictive, reliable, and data-efficient. However, recent generative models lack physical foundation and temporal reasoning required for clinical decision support. As scaling language models show diminishing returns for grounded clinical reasoning, world models are gaining traction because they learn multimodal, temporally coherent, and action-conditioned representations that reflect the physical and causal structure of care. This paper reviews World Models for healthcare systems that learn predictive dynamics to enable multistep rollouts, counterfactual evaluation and planning. We survey recent work across three domains: (i) medical imaging and diagnostics (e.g., longitudinal tumor simulation, projection-transition modeling, and Joint Embedding Predictive Architecture i.e., JEPA-style predictive representation learning), (ii) disease progression modeling from electronic health records (generative event forecasting at scale), and (iii) robotic surgery and surgical planning (action-conditioned guidance and control). We also introduce a capability rubric: L1 temporal prediction, L2 action-conditioned prediction, L3 counterfactual rollouts for decision support, and L4 planning/control. Most reviewed systems achieve L1--L2, with fewer instances of L3 and rare L4. We identify cross-cutting gaps that limit clinical reliability; under-specified action spaces and safety constraints, weak interventional validation, incomplete multimodal state construction, and limited trajectory-level uncertainty calibration. This review outlines a research agenda for clinically robust prediction-first world models that integrate generative backbones (transformers, diffusion, VAE) with causal/mechanical foundation for safe decision support in healthcare.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16314v1" target="_blank"><h2>Thermal equilibrium curves of accretion disks driven by magnetorotational instability <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Shigenobu Hirose<br><strong><u>Categories:</u></strong> astro-ph.HE, astro-ph.EP, astro-ph.SR<br><strong><u>Comments:</u></strong> This paper is based on an invited talk presented at the 87th Fujihara Seminar: The 50th Anniversary Workshop of the Disk Instability Model in Compact Binary Stars (DIM50TH2025), held on 22--26 September 2025 in Tomakomai, Japan. It has been accepted for publication in PoS (Proceedings of Science), and is scheduled to appear in February 2026 atthis https URL<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> Analogous to the HR diagram for stars, the thermal equilibrium curve encodes the thermodynamics of accretion disks by expressing the local balance between heating -- primarily via viscous dissipation -- and cooling -- typically through radiative transfer. These curves are commonly plotted as surface density versus effective temperature. When an S-shaped locus appears, local annuli become bistable, and limit-cycle oscillations arise when the external mass-transfer rate falls within an unstable band. This behavior underpins the disk instability model for recurring outbursts in cataclysmic variables. This paper reviews first-principles thermal equilibrium curves for accretion disks driven by magnetorotational instability (MRI), with emphasis on dwarf novae. Unlike the parameterized $α$-viscosity approach, the curves are obtained by solving the governing equations with radiation magnetohydrodynamics simulations, thereby reproducing S-shaped loci without prescribing $α$. The disk instability in dwarf-nova systems and the physical origin of angular-momentum transport (shear stresses) are also briefly reviewed. Notes on the stability of radiation-dominated accretion flows are included in the Appendix.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16256v1" target="_blank"><h2>Accelerating Reionization Constraints: An ANN-Emulator Framework for the SCRIPT Semi-numerical Model <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Saptarshi Sarkar, Tirthankar Roy Choudhury<br><strong><u>Categories:</u></strong> astro-ph.IM, astro-ph.CO<br><strong><u>Comments:</u></strong> 22 pages, 5 figures. To be submitted to JCAP<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Constraining the Epoch of Reionization (EoR) with physically motivated simulations is hampered by the high cost of conventional parameter inference. We present an efficient emulator-based framework that dramatically reduces this bottleneck for the photon-conserving semi-numerical code SCRIPT. Our approach combines (i) a reliable coarse-resolution MCMC to locate the high-likelihood region (exploiting the large-scale convergence of SCRIPT) with (ii) an adaptive, targeted sampling strategy to build a compact high-resolution training set for an artificial neural network based emulator of the model likelihood. With only $\approx 10^3$ high-resolution simulations, the trained emulators achieve excellent predictive accuracy ($R^2 \approx 0.97$--$0.99$) and, when embedded within an MCMC framework, reproduce posterior distributions from full high-resolution runs. Compared to conventional MCMC, our pipeline reduces the number of expensive simulations by a factor of $\sim 100$ and lowers total CPU cost by up to a factor of $\sim 70$, while retaining statistical fidelity. This computational speedup makes inference in much higher-dimensional models tractable (e.g., those needed to incorporate JWST and upcoming 21 cm datasets) and provides a general strategy for building efficient emulators for next generation of EoR constraints.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16248v1" target="_blank"><h2>Revisiting Fairness-aware Interactive Recommendation: Item Lifecycle as a Control Knob <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yun Lu, Xiaoyu Shi, Hong Xie, Chongjun Xia, Zhenhui Gong, Mingsheng Shang<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> 8 pages, 5 figures, conference<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> This paper revisits fairness-aware interactive recommendation (e.g., TikTok, KuaiShou) by introducing a novel control knob, i.e., the lifecycle of items. We make threefold contributions. First, we conduct a comprehensive empirical analysis and uncover that item lifecycles in short-video platforms follow a compressed three-phase pattern, i.e., rapid growth, transient stability, and sharp decay, which significantly deviates from the classical four-stage model (introduction, growth, maturity, decline). Second, we introduce LHRL, a lifecycle-aware hierarchical reinforcement learning framework that dynamically harmonizes fairness and accuracy by leveraging phase-specific exposure dynamics. LHRL consists of two key components: (1) PhaseFormer, a lightweight encoder combining STL decomposition and attention mechanisms for robust phase detection; (2) a two-level HRL agent, where the high-level policy imposes phase-aware fairness constraints, and the low-level policy optimizes immediate user engagement. This decoupled optimization allows for effective reconciliation between long-term equity and short-term utility. Third, experiments on multiple real-world interactive recommendation datasets demonstrate that LHRL significantly improves both fairness and user engagement. Furthermore, the integration of lifecycle-aware rewards into existing RL-based models consistently yields performance gains, highlighting the generalizability and practical value of our approach.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16231v1" target="_blank"><h2>Pass@k Metric for RLVR: A Diagnostic Tool of Exploration, But Not an Objective <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yang Yu<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> The ability of Large Language Models (LLMs) to perform complex, multi-step reasoning is a central focus of modern AI research. To evaluate and enhance this capability, the pass@k metric, which measures the probability of obtaining at least one correct solution in k independent samples, has received significant attention. Its intuitive appeal has led to its adoption not only as an evaluation standard but also as a direct optimization objective in reinforcement learning. In this paper, we analyze the pass@k objective, derive its gradient, and demonstrate that it is fundamentally a per-example positive reweighting of the simpler pass@1 objective. Our analysis reveals that the pass@k objective provides a vanishing learning signal in regimes where exploration is most critical. We further analyze the dynamics of "exploration collapse", showing that as the policy concentrates probability mass, the gap between pass@k and pass@1 diminishes. We conclude that while pass@k is a useful diagnostic tool, it may be an unsuitable direct objective for optimization. Instead, mechanisms explicitly encouraging efficient exploration could offer a more effective path forward for reinforcement learning in reasoning tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16229v1" target="_blank"><h2>Q-MLLM: Vector Quantization for Robust Multimodal Large Language Model Security <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Wei Zhao, Zhe Li, Yige Li, Jun Sun<br><strong><u>Categories:</u></strong> cs.CR, cs.AI<br><strong><u>Comments:</u></strong> Accepted by NDSS 2026<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities in cross-modal understanding, but remain vulnerable to adversarial attacks through visual inputs despite robust textual safety mechanisms. These vulnerabilities arise from two core weaknesses: the continuous nature of visual representations, which allows for gradient-based attacks, and the inadequate transfer of text-based safety mechanisms to visual content. We introduce Q-MLLM, a novel architecture that integrates two-level vector quantization to create a discrete bottleneck against adversarial attacks while preserving multimodal reasoning capabilities. By discretizing visual representations at both pixel-patch and semantic levels, Q-MLLM blocks attack pathways and bridges the cross-modal safety alignment gap. Our two-stage training methodology ensures robust learning while maintaining model utility. Experiments demonstrate that Q-MLLM achieves significantly better defense success rate against both jailbreak attacks and toxic image attacks than existing approaches. Notably, Q-MLLM achieves perfect defense success rate (100\%) against jailbreak attacks except in one arguable case, while maintaining competitive performance on multiple utility benchmarks with minimal inference overhead. This work establishes vector quantization as an effective defense mechanism for secure multimodal AI systems without requiring expensive safety-specific fine-tuning or detection overhead. Code is available at https://github.com/Amadeuszhao/QMLLM.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16226v1" target="_blank"><h2>Deep SOR Minimax Q-learning for Two-player Zero-sum Game <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Saksham Gautam, Lakshmi Mandal, Shalabh Bhatnagar<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> In this work, we consider the problem of a two-player zero-sum game. In the literature, the successive over-relaxation Q-learning algorithm has been developed and implemented, and it is seen to result in a lower contraction factor for the associated Q-Bellman operator resulting in a faster value iteration-based procedure. However, this has been presented only for the tabular case and not for the setting with function approximation that typically caters to real-world high-dimensional state-action spaces. Furthermore, such settings in the case of two-player zero-sum games have not been considered. We thus propose a deep successive over-relaxation minimax Q-learning algorithm that incorporates deep neural networks as function approximators and is suitable for high-dimensional spaces. We prove the finite-time convergence of the proposed algorithm. Through numerical experiments, we show the effectiveness of the proposed method over the existing Q-learning algorithm. Our ablation studies demonstrate the effect of different values of the crucial successive over-relaxation parameter.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16225v1" target="_blank"><h2>Real-Time Inference for Distributed Multimodal Systems under Communication Delay Uncertainty <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Victor Croisfelt, João Henrique Inacio de Souza, Shashi Raj Pandey, Beatriz Soret, Petar Popovski<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 6 pages, 3 figures, submitted to IEEE ICC 2026<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (title)<br><p><strong><u>Abstract:</u></strong> Connected cyber-physical systems perform inference based on real-time inputs from multiple data streams. Uncertain communication delays across data streams challenge the temporal flow of the inference process. State-of-the-art (SotA) non-blocking inference methods rely on a reference-modality paradigm, requiring one modality input to be fully received before processing, while depending on costly offline profiling. We propose a novel, neuro-inspired non-blocking inference paradigm that primarily employs adaptive temporal windows of integration (TWIs) to dynamically adjust to stochastic delay patterns across heterogeneous streams while relaxing the reference-modality requirement. Our communication-delay-aware framework achieves robust real-time inference with finer-grained control over the accuracy-latency tradeoff. Experiments on the audio-visual event localization (AVEL) task demonstrate superior adaptability to network dynamics compared to SotA approaches.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16205v1" target="_blank"><h2>ChemLabs on ChemO: A Multi-Agent System for Multimodal Reasoning on IChO 2025 <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Xu Qiang, Shengyuan Bai, Leqing Chen, Zijing Liu, Yu Li<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> 13 pages, 1 figures<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Olympiad-level benchmarks in mathematics and physics are crucial testbeds for advanced AI reasoning, but chemistry, with its unique multimodal symbolic language, has remained an open challenge. We introduce ChemO, a new benchmark built from the International Chemistry Olympiad (IChO) 2025. ChemO features two key innovations for automated assessment: Assessment-Equivalent Reformulation (AER), which converts problems requiring visual outputs (e.g., drawing molecules) into computationally tractable formats, and Structured Visual Enhancement (SVE), a diagnostic mechanism to disentangle a model's visual perception capabilities from its core chemical reasoning. To tackle this benchmark, we propose ChemLabs, a hierarchical multi-agent framework that mimics human expert collaboration through specialized agents for problem decomposition, perception, reasoning, and auditing. Experiments on state-of-the-art multimodal models demonstrate that combining SVE with our multi-agent system yields dramatic performance gains. Our top configuration achieves a score of 93.6 out of 100, surpassing an estimated human gold medal threshold and establishing a new state-of-the-art in automated chemical problem-solving. ChemO Dataset: https://huggingface.co/datasets/IDEA-AI4SCI/ChemO</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16203v1" target="_blank"><h2>When Alignment Fails: Multimodal Adversarial Attacks on Vision-Language-Action Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yuping Yan, Yuhan Xie, Yinxin Zhang, Lingjuan Lyu, Yaochu Jin<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Vision-Language-Action models (VLAs) have recently demonstrated remarkable progress in embodied environments, enabling robots to perceive, reason, and act through unified multimodal understanding. Despite their impressive capabilities, the adversarial robustness of these systems remains largely unexplored, especially under realistic multimodal and black-box conditions. Existing studies mainly focus on single-modality perturbations and overlook the cross-modal misalignment that fundamentally affects embodied reasoning and decision-making. In this paper, we introduce VLA-Fool, a comprehensive study of multimodal adversarial robustness in embodied VLA models under both white-box and black-box settings. VLA-Fool unifies three levels of multimodal adversarial attacks: (1) textual perturbations through gradient-based and prompt-based manipulations, (2) visual perturbations via patch and noise distortions, and (3) cross-modal misalignment attacks that intentionally disrupt the semantic correspondence between perception and instruction. We further incorporate a VLA-aware semantic space into linguistic prompts, developing the first automatically crafted and semantically guided prompting framework. Experiments on the LIBERO benchmark using a fine-tuned OpenVLA model reveal that even minor multimodal perturbations can cause significant behavioral deviations, demonstrating the fragility of embodied multimodal alignment.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16201v1" target="_blank"><h2>From Performance to Understanding: A Vision for Explainable Automated Algorithm Design <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Niki van Stein, Anna V. Kononova, Thomas Bäck<br><strong><u>Categories:</u></strong> cs.AI, cs.NE<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> explainable (title, abstract)<br><p><strong><u>Abstract:</u></strong> Automated algorithm design is entering a new phase: Large Language Models can now generate full optimisation (meta)heuristics, explore vast design spaces and adapt through iterative feedback. Yet this rapid progress is largely performance-driven and opaque. Current LLM-based approaches rarely reveal why a generated algorithm works, which components matter or how design choices relate to underlying problem structures. This paper argues that the next breakthrough will come not from more automation, but from coupling automation with understanding from systematic benchmarking. We outline a vision for explainable automated algorithm design, built on three pillars: (i) LLM-driven discovery of algorithmic variants, (ii) explainable benchmarking that attributes performance to components and hyperparameters and (iii) problem-class descriptors that connect algorithm behaviour to landscape structure. Together, these elements form a closed knowledge loop in which discovery, explanation and generalisation reinforce each other. We argue that this integration will shift the field from blind search to interpretable, class-specific algorithm design, accelerating progress while producing reusable scientific insight into when and why optimisation strategies succeed.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16191v1" target="_blank"><h2>CausalMamba: Interpretable State Space Modeling for Temporal Rumor Causality <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Xiaotong Zhan, Xi Cheng<br><strong><u>Categories:</u></strong> cs.LG, cs.SI<br><strong><u>Comments:</u></strong> Preprint. 9 pages, 3 figures, 2 tables. Code and implementation details available at:this https URL<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), explainable (abstract), causality (title)<br><p><strong><u>Abstract:</u></strong> Rumor detection on social media remains a challenging task due to the complex propagation dynamics and the limited interpretability of existing models. While recent neural architectures capture content and structural features, they often fail to reveal the underlying causal mechanisms of misinformation spread. We propose CausalMamba, a novel framework that integrates Mamba-based sequence modeling, graph convolutional networks (GCNs), and differentiable causal discovery via NOTEARS. CausalMamba learns joint representations of temporal tweet sequences and reply structures, while uncovering latent causal graphs to identify influential nodes within each propagation chain. Experiments on the Twitter15 dataset show that our model achieves competitive classification performance compared to strong baselines, and uniquely enables counterfactual intervention analysis. Qualitative results demonstrate that removing top-ranked causal nodes significantly alters graph connectivity, offering interpretable insights into rumor dynamics. Our framework provides a unified approach for rumor classification and influence analysis, paving the way for more explainable and actionable misinformation detection systems.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16183v1" target="_blank"><h2>FOOTPASS: A Multi-Modal Multi-Agent Tactical Context Dataset for Play-by-Play Action Spotting in Soccer Broadcast Videos <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Jeremie Ochin, Raphael Chekroun, Bogdan Stanciulescu, Sotiris Manitsaris<br><strong><u>Categories:</u></strong> cs.AI, cs.CV<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), multi-modal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Soccer video understanding has motivated the creation of datasets for tasks such as temporal action localization, spatiotemporal action detection (STAD), or multiobject tracking (MOT). The annotation of structured sequences of events (who does what, when, and where) used for soccer analytics requires a holistic approach that integrates both STAD and MOT. However, current action recognition methods remain insufficient for constructing reliable play-by-play data and are typically used to assist rather than fully automate annotation. Parallel research has advanced tactical modeling, trajectory forecasting, and performance analysis, all grounded in game-state and play-by-play data. This motivates leveraging tactical knowledge as a prior to support computer-vision-based predictions, enabling more automated and reliable extraction of play-by-play data. We introduce Footovision Play-by-Play Action Spotting in Soccer Dataset (FOOTPASS), the first benchmark for play-by-play action spotting over entire soccer matches in a multi-modal, multi-agent tactical context. It enables the development of methods for player-centric action spotting that exploit both outputs from computer-vision tasks (e.g., tracking, identification) and prior knowledge of soccer, including its tactical regularities over long time horizons, to generate reliable play-by-play data streams. These streams form an essential input for data-driven sports analytics.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16175v1" target="_blank"><h2>Mantis: A Versatile Vision-Language-Action Model with Disentangled Visual Foresight <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yi Yang, Xueqi Li, Yiyang Chen, Jin Song, Yihan Wang, Zipeng Xiao, Jiadi Su, You Qiaoben, Pengfei Liu, Zhijie Deng<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Recent advances in Vision-Language-Action (VLA) models demonstrate that visual signals can effectively complement sparse action supervisions. However, letting VLA directly predict high-dimensional visual states can distribute model capacity and incur prohibitive training cost, while compressing visual states into more compact supervisory signals inevitably incurs information bottlenecks. Moreover, existing methods often suffer from poor comprehension and reasoning capabilities due to the neglect of language supervision. This paper introduces Mantis, a novel framework featuring a Disentangled Visual Foresight (DVF) to tackle these issues. Specifically, Mantis decouples visual foresight prediction from the backbone with the combination of meta queries and a diffusion Transformer (DiT) head. With the current visual state provided to the DiT via a residual connection, a simple next-state prediction objective enables the meta queries to automatically capture the latent actions that delineate the visual trajectory, and hence boost the learning of explicit actions. The disentanglement reduces the burden of the VLA backbone, enabling it to maintain comprehension and reasoning capabilities through language supervision. Empirically, pretrained on human manipulation videos, robot demonstrations, and image-text pairs, Mantis achieves a 96.7% success rate on LIBERO benchmark after fine-tuning, surpassing powerful baselines while exhibiting high convergence speed. Real-world evaluations show that Mantis outperforms $π_{0.5}$, a leading open-source VLA model, particularly in instruction-following capability, generalization to unseen instructions, and reasoning ability. Code and weights are released to support the open-source community.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16149v1" target="_blank"><h2>Approximation rates of quantum neural networks for periodic functions via Jackson's inequality <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Ariel Neufeld, Philipp Schmocker, Viet Khoa Tran<br><strong><u>Categories:</u></strong> quant-ph, cs.LG, math.NA, stat.ML<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> Quantum neural networks (QNNs) are an analog of classical neural networks in the world of quantum computing, which are represented by a unitary matrix with trainable parameters. Inspired by the universal approximation property of classical neural networks, ensuring that every continuous function can be arbitrarily well approximated uniformly on a compact set of a Euclidean space, some recent works have established analogous results for QNNs, ranging from single-qubit to multi-qubit QNNs, and even hybrid classical-quantum models. In this paper, we study the approximation capabilities of QNNs for periodic functions with respect to the supremum norm. We use the Jackson inequality to approximate a given function by implementing its approximating trigonometric polynomial via a suitable QNN. In particular, we see that by restricting to the class of periodic functions, one can achieve a quadratic reduction of the number of parameters, producing better approximation results than in the literature. Moreover, the smoother the function, the fewer parameters are needed to construct a QNN to approximate the function.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16148v1" target="_blank"><h2>Enhancing Nuclear Reactor Core Simulation through Data-Based Surrogate Models <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Perceval Beja-Battais, Alain Grossetête, Nicolas Vayatis<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> In recent years, there has been an increasing need for Nuclear Power Plants (NPPs) to improve flexibility in order to match the rapid growth of renewable energies. The Operator Assistance Predictive System (OAPS) developed by Framatome addresses this problem through Model Predictive Control (MPC). In this work, we aim to improve MPC methods through data-driven simulation schemes. Thus, from a set of nonlinear stiff ordinary differential equations (ODEs), this paper introduces two surrogate models acting as alternative simulation schemes to enhance nuclear reactor core simulation. We show that both data-driven and physics-informed models can rapidly integrate complex dynamics, with a very low computational time (up to 1000x time reduction).</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16145v1" target="_blank"><h2>Labels Matter More Than Models: Quantifying the Benefit of Supervised Time Series Anomaly Detection <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Zhijie Zhong, Zhiwen Yu, Kaixiang Yang, C. L. Philip Chen<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 16 pages, 14 figures, 7 tables. Under review<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> anomaly detection (title, abstract)<br><p><strong><u>Abstract:</u></strong> Time series anomaly detection (TSAD) is a critical data mining task often constrained by label scarcity. Consequently, current research predominantly focuses on Unsupervised Time-series Anomaly Detection (UTAD), relying on complex architectures to model normal data distributions. However, this approach often overlooks the significant performance gains available from limited anomaly labels achievable in practical scenarios. This paper challenges the premise that architectural complexity is the optimal path for TSAD. We conduct the first methodical comparison between supervised and unsupervised paradigms and introduce STAND, a streamlined supervised baseline. Extensive experiments on five public datasets demonstrate that: (1) Labels matter more than models: under a limited labeling budget, simple supervised models significantly outperform complex state-of-the-art unsupervised methods; (2) Supervision yields higher returns: the performance gain from minimal supervision far exceeds that from architectural innovations; and (3) Practicality: STAND exhibits superior prediction consistency and anomaly localization compared to unsupervised counterparts. These findings advocate for a data-centric shift in TSAD research, emphasizing label utilization over purely algorithmic complexity. The code is publicly available at https://github.com/EmorZz1G/STAND.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16139v1" target="_blank"><h2>Multidimensional Rubric-oriented Reward Model Learning via Geometric Projection Reference Constraints <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yongnan Jin, Xurui Li, Feng Cao, Liucun Gao, Juanjuan Yao<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> The integration of large language models (LLMs) into medical practice holds transformative potential, yet their real-world clinical utility remains limited by critical alignment challenges: (1) a disconnect between static evaluation benchmarks and dynamic clinical cognitive needs, (2) difficulties in adapting to evolving, multi-source medical standards, and (3) the inability of conventional reward models to capture nuanced, multi-dimensional medical quality criteria. To address these gaps, we propose MR-RML (Multidimensional Rubric-oriented Reward Model Learning) via GPRC (Geometric Projection Reference Constraints), a novel alignment framework that integrates medical standards into a structured "Dimensions-Scenarios-Disciplines" matrix to guide data generation and model optimization. MR-RML introduces three core innovations: (1) a "Dimensions-Scenarios-Disciplines" medical standard system that embeds domain standards into the full training pipeline; (2) an independent multi-dimensional reward model that decomposes evaluation criteria, shifting from real-time rubric-based scoring to internalized reward modeling for improved consistency and cost-efficiency; (3) geometric projection reference constraints that transform medical cognitive logic into mathematical regularization, aligning scoring gradients with clinical reasoning and enabling synthetic data-driven training. Through extensive evaluations on the authoritative medical benchmark Healthbench, our method yields substantial performance gains over the base LLM Qwen-32B (45% on the full subset and 85% on Hard subset, respectively). It achieves a SOTA among open-source LLMs with scores of 62.7 (full subset) and 44.7 (hard subset), while also outperforming the majority of closed-source models.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16130v1" target="_blank"><h2>Creation of Viscous Dark Energy by the Hubble Flow: Comparison with SNe Ia Master Sample Binned Data <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Iolanda Navonea, Maria Giovanna Dainotti, Elisa Fazzari, Giovanni Montani, Naoto Maki<br><strong><u>Categories:</u></strong> astro-ph.CO<br><strong><u>Comments:</u></strong> 14 pages, 10 figures<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> We study a cosmological model featuring evolutionary dark energy, according to the idea that the creation of its constituents arises from the gravitational field of the expanding universe, whose non-equilibrium physics is described by a non-zero bulk viscosity coefficient. This physical scenario calls for two additional parameters with respect to the ΛCDM model, one of which is the equation of state parameter of the created dark energy. The model is constrained by the requirement that its deceleration parameter coincides with the one predicted by the ΛCDM model. Then, we construct the effective running Hubble constant, a theoretical function that corresponds to the ratio of the Hubble parameter in our model to the ΛCDM expansion rate. The model's theoretical predictions for the effective running Hubble constant are compared with the binned data of the Supernovae Ia Master Sample. The comparison is performed by a MCMC procedure for each bin, with three parameters left free to vary, while the particle creation rate is taken from a grid of values, each of which is fixed in the given MCMC run. The most important result emerging from this analysis is that the created dark energy constituent corresponds to an equation of state parameter with phantom character. Only if particle creation is removed do the dark energy constituents acquire a quintessence character. No matter the intrinsic nature of the constituents, their effective z-dependent equation of state parameter is, both with and without considering particle creation, entirely of phantom nature across the considered redshift range.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16105v1" target="_blank"><h2>Pathlet Variational Auto-Encoder for Robust Trajectory Generation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yuanbo Tang, Yan Tang, Zixuan Zhang, Zihui Zhao, Yang Li<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> variational autoencoder (abstract), VAE (abstract)<br><p><strong><u>Abstract:</u></strong> Trajectory generation has recently drawn growing interest in privacy-preserving urban mobility studies and location-based service applications. Although many studies have used deep learning or generative AI methods to model trajectories and have achieved promising results, the robustness and interpretability of such models are largely unexplored. This limits the application of trajectory generation algorithms on noisy real-world data and their trustworthiness in downstream tasks. To address this issue, we exploit the regular structure in urban trajectories and propose a deep generative model based on the pathlet representation, which encode trajectories with binary vectors associated with a learned dictionary of trajectory segments. Specifically, we introduce a probabilistic graphical model to describe the trajectory generation process, which includes a Variational Autoencoder (VAE) component and a linear decoder component. During training, the model can simultaneously learn the latent embedding of pathlet representations and the pathlet dictionary that captures mobility patterns in the trajectory dataset. The conditional version of our model can also be used to generate customized trajectories based on temporal and spatial constraints.
  Our model can effectively learn data distribution even using noisy data, achieving relative improvements of $35.4\%$ and $26.3\%$ over strong baselines on two real-world trajectory datasets. Moreover, the generated trajectories can be conveniently utilized for multiple downstream tasks, including trajectory prediction and data denoising. Lastly, the framework design offers a significant efficiency advantage, saving $64.8\%$ of the time and $56.5\%$ of GPU memory compared to previous approaches.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16101v1" target="_blank"><h2>HybSpecNet: A Critical Analysis of Architectural Instability in Hybrid-Domain Spectral GNNs <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Huseyin Goksu<br><strong><u>Categories:</u></strong> cs.LG, eess.SP<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Spectral Graph Neural Networks offer a principled approach to graph filtering but face a fundamental "Stability-vs-Adaptivity" trade-off. This trade-off is dictated by the choice of spectral domain. Filters in the finite [-1, 1] domain (e.g., ChebyNet) are numerically stable at high polynomial degrees (K) but are static and low-pass, causing them to fail on heterophilic graphs. Conversely, filters in the semi-infinite [0, infty) domain (e.g., KrawtchoukNet) are highly adaptive and achieve SOTA results on heterophily by learning non-low-pass responses. However, as we demonstrate, these adaptive filters can also suffer from numerical instability, leading to catastrophic performance collapse at high K. In this paper, we propose to resolve this trade-off by designing a hybrid-domain GNN, HybSpecNet, which combines a stable `ChebyNet` branch with an adaptive `KrawtchoukNet` branch. We first demonstrate that a "naive" hybrid architecture, which fuses the branches via concatenation, successfully unifies performance at low K, achieving strong results on both homophilic and heterophilic benchmarks. However, we then prove that this naive architecture fails the stability test. Our K-ablation experiments show that this architecture catastrophically collapses at K=25, exactly mirroring the collapse of its unstable `KrawtchoukNet` branch. We identify this critical finding as "Instability Poisoning," where `NaN`/`Inf` gradients from the adaptive branch destroy the training of the model. Finally, we propose and validate an advanced architecture that uses "Late Fusion" to completely isolate the gradient pathways. We demonstrate that this successfully solves the instability problem, remaining perfectly stable up to K=30 while retaining its SOTA performance across all graph types. This work identifies a critical architectural pitfall in hybrid GNN design and provides the robust architectural solution.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16087v1" target="_blank"><h2>AssayMatch: Learning to Select Data for Molecular Activity Models <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Vincent Fan, Regina Barzilay<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> The performance of machine learning models in drug discovery is highly dependent on the quality and consistency of the underlying training data. Due to limitations in dataset sizes, many models are trained by aggregating bioactivity data from diverse sources, including public databases such as ChEMBL. However, this approach often introduces significant noise due to variability in experimental protocols. We introduce AssayMatch, a framework for data selection that builds smaller, more homogenous training sets attuned to the test set of interest. AssayMatch leverages data attribution methods to quantify the contribution of each training assay to model performance. These attribution scores are used to finetune language embeddings of text-based assay descriptions to capture not just semantic similarity, but also the compatibility between assays. Unlike existing data attribution methods, our approach enables data selection for a test set with unknown labels, mirroring real-world drug discovery campaigns where the activities of candidate molecules are not known in advance. At test time, embeddings finetuned with AssayMatch are used to rank all available training data. We demonstrate that models trained on data selected by AssayMatch are able to surpass the performance of the model trained on the complete dataset, highlighting its ability to effectively filter out harmful or noisy experiments. We perform experiments on two common machine learning architectures and see increased prediction capability over a strong language-only baseline for 9/12 model-target pairs. AssayMatch provides a data-driven mechanism to curate higher-quality datasets, reducing noise from incompatible experiments and improving the predictive power and data efficiency of models for drug discovery. AssayMatch is available at https://github.com/Ozymandias314/AssayMatch.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16062v1" target="_blank"><h2>Gauge-Equivariant Graph Networks via Self-Interference Cancellation <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Yoonhyuk Choi, Chong-Kwon Kim<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Graph Neural Networks (GNNs) excel on homophilous graphs but often fail under heterophily due to self-reinforcing and phase-inconsistent signals. We propose a Gauge-Equivariant Graph Network with Self-Interference Cancellation (GESC), which replaces additive aggregation with a projection-based interference mechanism. Unlike prior magnetic or gauge-equivariant GNNs that typically focus on phase handling in spectral filtering while largely relying on scalar weighting, GESC introduces a $\mathrm{U}(1)$ phase connection followed by a rank-1 projection that attenuates self-parallel components before attention. A sign- and phase-aware gate further regulates neighbor influence, attenuating components aligned with current node states and acting as a local notch on low-frequency modes. Across diverse graph benchmarks, our method consistently outperforms recent state-of-the-art models while offering a unified, interference-aware view of message passing. Our code is available at \href{here}{https://anonymous.4open.science/r/GESC-1B22}.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16060v1" target="_blank"><h2>Neuromorphic Astronomy: An End-to-End SNN Pipeline for RFI Detection Hardware <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Nicholas J. Pritchard, Andreas Wicenec, Richard Dodson, Mohammed Bennamoun, Dylan R. Muir<br><strong><u>Categories:</u></strong> cs.NE, astro-ph.IM<br><strong><u>Comments:</u></strong> 21 pages, 4 figures, 12 tables<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Imminent radio telescope observatories provide massive data rates making deep learning based processing appealing while simultaneously demanding real-time performance at low-energy; prohibiting the use of many artificial neural network based approaches. We begin tackling the scientifically existential challenge of Radio Frequency Interference (RFI) detection by deploying deep Spiking Neural Networks (SNNs) on resource-constrained neuromorphic hardware. Our approach partitions large, pre-trained networks onto SynSense Xylo hardware using maximal splitting, a novel greedy algorithm. We validate this pipeline with on-chip power measurements, achieving instrument-scaled inference at 100mW. While our full-scale SNN achieves state-of-the-art accuracy among SNN baselines, our experiments reveal a more important insight that a smaller un-partitioned model significantly outperforms larger, split models. This finding highlights that hardware co-design is paramount for optimal performance. Our work thus provides a practical deployment blueprint, a key insight into the challenges of model scaling, and reinforces radio astronomy as a demanding yet ideal domain for advancing applied neuromorphic computing.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16048v1" target="_blank"><h2>Semantic Glitch: Agency and Artistry in an Autonomous Pixel Cloud <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Qing Zhang, Jing Huang, Mingyang Xu, Jun Rekimoto<br><strong><u>Categories:</u></strong> cs.RO, cs.AI, cs.HC<br><strong><u>Comments:</u></strong> NeurIPS 2025 Creative AI Track, The Thirty-Ninth Annual Conference on Neural Information Processing Systems<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> While mainstream robotics pursues metric precision and flawless performance, this paper explores the creative potential of a deliberately "lo-fi" approach. We present the "Semantic Glitch," a soft flying robotic art installation whose physical form, a 3D pixel style cloud, is a "physical glitch" derived from digital archaeology. We detail a novel autonomous pipeline that rejects conventional sensors like LiDAR and SLAM, relying solely on the qualitative, semantic understanding of a Multimodal Large Language Model to navigate. By authoring a bio-inspired personality for the robot through a natural language prompt, we create a "narrative mind" that complements the "weak," historically, loaded body. Our analysis begins with a 13-minute autonomous flight log, and a follow-up study statistically validates the framework's robustness for authoring quantifiably distinct personas. The combined analysis reveals emergent behaviors, from landmark-based navigation to a compelling "plan to execution" gap, and a character whose unpredictable, plausible behavior stems from a lack of precise proprioception. This demonstrates a lo-fi framework for creating imperfect companions whose success is measured in character over efficiency.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16027v1" target="_blank"><h2>HGCN2SP: Hierarchical Graph Convolutional Network for Two-Stage Stochastic Programming <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Yang Wu, Yifan Zhang, Zhenxing Liang, Jian Cheng<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 17 pages, 4 figures<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> convolutional (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Two-stage Stochastic Programming (2SP) is a standard framework for modeling decision-making problems under uncertainty. While numerous methods exist, solving such problems with many scenarios remains challenging. Selecting representative scenarios is a practical method for accelerating solutions. However, current approaches typically rely on clustering or Monte Carlo sampling, failing to integrate scenario information deeply and overlooking the significant impact of the scenario order on solving time. To address these issues, we develop HGCN2SP, a novel model with a hierarchical graph designed for 2SP problems, encoding each scenario and modeling their relationships hierarchically. The model is trained in a reinforcement learning paradigm to utilize the feedback of the solver. The policy network is equipped with a hierarchical graph convolutional network for feature encoding and an attention-based decoder for scenario selection in proper order. Evaluation of two classic 2SP problems demonstrates that HGCN2SP provides high-quality decisions in a short computational time. Furthermore, HGCN2SP exhibits remarkable generalization capabilities in handling large-scale instances, even with a substantial number of variables or scenarios that were unseen during the training phase.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16026v1" target="_blank"><h2>Towards a Safer and Sustainable Manufacturing Process: Material classification in Laser Cutting Using Deep Learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Mohamed Abdallah Salem, Hamdy Ahmed Ashur, Ahmed Elshinnawy<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.LG, cs.RO<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Laser cutting is a widely adopted technology in material processing across various industries, but it generates a significant amount of dust, smoke, and aerosols during operation, posing a risk to both the environment and workers' health. Speckle sensing has emerged as a promising method to monitor the cutting process and identify material types in real-time. This paper proposes a material classification technique using a speckle pattern of the material's surface based on deep learning to monitor and control the laser cutting process. The proposed method involves training a convolutional neural network (CNN) on a dataset of laser speckle patterns to recognize distinct material types for safe and efficient cutting. Previous methods for material classification using speckle sensing may face issues when the color of the laser used to produce the speckle pattern is changed. Experiments conducted in this study demonstrate that the proposed method achieves high accuracy in material classification, even when the laser color is changed. The model achieved an accuracy of 98.30 % on the training set and 96.88% on the validation set. Furthermore, the model was evaluated on a set of 3000 new images for 30 different materials, achieving an F1-score of 0.9643. The proposed method provides a robust and accurate solution for material-aware laser cutting using speckle sensing.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16020v1" target="_blank"><h2>Physically Realistic Sequence-Level Adversarial Clothing for Robust Human-Detection Evasion <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Dingkun Zhou, Patrick P. K. Chan, Hengxu Wu, Shikang Zheng, Ruiqi Huang, Yuanjie Zhao<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Deep neural networks used for human detection are highly vulnerable to adversarial manipulation, creating safety and privacy risks in real surveillance environments. Wearable attacks offer a realistic threat model, yet existing approaches usually optimize textures frame by frame and therefore fail to maintain concealment across long video sequences with motion, pose changes, and garment deformation. In this work, a sequence-level optimization framework is introduced to generate natural, printable adversarial textures for shirts, trousers, and hats that remain effective throughout entire walking videos in both digital and physical settings. Product images are first mapped to UV space and converted into a compact palette and control-point parameterization, with ICC locking to keep all colors printable. A physically based human-garment pipeline is then employed to simulate motion, multi-angle camera viewpoints, cloth dynamics, and illumination variation. An expectation-over-transformation objective with temporal weighting is used to optimize the control points so that detection confidence is minimized across whole sequences. Extensive experiments demonstrate strong and stable concealment, high robustness to viewpoint changes, and superior cross-model transferability. Physical garments produced with sublimation printing achieve reliable suppression under indoor and outdoor recordings, confirming real-world feasibility.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16014v1" target="_blank"><h2>MUSEKG: A Knowledge Graph Over Museum Collections <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jinhao Li, Jianzhong Qi, Soyeon Caren Han, Eun-Jung Holden<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Digital transformation in the cultural heritage sector has produced vast yet fragmented collections of artefact data. Existing frameworks for museum information systems struggle to integrate heterogeneous metadata, unstructured documents, and multimodal artefacts into a coherent and queryable form. We present MuseKG, an end-to-end knowledge-graph framework that unifies structured and unstructured museum data through symbolic-neural integration. MuseKG constructs a typed property graph linking objects, people, organisations, and visual or textual labels, and supports natural language queries. Evaluations on real museum collections demonstrate robust performance across queries over attributes, relations, and related entities, surpassing large-language-model zero-shot, few-shot and SPARQL prompt baselines. The results highlight the importance of symbolic grounding for interpretable and scalable cultural heritage reasoning, and pave the way for web-scale integration of digital heritage knowledge.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16013v1" target="_blank"><h2>Physics-Guided Inductive Spatiotemporal Kriging for PM2.5 with Satellite Gradient Constraints <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Shuo Wang, Mengfan Teng, Yun Cheng, Lothar Thiele, Olga Saukh, Shuangshuang He, Yuanting Zhang, Jiang Zhang, Gangfeng Zhang, Xingyuan Yuan, Jingfang Fan<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> High-resolution mapping of fine particulate matter (PM2.5) is a cornerstone of sustainable urbanism but remains critically hindered by the spatial sparsity of ground monitoring networks. While traditional data-driven methods attempt to bridge this gap using satellite Aerosol Optical Depth (AOD), they often suffer from severe, non-random data missingness (e.g., due to cloud cover or nighttime) and inversion biases. To overcome these limitations, this study proposes the Spatiotemporal Physics-Guided Inference Network (SPIN), a novel framework designed for inductive spatiotemporal kriging. Unlike conventional approaches, SPIN synergistically integrates domain knowledge into deep learning by explicitly modeling physical advection and diffusion processes via parallel graph kernels. Crucially, we introduce a paradigm-shifting training strategy: rather than using error-prone AOD as a direct input, we repurpose it as a spatial gradient constraint within the loss function. This allows the model to learn structural pollution patterns from satellite data while remaining robust to data voids. Validated in the highly polluted Beijing-Tianjin-Hebei and Surrounding Areas (BTHSA), SPIN achieves a new state-of-the-art with a Mean Absolute Error (MAE) of 9.52 ug/m^3, effectively generating continuous, physically plausible pollution fields even in unmonitored areas. This work provides a robust, low-cost, and all-weather solution for fine-grained environmental management.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16006v1" target="_blank"><h2>Synergizing Deconfounding and Temporal Generalization For Time-series Counterfactual Outcome Estimation <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Yiling Liu, Juncheng Dong, Chen Fu, Wei Shi, Ziyang Jiang, Zhigang Hua, David Carlson<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> latent space (abstract)<br><p><strong><u>Abstract:</u></strong> Estimating counterfactual outcomes from time-series observations is crucial for effective decision-making, e.g. when to administer a life-saving treatment, yet remains significantly challenging because (i) the counterfactual trajectory is never observed and (ii) confounders evolve with time and distort estimation at every step. To address these challenges, we propose a novel framework that synergistically integrates two complementary approaches: Sub-treatment Group Alignment (SGA) and Random Temporal Masking (RTM). Instead of the coarse practice of aligning marginal distributions of the treatments in latent space, SGA uses iterative treatment-agnostic clustering to identify fine-grained sub-treatment groups. Aligning these fine-grained groups achieves improved distributional matching, thus leading to more effective deconfounding. We theoretically demonstrate that SGA optimizes a tighter upper bound on counterfactual risk and empirically verify its deconfounding efficacy. RTM promotes temporal generalization by randomly replacing input covariates with Gaussian noises during training. This encourages the model to rely less on potentially noisy or spuriously correlated covariates at the current step and more on stable historical patterns, thereby improving its ability to generalize across time and better preserve underlying causal relationships. Our experiments demonstrate that while applying SGA and RTM individually improves counterfactual outcome estimation, their synergistic combination consistently achieves state-of-the-art performance. This success comes from their distinct yet complementary roles: RTM enhances temporal generalization and robustness across time steps, while SGA improves deconfounding at each specific time point.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15997v1" target="_blank"><h2>Sensorium Arc: AI Agent System for Oceanic Data Exploration and Interactive Eco-Art <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Noah Bissell, Ethan Paley, Joshua Harrison, Juliano Calil, Myungin Lee<br><strong><u>Categories:</u></strong> cs.AI, cs.MM<br><strong><u>Comments:</u></strong> (to appear) NeurIPS 2025 Creative AI Track<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Sensorium Arc (AI reflects on climate) is a real-time multimodal interactive AI agent system that personifies the ocean as a poetic speaker and guides users through immersive explorations of complex marine data. Built on a modular multi-agent system and retrieval-augmented large language model (LLM) framework, Sensorium enables natural spoken conversations with AI agents that embodies the ocean's perspective, generating responses that blend scientific insight with ecological poetics. Through keyword detection and semantic parsing, the system dynamically triggers data visualizations and audiovisual playback based on time, location, and thematic cues drawn from the dialogue. Developed in collaboration with the Center for the Study of the Force Majeure and inspired by the eco-aesthetic philosophy of Newton Harrison, Sensorium Arc reimagines ocean data not as an abstract dataset but as a living narrative. The project demonstrates the potential of conversational AI agents to mediate affective, intuitive access to high-dimensional environmental data and proposes a new paradigm for human-machine-ecosystem.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15986v1" target="_blank"><h2>Fairness in Multi-modal Medical Diagnosis with Demonstration Selection <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Dawei Li, Zijian Gu, Peng Wang, Chuhan Song, Zhen Tan, Mohan Zhang, Tianlong Chen, Yu Tian, Song Wang<br><strong><u>Categories:</u></strong> cs.CV, cs.CY, cs.LG<br><strong><u>Comments:</u></strong> 10 pages (including 2 pages of references), 4 figures. This work explores fairness in multi-modal medical image reasoning using in-context learning<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract), multi-modal (title)<br><p><strong><u>Abstract:</u></strong> Multimodal large language models (MLLMs) have shown strong potential for medical image reasoning, yet fairness across demographic groups remains a major concern. Existing debiasing methods often rely on large labeled datasets or fine-tuning, which are impractical for foundation-scale models. We explore In-Context Learning (ICL) as a lightweight, tuning-free alternative for improving fairness. Through systematic analysis, we find that conventional demonstration selection (DS) strategies fail to ensure fairness due to demographic imbalance in selected exemplars. To address this, we propose Fairness-Aware Demonstration Selection (FADS), which builds demographically balanced and semantically relevant demonstrations via clustering-based sampling. Experiments on multiple medical imaging benchmarks show that FADS consistently reduces gender-, race-, and ethnicity-related disparities while maintaining strong accuracy, offering an efficient and scalable path toward fair medical image reasoning. These results highlight the potential of fairness-aware in-context learning as a scalable and data-efficient solution for equitable medical image reasoning.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15975v1" target="_blank"><h2>SN 2019vxm: A Shocking Coincidence between Fermi and TESS <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zachary G. Lane, Ryan Ridden-Harper, Sofia Rest, Armin Rest, Conor L. Ransome, Qinan Wang, Clarinda Montilla, Micaela Steed, Igor Andreoni, Patrick Armstrong, Peter J. Brown, Jeffrey Cooke, David A. Coulter, Ori Fox, James Freeburn, Marco Galoppo, Avishay Gal-Yam, Jared A. Goldberg, Christopher Harvey-Hawes, Rebekah Hounsell, Brayden Leicester, Itai Linial, Thomas Moore, Pierre Mourier, Anya E. Nugent, David O'Neill, Hugh Roxburgh, Koji Shukawa, Stephen J. Smartt, Nathan Smith, Ken W. Smith, Sebastian Vergara Carrasco, V. Ashley Villar, Tal Wasserman, Zenati Yossef, Erez Zimmerman<br><strong><u>Categories:</u></strong> astro-ph.HE<br><strong><u>Comments:</u></strong> 25 pages, 9 figures. Submitted to The Astrophysical Journal<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> Shock breakout and, in some cases, jet-driven high-energy emission are increasingly recognized as key signatures of the earliest phases of core-collapse supernovae, especially in Type IIn systems due to their dense, interaction-dominated circumstellar environments. We present a comprehensive photometric analysis of SN 2019vxm, a long-duration, luminous Type IIn supernova, $M_V^{}=-21.41\pm0.05\;{\rm mag}$, observed from X-ray to near-infrared. SN 2019vxm is the first superluminous supernovae Type IIn to be caught with well-sampled TESS photometric data on the rise and has a convincing coincident X-ray source at the time of first light. The high-cadence TESS light curve captures the early-time rise, which is well described by a broken power law with an index of $n=1.41\pm0.04$, significantly shallower than the canonical $n=2$ behavior. From this, we constrain the time of first light to within 7.2 hours. We identify a spatial and temporal coincidence between SN 2019vxm and the X-ray transient GRB191117A, corresponding to a $3.3σ$ association confidence. Both the short-duration X-ray event and the lightcurve modeling are consistent with shock breakout into a dense, asymmetric circumstellar medium, indicative of a massive, compact progenitor such as a luminous blue variable transitioning to Wolf-Rayet phase embedded in a clumpy, asymmetric environment.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15974v2" target="_blank"><h2>KRAL: Knowledge and Reasoning Augmented Learning for LLM-assisted Clinical Antimicrobial Therapy <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zhe Li, Yehan Qiu, Yujie Chen, Xiang Zhou<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> data augmentation (abstract)<br><p><strong><u>Abstract:</u></strong> Clinical antimicrobial therapy requires the dynamic integration of pathogen profiles,host factors, pharmacological properties of antimicrobials,and the severity of infection. This complexity imposes fundamental limitations on the applicability of Large Language Models (LLMs) in high-stakes clinical decision-making including knowledge gaps, data privacy concerns, high deployment costs, and limited reasoning capabilities. To address these challenges, we propose KRAL (Knowledge and Reasoning Augmented Learning), a low-cost, scalable, privacy-preserving paradigm that leverages teacher-model reasoning to automatically distill knowledge and reasoning trajectories via answer-to-question reverse generation, employs heuristic learning for semi-supervised data augmentation (reducing manual annotation requirements by approximately 80%), and utilizes agentic reinforcement learning to jointly enhance medical knowledge and reasoning while optimizing computational and memory efficiency. A hierarchical evaluation employing diverse teacher-model proxies reduces assessment costs, while modular interface design facilitates seamless system updates. Experimental results demonstrate that KRAL significantly outperforms traditional Retrieval-Augmented Generation (RAG) and Supervised Fine-Tuning (SFT) methods. It improves knowledge question-answering capability (Accuracy@1 on the external open-source benchmark MEDQA increased by 1.8% vs. SFT and 3.6% vs. RAG) and reasoning capability (Pass@1 on the external benchmark PUMCH Antimicrobial increased by 27% vs. SFT and 27.2% vs. RAG), achieved at about 20% of SFT's long-term training costs. This establishes KRAL as an effective solution for enhancing local LLMs' clinical diagnostic capabilities, enabling low-cost, high-safety deployment in complex medical decision support.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15965v1" target="_blank"><h2>Self-supervised and Multi-fidelity Learning for Extended Predictive Soil Spectroscopy <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Luning Sun, José L. Safanelli, Jonathan Sanderman, Katerina Georgiou, Colby Brungard, Kanchan Grover, Bryan G. Hopkins, Shusen Liu, Timo Bremer<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 49 pages, 9 figures, submitted to Geoderma<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> variational autoencoder (abstract), latent space (abstract)<br><p><strong><u>Abstract:</u></strong> We propose a self-supervised machine learning (SSML) framework for multi-fidelity learning and extended predictive soil spectroscopy based on latent space embeddings. A self-supervised representation was pretrained with the large MIR spectral library and the Variational Autoencoder algorithm to obtain a compressed latent space for generating spectral embeddings. At this stage, only unlabeled spectral data were used, allowing us to leverage the full spectral database and the availability of scan repeats for augmented training. We also leveraged and froze the trained MIR decoder for a spectrum conversion task by plugging it into a NIR encoder to learn the mapping between NIR and MIR spectra in an attempt to leverage the predictive capabilities contained in the large MIR library with a low cost portable NIR scanner. This was achieved by using a smaller subset of the KSSL library with paired NIR and MIR spectra. Downstream machine learning models were then trained to map between original spectra, predicted spectra, and latent space embeddings for nine soil properties. The performance of was evaluated independently of the KSSL training data using a gold-standard test set, along with regression goodness-of-fit metrics. Compared to baseline models, the proposed SSML and its embeddings yielded similar or better accuracy in all soil properties prediction tasks. Predictions derived from the spectrum conversion (NIR to MIR) task did not match the performance of the original MIR spectra but were similar or superior to predictive performance of NIR-only models, suggesting the unified spectral latent space can effectively leverage the larger and more diverse MIR dataset for prediction of soil properties not well represented in current NIR libraries.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15960v1" target="_blank"><h2>Machine Learning vs. Randomness: Challenges in Predicting Binary Options Movements <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Gabriel M. Arantes, Richard F. Pinto, Bruno L. Dalmazo, Eduardo N. Borges, Giancarlo Lucca, Viviane L. D. de Mattos, Fabian C. Cardoso, Rafael A. Berri<br><strong><u>Categories:</u></strong> q-fin.CP, cs.LG<br><strong><u>Comments:</u></strong> Accepted for publication at the 26th International Conference on Intelligent Data Engineering and Automated Learning (IDEAL 2025)<br><strong><u>Published:</u></strong> 2025-11-20<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Binary options trading is often marketed as a field where predictive models can generate consistent profits. However, the inherent randomness and stochastic nature of binary options make price movements highly unpredictable, posing significant challenges for any forecasting approach. This study demonstrates that machine learning algorithms struggle to outperform a simple baseline in predicting binary options movements. Using a dataset of EUR/USD currency pairs from 2021 to 2023, we tested multiple models, including Random Forest, Logistic Regression, Gradient Boosting, and k-Nearest Neighbors (kNN), both before and after hyperparameter optimization. Furthermore, several neural network architectures, including Multi-Layer Perceptrons (MLP) and a Long Short-Term Memory (LSTM) network, were evaluated under different training conditions. Despite these exhaustive efforts, none of the models surpassed the ZeroR baseline accuracy, highlighting the inherent randomness of binary options. These findings reinforce the notion that binary options lack predictable patterns, making them unsuitable for machine learning-based forecasting.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15927v1" target="_blank"><h2>Breaking the Bottleneck with DiffuApriel: High-Throughput Diffusion LMs with Mamba Backbone <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Vaibhav Singh, Oleksiy Ostapenko, Pierre-André Noël, Torsten Scholak<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 9 pages, 4 figures<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> time sequence (abstract), transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Diffusion-based language models have recently emerged as a promising alternative to autoregressive generation, yet their reliance on Transformer backbones limits inference efficiency due to quadratic attention and KV-cache overhead. In this work, we introduce DiffuApriel, a masked diffusion language model built on a bidirectional Mamba backbone that combines the diffusion objective with linear-time sequence modeling. DiffuApriel matches the performance of Transformer-based diffusion models while achieving up to 4.4x higher inference throughput for long sequences with a 1.3B model. We further propose DiffuApriel-H, a hybrid variant that interleaves attention and mamba layers, offering up to 2.6x throughput improvement with balanced global and local context modeling. Our results demonstrate that bidirectional state-space architectures serve as strong denoisers in masked diffusion LMs, providing a practical and scalable foundation for faster, memory-efficient text generation.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15902v1" target="_blank"><h2>EEG Emotion Recognition Through Deep Learning <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Roman Dolgopolyi, Antonis Chatzipanagiotou<br><strong><u>Categories:</u></strong> eess.SP, cs.LG<br><strong><u>Comments:</u></strong> This version corresponds to the original manuscript submitted to the 22nd EMCIS conference prior to peer review. The peer-reviewed and accepted version will appear in the Springer conference proceedings<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> An advanced emotion classification model was developed using a CNN-Transformer architecture for emotion recognition from EEG brain wave signals, effectively distinguishing among three emotional states, positive, neutral and negative. The model achieved a testing accuracy of 91%, outperforming traditional models such as SVM, DNN, and Logistic Regression. Training was conducted on a custom dataset created by merging data from SEED, SEED-FRA, and SEED-GER repositories, comprising 1,455 samples with EEG recordings labeled according to emotional states. The combined dataset represents one of the largest and most culturally diverse collections available. Additionally, the model allows for the reduction of the requirements of the EEG apparatus, by leveraging only 5 electrodes of the 62. This reduction demonstrates the feasibility of deploying a more affordable consumer-grade EEG headset, thereby enabling accessible, at-home use, while also requiring less computational power. This advancement sets the groundwork for future exploration into mood changes induced by media content consumption, an area that remains underresearched. Integration into medical, wellness, and home-health platforms could enable continuous, passive emotional monitoring, particularly beneficial in clinical or caregiving settings where traditional behavioral cues, such as facial expressions or vocal tone, are diminished, restricted, or difficult to interpret, thus potentially transforming mental health diagnostics and interventions...</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15870v1" target="_blank"><h2>AquaSentinel: Next-Generation AI System Integrating Sensor Networks for Urban Underground Water Pipeline Anomaly Detection via Collaborative MoE-LLM Agent Architecture <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Qiming Guo, Bishal Khatri, Wenbo Sun, Jinwen Tang, Hua Zhang, Wenlu Wang<br><strong><u>Categories:</u></strong> cs.CE, cs.AI<br><strong><u>Comments:</u></strong> 7 pages, 1 figure, 2 tables, Accepted to the 40th AAAI Conference on Artificial Intelligence (AAAI 2026), IAAI Deployed Applications Track<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> anomaly detection (title, abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Underground pipeline leaks and infiltrations pose significant threats to water security and environmental safety. Traditional manual inspection methods provide limited coverage and delayed response, often missing critical anomalies. This paper proposes AquaSentinel, a novel physics-informed AI system for real-time anomaly detection in urban underground water pipeline networks. We introduce four key innovations: (1) strategic sparse sensor deployment at high-centrality nodes combined with physics-based state augmentation to achieve network-wide observability from minimal infrastructure; (2) the RTCA (Real-Time Cumulative Anomaly) detection algorithm, which employs dual-threshold monitoring with adaptive statistics to distinguish transient fluctuations from genuine anomalies; (3) a Mixture of Experts (MoE) ensemble of spatiotemporal graph neural networks that provides robust predictions by dynamically weighting model contributions; (4) causal flow-based leak localization that traces anomalies upstream to identify source nodes and affected pipe segments. Our system strategically deploys sensors at critical network junctions and leverages physics-based modeling to propagate measurements to unmonitored nodes, creating virtual sensors that enhance data availability across the entire network. Experimental evaluation using 110 leak scenarios demonstrates that AquaSentinel achieves 100% detection accuracy. This work advances pipeline monitoring by demonstrating that physics-informed sparse sensing can match the performance of dense deployments at a fraction of the cost, providing a practical solution for aging urban infrastructure.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15853v1" target="_blank"><h2>The Ensemble Kalman Inversion Race <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Rebecca Gjini, Matthias Morzfeld, Oliver R. A. Dunbar, Tapio Schneider<br><strong><u>Categories:</u></strong> physics.data-an, stat.ML<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Ensemble Kalman methods were initially developed to solve nonlinear data assimilation problems in oceanography, but are now popular in applications far beyond their original use cases. Of particular interest is climate model calibration. As hybrid physics and machine-learning models evolve, the number of parameters and complexity of parameterizations in climate models will continue to grow. Thus, robust calibration of these parameters plays an increasingly important role. We focus on learning climate model parameters from minimizing the misfit between modeled and observed climate statistics in an idealized setting. Ensemble Kalman methods are a natural choice for this problem because they are derivative-free, scalable to high dimensions, and robust to noise caused by statistical observations. Given the many variants of ensemble methods proposed, an important question is: Which ensemble Kalman method should be used for climate model calibration? To answer this question, we perform systematic numerical experiments to explore the relative computational efficiencies of several ensemble Kalman methods. The numerical experiments involve statistical observations of Lorenz-type models of increasing complexity, frequently used to represent simplified atmospheric systems, and some feature neural network parameterizations. For each test problem, several ensemble Kalman methods and a derivative-based method "race" to reach a specified accuracy, and we measure the computational cost required to achieve the desired accuracy. We investigate how prior information and the parameter or data dimensions play a role in choosing the ensemble method variant. The derivative-based method consistently fails to complete the race because it does not adaptively handle the noisy loss landscape.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15848v1" target="_blank"><h2>Step-Audio-R1 Technical Report <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Fei Tian, Xiangyu Tony Zhang, Yuxin Zhang, Haoyang Zhang, Yuxin Li, Daijiao Liu, Yayue Deng, Donghang Wu, Jun Chen, Liang Zhao, Chengyuan Yao, Hexin Liu, Eng Siong Chng, Xuerui Yang, Xiangyu Zhang, Daxin Jiang, Gang Yu<br><strong><u>Categories:</u></strong> cs.AI, cs.CL, cs.SD<br><strong><u>Comments:</u></strong> 15 pages, 5 figures. Technical Report<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Recent advances in reasoning models have demonstrated remarkable success in text and vision domains through extended chain-of-thought deliberation. However, a perplexing phenomenon persists in audio language models: they consistently perform better with minimal or no reasoning, raising a fundamental question - can audio intelligence truly benefit from deliberate thinking? We introduce Step-Audio-R1, the first audio reasoning model that successfully unlocks reasoning capabilities in the audio domain. Through our proposed Modality-Grounded Reasoning Distillation (MGRD) framework, Step-Audio-R1 learns to generate audio-relevant reasoning chains that genuinely ground themselves in acoustic features rather than hallucinating disconnected deliberations. Our model exhibits strong audio reasoning capabilities, surpassing Gemini 2.5 Pro and achieving performance comparable to the state-of-the-art Gemini 3 Pro across comprehensive audio understanding and reasoning benchmarks spanning speech, environmental sounds, and music. These results demonstrate that reasoning is a transferable capability across modalities when appropriately anchored, transforming extended deliberation from a liability into a powerful asset for audio intelligence. By establishing the first successful audio reasoning model, Step-Audio-R1 opens new pathways toward building truly multimodal reasoning systems that think deeply across all sensory modalities.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15847v1" target="_blank"><h2>Transparent Early ICU Mortality Prediction with Clinical Transformer and Per-Case Modality Attribution <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Alexander Bakumenko, Janine Hoelscher, Hudson Smith<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract), transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> Early identification of intensive care patients at risk of in-hospital mortality enables timely intervention and efficient resource allocation. Despite high predictive performance, existing machine learning approaches lack transparency and robustness, limiting clinical adoption. We present a lightweight, transparent multimodal ensemble that fuses physiological time-series measurements with unstructured clinical notes from the first 48 hours of an ICU stay. A logistic regression model combines predictions from two modality-specific models: a bidirectional LSTM for vitals and a finetuned ClinicalModernBERT transformer for notes. This traceable architecture allows for multilevel interpretability: feature attributions within each modality and direct per-case modality attributions quantifying how vitals and notes influence each decision. On the MIMIC-III benchmark, our late-fusion ensemble improves discrimination over the best single model (AUPRC 0.565 vs. 0.526; AUROC 0.891 vs. 0.876) while maintaining well-calibrated predictions. The system remains robust through a calibrated fallback when a modality is missing. These results demonstrate competitive performance with reliable, auditable risk estimates and transparent, predictable operation, which together are crucial for clinical use.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15846v2" target="_blank"><h2>The Loss of Control Playbook: Degrees, Dynamics, and Preparedness <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Charlotte Stix, Annika Hallensleben, Alejandro Ortega, Matteo Pistillo<br><strong><u>Categories:</u></strong> cs.CY, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> attention (abstract), literature review (abstract)<br><p><strong><u>Abstract:</u></strong> This research report addresses the absence of an actionable definition for Loss of Control (LoC) in AI systems by developing a novel taxonomy and preparedness framework. Despite increasing policy and research attention, existing LoC definitions vary significantly in scope and timeline, hindering effective LoC assessment and mitigation. To address this issue, we draw from an extensive literature review and propose a graded LoC taxonomy, based on the metrics of severity and persistence, that distinguishes between Deviation, Bounded LoC, and Strict LoC. We model pathways toward a societal state of vulnerability in which sufficiently advanced AI systems have acquired or could acquire the means to cause Bounded or Strict LoC once a catalyst, either misalignment or pure malfunction, materializes. We argue that this state becomes increasingly likely over time, absent strategic intervention, and propose a strategy to avoid reaching a state of vulnerability. Rather than focusing solely on intervening on AI capabilities and propensities potentially relevant for LoC or on preventing potential catalysts, we introduce a complementary framework that emphasizes three extrinsic factors: Deployment context, Affordances, and Permissions (the DAP framework). Compared to work on intrinsic factors and catalysts, this framework has the unfair advantage of being actionable today. Finally, we put forward a plan to maintain preparedness and prevent the occurrence of LoC outcomes should a state of societal vulnerability be reached, focusing on governance measures (threat modeling, deployment policies, emergency response) and technical controls (pre-deployment testing, control measures, monitoring) that could maintain a condition of perennial suspension.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15838v1" target="_blank"><h2>Attention-Based Feature Online Conformal Prediction for Time Series <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Meiyi Zhu, Caili Guo, Chunyan Feng, Osvaldo Simeone<br><strong><u>Categories:</u></strong> cs.LG, cs.IT, eess.SP<br><strong><u>Comments:</u></strong> 25 pages, 24 figures<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Online conformal prediction (OCP) wraps around any pre-trained predictor to produce prediction sets with coverage guarantees that hold irrespective of temporal dependencies or distribution shifts. However, standard OCP faces two key limitations: it operates in the output space using simple nonconformity (NC) scores, and it treats all historical observations uniformly when estimating quantiles. This paper introduces attention-based feature OCP (AFOCP), which addresses both limitations through two key innovations. First, AFOCP operates in the feature space of pre-trained neural networks, leveraging learned representations to construct more compact prediction sets by concentrating on task-relevant information while suppressing nuisance variation. Second, AFOCP incorporates an attention mechanism that adaptively weights historical observations based on their relevance to the current test point, effectively handling non-stationarity and distribution shifts. We provide theoretical guarantees showing that AFOCP maintains long-term coverage while provably achieving smaller prediction intervals than standard OCP under mild regularity conditions. Extensive experiments on synthetic and real-world time series datasets demonstrate that AFOCP consistently reduces the size of prediction intervals by as much as $88\%$ as compared to OCP, while maintaining target coverage levels, validating the benefits of both feature-space calibration and attention-based adaptive weighting.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15828v1" target="_blank"><h2>Observational constraints on the product of dark energy chemical potential and number density in out-of-equilibrium models <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> J. M. Costa Netto, Javier E. Gonzalez, H. H. B. Silva<br><strong><u>Categories:</u></strong> astro-ph.CO<br><strong><u>Comments:</u></strong> 11 pages, 6 figures, 3 tables<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> In this work, we impose observational limits on the product of dark energy chemical potential, $μ$, and number density, $n$, at the present time in out-of-equilibrium models, considering that particles can be created or destroyed in the fluid at a rate $Γ=3αH(a)$, where $α$ is a constant and $H(a)\equiv\dot{a}/a$ is the Hubble parameter. We combine the bounds derived from the positivity of entropy and the second law of thermodynamics with observational constraints on the Chevallier-Polarski-Linder (CPL) and Barboza-Alcaniz (BA) parameterizations of the equation of state (EoS) of the component. We use Type Ia supernovae (SN Ia) data from Pantheon+; baryon acoustic oscillation (BAO) data from DESI DR2; and cosmic microwave background (CMB) measurements from Planck. For $α>0$ (particle creation), the thermodynamic restrictions yield only upper limits for the $μ_{0}n_{0}$ product, while in the case of $α<0$ (particle destruction) they establish both upper and lower limits, allowing for a range of values to be obtained. In both scenarios, however, we find that the chemical potential of dark energy must be negative, $μ<0$, which indicates a preference for the phantom regime. In particular, when $α<0$, it is noted that the thermodynamic bounds are simultaneously compatible only for very small absolute values of $α$, with $α=-0.0002$ being the limiting case and resulting in $μ_{0}n_{0}(α=-0.0002)=-2.2_{-0.7}^{+1.0}\,\,GeV/m^{3}$.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15822v1" target="_blank"><h2>Atlas Gaussian processes on restricted domains and point clouds <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Mu Niu, Yue Zhang, Ke Ye, Pokman Cheung, Yizhu Wang, Xiaochen Yang<br><strong><u>Categories:</u></strong> stat.ML, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> latent space (abstract)<br><p><strong><u>Abstract:</u></strong> In real-world applications, data often reside in restricted domains with unknown boundaries, or as high-dimensional point clouds lying on a lower-dimensional, nontrivial, unknown manifold. Traditional Gaussian Processes (GPs) struggle to capture the underlying geometry in such settings. Some existing methods assume a flat space embedded in a point cloud, which can be represented by a single latent chart (latent space), while others exhibit weak performance when the point cloud is sparse or irregularly sampled. The goal of this work is to address these challenges. The main contributions are twofold: (1) We establish the Atlas Brownian Motion (BM) framework for estimating the heat kernel on point clouds with unknown geometries and nontrivial topological structures; (2) Instead of directly using the heat kernel estimates, we construct a Riemannian corrected kernel by combining the global heat kernel with local RBF kernel and leading to the formulation of Riemannian-corrected Atlas Gaussian Processes (RC-AGPs). The resulting RC-AGPs are applied to regression tasks across synthetic and real-world datasets. These examples demonstrate that our method outperforms existing approaches in both heat kernel estimation and regression accuracy. It improves statistical inference by effectively bridging the gap between complex, high-dimensional observations and manifold-based inferences.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15809v1" target="_blank"><h2>Unveiling Chemical Enrichment in the Abell 2029 Core with XRISM, XMM-Newton, and Chandra <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Arnab Sarkar, Eric D. Miller, Brian McNamara, Ming Sun, Richard Mushotzky, Stefano Ettori, Lorenzo Lovisari, Irina Zhuravleva, Naomi Ota<br><strong><u>Categories:</u></strong> astro-ph.GA, astro-ph.CO, astro-ph.HE<br><strong><u>Comments:</u></strong> 5 figures, two tables; Accepted for publication in The Astrophysical Journal Letters<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> We present new measurements of the chemical abundance pattern in the core of the nearby galaxy cluster Abell~2029, based on XRISM observations with Resolve (37 ks) and Xtend (500 ks), combined with archival data from XMM-Newton (EPIC, RGS) and Chandra. Fe abundances derived from Resolve, Xtend, and EPIC are broadly consistent, while RGS gives systematically lower values. Because the XRISM gate valve remained closed during these observations, Resolve spectral fitting is restricted to the 2--10 keV band, providing reliable constraints only for elements with strong lines in this band (S, Ar, Ca, Fe, Ni). Abundances of the $α$-elements are therefore derived using complementary observations from Xtend, EPIC, RGS, and Chandra. We construct an average X/Fe pattern in the cluster core by using Resolve exclusively for S/Fe, Ar/Fe, Ca/Fe, and Ni/Fe, and RGS + Xtend for O/Fe. The Ne/Fe ratio is averaged from Xtend, EPIC, RGS, and Chandra measurements; Mg/Fe from EPIC and Chandra measurements; and Si/Fe from Xtend, EPIC, and Chandra. Comparison with the supernovae yield models indicates that the observed abundance pattern in A2029 core is best reproduced by a combination of core-collapsed yields from low-metallicity progenitors ($Z_{\rm init}=0.001$) and a sub-Chandrasekhar-mass, double-degenerate Type Ia model. Additionally, we find an excess in Ca abundance in the core of A2029 that cannot be reproduced by the standard supernovae yield models.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15807v1" target="_blank"><h2>TopoReformer: Mitigating Adversarial Attacks Using Topological Purification in OCR Models <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Bhagyesh Kumar, A S Aravinthakashan, Akshat Satyanarayan, Ishaan Gakhar, Ujjwal Verma<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> Accepted at AAAI 2026 AI for CyberSecurity (AICS) Workshop<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> latent space (abstract)<br><p><strong><u>Abstract:</u></strong> Adversarially perturbed images of text can cause sophisticated OCR systems to produce misleading or incorrect transcriptions from seemingly invisible changes to humans. Some of these perturbations even survive physical capture, posing security risks to high-stakes applications such as document processing, license plate recognition, and automated compliance systems. Existing defenses, such as adversarial training, input preprocessing, or post-recognition correction, are often model-specific, computationally expensive, and affect performance on unperturbed inputs while remaining vulnerable to unseen or adaptive attacks. To address these challenges, TopoReformer is introduced, a model-agnostic reformation pipeline that mitigates adversarial perturbations while preserving the structural integrity of text images. Topology studies properties of shapes and spaces that remain unchanged under continuous deformations, focusing on global structures such as connectivity, holes, and loops rather than exact distance. Leveraging these topological features, TopoReformer employs a topological autoencoder to enforce manifold-level consistency in latent space and improve robustness without explicit gradient regularization. The proposed method is benchmarked on EMNIST, MNIST, against standard adversarial attacks (FGSM, PGD, Carlini-Wagner), adaptive attacks (EOT, BDPA), and an OCR-specific watermark attack (FAWA).</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15796v1" target="_blank"><h2>Teukolsky by Design: A Hybrid Spectral-PINN solver for Kerr Quasinormal Modes <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Alexandre M. Pombo, Lorenzo Pizzuti<br><strong><u>Categories:</u></strong> gr-qc, astro-ph.HE<br><strong><u>Comments:</u></strong> 23 pages, 5 figures, 3 tables<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> We introduce SpectralPINN, a hybrid pseudo-spectral/physics-informed neural network (PINN) solver for Kerr quasinormal modes that targets the Teukolsky equation in both the separated (radial/angular) and joint two-dimensional formulations. The solver replaces standard neural activation functions with Chebyshev polynomials of the first kind and supports both soft -- via loss penalties -- and hard -- enforced by analytic masks -- implementations of Leaver's normalization. Benchmarking against Leaver's continued-fraction method shows cumulative (real+imaginary part) relative frequency errors of $\sim 0.001\%$ for the separated formulation with hard normalization, $\sim 0.1\%$ for both the soft separated and soft joint formulations, and $\sim 0.01\%$ for the hard joint case. Exploiting our ability to solve the joint equation, we add a small quadrupolar perturbation to the Teukolsky operator, effectively rendering the problem non-separable. The resulting perturbed quasinormal modes are compared against the expected precision of the Einstein Telescope, allowing us to constrain the magnitude of the perturbation. These proof-of-concept results demonstrate that hybrid spectral-PINN solvers can provide a flexible pathway to quasinormal spectra in settings where separability, asymptotics, or field content become more intricate and high accuracy is required.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15704v1" target="_blank"><h2>In-N-On: Scaling Egocentric Manipulation with in-the-wild and on-task Data <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Xiongyi Cai, Ri-Zhao Qiu, Geng Chen, Lai Wei, Isabella Liu, Tianshu Huang, Xuxin Cheng, Xiaolong Wang<br><strong><u>Categories:</u></strong> cs.RO, cs.AI, cs.CV<br><strong><u>Comments:</u></strong> Project webpage:this https URL<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> domain adaptation (abstract)<br><p><strong><u>Abstract:</u></strong> Egocentric videos are a valuable and scalable data source to learn manipulation policies. However, due to significant data heterogeneity, most existing approaches utilize human data for simple pre-training, which does not unlock its full potential. This paper first provides a scalable recipe for collecting and using egocentric data by categorizing human data into two categories: in-the-wild and on-task alongside with systematic analysis on how to use the data. We first curate a dataset, PHSD, which contains over 1,000 hours of diverse in-the-wild egocentric data and over 20 hours of on-task data directly aligned to the target manipulation tasks. This enables learning a large egocentric language-conditioned flow matching policy, Human0. With domain adaptation techniques, Human0 minimizes the gap between humans and humanoids. Empirically, we show Human0 achieves several novel properties from scaling human data, including language following of instructions from only human data, few-shot learning, and improved robustness using on-task data. Project website: https://xiongyicai.github.io/In-N-On/</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15699v1" target="_blank"><h2>Joint Semantic-Channel Coding and Modulation for Token Communications <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jingkai Ying, Zhijin Qin, Yulong Feng, Liejun Wang, Xiaoming Tao<br><strong><u>Categories:</u></strong> eess.SP, cs.AI<br><strong><u>Comments:</u></strong> 14 pages, 14 figures, 2 tables<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> In recent years, the Transformer architecture has achieved outstanding performance across a wide range of tasks and modalities. Token is the unified input and output representation in Transformer-based models, which has become a fundamental information unit. In this work, we consider the problem of token communication, studying how to transmit tokens efficiently and reliably. Point cloud, a prevailing three-dimensional format which exhibits a more complex spatial structure compared to image or video, is chosen to be the information source. We utilize the set abstraction method to obtain point tokens. Subsequently, to get a more informative and transmission-friendly representation based on tokens, we propose a joint semantic-channel and modulation (JSCCM) scheme for the token encoder, mapping point tokens to standard digital constellation points (modulated tokens). Specifically, the JSCCM consists of two parallel Point Transformer-based encoders and a differential modulator which combines the Gumel-softmax and soft quantization methods. Besides, the rate allocator and channel adapter are developed, facilitating adaptive generation of high-quality modulated tokens conditioned on both semantic information and channel conditions. Extensive simulations demonstrate that the proposed method outperforms both joint semantic-channel coding and traditional separate coding, achieving over 1dB gain in reconstruction and more than 6x compression ratio in modulated symbols.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15687v1" target="_blank"><h2>Impact of cosmic expansion on gravitational wave spectra from strongly supercooled first-order phase transitions <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Marek Lewicki, Ville Vaskonen<br><strong><u>Categories:</u></strong> astro-ph.CO, hep-ph<br><strong><u>Comments:</u></strong> 6 pages, 4 figures<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> causality (abstract)<br><p><strong><u>Abstract:</u></strong> We compute the gravitational wave spectra from strongly supercooled first-order phase transitions, explicitly incorporating the evolution of the background metric across the transition from thermal inflation to radiation domination. We find that the spectral shape remains largely unchanged apart from a causality-induced super-horizon tail. However, in contrast to standard expectations, for slow transitions we show that the peak amplitude and frequency exhibit a weaker dependence on the transition rate $β$ than the usual scaling of $\propto β^{-2}$ and $\proptoβ$, respectively.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15684v1" target="_blank"><h2>Walrus: A Cross-Domain Foundation Model for Continuum Dynamics <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Michael McCabe, Payel Mukhopadhyay, Tanya Marwah, Bruno Regaldo-Saint Blancard, Francois Rozet, Cristiana Diaconu, Lucas Meyer, Kaze W. K. Wong, Hadi Sotoudeh, Alberto Bietti, Irina Espejo, Rio Fear, Siavash Golkar, Tom Hehir, Keiya Hirashima, Geraud Krawezik, Francois Lanusse, Rudy Morel, Ruben Ohana, Liam Parker, Mariel Pettee, Jeff Shen, Kyunghyun Cho, Miles Cranmer, Shirley Ho<br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.CE<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Foundation models have transformed machine learning for language and vision, but achieving comparable impact in physical simulation remains a challenge. Data heterogeneity and unstable long-term dynamics inhibit learning from sufficiently diverse dynamics, while varying resolutions and dimensionalities challenge efficient training on modern hardware. Through empirical and theoretical analysis, we incorporate new approaches to mitigate these obstacles, including a harmonic-analysis-based stabilization method, load-balanced distributed 2D and 3D training strategies, and compute-adaptive tokenization. Using these tools, we develop Walrus, a transformer-based foundation model developed primarily for fluid-like continuum dynamics. Walrus is pretrained on nineteen diverse scenarios spanning astrophysics, geoscience, rheology, plasma physics, acoustics, and classical fluids. Experiments show that Walrus outperforms prior foundation models on both short and long term prediction horizons on downstream tasks and across the breadth of pretraining data, while ablation studies confirm the value of our contributions to forecast stability, training throughput, and transfer performance over conventional approaches. Code and weights are released for community use.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15675v2" target="_blank"><h2>MF-GCN: A Multi-Frequency Graph Convolutional Network for Tri-Modal Depression Detection Using Eye-Tracking, Facial, and Acoustic Features <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Sejuti Rahman, Swakshar Deb, MD. Sameer Iqbal Chowdhury, MD. Jubair Ahmed Sourov, Mohammad Shamsuddin<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> convolutional (title, abstract), multimodal (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Depression is a prevalent global mental health disorder, characterised by persistent low mood and anhedonia. However, it remains underdiagnosed because current diagnostic methods depend heavily on subjective clinical assessments. To enable objective detection, we introduce a gold standard dataset of 103 clinically assessed participants collected through a tripartite data approach which uniquely integrated eye tracking data with audio and video to give a comprehensive representation of depressive symptoms. Eye tracking data quantifies the attentional bias towards negative stimuli that is frequently observed in depressed groups. Audio and video data capture the affective flattening and psychomotor retardation characteristic of depression. Statistical validation confirmed their significant discriminative power in distinguishing depressed from non depressed groups. We address a critical limitation of existing graph-based models that focus on low-frequency information and propose a Multi-Frequency Graph Convolutional Network (MF-GCN). This framework consists of a novel Multi-Frequency Filter Bank Module (MFFBM), which can leverage both low and high frequency signals. Extensive evaluation against traditional machine learning algorithms and deep learning frameworks demonstrates that MF-GCN consistently outperforms baselines. In binary classification, the model achieved a sensitivity of 0.96 and F2 score of 0.94. For the 3 class classification task, the proposed method achieved a sensitivity of 0.79 and specificity of 0.87 and siginificantly suprassed other models. To validate generalizability, the model was also evaluated on the Chinese Multimodal Depression Corpus (CMDC) dataset and achieved a sensitivity of 0.95 and F2 score of 0.96. These results confirm that our trimodal, multi frequency framework effectively captures cross modal interaction for accurate depression detection.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15661v2" target="_blank"><h2>VisPlay: Self-Evolving Vision-Language Models from Images <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yicheng He, Chengsong Huang, Zongxia Li, Jiaxin Huang, Yonghui Yang<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.CL, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Reinforcement learning (RL) provides a principled framework for improving Vision-Language Models (VLMs) on complex reasoning tasks. However, existing RL approaches often rely on human-annotated labels or task-specific heuristics to define verifiable rewards, both of which are costly and difficult to scale. We introduce VisPlay, a self-evolving RL framework that enables VLMs to autonomously improve their reasoning abilities using large amounts of unlabeled image data. Starting from a single base VLM, VisPlay assigns the model into two interacting roles: an Image-Conditioned Questioner that formulates challenging yet answerable visual questions, and a Multimodal Reasoner that generates silver responses. These roles are jointly trained with Group Relative Policy Optimization (GRPO), which incorporates diversity and difficulty rewards to balance the complexity of generated questions with the quality of the silver answers. VisPlay scales efficiently across two model families. When trained on Qwen2.5-VL and MiMo-VL, VisPlay achieves consistent improvements in visual reasoning, compositional generalization, and hallucination reduction across eight benchmarks, including MM-Vet and MMMU, demonstrating a scalable path toward self-evolving multimodal intelligence. The project page is available at https://bruno686.github.io/VisPlay/</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15652v1" target="_blank"><h2>Continual Reinforcement Learning for Cyber-Physical Systems: Lessons Learned and Open Challenges <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Kim N. Nolle, Ivana Dusparic, Rhodri Cusack, Vinny Cahill<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 5 pages, 5 figures, Accepted to RLDM 2025<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Continual learning (CL) is a branch of machine learning that aims to enable agents to adapt and generalise previously learned abilities so that these can be reapplied to new tasks or environments. This is particularly useful in multi-task settings or in non-stationary environments, where the dynamics can change over time. This is particularly relevant in cyber-physical systems such as autonomous driving. However, despite recent advances in CL, successfully applying it to reinforcement learning (RL) is still an open problem.
  This paper highlights open challenges in continual RL (CRL) based on experiments in an autonomous driving environment. In this environment, the agent must learn to successfully park in four different scenarios corresponding to parking spaces oriented at varying angles. The agent is successively trained in these four scenarios one after another, representing a CL environment, using Proximal Policy Optimisation (PPO). These experiments exposed a number of open challenges in CRL: finding suitable abstractions of the environment, oversensitivity to hyperparameters, catastrophic forgetting, and efficient use of neural network capacity.
  Based on these identified challenges, we present open research questions that are important to be addressed for creating robust CRL systems. In addition, the identified challenges call into question the suitability of neural networks for CL. We also identify the need for interdisciplinary research, in particular between computer science and neuroscience.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15633v1" target="_blank"><h2>Hierarchical Semantic Tree Anchoring for CLIP-Based Class-Incremental Learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Tao Hu, Lan Li, Zhen-Hao Xie, Da-Wei Zhou<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multi-modal (abstract)<br><p><strong><u>Abstract:</u></strong> Class-Incremental Learning (CIL) enables models to learn new classes continually while preserving past knowledge. Recently, vision-language models like CLIP offer transferable features via multi-modal pre-training, making them well-suited for CIL. However, real-world visual and linguistic concepts are inherently hierarchical: a textual concept like "dog" subsumes fine-grained categories such as "Labrador" and "Golden Retriever," and each category entails its images. But existing CLIP-based CIL methods fail to explicitly capture this inherent hierarchy, leading to fine-grained class features drift during incremental updates and ultimately to catastrophic forgetting. To address this challenge, we propose HASTEN (Hierarchical Semantic Tree Anchoring) that anchors hierarchical information into CIL to reduce catastrophic forgetting. First, we employ an external knowledge graph as supervision to embed visual and textual features in hyperbolic space, effectively preserving hierarchical structure as data evolves. Second, to mitigate catastrophic forgetting, we project gradients onto the null space of the shared hyperbolic mapper, preventing interference with prior tasks. These two steps work synergistically to enable the model to resist forgetting by maintaining hierarchical relationships. Extensive experiments show that HASTEN consistently outperforms existing methods while providing a unified structured representation.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15632v1" target="_blank"><h2>CODE-II: A large-scale dataset for artificial intelligence in ECG analysis <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Petrus E. O. G. B. Abreu, Gabriela M. M. Paixão, Jiawei Li, Paulo R. Gomes, Peter W. Macfarlane, Ana C. S. Oliveira, Vinicius T. Carvalho, Thomas B. Schön, Antonio Luiz P. Ribeiro, Antônio H. Ribeiro<br><strong><u>Categories:</u></strong> eess.SP, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Data-driven methods for electrocardiogram (ECG) interpretation are rapidly progressing. Large datasets have enabled advances in artificial intelligence (AI) based ECG analysis, yet limitations in annotation quality, size, and scope remain major challenges. Here we present CODE-II, a large-scale real-world dataset of 2,735,269 12-lead ECGs from 2,093,807 adult patients collected by the Telehealth Network of Minas Gerais (TNMG), Brazil. Each exam was annotated using standardized diagnostic criteria and reviewed by cardiologists. A defining feature of CODE-II is a set of 66 clinically meaningful diagnostic classes, developed with cardiologist input and routinely used in telehealth practice. We additionally provide an open available subset: CODE-II-open, a public subset of 15,000 patients, and the CODE-II-test, a non-overlapping set of 8,475 exams reviewed by multiple cardiologists for blinded evaluation. A neural network pre-trained on CODE-II achieved superior transfer performance on external benchmarks (PTB-XL and CPSC 2018) and outperformed alternatives trained on larger datasets.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15623v1" target="_blank"><h2>Sufficient Explanations in Databases and their Connections to Necessary Explanations and Repairs <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Leopoldo Bertossi, Nina Pardal<br><strong><u>Categories:</u></strong> cs.DB, cs.AI, cs.LO<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> causality (abstract)<br><p><strong><u>Abstract:</u></strong> The notion of cause, as formalized by Halpern and Pearl, has been recently applied to relational databases, to characterize and compute causal explanations for query answers. In this work we consider the alternative notion of sufficient explanation. We investigate its connections with database repairs as used for dealing with inconsistent databases, and with causality-based necessary explanations. We also obtain some computational results.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15619v1" target="_blank"><h2>CODE: A global approach to ODE dynamics learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Nils Wildt, Daniel M. Tartakovsky, Sergey Oladyshkin, Wolfgang Nowak<br><strong><u>Categories:</u></strong> cs.LG, stat.ML<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Ordinary differential equations (ODEs) are a conventional way to describe the observed dynamics of physical systems. Scientists typically hypothesize about dynamical behavior, propose a mathematical model, and compare its predictions to data. However, modern computing and algorithmic advances now enable purely data-driven learning of governing dynamics directly from observations. In data-driven settings, one learns the ODE's right-hand side (RHS). Dense measurements are often assumed, yet high temporal resolution is typically both cumbersome and expensive. Consequently, one usually has only sparsely sampled data. In this work we introduce ChaosODE (CODE), a Polynomial Chaos ODE Expansion in which we use an arbitrary Polynomial Chaos Expansion (aPCE) for the ODE's right-hand side, resulting in a global orthonormal polynomial representation of dynamics. We evaluate the performance of CODE in several experiments on the Lotka-Volterra system, across varying noise levels, initial conditions, and predictions far into the future, even on previously unseen initial conditions. CODE exhibits remarkable extrapolation capabilities even when evaluated under novel initial conditions and shows advantages compared to well-examined methods using neural networks (NeuralODE) or kernel approximators (KernelODE) as the RHS representer. We observe that the high flexibility of NeuralODE and KernelODE degrades extrapolation capabilities under scarce data and measurement noise. Finally, we provide practical guidelines for robust optimization of dynamics-learning problems and illustrate them in the accompanying code.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15600v1" target="_blank"><h2>US-X Complete: A Multi-Modal Approach to Anatomical 3D Shape Recovery <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Miruna-Alexandra Gafencu, Yordanka Velikova, Nassir Navab, Mohammad Farid Azampour<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> Accepted at the Workshop on Shape in Medical Imaging at MICCAI 2025<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multi-modal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Ultrasound offers a radiation-free, cost-effective solution for real-time visualization of spinal landmarks, paraspinal soft tissues and neurovascular structures, making it valuable for intraoperative guidance during spinal procedures. However, ultrasound suffers from inherent limitations in visualizing complete vertebral anatomy, in particular vertebral bodies, due to acoustic shadowing effects caused by bone. In this work, we present a novel multi-modal deep learning method for completing occluded anatomical structures in 3D ultrasound by leveraging complementary information from a single X-ray image. To enable training, we generate paired training data consisting of: (1) 2D lateral vertebral views that simulate X-ray scans, and (2) 3D partial vertebrae representations that mimic the limited visibility and occlusions encountered during ultrasound spine imaging. Our method integrates morphological information from both imaging modalities and demonstrates significant improvements in vertebral reconstruction (p < 0.001) compared to state of art in 3D ultrasound vertebral completion. We perform phantom studies as an initial step to future clinical translation, and achieve a more accurate, complete volumetric lumbar spine visualization overlayed on the ultrasound scan without the need for registration with preoperative modalities such as computed tomography. This demonstrates that integrating a single X-ray projection mitigates ultrasound's key limitation while preserving its strengths as the primary imaging modality. Code and data can be found at https://github.com/miruna20/US-X-Complete</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15552v2" target="_blank"><h2>Multimodal Evaluation of Russian-language Architectures <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Artem Chervyakov, Ulyana Isaeva, Anton Emelyanov, Artem Safin, Maria Tikhonova, Alexander Kharitonov, Yulia Lyakh, Petr Surovtsev, Denis Shevelev, Vildan Saburov, Vasily Konovalov, Elisei Rykov, Ivan Sviridov, Amina Miftakhova, Ilseyar Alimova, Alexander Panchenko, Alexander Kapitanov, Alena Fenogenova<br><strong><u>Categories:</u></strong> cs.CL, cs.AI, cs.CV<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15543v1" target="_blank"><h2>A Physics Informed Machine Learning Framework for Optimal Sensor Placement and Parameter Estimation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Georgios Venianakis, Constantinos Theodoropoulos, Michail Kavousanakis<br><strong><u>Categories:</u></strong> stat.ML, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Parameter estimation remains a challenging task across many areas of engineering. Because data acquisition can often be costly, limited, or prone to inaccuracies (noise, uncertainty) it is crucial to identify sensor configurations that provide the maximum amount of information about the unknown parameters, in particular for the case of distributed-parameter systems, where spatial variations are important. Physics-Informed Neural Networks (PINNs) have recently emerged as a powerful machine-learning (ML) tool for parameter estimation, particularly in cases with sparse or noisy measurements, overcoming some of the limitations of traditional optimization-based and Bayesian approaches. Despite the widespread use of PINNs for solving inverse problems, relatively little attention has been given to how their performance depends on sensor placement. This study addresses this gap by introducing a comprehensive PINN-based framework that simultaneously tackles optimal sensor placement and parameter estimation. Our approach involves training a PINN model in which the parameters of interest are included as additional inputs. This enables the efficient computation of sensitivity functions through automatic differentiation, which are then used to determine optimal sensor locations exploiting the D-optimality criterion. The framework is validated on two illustrative distributed-parameter reaction-diffusion-advection problems of increasing complexity. The results demonstrate that our PINNs-based methodology consistently achieves higher accuracy compared to parameter values estimated from intuitively or randomly selected sensor positions.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15541v1" target="_blank"><h2>The VVVX quest for satellites around the Circinus galaxy <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> L. D. Baravalle, A. L. O'Mill, M. V. Alonso, C. Obasi, D. Minniti, M. Gómez, C. Villalon, J. Nilo-Castellón, C. Valotto, M. Soto, I. V. Daza Perilla, M. A. Sgró, J. G. Fernández-Trincado<br><strong><u>Categories:</u></strong> astro-ph.GA<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> The Circinus galaxy is the nearest type-2 Seyfert galaxy, which is at a distance of 4.2 Mpc. Its environment is challenging to explore because it is located at low Galactic latitudes, behind the Galactic disc. The long-term goal is to characterise the Circinus galaxy halo and determine the presence of dwarf satellites using near-infrared data. We selected 1,542 galaxies from the VVV NIRGC within a 2-degree radius around Circinus, representing 2/3 of the virial radius. Structural parameters such as half-light radii and colours were used, and correlations were examined. A neural network was trained with 486 galaxies with known spectroscopic redshifts to estimate photometric redshifts for all galaxies. Potential satellites were defined based on half-light radii compatible with the typical sizes of dwarf satellites, and combined with photometric redshifts. The galaxy properties are reliably characterised down to $K_{s}$ $\sim$ 15.5 mag, which represents about 90% completeness of detections. At the distance of Circinus, this limiting magnitude corresponds to $K_{s}$ absolute magnitude of $-12.6$ mag, which allows us to find dwarf galaxies. There are 20 galaxies with half-light radii larger than 2.45 arcsec, only 8 have photometric redshifts below 0.04. None of these galaxies is close to Circinus, which has a redshift of 0.0015. The ANNz model exhibited a high degree of accuracy in the range $0.001 < z_{phot} < 0.023$. The presence of dwarf satellites could not be confirmed with the available data in the studied region. The apparent lack of satellites may be genuine, possibly related to AGN feedback effects. This work demonstrates the effectiveness of combining near-infrared data and machine learning techniques to estimate photometric redshifts at low Galactic latitudes, providing useful information for future spectroscopic follow-up campaigns.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15530v1" target="_blank"><h2>Convergence and Sketching-Based Efficient Computation of Neural Tangent Kernel Weights in Physics-Based Loss <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Max Hirsch, Federico Pichi<br><strong><u>Categories:</u></strong> math.NA, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> In multi-objective optimization, multiple loss terms are weighted and added together to form a single objective. These weights are chosen to properly balance the competing losses according to some meta-goal. For example, in physics-informed neural networks (PINNs), these weights are often adaptively chosen to improve the network's generalization error. A popular choice of adaptive weights is based on the neural tangent kernel (NTK) of the PINN, which describes the evolution of the network in predictor space during training. The convergence of such an adaptive weighting algorithm is not clear a priori. Moreover, these NTK-based weights would be updated frequently during training, further increasing the computational burden of the learning process. In this paper, we prove that under appropriate conditions, gradient descent enhanced with adaptive NTK-based weights is convergent in a suitable sense. We then address the problem of computational efficiency by developing a randomized algorithm inspired by a predictor-corrector approach and matrix sketching, which produces unbiased estimates of the NTK up to an arbitrarily small discretization error. Finally, we provide numerical experiments to support our theoretical findings and to show the efficacy of our randomized algorithm. Code Availability: https://github.com/maxhirsch/Efficient-NTK</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15522v1" target="_blank"><h2>PCARNN-DCBF: Minimal-Intervention Geofence Enforcement for Ground Vehicles <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yinan Yu, Samuel Scheidegger<br><strong><u>Categories:</u></strong> cs.LG, cs.RO<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Runtime geofencing for ground vehicles is rapidly emerging as a critical technology for enforcing Operational Design Domains (ODDs). However, existing solutions struggle to reconcile high-fidelity learning with the structural requirements of verifiable control. We address this by introducing PCARNN-DCBF, a novel pipeline integrating a Physics-encoded Control-Affine Residual Neural Network with a preview-based Discrete Control Barrier Function. Unlike generic learned models, PCARNN explicitly preserves the control-affine structure of vehicle dynamics, ensuring the linearity required for reliable optimization. This enables the DCBF to enforce polygonal keep-in constraints via a real-time Quadratic Program (QP) that handles high relative degree and mitigates actuator saturation. Experiments in CARLA across electric and combustion platforms demonstrate that this structure-preserving approach significantly outperforms analytical and unstructured neural baselines.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15520v1" target="_blank"><h2>Theoretical Closed-loop Stability Bounds for Dynamical System Coupled with Diffusion Policies <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Gabriel Lauzier, Alexandre Girard, François Ferland<br><strong><u>Categories:</u></strong> cs.RO, cs.AI<br><strong><u>Comments:</u></strong> 5 pages, 3 figures<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Diffusion Policy has shown great performance in robotic manipulation tasks under stochastic perturbations, due to its ability to model multimodal action distributions. Nonetheless, its reliance on a computationally expensive reverse-time diffusion (denoising) process, for action inference, makes it challenging to use for real-time applications where quick decision-making is mandatory. This work studies the possibility of conducting the denoising process only partially before executing an action, allowing the plant to evolve according to its dynamics in parallel to the reverse-time diffusion dynamics ongoing on the computer. In a classical diffusion policy setting, the plant dynamics are usually slow and the two dynamical processes are uncoupled. Here, we investigate theoretical bounds on the stability of closed-loop systems using diffusion policies when the plant dynamics and the denoising dynamics are coupled. The contribution of this work gives a framework for faster imitation learning and a metric that yields if a controller will be stable based on the variance of the demonstrations.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15763v1" target="_blank"><h2>Identifying the Supply Chain of AI for Trustworthiness and Risk Management in Critical Applications <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Raymond K. Sheh, Karen Geappen<br><strong><u>Categories:</u></strong> cs.AI, cs.CR, cs.SE<br><strong><u>Comments:</u></strong> Presented at the 2025 AAAI Fall Symposium - AI Trustworthiness and Risk Assessment for Challenged Contexts (ATRACC)<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Risks associated with the use of AI, ranging from algorithmic bias to model hallucinations, have received much attention and extensive research across the AI community, from researchers to end-users. However, a gap exists in the systematic assessment of supply chain risks associated with the complex web of data sources, pre-trained models, agents, services, and other systems that contribute to the output of modern AI systems. This gap is particularly problematic when AI systems are used in critical applications, such as the food supply, healthcare, utilities, law, insurance, and transport.
  We survey the current state of AI risk assessment and management, with a focus on the supply chain of AI and risks relating to the behavior and outputs of the AI system. We then present a proposed taxonomy specifically for categorizing AI supply chain entities. This taxonomy helps stakeholders, especially those without extensive AI expertise, to "consider the right questions" and systematically inventory dependencies across their organization's AI systems. Our contribution bridges a gap between the current state of AI governance and the urgent need for actionable risk assessment and management of AI use in critical applications.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15476v1" target="_blank"><h2>RS-CA-HSICT: A Residual and Spatial Channel Augmented CNN Transformer Framework for Monkeypox Detection <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Rashid Iqbal, Saddam Hussain Khan<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> 33 Pages, 12 Figure, 4 Tables<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> This work proposes a hybrid deep learning approach, namely Residual and Spatial Learning based Channel Augmented Integrated CNN-Transformer architecture, that leverages the strengths of CNN and Transformer towards enhanced MPox detection. The proposed RS-CA-HSICT framework is composed of an HSICT block, a residual CNN module, a spatial CNN block, and a CA, which enhances the diverse feature space, detailed lesion information, and long-range dependencies. The new HSICT module first integrates an abstract representation of the stem CNN and customized ICT blocks for efficient multihead attention and structured CNN layers with homogeneous (H) and structural (S) operations. The customized ICT blocks learn global contextual interactions and local texture extraction. Additionally, H and S layers learn spatial homogeneity and fine structural details by reducing noise and modeling complex morphological variations. Moreover, inverse residual learning enhances vanishing gradient, and stage-wise resolution reduction ensures scale invariance. Furthermore, the RS-CA-HSICT framework augments the learned HSICT channels with the TL-driven Residual and Spatial CNN maps for enhanced multiscale feature space capturing global and localized structural cues, subtle texture, and contrast variations. These channels, preceding augmentation, are refined through the Channel-Fusion-and-Attention block, which preserves discriminative channels while suppressing redundant ones, thereby enabling efficient computation. Finally, the spatial attention mechanism refines pixel selection to detect subtle patterns and intra-class contrast variations in Mpox. Experimental results on both the Kaggle benchmark and a diverse MPox dataset reported classification accuracy as high as 98.30% and an F1-score of 98.13%, which outperforms the existing CNNs and ViTs.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15475v1" target="_blank"><h2>LCS: A Learnlet-Based Sparse Framework for Blind Source Separation <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> V. Bonjean, A. Gkogkou, J. L. Starck, P. Tsakalides<br><strong><u>Categories:</u></strong> astro-ph.IM, astro-ph.CO<br><strong><u>Comments:</u></strong> 11 pages, 8 figures<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Blind source separation (BSS) plays a pivotal role in modern astrophysics by enabling the extraction of scientifically meaningful signals from multi-frequency observations. Traditional BSS methods, such as those relying on fixed wavelet dictionaries, enforce sparsity during component separation, but may fall short when faced with the inherent complexity of real astrophysical signals. In this work, we introduce the Learnlet Component Separator (LCS), a novel BSS framework that bridges classical sparsity-based techniques with modern deep learning. LCS utilizes the Learnlet transform: a structured convolutional neural network designed to serve as a learned, wavelet-like multiscale representation. This hybrid design preserves the interpretability and sparsity, promoting properties of wavelets while gaining the adaptability and expressiveness of learned models. The LCS algorithm integrates this learned sparse representation into an iterative source separation process, enabling effective decomposition of multi-channel observations. While conceptually inspired by sparse BSS methods, LCS introduces a learned representation layer that significantly departs from classical fixed-basis assumptions. We evaluate LCS on both synthetic and real datasets, demonstrating superior separation performance compared to state-of-the-art methods (average gain of about 5 dB on toy model examples). Our results highlight the potential of hybrid approaches that combine signal processing priors with deep learning to address the challenges of next-generation cosmological experiments.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15470v1" target="_blank"><h2>Advancing Identification method of Gamma-Ray Bursts with Data and Feature Enhancement <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Peng Zhang, Bing Li, Ren-Zhou Gui, Shao-Lin Xiong, Yu Wang, Shi-Jie Zheng, Guang-Cheng Xiao, Xiao-Bo Li, Yue Huang, Chen-Wei Wang, Jia-Cong Liu, Yan-Qiu Zhang, Wang-Chen Xue, Chao Zheng, Yue Wang<br><strong><u>Categories:</u></strong> astro-ph.HE<br><strong><u>Comments:</u></strong> Under review. Dataset and model related discussions are welcome!<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), dimensionality reduction (abstract), neural network (abstract), time-domain (abstract), data augmentation (abstract)<br><p><strong><u>Abstract:</u></strong> Gamma-ray bursts (GRBs) are challenging to identify due to their transient nature, complex temporal profiles, and limited observational datasets. We address this with a one-dimensional convolutional neural network integrated with an Adaptive Frequency Feature Enhancement module and physics-informed data augmentation. Our framework generates 100,000 synthetic GRB samples, expanding training data diversity and volume while preserving physical fidelity-especially for low-significance events. The model achieves 97.46% classification accuracy, outperforming all tested variants with conventional enhancement modules, highlighting enhanced domain-specific feature capture. Feature visualization shows model focuses on deep-seated morphological features and confirms the capability of extracting physically meaningful burst characteristics. Dimensionality reduction and clustering reveal GRBs with similar morphologies or progenitor origins cluster in the feature space, linking learned features to physical properties. This perhaps offers a novel diagnostic tool for identifying kilonova- and supernova-associated GRB candidates, establishing criteria to enhance multi-messenger early-warning systems. The framework aids current time-domain surveys, generalizes to other rare transients, and advances automated detection in large-volume observational data.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15464v1" target="_blank"><h2>SIGMMA: Hierarchical Graph-Based Multi-Scale Multi-modal Contrastive Alignment of Histopathology Image and Spatial Transcriptome <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Dabin Jeong, Amirhossein Vahidi, Ciro Ramírez-Suástegui, Marie Moullet, Kevin Ly, Mohammad Vali Sanian, Sebastian Birk, Yinshui Chang, Adam Boxall, Daniyal Jafree, Lloyd Steele, Vijaya Baskar MS, Muzlifah Haniffa, Mohammad Lotfollahi<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multi-modal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Recent advances in computational pathology have leveraged vision-language models to learn joint representations of Hematoxylin and Eosin (HE) images with spatial transcriptomic (ST) profiles. However, existing approaches typically align HE tiles with their corresponding ST profiles at a single scale, overlooking fine-grained cellular structures and their spatial organization. To address this, we propose Sigmma, a multi-modal contrastive alignment framework for learning hierarchical representations of HE images and spatial transcriptome profiles across multiple scales. Sigmma introduces multi-scale contrastive alignment, ensuring that representations learned at different scales remain coherent across modalities. Furthermore, by representing cell interactions as a graph and integrating inter- and intra-subgraph relationships, our approach effectively captures cell-cell interactions, ranging from fine to coarse, within the tissue microenvironment. We demonstrate that Sigmm learns representations that better capture cross-modal correspondences, leading to an improvement of avg. 9.78\% in the gene-expression prediction task and avg. 26.93\% in the cross-modal retrieval task across datasets. We further show that it learns meaningful multi-tissue organization in downstream analyses.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15456v1" target="_blank"><h2>Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Qian'ang Mao, Yuxuan Zhang, Jiaman Chen, Wenjun Zhou, Jiaqi Yan<br><strong><u>Categories:</u></strong> cs.AI, q-fin.GN<br><strong><u>Comments:</u></strong> Written in 2025 Q1<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multi-modal (abstract)<br><p><strong><u>Abstract:</u></strong> As Decentralized Finance (DeFi) develops, understanding user intent behind DeFi transactions is crucial yet challenging due to complex smart contract interactions, multifaceted on-/off-chain factors, and opaque hex logs. Existing methods lack deep semantic insight. To address this, we propose the Transaction Intent Mining (TIM) framework. TIM leverages a DeFi intent taxonomy built on grounded theory and a multi-agent Large Language Model (LLM) system to robustly infer user intents. A Meta-Level Planner dynamically coordinates domain experts to decompose multiple perspective-specific intent analyses into solvable subtasks. Question Solvers handle the tasks with multi-modal on/off-chain data. While a Cognitive Evaluator mitigates LLM hallucinations and ensures verifiability. Experiments show that TIM significantly outperforms machine learning models, single LLMs, and single Agent baselines. We also analyze core challenges in intent inference. This work helps provide a more reliable understanding of user motivations in DeFi, offering context-aware explanations for complex blockchain activity.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15762v1" target="_blank"><h2>A time for monsters: Organizational knowing after LLMs <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Samer Faraj, Joel Perez Torrents, Saku Mantere, Anand Bhardwaj<br><strong><u>Categories:</u></strong> cs.CY, cs.AI<br><strong><u>Comments:</u></strong> Forthcoming at Strategic Organization<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Large Language Models (LLMs) are reshaping organizational knowing by unsettling the epistemological foundations of representational and practice-based perspectives. We conceptualize LLMs as Haraway-ian monsters, that is, hybrid, boundary-crossing entities that destabilize established categories while opening new possibilities for inquiry. Focusing on analogizing as a fundamental driver of knowledge, we examine how LLMs generate connections through large-scale statistical inference. Analyzing their operation across the dimensions of surface/deep analogies and near/far domains, we highlight both their capacity to expand organizational knowing and the epistemic risks they introduce. Building on this, we identify three challenges of living with such epistemic monsters: the transformation of inquiry, the growing need for dialogical vetting, and the redistribution of agency. By foregrounding the entangled dynamics of knowing-with-LLMs, the paper extends organizational theory beyond human-centered epistemologies and invites renewed attention to how knowledge is created, validated, and acted upon in the age of intelligent technologies.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15445v1" target="_blank"><h2>Neural network-driven domain decomposition for efficient solutions to the Helmholtz equation <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Victorita Dolean, Daria Hrebenshchykova, Stéphane Lanteri, Victor Michel-Dansac<br><strong><u>Categories:</u></strong> math.NA, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> Accurately simulating wave propagation is crucial in fields such as acoustics, electromagnetism, and seismic analysis. Traditional numerical methods, like finite difference and finite element approaches, are widely used to solve governing partial differential equations (PDEs) such as the Helmholtz equation. However, these methods face significant computational challenges when applied to high-frequency wave problems in complex two-dimensional domains. This work investigates Finite Basis Physics-Informed Neural Networks (FBPINNs) and their multilevel extensions as a promising alternative. These methods leverage domain decomposition, partitioning the computational domain into overlapping sub-domains, each governed by a local neural network. We assess their accuracy and computational efficiency in solving the Helmholtz equation for the homogeneous case, demonstrating their potential to mitigate the limitations of traditional approaches.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15435v1" target="_blank"><h2>HV-Attack: Hierarchical Visual Attack for Multimodal Retrieval Augmented Generation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Linyin Luo, Yujuan Ding, Yunshan Ma, Wenqi Fan, Hanjiang Lai<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.IR<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Advanced multimodal Retrieval-Augmented Generation (MRAG) techniques have been widely applied to enhance the capabilities of Large Multimodal Models (LMMs), but they also bring along novel safety issues. Existing adversarial research has revealed the vulnerability of MRAG systems to knowledge poisoning attacks, which fool the retriever into recalling injected poisoned contents. However, our work considers a different setting: visual attack of MRAG by solely adding imperceptible perturbations at the image inputs of users, without manipulating any other components. This is challenging due to the robustness of fine-tuned retrievers and large-scale generators, and the effect of visual perturbation may be further weakened by propagation through the RAG chain. We propose a novel Hierarchical Visual Attack that misaligns and disrupts the two inputs (the multimodal query and the augmented knowledge) of MRAG's generator to confuse its generation. We further design a hierarchical two-stage strategy to obtain misaligned augmented knowledge. We disrupt the image input of the retriever to make it recall irrelevant knowledge from the original database, by optimizing the perturbation which first breaks the cross-modal alignment and then disrupts the multimodal semantic alignment. We conduct extensive experiments on two widely-used MRAG datasets: OK-VQA and InfoSeek. We use CLIP-based retrievers and two LMMs BLIP-2 and LLaVA as generators. Results demonstrate the effectiveness of our visual attack on MRAG through the significant decrease in both retrieval and generation performance.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15432v1" target="_blank"><h2>Towards Understanding Layer Contributions in Tabular In-Context Learning Models <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Amir Rezaei Balef, Mykhailo Koshil, Katharina Eggensperger<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> Accepted at the EurIPS 2025 Workshop on AI for Tabular Data<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> latent space (abstract)<br><p><strong><u>Abstract:</u></strong> Despite the architectural similarities between tabular in-context learning (ICL) models and large language models (LLMs), little is known about how individual layers contribute to tabular prediction. In this paper, we investigate how the latent spaces evolve across layers in tabular ICL models, identify potential redundant layers, and compare these dynamics with those observed in LLMs. We analyze TabPFN and TabICL through the "layers as painters" perspective, finding that only subsets of layers share a common representational language, suggesting structural redundancy and offering opportunities for model compression and improved interpretability.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15414v1" target="_blank"><h2>RRT*former: Environment-Aware Sampling-Based Motion Planning using Transformer <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Mingyang Feng, Shaoyuan Li, Xiang Yin<br><strong><u>Categories:</u></strong> cs.RO, cs.AI<br><strong><u>Comments:</u></strong> Accepted to IROS 2025<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> We investigate the sampling-based optimal path planning problem for robotics in complex and dynamic environments. Most existing sampling-based algorithms neglect environmental information or the information from previous samples. Yet, these pieces of information are highly informative, as leveraging them can provide better heuristics when sampling the next state. In this paper, we propose a novel sampling-based planning algorithm, called \emph{RRT*former}, which integrates the standard RRT* algorithm with a Transformer network in a novel way. Specifically, the Transformer is used to extract features from the environment and leverage information from previous samples to better guide the sampling process. Our extensive experiments demonstrate that, compared to existing sampling-based approaches such as RRT*, Neural RRT*, and their variants, our algorithm achieves considerable improvements in both the optimality of the path and sampling efficiency. The code for our implementation is available on https://github.com/fengmingyang666/RRTformer.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15407v1" target="_blank"><h2>IPR-1: Interactive Physical Reasoner <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Mingyu Zhang, Lifeng Zhuo, Tianxi Tan, Guocan Xie, Xian Nie, Yan Li, Renjie Zhao, Zizhu He, Ziyu Wang, Jiting Cai, Yong-Lu Li<br><strong><u>Categories:</u></strong> cs.AI, cs.CV<br><strong><u>Comments:</u></strong> 11 pages, 5 figures<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> causality (abstract)<br><p><strong><u>Abstract:</u></strong> Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. We study this in a Game-to-Unseen (G2U) setting, curating 1,000+ heterogeneous games with diverse physical and causal mechanisms, and evaluate at three human-like levels: Survival, Curiosity, Utility, from primitive intuition to goal-driven reasoning. Our analysis reveals complementary failures: VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on three levels, matches GPT-5 overall, and surpasses it on Curiosity. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15401v1" target="_blank"><h2>Explosions in the Empty: A Survey of Transients in Local Void Galaxies <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Suo-Ning Wang, Bin-Bin Zhang, Rubén García Benito<br><strong><u>Categories:</u></strong> astro-ph.HE, astro-ph.GA<br><strong><u>Comments:</u></strong> 52 pages, 4 figures, 6 tables<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> We present a systematic analysis of transient astrophysical events -- including supernovae (SNe), gamma-ray bursts (GRBs), and fast radio bursts (FRBs) -- in void and non-void galaxies within the local universe ($0.005 < z < 0.05$). Cosmic voids, defined by low galaxy densities and characterized by minimal environmental interactions, offer a natural laboratory for isolating the impact of large-scale underdensities on stellar evolution and transient production. Using multi-wavelength data from the Sloan Digital Sky Survey, the Sternberg Astronomical Institute Supernova Catalogue, and high-energy space observatories, we compare transient occurrence rates and host galaxy properties across environments. We find that core-collapse supernovae (CCSNe) are significantly more common in void galaxies, indicating that massive star formation remains active in underdense regions. In contrast, Type Ia supernovae are less frequent in voids, consistent with a scarcity of older stellar populations. Notably, we identify a short-duration GRB hosted by a void galaxy, demonstrating that compact object mergers can occur in isolated environments. Additionally, we find no FRBs associated with void galaxies. Taken together, these results show that cosmic voids exert a measurable influence on the star formation history of galaxies and hence on the production of transients.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15393v1" target="_blank"><h2>EVA-Net: Interpretable Brain Age Prediction via Continuous Aging Prototypes from EEG <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Kunyu Zhang, Mingxuan Wang, Xiangjie Shi, Haoxing Xu, Chao Zhang<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> anomaly detection (abstract), transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> The brain age is a key indicator of brain health. While electroencephalography (EEG) is a practical tool for this task, existing models struggle with the common challenge of imperfect medical data, such as learning a ``normal'' baseline from weakly supervised, healthy-only cohorts. This is a critical anomaly detection task for identifying disease, but standard models are often black boxes lacking an interpretable structure. We propose EVA-Net, a novel framework that recasts brain age as an interpretable anomaly detection problem. EVA-Net uses an efficient, sparsified-attention Transformer to model long EEG sequences. To handle noise and variability in imperfect data, it employs a Variational Information Bottleneck to learn a robust, compressed representation. For interpretability, this representation is aligned to a continuous prototype network that explicitly learns the normative healthy aging manifold. Trained on 1297 healthy subjects, EVA-Net achieves state-of-the-art accuracy. We validated its anomaly detection capabilities on an unseen cohort of 27 MCI and AD patients. This pathological group showed significantly higher brain-age gaps and a novel Prototype Alignment Error, confirming their deviation from the healthy manifold. EVA-Net provides an interpretable framework for healthcare intelligence using imperfect medical data.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15375v1" target="_blank"><h2>Parameter Importance-Driven Continual Learning for Foundation Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Lingxiang Wang, Hainan Zhang, Zhiming Zheng<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Domain-specific post-training often causes catastrophic forgetting, making foundation models lose their general reasoning ability and limiting their adaptability to dynamic real-world environments. Preserving general capabilities while acquiring downstream domain knowledge is a central challenge for large language and multimodal models. Traditional continual learning methods, such as regularization, replay and architectural isolation, suffer from poor downstream performance, reliance on inaccessible historical data, or additional parameter overhead. While recent parameter-efficient tuning (PET) methods can alleviate forgetting, their effectiveness strongly depends on the choice of parameters and update strategies. In this paper, we introduce PIECE, a Parameter Importance Estimation-based Continual Enhancement method that preserves general ability while efficiently learning domain knowledge without accessing prior training data or increasing model parameters. PIECE selectively updates only 0.1% of core parameters most relevant to new tasks, guided by two importance estimators: PIECE-F based on Fisher Information, and PIECE-S based on a second-order normalization that combines gradient and curvature information. Experiments across three language models and two multimodal models show that PIECE maintains general capabilities and achieves state-of-the-art continual learning performance across diverse downstream tasks. Our results highlight a practical path to scalable, domain-adaptive foundation models without catastrophic forgetting.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15370v1" target="_blank"><h2>The Empowerment of Science of Science by Large Language Models: New Tools and Methods <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Guoqiang Liang, Jingqian Gong, Mengxuan Li, Gege Lin, Shuo Zhang<br><strong><u>Categories:</u></strong> cs.CL, cs.AI<br><strong><u>Comments:</u></strong> The manuscript is currently ongoing the underreview process of the journal of information science<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Large language models (LLMs) have exhibited exceptional capabilities in natural language understanding and generation, image recognition, and multimodal tasks, charting a course towards AGI and emerging as a central issue in the global technological race. This manuscript conducts a comprehensive review of the core technologies that support LLMs from a user standpoint, including prompt engineering, knowledge-enhanced retrieval augmented generation, fine tuning, pretraining, and tool learning. Additionally, it traces the historical development of Science of Science (SciSci) and presents a forward looking perspective on the potential applications of LLMs within the scientometric domain. Furthermore, it discusses the prospect of an AI agent based model for scientific evaluation, and presents new research fronts detection and knowledge graph building methods with LLMs.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15369v1" target="_blank"><h2>IPTQ-ViT: Post-Training Quantization of Non-linear Functions for Integer-only Vision Transformers <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Gihwan Kim, Jemin Lee, Hyungshin Kim<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> accepted in WACV 2026 (10 pages)<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> Previous Quantization-Aware Training (QAT) methods for vision transformers rely on expensive retraining to recover accuracy loss in non-linear layer quantization, limiting their use in resource-constrained environments. In contrast, existing Post-Training Quantization (PTQ) methods either partially quantize non-linear functions or adjust activation distributions to maintain accuracy but fail to achieve fully integer-only inference. In this paper, we introduce IPTQ-ViT, a novel PTQ framework for fully integer-only vision transformers without retraining. We present approximation functions: a polynomial-based GELU optimized for vision data and a bit-shifting-based Softmax designed to improve approximation accuracy in PTQ. In addition, we propose a unified metric integrating quantization sensitivity, perturbation, and computational cost to select the optimal approximation function per activation layer. IPTQ-ViT outperforms previous PTQ methods, achieving up to 6.44\%p (avg. 1.78\%p) top-1 accuracy improvement for image classification, 1.0 mAP for object detection. IPTQ-ViT outperforms partial floating-point PTQ methods under W8A8 and W4A8, and achieves accuracy and latency comparable to integer-only QAT methods. We plan to release our code https://github.com/gihwan-kim/IPTQ-ViT.git.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15351v1" target="_blank"><h2>Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Yifu Guo, Zishan Xu, Zhiyuan Yao, Yuquan Lu, Jiaye Lin, Sen Hu, Zhenheng Tang, Yingchao Li, Huacan Wang, Ronghao Chen<br><strong><u>Categories:</u></strong> cs.AI, cs.CV<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Existing multimodal reasoning models and frameworks suffer from fundamental architectural limitations: most lack the human-like ability to autonomously explore diverse reasoning pathways-whether in direct inference, tool-driven visual exploration, programmatic visual manipulation, or intrinsic visual imagination. Consequently, they struggle to adapt to dynamically changing capability requirements in real-world tasks. Meanwhile, humans exhibit a complementary set of thinking abilities when addressing such tasks, whereas existing methods typically cover only a subset of these dimensions. Inspired by this, we propose Octopus: Agentic Multimodal Reasoning with Six-Capability Orchestration, a new paradigm for multimodal agentic reasoning. We define six core capabilities essential for multimodal reasoning and organize a comprehensive evaluation benchmark, Octopus-Bench, accordingly. Octopus is capable of autonomously exploring during reasoning and dynamically selecting the most appropriate capability based on the current state. Experimental results show that Octopus achieves the best performance on the vast majority of tasks in Octopus-Bench, highlighting the crucial role of capability coordination in agentic multimodal reasoning.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15349v2" target="_blank"><h2>Type Iax supernovae as a source of iron-rich silicate dust <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Aman Kumar, Arkaprabha Sarangi<br><strong><u>Categories:</u></strong> astro-ph.SR, astro-ph.GA, astro-ph.HE<br><strong><u>Comments:</u></strong> Submitted to The Astrophysical Journal<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> VAE (title, abstract)<br><p><strong><u>Abstract:</u></strong> We model the formation of dust in the ejecta of Type Iax supernovae (SNe), which is a low-luminosity subclass of Type Ia SNe. A non-equilibrium chemical kinetic approach is adopted to trace the synthesis of molecules, molecular clusters, and dust grains in the ejecta of thermonuclear SNe. We find that Type Iax SNe provide conditions conducive to the formation of several O-rich dust species in the ejecta. Particularly, iron-rich silicates of chemical type FeSiO3, Fe2SiO4, and MgFeSiO4 are found to form in abundance, suggesting that the ejecta of low-luminosity thermonuclear SNe can be a site where a large fraction of iron is locked up in dust, unlike other stellar sources. The final mass of dust formed in the ejecta ranges between 10^{-5} and 10^{-4} Msun, where most of the dust forms between 1000 and 2000 days post-explosion. Apart from Fe-rich silicates, Mg-silicates, and silicon carbide are also formed in the ejecta of Type Iax SNe. When compared to the dust budget of typical Type Ia SNe, we find that the expected dust-to-ejecta mass ratio is one or two orders of magnitude larger in Type Iax SNe. We conclude that the ejecta of typical Type Ia SNe form a negligible amount of dust, in agreement with observation, while the low-luminosity subclass Type Iax SNe are potential producers of iron-rich silicates.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15342v1" target="_blank"><h2>Reflexive Evidence-Based Multimodal Learning for Clean Energy Transitions: Causal Insights on Cooking Fuel Access, Urbanization, and Carbon Emissions <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Shan Shan<br><strong><u>Categories:</u></strong> cs.HC, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (title), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Achieving Sustainable Development Goal 7 (Affordable and Clean Energy) requires not only technological innovation but also a deeper understanding of the socioeconomic factors influencing energy access and carbon emissions. While these factors are gaining attention, critical questions remain, particularly regarding how to quantify their impacts on energy systems, model their cross-domain interactions, and capture feedback dynamics in the broader context of energy transitions. To address these gaps, this study introduces ClimateAgents, an AI-based framework that combines large language models with domain-specialized agents to support hypothesis generation and scenario exploration. Leveraging 20 years of socioeconomic and emissions data from 265 economies, countries and regions, and 98 indicators drawn from the World Bank database, the framework applies a machine learning based causal inference approach to identify key determinants of carbon emissions in an evidence-based, data driven manner. The analysis highlights three primary drivers: access to clean cooking fuels in rural areas, access to clean cooking fuels in urban areas, and the percentage of population living in urban areas. These findings underscore the critical role of clean cooking technologies and urbanization patterns in shaping emission outcomes. In line with growing calls for evidence-based AI policy, ClimateAgents offers a modular and reflexive learning system that supports the generation of credible and actionable insights for policy. By integrating heterogeneous data modalities, including structured indicators, policy documents, and semantic reasoning, the framework contributes to adaptive policymaking infrastructures that can evolve with complex socio-technical challenges. This approach aims to support a shift from siloed modeling to reflexive, modular systems designed for dynamic, context-aware climate action.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15339v1" target="_blank"><h2>STREAM-VAE: Dual-Path Routing for Slow and Fast Dynamics in Vehicle Telemetry Anomaly Detection <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Kadir-Kaan Özer, René Ebeling, Markus Enzweiler<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 8 Pages, 4 Figures, 4 Tables<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> variational autoencoder (abstract), VAE (title, abstract), anomaly detection (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Automotive telemetry data exhibits slow drifts and fast spikes, often within the same sequence, making reliable anomaly detection challenging. Standard reconstruction-based methods, including sequence variational autoencoders (VAEs), use a single latent process and therefore mix heterogeneous time scales, which can smooth out spikes or inflate variances and weaken anomaly separation.
  In this paper, we present STREAM-VAE, a variational autoencoder for anomaly detection in automotive telemetry time-series data. Our model uses a dual-path encoder to separate slow drift and fast spike signal dynamics, and a decoder that represents transient deviations separately from the normal operating pattern. STREAM-VAE is designed for deployment, producing stable anomaly scores across operating modes for both in-vehicle monitors and backend fleet analytics.
  Experiments on an automotive telemetry dataset and the public SMD benchmark show that explicitly separating drift and spike dynamics improves robustness compared to strong forecasting, attention, graph, and VAE baselines.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15328v1" target="_blank"><h2>LaguerreNet: Advancing a Unified Solution for Heterophily and Over-smoothing with Adaptive Continuous Polynomials <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Huseyin Goksu<br><strong><u>Categories:</u></strong> cs.LG, eess.SP<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Spectral Graph Neural Networks (GNNs) suffer from two critical limitations: poor performance on "heterophilic" graphs and performance collapse at high polynomial degrees (K), known as over-smoothing. Both issues stem from the static, low-pass nature of standard filters (e.g., ChebyNet). While adaptive polynomial filters, such as the discrete MeixnerNet, have emerged as a potential unified solution, their extension to the continuous domain and stability with unbounded coefficients remain open questions. In this work, we propose `LaguerreNet`, a novel GNN filter based on continuous Laguerre polynomials. `LaguerreNet` learns the filter's spectral shape by making its core alpha parameter trainable, thereby advancing the adaptive polynomial approach. We solve the severe O(k^2) numerical instability of these unbounded polynomials using a `LayerNorm`-based stabilization technique. We demonstrate experimentally that this approach is highly effective: 1) `LaguerreNet` achieves state-of-the-art results on challenging heterophilic benchmarks. 2) It is exceptionally robust to over-smoothing, with performance peaking at K=10, an order of magnitude beyond where ChebyNet collapses.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15327v1" target="_blank"><h2>KrawtchoukNet: A Unified GNN Solution for Heterophily and Over-smoothing with Adaptive Bounded Polynomials <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Huseyin Goksu<br><strong><u>Categories:</u></strong> cs.LG, eess.SP<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Spectral Graph Neural Networks (GNNs) based on polynomial filters, such as ChebyNet, suffer from two critical limitations: 1) performance collapse on "heterophilic" graphs and 2) performance collapse at high polynomial degrees (K), known as over-smoothing. Both issues stem from the static, low-pass nature of standard filters. In this work, we propose `KrawtchoukNet`, a GNN filter based on the discrete Krawtchouk polynomials. We demonstrate that `KrawtchoukNet` provides a unified solution to both problems through two key design choices. First, by fixing the polynomial's domain N to a small constant (e.g., N=20), we create the first GNN filter whose recurrence coefficients are \textit{inherently bounded}, making it exceptionally robust to over-smoothing (achieving SOTA results at K=10). Second, by making the filter's shape parameter p learnable, the filter adapts its spectral response to the graph data. We show this adaptive nature allows `KrawtchoukNet` to achieve SOTA performance on challenging heterophilic benchmarks (Texas, Cornell), decisively outperforming standard GNNs like GAT and APPNP.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15324v1" target="_blank"><h2>On the Internal Semantics of Time-Series Foundation Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Atharva Pandey, Abhilash Neog, Gautam Jajoo<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> time-domain (abstract)<br><p><strong><u>Abstract:</u></strong> Time-series Foundation Models (TSFMs) have recently emerged as a universal paradigm for learning across diverse temporal domains. However, despite their empirical success, the internal mechanisms by which these models represent fundamental time-series concepts remain poorly understood. In this work, we undertake a systematic investigation of concept interpretability in TSFMs. Specifically, we examine: (i) which layers encode which concepts, (ii) whether concept parameters are linearly recoverable, (iii) how representations evolve in terms of concept disentanglement and abstraction across model depth, and (iv) how models process compositions of concepts. We systematically probe these questions using layer-wise analyses, linear recoverability tests, and representation similarity measures, providing a structured account of TSFM semantics. The resulting insights show that early layers mainly capture local, time-domain patterns (e.g., AR(1), level shifts, trends), while deeper layers encode dispersion and change-time signals, with spectral and warping factors remaining the hardest to recover linearly. In compositional settings, however, probe performance degrades, revealing interference between concepts. This highlights that while atomic concepts are reliably localized, composition remains a challenge, underscoring a key limitation in current TSFMs' ability to represent interacting temporal phenomena.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15313v1" target="_blank"><h2>A Method for Gamma-Ray Energy Spectrum Inversion and Correction <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zhi-Qiang Ding, Xin-Qiao Li, Da-Li Zhang, Zheng-Hua An, Zhen-Xia Zhang, Roberto Battiston, Roberto Iuppa, Zhuo Li, Yan-Qiu Zhang, Yan Huang, Chao Zheng, Yan-Bing Xu, Xiao-Yun Zhao, Lu Wang, Ping Wang, Hong Lu<br><strong><u>Categories:</u></strong> astro-ph.HE, astro-ph.IM<br><strong><u>Comments:</u></strong> The Astrophysical Journal has accepted<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Accurate spectral analysis of high-energy astrophysical sources often relies on comparing observed data to incident spectral models convolved with the instrument response. However, for Gamma-Ray Bursts and other high-energy transient events observed at high count rates, significant distortions (e.g., pile-up, dead time, and large signal trailing) are introduced, complicating this analysis. We present a method framework to address the model dependence problem, especially to solve the problem of energy spectrum distortion caused by instrument signal pile-up due to high counting rate and high-rate effects, applicable to X-ray, gamma-ray, and particle detectors. Our approach combines physics-based Monte Carlo (MC) simulations with a model-independent spectral inversion technique. The MC simulations quantify instrumental effects and enable correction of the distorted spectrum. Subsequently, the inversion step reconstructs the incident spectrum using an inverse response matrix approach, conceptually equivalent to deconvolving the detector response. The inversion employs a Convolutional Neural Network, selected for its numerical stability and effective handling of complex detector responses. Validation using simulations across diverse input spectra demonstrates high fidelity. Specifically, for 27 different parameter sets of the brightest gamma-ray bursts, goodness-of-fit tests confirm the reconstructed spectra are in excellent statistical agreement with the input spectra, and residuals are typically within $\pm 2σ$. This method enables precise analysis of intense transients and other high-flux events, overcoming limitations imposed by instrumental effects in traditional analyses.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15759v1" target="_blank"><h2>Securing AI Agents Against Prompt Injection Attacks <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Badrinath Ramakrishnan, Akshaya Balaji<br><strong><u>Categories:</u></strong> cs.CR, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> anomaly detection (abstract)<br><p><strong><u>Abstract:</u></strong> Retrieval-augmented generation (RAG) systems have become widely used for enhancing large language model capabilities, but they introduce significant security vulnerabilities through prompt injection attacks. We present a comprehensive benchmark for evaluating prompt injection risks in RAG-enabled AI agents and propose a multi-layered defense framework. Our benchmark includes 847 adversarial test cases across five attack categories: direct injection, context manipulation, instruction override, data exfiltration, and cross-context contamination. We evaluate three defense mechanisms: content filtering with embedding-based anomaly detection, hierarchical system prompt guardrails, and multi-stage response verification, across seven state-of-the-art language models. Our combined framework reduces successful attack rates from 73.2% to 8.7% while maintaining 94.3% of baseline task performance. We release our benchmark dataset and defense implementation to support future research in AI agent security.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15271v1" target="_blank"><h2>Graph Query Networks for Object Detection with Automotive Radar <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Loveneet Saini, Hasan Tercan, Tobias Meisen<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> Accepted in WACV 2026 Main Conference<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Object detection with 3D radar is essential for 360-degree automotive perception, but radar's long wavelengths produce sparse and irregular reflections that challenge traditional grid and sequence-based convolutional and transformer detectors. This paper introduces Graph Query Networks (GQN), an attention-based framework that models objects sensed by radar as graphs, to extract individualized relational and contextual features. GQN employs a novel concept of graph queries to dynamically attend over the bird's-eye view (BEV) space, constructing object-specific graphs processed by two novel modules: EdgeFocus for relational reasoning and DeepContext Pooling for contextual aggregation. On the NuScenes dataset, GQN improves relative mAP by up to +53%, including a +8.2% gain over the strongest prior radar method, while reducing peak graph construction overhead by 80% with moderate FLOPs cost.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15262v1" target="_blank"><h2>Reinforcement Learning in Queue-Reactive Models: Application to Optimal Execution <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Tomas Espana, Yadh Hafsi, Fabrizio Lillo, Edoardo Vittori<br><strong><u>Categories:</u></strong> q-fin.TR, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> We investigate the use of Reinforcement Learning for the optimal execution of meta-orders, where the objective is to execute incrementally large orders while minimizing implementation shortfall and market impact over an extended period of time. Departing from traditional parametric approaches to price dynamics and impact modeling, we adopt a model-free, data-driven framework. Since policy optimization requires counterfactual feedback that historical data cannot provide, we employ the Queue-Reactive Model to generate realistic and tractable limit order book simulations that encompass transient price impact, and nonlinear and dynamic order flow responses. Methodologically, we train a Double Deep Q-Network agent on a state space comprising time, inventory, price, and depth variables, and evaluate its performance against established benchmarks. Numerical simulation results show that the agent learns a policy that is both strategic and tactical, adapting effectively to order book conditions and outperforming standard approaches across multiple training configurations. These findings provide strong evidence that model-free Reinforcement Learning can yield adaptive and robust solutions to the optimal execution problem.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15253v1" target="_blank"><h2>PresentCoach: Dual-Agent Presentation Coaching through Exemplars and Interactive Feedback <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Sirui Chen, Jinsong Zhou, Xinli Xu, Xiaoyu Yang, Litao Guo, Ying-Cong Chen<br><strong><u>Categories:</u></strong> cs.HC, cs.AI<br><strong><u>Comments:</u></strong> 13pages,6figures<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Effective presentation skills are essential in education, professional communication, and public speaking, yet learners often lack access to high-quality exemplars or personalized coaching. Existing AI tools typically provide isolated functionalities such as speech scoring or script generation without integrating reference modeling and interactive feedback into a cohesive learning experience. We introduce a dual-agent system that supports presentation practice through two complementary roles: the Ideal Presentation Agent and the Coach Agent. The Ideal Presentation Agent converts user-provided slides into model presentation videos by combining slide processing, visual-language analysis, narration script generation, personalized voice synthesis, and synchronized video assembly. The Coach Agent then evaluates user-recorded presentations against these exemplars, conducting multimodal speech analysis and delivering structured feedback in an Observation-Impact-Suggestion (OIS) format. To enhance the authenticity of the learning experience, the Coach Agent incorporates an Audience Agent, which simulates the perspective of a human listener and provides humanized feedback reflecting audience reactions and engagement. Together, these agents form a closed loop of observation, practice, and feedback. Implemented on a robust backend with multi-model integration, voice cloning, and error handling mechanisms, the system demonstrates how AI-driven agents can provide engaging, human-centered, and scalable support for presentation skill development in both educational and professional contexts.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15251v1" target="_blank"><h2>PLATONT: Learning a Platonic Representation for Unified Network Tomography <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Chengze Du, Heng Xu, Zhiwei Yu, Bo Liu, Jialong Li<br><strong><u>Categories:</u></strong> cs.LG, cs.NI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> latent space (abstract), multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Network tomography aims to infer hidden network states, such as link performance, traffic load, and topology, from external observations. Most existing methods solve these problems separately and depend on limited task-specific signals, which limits generalization and interpretability. We present PLATONT, a unified framework that models different network indicators (e.g., delay, loss, bandwidth) as projections of a shared latent network state. Guided by the Platonic Representation Hypothesis, PLATONT learns this latent state through multimodal alignment and contrastive learning. By training multiple tomography tasks within a shared latent space, it builds compact and structured representations that improve cross-task generalization. Experiments on synthetic and real-world datasets show that PLATONT consistently outperforms existing methods in link estimation, topology inference, and traffic prediction, achieving higher accuracy and stronger robustness under varying network conditions.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15246v1" target="_blank"><h2>D2D Power Allocation via Quantum Graph Neural Network <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Tung Giang Le, Xuan Tung Nguyen, Won-Joo Hwang<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> Increasing wireless network complexity demands scalable resource management. Classical GNNs excel at graph learning but incur high computational costs in large-scale settings. We present a fully quantum Graph Neural Network (QGNN) that implements message passing via Parameterized Quantum Circuits (PQCs). Our Quantum Graph Convolutional Layers (QGCLs) encode features into quantum states, process graphs with NISQ-compatible unitaries, and retrieve embeddings through measurement. Applied to D2D power control for SINR maximization, our QGNN matches classical performance with fewer parameters and inherent parallelism. This end-to-end PQC-based GNN marks a step toward quantum-accelerated wireless optimization.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15222v1" target="_blank"><h2>Why Physics Still Matters: Improving Machine Learning Prediction of Material Properties with Phonon-Informed Datasets <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Pol Benítez, Cibrán López, Edgardo Saucedo, Teruyasu Mizoguchi, Claudio Cazorla<br><strong><u>Categories:</u></strong> cond-mat.mtrl-sci, cs.LG<br><strong><u>Comments:</u></strong> 12 pages; 5 figures<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> explainability (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Machine learning (ML) methods have become powerful tools for predicting material properties with near first-principles accuracy and vastly reduced computational cost. However, the performance of ML models critically depends on the quality, size, and diversity of the training dataset. In materials science, this dependence is particularly important for learning from low-symmetry atomistic configurations that capture thermal excitations, structural defects, and chemical disorder, features that are ubiquitous in real materials but underrepresented in most datasets. The absence of systematic strategies for generating representative training data may therefore limit the predictive power of ML models in technologically critical fields such as energy conversion and photonics. In this work, we assess the effectiveness of graph neural network (GNN) models trained on two fundamentally different types of datasets: one composed of randomly generated atomic configurations and another constructed using physically informed sampling based on lattice vibrations. As a case study, we address the challenging task of predicting electronic and mechanical properties of a prototypical family of optoelectronic materials under realistic finite-temperature conditions. We find that the phonons-informed model consistently outperforms the randomly trained counterpart, despite relying on fewer data points. Explainability analyses further reveal that high-performing models assign greater weight to chemically meaningful bonds that control property variations, underscoring the importance of physically guided data generation. Overall, this work demonstrates that larger datasets do not necessarily yield better GNN predictive models and introduces a simple and general strategy for efficiently constructing high-quality training data in materials informatics.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15211v2" target="_blank"><h2>OEMA: Ontology-Enhanced Multi-Agent Collaboration Framework for Zero-Shot Clinical Named Entity Recognition <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Xinli Tao, Xin Dong, Xuezhong Zhou<br><strong><u>Categories:</u></strong> cs.CL, cs.AI<br><strong><u>Comments:</u></strong> 12 pages, 4 figures, 4 tables<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> VAE (abstract), domain adaptation (abstract)<br><p><strong><u>Abstract:</u></strong> With the rapid expansion of unstructured clinical texts in electronic health records (EHRs), clinical named entity recognition (NER) has become a crucial technique for extracting medical information. However, traditional supervised models such as CRF and BioClinicalBERT suffer from high annotation costs. Although zero-shot NER based on large language models (LLMs) reduces the dependency on labeled data, challenges remain in aligning example selection with task granularity and effectively integrating prompt design with self-improvement frameworks. To address these limitations, we propose OEMA, a novel zero-shot clinical NER framework based on multi-agent collaboration. OEMA consists of three core components: (1) a self-annotator that autonomously generates candidate examples; (2) a discriminator that leverages SNOMED CT to filter token-level examples by clinical relevance; and (3) a predictor that incorporates entity-type descriptions to enhance inference accuracy. Experimental results on two benchmark datasets, MTSamples and VAERS, demonstrate that OEMA achieves state-of-the-art performance under exact-match evaluation. Moreover, under related-match criteria, OEMA performs comparably to the supervised BioClinicalBERT model while significantly outperforming the traditional CRF method. OEMA improves zero-shot clinical NER, achieving near-supervised performance under related-match criteria. Future work will focus on continual learning and open-domain adaptation to expand its applicability in clinical NLP.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15204v1" target="_blank"><h2>Physics-Based Benchmarking Metrics for Multimodal Synthetic Images <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Kishor Datta Gupta, Marufa Kamal, Md. Mahfuzur Rahman, Fahad Rahman, Mohd Ariful Haque, Sunzida Siddique<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Current state of the art measures like BLEU, CIDEr, VQA score, SigLIP-2 and CLIPScore are often unable to capture semantic or structural accuracy, especially for domain-specific or context-dependent scenarios. For this, this paper proposes a Physics-Constrained Multimodal Data Evaluation (PCMDE) metric combining large language models with reasoning, knowledge based mapping and vision-language models to overcome these limitations. The architecture is comprised of three main stages: (1) feature extraction of spatial and semantic information with multimodal features through object detection and VLMs; (2) Confidence-Weighted Component Fusion for adaptive component-level validation; and (3) physics-guided reasoning using large language models for structural and relational constraints (e.g., alignment, position, consistency) enforcement.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15199v1" target="_blank"><h2>Learning Where, What and How to Transfer: A Multi-Role Reinforcement Learning Approach for Evolutionary Multitasking <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jiajun Zhan, Zeyuan Ma, Yue-Jiao Gong, Kay Chen Tan<br><strong><u>Categories:</u></strong> cs.NE, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Evolutionary multitasking (EMT) algorithms typically require tailored designs for knowledge transfer, in order to assure convergence and optimality in multitask optimization. In this paper, we explore designing a systematic and generalizable knowledge transfer policy through Reinforcement Learning. We first identify three major challenges: determining the task to transfer (where), the knowledge to be transferred (what) and the mechanism for the transfer (how). To address these challenges, we formulate a multi-role RL system where three (groups of) policy networks act as specialized agents: a task routing agent incorporates an attention-based similarity recognition module to determine source-target transfer pairs via attention scores; a knowledge control agent determines the proportion of elite solutions to transfer; and a group of strategy adaptation agents control transfer strength by dynamically controlling hyper-parameters in the underlying EMT framework. Through pre-training all network modules end-to-end over an augmented multitask problem distribution, a generalizable meta-policy is obtained. Comprehensive validation experiments show state-of-the-art performance of our method against representative baselines. Further in-depth analysis not only reveals the rationale behind our proposal but also provide insightful interpretations on what the system have learned.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15196v1" target="_blank"><h2>Particle Monte Carlo methods for Lattice Field Theory <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> David Yallup<br><strong><u>Categories:</u></strong> stat.ML, cs.LG, hep-lat<br><strong><u>Comments:</u></strong> To appear in the NeurIPS 2025 workshop, Frontiers in Probabilistic Inference: Sampling Meets Learning<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> High-dimensional multimodal sampling problems from lattice field theory (LFT) have become important benchmarks for machine learning assisted sampling methods. We show that GPU-accelerated particle methods, Sequential Monte Carlo (SMC) and nested sampling, provide a strong classical baseline that matches or outperforms state-of-the-art neural samplers in sample quality and wall-clock time on standard scalar field theory benchmarks, while also estimating the partition function. Using only a single data-driven covariance for tuning, these methods achieve competitive performance without problem-specific structure, raising the bar for when learned proposals justify their training cost.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15194v1" target="_blank"><h2>Eq.Bot: Enhance Robotic Manipulation Learning via Group Equivariant Canonicalization <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jian Deng, Yuandong Wang, Yangfu Zhu, Tao Feng, Tianyu Wo, Zhenzhou Shao<br><strong><u>Categories:</u></strong> cs.RO, cs.AI<br><strong><u>Comments:</u></strong> 12 pages, 4 figures and 3 tables<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multi-modal (abstract), transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Robotic manipulation systems are increasingly deployed across diverse domains. Yet existing multi-modal learning frameworks lack inherent guarantees of geometric consistency, struggling to handle spatial transformations such as rotations and translations. While recent works attempt to introduce equivariance through bespoke architectural modifications, these methods suffer from high implementation complexity, computational cost, and poor portability. Inspired by human cognitive processes in spatial reasoning, we propose Eq.Bot, a universal canonicalization framework grounded in SE(2) group equivariant theory for robotic manipulation learning. Our framework transforms observations into a canonical space, applies an existing policy, and maps the resulting actions back to the original space. As a model-agnostic solution, Eq.Bot aims to endow models with spatial equivariance without requiring architectural modifications. Extensive experiments demonstrate the superiority of Eq.Bot under both CNN-based (e.g., CLIPort) and Transformer-based (e.g., OpenVLA-OFT) architectures over existing methods on various robotic manipulation tasks, where the most significant improvement can reach 50.0%.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15191v1" target="_blank"><h2>HISE-KT: Synergizing Heterogeneous Information Networks and LLMs for Explainable Knowledge Tracing with Meta-Path Optimization <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zhiyi Duan, Zixing Shi, Hongyu Yuan, Qi Wang<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> explainable (title, abstract)<br><p><strong><u>Abstract:</u></strong> Knowledge Tracing (KT) aims to mine students' evolving knowledge states and predict their future question-answering performance. Existing methods based on heterogeneous information networks (HINs) are prone to introducing noises due to manual or random selection of meta-paths and lack necessary quality assessment of meta-path instances. Conversely, recent large language models (LLMs)-based methods ignore the rich information across students, and both paradigms struggle to deliver consistently accurate and evidence-based explanations. To address these issues, we propose an innovative framework, HIN-LLM Synergistic Enhanced Knowledge Tracing (HISE-KT), which seamlessly integrates HINs with LLMs. HISE-KT first builds a multi-relationship HIN containing diverse node types to capture the structural relations through multiple meta-paths. The LLM is then employed to intelligently score and filter meta-path instances and retain high-quality paths, pioneering automated meta-path quality assessment. Inspired by educational psychology principles, a similar student retrieval mechanism based on meta-paths is designed to provide a more valuable context for prediction. Finally, HISE-KT uses a structured prompt to integrate the target student's history with the retrieved similar trajectories, enabling the LLM to generate not only accurate predictions but also evidence-backed, explainable analysis reports. Experiments on four public datasets show that HISE-KT outperforms existing KT baselines in both prediction performance and interpretability.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15188v2" target="_blank"><h2>BrainRotViT: Transformer-ResNet Hybrid for Explainable Modeling of Brain Aging from 3D sMRI <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Wasif Jalal, Md Nafiu Rahman, Atif Hasan Rahman, M. Sohel Rahman<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> explainable (title), transformer (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Accurate brain age estimation from structural MRI is a valuable biomarker for studying aging and neurodegeneration. Traditional regression and CNN-based methods face limitations such as manual feature engineering, limited receptive fields, and overfitting on heterogeneous data. Pure transformer models, while effective, require large datasets and high computational cost. We propose Brain ResNet over trained Vision Transformer (BrainRotViT), a hybrid architecture that combines the global context modeling of vision transformers (ViT) with the local refinement of residual CNNs. A ViT encoder is first trained on an auxiliary age and sex classification task to learn slice-level features. The frozen encoder is then applied to all sagittal slices to generate a 2D matrix of embedding vectors, which is fed into a residual CNN regressor that incorporates subject sex at the final fully-connected layer to estimate continuous brain age. Our method achieves an MAE of 3.34 years (Pearson $r=0.98$, Spearman $ρ=0.97$, $R^2=0.95$) on validation across 11 MRI datasets encompassing more than 130 acquisition sites, outperforming baseline and state-of-the-art models. It also generalizes well across 4 independent cohorts with MAEs between 3.77 and 5.04 years. Analyses on the brain age gap (the difference between the predicted age and actual age) show that aging patterns are associated with Alzheimer's disease, cognitive impairment, and autism spectrum disorder. Model attention maps highlight aging-associated regions of the brain, notably the cerebellar vermis, precentral and postcentral gyri, temporal lobes, and medial superior frontal gyrus. Our results demonstrate that this method provides an efficient, interpretable, and generalizable framework for brain-age prediction, bridging the gap between CNN- and transformer-based approaches while opening new avenues for aging and neurodegeneration research.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15183v1" target="_blank"><h2>HinTel-AlignBench: A Framework and Benchmark for Hindi-Telugu with English-Aligned Samples <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Rishikant Chigrupaatii, Ponnada Sai Tulasi Kanishka, Lalit Chandra Routhu, Martin Patel Sama Supratheek Reddy, Divyam Gupta, Dasari Srikar, Krishna Teja Kuchimanchi, Rajiv Misra, Rohun Tripathi<br><strong><u>Categories:</u></strong> cs.CL, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> With nearly 1.5 billion people and more than 120 major languages, India represents one of the most diverse regions in the world. As multilingual Vision-Language Models (VLMs) gain prominence, robust evaluation methodologies are essential to drive progress toward equitable AI for low-resource languages. Current multilingual VLM evaluations suffer from four major limitations: reliance on unverified auto-translations, narrow task/domain coverage, limited sample sizes, and lack of cultural and natively sourced Question-Answering (QA). To address these gaps, we present a scalable framework to evaluate VLMs in Indian languages and compare it with performance in English. Using the framework, we generate HinTel-AlignBench, a benchmark that draws from diverse sources in Hindi and Telugu with English-aligned samples. Our contributions are threefold: (1) a semi-automated dataset creation framework combining back-translation, filtering, and human verification; (2) the most comprehensive vision-language benchmark for Hindi and and Telugu, including adapted English datasets (VQAv2, RealWorldQA, CLEVR-Math) and native novel Indic datasets (JEE for STEM, VAANI for cultural grounding) with approximately 4,000 QA pairs per language; and (3) a detailed performance analysis of various State-of-the-Art (SOTA) open-weight and closed-source VLMs. We find a regression in performance for tasks in English versus in Indian languages for 4 out of 5 tasks across all the models, with an average regression of 8.3 points in Hindi and 5.5 points for Telugu. We categorize common failure modes to highlight concrete areas of improvement in multilingual multimodal understanding.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15175v1" target="_blank"><h2>Vehicle Routing Problems via Quantum Graph Attention Network Deep Reinforcement Learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Le Tung Giang, Vu Hoang Viet, Nguyen Xuan Tung, Trinh Van Chien, Won-Joo Hwang<br><strong><u>Categories:</u></strong> cs.LG, cs.IT, quant-ph<br><strong><u>Comments:</u></strong> 11 pages, 3 figures, 2 tables. Accepted by SOICT 2025<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> The vehicle routing problem (VRP) is a fundamental NP-hard task in intelligent transportation systems with broad applications in logistics and distribution. Deep reinforcement learning (DRL) with Graph Neural Networks (GNNs) has shown promise, yet classical models rely on large multi-layer perceptrons (MLPs) that are parameter-heavy and memory-bound. We propose a Quantum Graph Attention Network (Q-GAT) within a DRL framework, where parameterized quantum circuits (PQCs) replace conventional MLPs at critical readout stages. The hybrid model maintains the expressive capacity of graph attention encoders while reducing trainable parameters by more than 50%. Using proximal policy optimization (PPO) with greedy and stochastic decoding, experiments on VRP benchmarks show that Q-GAT achieves faster convergence and reduces routing cost by about 5% compared with classical GAT baselines. These results demonstrate the potential of PQC-enhanced GNNs as compact and effective solvers for large-scale routing and logistics optimization.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15174v1" target="_blank"><h2>FaultDiffusion: Few-Shot Fault Time Series Generation with Diffusion Model <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Yi Xu, Zhigang Chen, Rui Wang, Yangfan Li, Fengxiao Tang, Ming Zhao, Jiaqi Liu<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 4 figures, 5 tables ,8 pages<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> In industrial equipment monitoring, fault diagnosis is critical for ensuring system reliability and enabling predictive maintenance. However, the scarcity of fault data, due to the rarity of fault events and the high cost of data annotation, significantly hinders data-driven approaches. Existing time-series generation models, optimized for abundant normal data, struggle to capture fault distributions in few-shot scenarios, producing samples that lack authenticity and diversity due to the large domain gap and high intra-class variability of faults. To address this, we propose a novel few-shot fault time-series generation framework based on diffusion models. Our approach employs a positive-negative difference adapter, leveraging pre-trained normal data distributions to model the discrepancies between normal and fault domains for accurate fault synthesis. Additionally, a diversity loss is introduced to prevent mode collapse, encouraging the generation of diverse fault samples through inter-sample difference regularization. Experimental results demonstrate that our model significantly outperforms traditional methods in authenticity and diversity, achieving state-of-the-art performance on key benchmarks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15173v1" target="_blank"><h2>Data-driven Prediction of Species-Specific Plant Responses to Spectral-Shifting Films from Leaf Phenotypic and Photosynthetic Traits <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jun Hyeun Kang, Jung Eek Son, Tae In Ahn<br><strong><u>Categories:</u></strong> q-bio.QM, cs.CV, cs.LG, eess.IV<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> data-driven (title), variational autoencoder (abstract), neural network (abstract), data augmentation (abstract)<br><p><strong><u>Abstract:</u></strong> The application of spectral-shifting films in greenhouses to shift green light to red light has shown variable growth responses across crop species. However, the yield enhancement of crops under altered light quality is related to the collective effects of the specific biophysical characteristics of each species. Considering only one attribute of a crop has limitations in understanding the relationship between sunlight quality adjustments and crop growth performance. Therefore, this study aims to comprehensively link multiple plant phenotypic traits and daily light integral considering the physiological responses of crops to their growth outcomes under SF using artificial intelligence. Between 2021 and 2024, various leafy, fruiting, and root crops were grown in greenhouses covered with either PEF or SF, and leaf reflectance, leaf mass per area, chlorophyll content, daily light integral, and light saturation point were measured from the plants cultivated in each condition. 210 data points were collected, but there was insufficient data to train deep learning models, so a variational autoencoder was used for data augmentation. Most crop yields showed an average increase of 22.5% under SF. These data were used to train several models, including logistic regression, decision tree, random forest, XGBoost, and feedforward neural network (FFNN), aiming to binary classify whether there was a significant effect on yield with SF application. The FFNN achieved a high classification accuracy of 91.4% on a test dataset that was not used for training. This study provide insight into the complex interactions between leaf phenotypic and photosynthetic traits, environmental conditions, and solar spectral components by improving the ability to predict solar spectral shift effects using SF.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15172v2" target="_blank"><h2>Complex variational autoencoders admit Kähler structure <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Andrew Gracyk<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> variational autoencoder (title, abstract), VAE (abstract), latent space (abstract)<br><p><strong><u>Abstract:</u></strong> It has been discovered that latent-Euclidean variational autoencoders (VAEs) admit, in various capacities, Riemannian structure. We adapt these arguments but for complex VAEs with a complex latent stage. We show that complex VAEs reveal to some level Kähler geometric structure. Our methods will be tailored for decoder geometry. We derive the Fisher information metric in the complex case under a latent complex Gaussian regularization with trivial relation matrix. It is well known from statistical information theory that the Fisher information coincides with the Hessian of the Kullback-Leibler (KL) divergence. Thus, the metric Kähler potential relation is exactly achieved under relative entropy. We propose a Kähler potential derivative of complex Gaussian mixtures that has rough equivalence to the Fisher information metric while still being faithful to the underlying Kähler geometry. Computation of the metric via this potential is efficient, and through our potential, valid as a plurisubharmonic (PSH) function, large scale computational burden of automatic differentiation is displaced to small scale. We show that we can regularize the latent space with decoder geometry, and that we can sample in accordance with a weighted complex volume element. We demonstrate these strategies, at the exchange of sample variation, yield consistently smoother representations and fewer semantic outliers.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15167v1" target="_blank"><h2>Learning Depth from Past Selves: Self-Evolution Contrast for Robust Depth Estimation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jing Cao, Kui Jiang, Shenyi Li, Xiaocheng Feng, Yong Huang<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Self-supervised depth estimation has gained significant attention in autonomous driving and robotics. However, existing methods exhibit substantial performance degradation under adverse weather conditions such as rain and fog, where reduced visibility critically impairs depth prediction. To address this issue, we propose a novel self-evolution contrastive learning framework called SEC-Depth for self-supervised robust depth estimation tasks. Our approach leverages intermediate parameters generated during training to construct temporally evolving latency models. Using these, we design a self-evolution contrastive scheme to mitigate performance loss under challenging conditions. Concretely, we first design a dynamic update strategy of latency models for the depth estimation task to capture optimization states across training stages. To effectively leverage latency models, we introduce a self-evolution contrastive Loss (SECL) that treats outputs from historical latency models as negative samples. This mechanism adaptively adjusts learning objectives while implicitly sensing weather degradation severity, reducing the needs for manual intervention. Experiments show that our method integrates seamlessly into diverse baseline models and significantly enhances robustness in zero-shot evaluations.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15165v1" target="_blank"><h2>Can MLLMs Detect Phishing? A Comprehensive Security Benchmark Suite Focusing on Dynamic Threats and Multimodal Evaluation in Academic Environments <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Jingzhuo Zhou<br><strong><u>Categories:</u></strong> cs.CR, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> The rapid proliferation of Multimodal Large Language Models (MLLMs) has introduced unprecedented security challenges, particularly in phishing detection within academic environments. Academic institutions and researchers are high-value targets, facing dynamic, multilingual, and context-dependent threats that leverage research backgrounds, academic collaborations, and personal information to craft highly tailored attacks. Existing security benchmarks largely rely on datasets that do not incorporate specific academic background information, making them inadequate for capturing the evolving attack patterns and human-centric vulnerability factors specific to academia. To address this gap, we present AdapT-Bench, a unified methodological framework and benchmark suite for systematically evaluating MLLM defense capabilities against dynamic phishing attacks in academic settings.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15162v1" target="_blank"><h2>Multimodal Wireless Foundation Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Ahmed Aboulfotouh, Hatem Abou-Zeid<br><strong><u>Categories:</u></strong> eess.SP, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Wireless foundation models (WFMs) have recently demonstrated promising capabilities, jointly performing multiple wireless functions and adapting effectively to new environments. However, while current WFMs process only one modality, depending on the task and operating conditions, the most informative modality changes and no single modality is best for all tasks. WFMs should therefore be designed to accept multiple modalities to enable a broader and more diverse range of tasks and scenarios. In this work, we propose and build the first multimodal wireless foundation model capable of processing both raw IQ streams and image-like wireless modalities (e.g., spectrograms and CSI) and performing multiple tasks across both. We introduce masked wireless modeling for the multimodal setting, a self-supervised objective and pretraining recipe that learns a joint representation from IQ streams and image-like wireless modalities. We evaluate the model on five tasks across both modality families: image-based (human activity sensing, RF signal classification, 5G NR positioning) and IQ-based (RF device fingerprinting, interference detection/classification). The multimodal WFM is competitive with single-modality WFMs, and in several cases surpasses their performance. Our results demonstrates the strong potential of developing multimodal WFMs that support diverse wireless tasks across different modalities. We believe this provides a concrete step toward both AI-native 6G and the vision of joint sensing, communication, and localization.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15158v1" target="_blank"><h2>Non-thermal processes in standard big bang nucleosynthesis: III. Reactions with slow nuclei and the overall effect <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Victor T. Voronchev<br><strong><u>Categories:</u></strong> astro-ph.CO<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> The present paper completes a series of our works on non-thermal nuclear processes in big bang nucleosynthesis (BBN) started in JCAP05(2008)010 (Part I) and 05(2009)001 (Part II). The processes are triggered by non-Maxwellian particles naturally born in the main BBN reactions. Half of these reactions generate fast particles k^+ (= n,p,t,3He,alpha). The other half, being radiative capture processes, produce slow nuclei k^- (= d,t,3He,7Li,7Be) which can undergo (k^-,n) reactions with neutrons having large cross sections. The particle production rate R_k, thermalization time tau_k, and effective number density n_k are determined. It is shown that the values of n_k at the Universe temperatures T > 65 keV can exceed the number densities of Maxwellian 7Li and 7Be ions. To clarify the overall non-Maxwellian effect on BBN, both types of the non-Maxwellian particles are taken into account in the reaction network. Particular attention is paid to two-step sequential processes like p(n,gamma)d^-(n,gamma)t, d(p,gamma)3He^-(n,p)t, t(alpha,gamma)7Li^-(n,gamma)8Li, 3He(alpha,gamma)7Be^-(n,p)7Li, d(t,alpha)n^+(A,n)a_1a_2, and d(3He,alpha)p^+(A,p)a_1a_2 with (A,a_1,a_2) = 7Li,t,alpha) and (7Be,3He,alpha. It is obtained that the non-Maxwellian particles can selectively affect the element abundances, e.g., improve the prediction on 7Li/H by ~1.5% and at the same time leave unchanged the 4He abundance. The main conclusion however is that these particles are unable to significantly change the standard picture of BBN in general, and provide a pathway toward a solution of the cosmological lithium problem in particular.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15151v1" target="_blank"><h2>DCL-SE: Dynamic Curriculum Learning for Spatiotemporal Encoding of Brain Imaging <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Meihua Zhou, Xinyu Tong, Jiarui Zhao, Min Cheng, Li Yang, Lei Tian, Nan Wan<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> High-dimensional neuroimaging analyses for clinical diagnosis are often constrained by compromises in spatiotemporal fidelity and by the limited adaptability of large-scale, general-purpose models. To address these challenges, we introduce Dynamic Curriculum Learning for Spatiotemporal Encoding (DCL-SE), an end-to-end framework centered on data-driven spatiotemporal encoding (DaSE). We leverage Approximate Rank Pooling (ARP) to efficiently encode three-dimensional volumetric brain data into information-rich, two-dimensional dynamic representations, and then employ a dynamic curriculum learning strategy, guided by a Dynamic Group Mechanism (DGM), to progressively train the decoder, refining feature extraction from global anatomical structures to fine pathological details. Evaluated across six publicly available datasets, including Alzheimer's disease and brain tumor classification, cerebral artery segmentation, and brain age prediction, DCL-SE consistently outperforms existing methods in accuracy, robustness, and interpretability. These findings underscore the critical importance of compact, task-specific architectures in the era of large-scale pretrained networks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.16703v1" target="_blank"><h2>Pedagogic Null Tests of Dynamical Dark Energy Hints: Reconstructing LambdaCDM with Consistent BAO, CMB, and SNe Mocks <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Seokcheon Lee<br><strong><u>Categories:</u></strong> astro-ph.CO<br><strong><u>Comments:</u></strong> 23 pages, 11 figures<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> Hints of a dynamical dark-energy equation of state have appeared in several combined cosmological probes. However, such indications may instead arise from the intrinsic likelihood geometry of individual datasets, residual inter-probe tension, or restrictive priors. These factors can mimic evidence for dynamical dark energy. To clarify these issues, we perform a controlled null test. We use realistic mock BAO, CMB, and Type~Ia supernova datasets generated from a common fiducial LambdaCDM cosmology. These mocks include the DESI~DR2 BAO covariance, the Planck~2018 distance-prior covariance, and the full Pantheon+ SH0ES supernova covariance. This setup isolates physical information from geometric or statistical effects in the CPL parametrization. We find that individual probes and most two-probe combinations show apparent deviations in the (w0,wa) plane. These could be mistaken for phantom crossing or evolving dark energy. Two-probe combinations including supernovae (BAO+SNe, CMB+SNe) recover values near (w0,wa)=(-1,0) but fail to reconstruct (Omegam0,H0), because SNe do not determine the absolute distance scale. Combinations without SNe (BAO+CMB), as well as any single dataset, retain strong degeneracy directions. These produce significant shifts driven purely by likelihood geometry. These behaviors arise because BAO, CMB, and SNe each constrain only one principal direction in (w0,wa) space. Their degeneracy ridges are misaligned due to distinct redshift sensitivities. In contrast, the full BAO+CMB+SNe likelihood with proper covariance breaks all degeneracies simultaneously. It cleanly recovers the fiducial cosmology, including (w0,wa)=(-1,0) and (Omegam0,H0). Our results provide a transparent benchmark for assessing future claims of omega(z) neq -1. They emphasize the need for complete multi-probe analyses with flexible H0 and rd priors.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15139v1" target="_blank"><h2>CASPER: Cross-modal Alignment of Spatial and single-cell Profiles for Expression Recovery <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Amit Kumar, Maninder Kaur, Raghvendra Mall, Sukrit Gupta<br><strong><u>Categories:</u></strong> q-bio.GN, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Spatial Transcriptomics enables mapping of gene expression within its native tissue context, but current platforms measure only a limited set of genes due to experimental constraints and excessive costs. To overcome this, computational models integrate Single-Cell RNA Sequencing data with Spatial Transcriptomics to predict unmeasured genes. We propose CASPER, a cross-attention based framework that predicts unmeasured gene expression in Spatial Transcriptomics by leveraging centroid-level representations from Single-Cell RNA Sequencing. We performed rigorous testing over four state-of-the-art Spatial Transcriptomics/Single-Cell RNA Sequencing dataset pairs across four existing baseline models. CASPER shows significant improvement in nine out of the twelve metrics for our experiments. This work paves the way for further work in Spatial Transcriptomics to Single-Cell RNA Sequencing modality translation. The code for CASPER is available at https://github.com/AI4Med-Lab/CASPER.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15138v1" target="_blank"><h2>Cross-Modal Consistency-Guided Active Learning for Affective BCI Systems <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Hyo-Jeong Jang, Hye-Bin Shin, Kang Yin<br><strong><u>Categories:</u></strong> cs.LG, cs.HC<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> latent space (abstract)<br><p><strong><u>Abstract:</u></strong> Deep learning models perform best with abundant, high-quality labels, yet such conditions are rarely achievable in EEG-based emotion recognition. Electroencephalogram (EEG) signals are easily corrupted by artifacts and individual variability, while emotional labels often stem from subjective and inconsistent reports-making robust affective decoding particularly difficult. We propose an uncertainty-aware active learning framework that enhances robustness to label noise by jointly leveraging model uncertainty and cross-modal consistency. Instead of relying solely on EEG-based uncertainty estimates, the method evaluates cross-modal alignment to determine whether uncertainty originates from cognitive ambiguity or sensor noise. A representation alignment module embeds EEG and face features into a shared latent space, enforcing semantic coherence between modalities. Residual discrepancies are treated as noise-induced inconsistencies, and these samples are selectively queried for oracle feedback during active learning. This feedback-driven process guides the network toward reliable, informative samples and reduces the impact of noisy labels. Experiments on the ASCERTAIN dataset examine the efficiency and robustness of ours, highlighting its potential as a data-efficient and noise-tolerant approach for EEG-based affective decoding in brain-computer interface systems.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15752v1" target="_blank"><h2>Build AI Assistants using Large Language Models and Agents to Enhance the Engineering Education of Biomechanics <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Hanzhi Yan, Qin Lu, Xianqiao Wang, Xiaoming Zhai, Tianming Liu, He Li<br><strong><u>Categories:</u></strong> cs.AI, cs.MA<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> explainable (abstract)<br><p><strong><u>Abstract:</u></strong> While large language models (LLMs) have demonstrated remarkable versatility across a wide range of general tasks, their effectiveness often diminishes in domain-specific applications due to inherent knowledge gaps. Moreover, their performance typically declines when addressing complex problems that require multi-step reasoning and analysis. In response to these challenges, we propose leveraging both LLMs and AI agents to develop education assistants aimed at enhancing undergraduate learning in biomechanics courses that focus on analyzing the force and moment in the musculoskeletal system of the human body. To achieve our goal, we construct a dual-module framework to enhance LLM performance in biomechanics educational tasks: 1) we apply Retrieval-Augmented Generation (RAG) to improve the specificity and logical consistency of LLM's responses to the conceptual true/false questions; 2) we build a Multi-Agent System (MAS) to solve calculation-oriented problems involving multi-step reasoning and code execution. Specifically, we evaluate the performance of several LLMs, i.e., Qwen-1.0-32B, Qwen-2.5-32B, and Llama-70B, on a biomechanics dataset comprising 100 true/false conceptual questions and problems requiring equation derivation and calculation. Our results demonstrate that RAG significantly enhances the performance and stability of LLMs in answering conceptual questions, surpassing those of vanilla models. On the other hand, the MAS constructed using multiple LLMs demonstrates its ability to perform multi-step reasoning, derive equations, execute code, and generate explainable solutions for tasks that require calculation. These findings demonstrate the potential of applying RAG and MAS to enhance LLM performance for specialized courses in engineering curricula, providing a promising direction for developing intelligent tutoring in engineering education.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15125v1" target="_blank"><h2>Efficient RF Passive Components Modeling with Bayesian Online Learning and Uncertainty Aware Sampling <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Huifan Zhang, Pingqiang Zhou<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Conventional radio frequency (RF) passive components modeling based on machine learning requires extensive electromagnetic (EM) simulations to cover geometric and frequency design spaces, creating computational bottlenecks. In this paper, we introduce an uncertainty-aware Bayesian online learning framework for efficient parametric modeling of RF passive components, which includes: 1) a Bayesian neural network with reconfigurable heads for joint geometric-frequency domain modeling while quantifying uncertainty; 2) an adaptive sampling strategy that simultaneously optimizes training data sampling across geometric parameters and frequency domain using uncertainty guidance. Validated on three RF passive components, the framework achieves accurate modeling while using only 2.86% EM simulation time compared to traditional ML-based flow, achieving a 35 times speedup.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15122v1" target="_blank"><h2>Multi-Aspect Cross-modal Quantization for Generative Recommendation <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Fuwei Zhang, Xiaoyu Liu, Dongbo Xi, Jishen Yin, Huan Chen, Peng Yan, Fuzhen Zhuang, Zhao Zhang<br><strong><u>Categories:</u></strong> cs.IR, cs.AI<br><strong><u>Comments:</u></strong> Accepted by AAAI 2026 (Oral)<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Generative Recommendation (GR) has emerged as a new paradigm in recommender systems. This approach relies on quantized representations to discretize item features, modeling users' historical interactions as sequences of discrete tokens. Based on these tokenized sequences, GR predicts the next item by employing next-token prediction methods. The challenges of GR lie in constructing high-quality semantic identifiers (IDs) that are hierarchically organized, minimally conflicting, and conducive to effective generative model training. However, current approaches remain limited in their ability to harness multimodal information and to capture the deep and intricate interactions among diverse modalities, both of which are essential for learning high-quality semantic IDs and for effectively training GR models. To address this, we propose Multi-Aspect Cross-modal quantization for generative Recommendation (MACRec), which introduces multimodal information and incorporates it into both semantic ID learning and generative model training from different aspects. Specifically, we first introduce cross-modal quantization during the ID learning process, which effectively reduces conflict rates and thus improves codebook usability through the complementary integration of multimodal information. In addition, to further enhance the generative ability of our GR model, we incorporate multi-aspect cross-modal alignments, including the implicit and explicit alignments. Finally, we conduct extensive experiments on three well-known recommendation datasets to demonstrate the effectiveness of our proposed method.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15120v1" target="_blank"><h2>Neural Networks Learn Generic Multi-Index Models Near Information-Theoretic Limit <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Bohan Zhang, Zihao Wang, Hengyu Fu, Jason D. Lee<br><strong><u>Categories:</u></strong> stat.ML, cs.AI, cs.IT, cs.LG, math.ST<br><strong><u>Comments:</u></strong> 86 pages, 2 figures. The order of the first two authors was determined by a coin flip<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> In deep learning, a central issue is to understand how neural networks efficiently learn high-dimensional features. To this end, we explore the gradient descent learning of a general Gaussian Multi-index model $f(\boldsymbol{x})=g(\boldsymbol{U}\boldsymbol{x})$ with hidden subspace $\boldsymbol{U}\in \mathbb{R}^{r\times d}$, which is the canonical setup to study representation learning. We prove that under generic non-degenerate assumptions on the link function, a standard two-layer neural network trained via layer-wise gradient descent can agnostically learn the target with $o_d(1)$ test error using $\widetilde{\mathcal{O}}(d)$ samples and $\widetilde{\mathcal{O}}(d^2)$ time. The sample and time complexity both align with the information-theoretic limit up to leading order and are therefore optimal. During the first stage of gradient descent learning, the proof proceeds via showing that the inner weights can perform a power-iteration process. This process implicitly mimics a spectral start for the whole span of the hidden subspace and eventually eliminates finite-sample noise and recovers this span. It surprisingly indicates that optimal results can only be achieved if the first layer is trained for more than $\mathcal{O}(1)$ steps. This work demonstrates the ability of neural networks to effectively learn hierarchical functions with respect to both sample and time efficiency.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15097v1" target="_blank"><h2>MAIF: Enforcing AI Trust and Provenance with an Artifact-Centric Agentic Paradigm <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Vineeth Sai Narajala, Manish Bhatt, Idan Habler, Ronald F. Del Rosario<br><strong><u>Categories:</u></strong> cs.CR, cs.AI<br><strong><u>Comments:</u></strong> 7 Pages, 2 Figures, 6 Tables, Repo:this https URL<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> explainability (abstract), multimodal (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> The AI trustworthiness crisis threatens to derail the artificial intelligence revolution, with regulatory barriers, security vulnerabilities, and accountability gaps preventing deployment in critical domains. Current AI systems operate on opaque data structures that lack the audit trails, provenance tracking, or explainability required by emerging regulations like the EU AI Act. We propose an artifact-centric AI agent paradigm where behavior is driven by persistent, verifiable data artifacts rather than ephemeral tasks, solving the trustworthiness problem at the data architecture level. Central to this approach is the Multimodal Artifact File Format (MAIF), an AI-native container embedding semantic representations, cryptographic provenance, and granular access controls. MAIF transforms data from passive storage into active trust enforcement, making every AI operation inherently auditable. Our production-ready implementation demonstrates ultra-high-speed streaming (2,720.7 MB/s), optimized video processing (1,342 MB/s), and enterprise-grade security. Novel algorithms for cross-modal attention, semantic compression, and cryptographic binding achieve up to 225 compression while maintaining semantic fidelity. Advanced security features include stream-level access control, real-time tamper detection, and behavioral anomaly analysis with minimal overhead. This approach directly addresses the regulatory, security, and accountability challenges preventing AI deployment in sensitive domains, offering a viable path toward trustworthy AI systems at scale.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15090v1" target="_blank"><h2>BBox DocVQA: A Large Scale Bounding Box Grounded Dataset for Enhancing Reasoning in Document Visual Question Answer <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Wenhan Yu, Wang Chen, Guanqiang Qi, Weikang Li, Yang Li, Lei Sha, Deguo Xia, Jizhou Huang<br><strong><u>Categories:</u></strong> cs.DB, cs.AI, cs.CV<br><strong><u>Comments:</u></strong> 22 pages, 4 figures<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Document Visual Question Answering (DocVQA) is a fundamental task for multimodal document understanding and a key testbed for vision language reasoning. However, most existing DocVQA datasets are limited to the page level and lack fine grained spatial grounding, constraining the interpretability and reasoning capability of Vision Language Models (VLMs). To address this gap, we introduce BBox DocVQA a large scale, bounding box grounded dataset designed to enhance spatial reasoning and evidence localization in visual documents. We further present an automated construction pipeline, Segment Judge and Generate, which integrates a segment model for region segmentation, a VLM for semantic judgment, and another advanced VLM for question answer generation, followed by human verification for quality assurance. The resulting dataset contains 3.6 K diverse documents and 32 K QA pairs, encompassing single and multi region as well as single and multi page scenarios. Each QA instance is grounded on explicit bounding boxes, enabling fine grained evaluation of spatial semantic alignment. Benchmarking multiple state of the art VLMs (e.g., GPT 5, Qwen2.5 VL, and InternVL) on BBox DocVQA reveals persistent challenges in spatial grounding and reasoning accuracy. Furthermore, fine tuning on BBox DocVQA substantially improves both bounding box localization and answer generation, validating its effectiveness for enhancing the reasoning ability of VLMs. Our dataset and code will be publicly released to advance research on interpretable and spatially grounded vision language reasoning.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15083v1" target="_blank"><h2>Fourier-KAN-Mamba: A Novel State-Space Equation Approach for Time-Series Anomaly Detection <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Xiancheng Wang, Lin Wang, Rui Wang, Zhibo Zhang, Minghang Zhao<br><strong><u>Categories:</u></strong> cs.LG, eess.SP<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> anomaly detection (title, abstract)<br><p><strong><u>Abstract:</u></strong> Time-series anomaly detection plays a critical role in numerous real-world applications, including industrial monitoring and fault diagnosis. Recently, Mamba-based state-space models have shown remarkable efficiency in long-sequence modeling. However, directly applying Mamba to anomaly detection tasks still faces challenges in capturing complex temporal patterns and nonlinear dynamics. In this paper, we propose Fourier-KAN-Mamba, a novel hybrid architecture that integrates Fourier layer, Kolmogorov-Arnold Networks (KAN), and Mamba selective state-space model. The Fourier layer extracts multi-scale frequency features, KAN enhances nonlinear representation capability, and a temporal gating control mechanism further improves the model's ability to distinguish normal and anomalous patterns. Extensive experiments on MSL, SMAP, and SWaT datasets demonstrate that our method significantly outperforms existing state-of-the-art approaches.
  Keywords: time-series anomaly detection, state-space model, Mamba, Fourier transform, Kolmogorov-Arnold Network</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15062v1" target="_blank"><h2>Interpretable temporal fusion network of multi- and multi-class arrhythmia classification <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Yun Kwan Kim<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> [Doctoral dissertation, Korea University, 2025]<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Clinical decision support systems (CDSSs) have been widely utilized to support the decisions made by cardiologists when detecting and classifying arrhythmia from electrocardiograms. However, forming a CDSS for the arrhythmia classification task is challenging due to the varying lengths of arrhythmias. Although the onset time of arrhythmia varies, previously developed methods have not considered such conditions. Thus, we propose a framework that consists of (i) local and global extraction and (ii) local-global information fusion with attention to enable arrhythmia detection and classification within a constrained input length. The framework's performance was evaluated in terms of 10-class and 4-class arrhythmia detection, focusing on identifying the onset and ending point of arrhythmia episodes and their duration using the MIT-BIH arrhythmia database (MITDB) and the MIT-BIH atrial fibrillation database (AFDB). Duration, episode, and Dice score performances resulted in overall F1-scores of 96.45%, 82.05%, and 96.31% on the MITDB and 97.57%, 98.31%, and 97.45% on the AFDB, respectively. The results demonstrated statistically superior performance compared to those of the benchmark models. To assess the generalization capability of the proposed method, an MITDB-trained model and MIT-BIH malignant ventricular arrhythmia database-trained model were tested AFDB and MITDB, respectively. Superior performance was attained compared with that of a state-of-the-art model. The proposed method effectively captures both local and global information and dynamics without significant information loss. Consequently, arrhythmias can be detected with greater accuracy, and their occurrence times can be precisely determined, enabling the clinical field to develop more accurate treatment plans based on the proposed method.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15055v1" target="_blank"><h2>Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jian-Ting Guo, Yu-Cheng Chen, Ping-Chun Hsieh, Kuo-Hao Ho, Po-Wei Huang, Ti-Rong Wu, I-Chen Wu<br><strong><u>Categories:</u></strong> cs.AI, cs.LG, cs.RO<br><strong><u>Comments:</u></strong> Accepted by the Thirty-Ninth Annual Conference on Neural Information Processing Systems (NeurIPS 2025)<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> VAE (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Human-like agents have long been one of the goals in pursuing artificial intelligence. Although reinforcement learning (RL) has achieved superhuman performance in many domains, relatively little attention has been focused on designing human-like RL agents. As a result, many reward-driven RL agents often exhibit unnatural behaviors compared to humans, raising concerns for both interpretability and trustworthiness. To achieve human-like behavior in RL, this paper first formulates human-likeness as trajectory optimization, where the objective is to find an action sequence that closely aligns with human behavior while also maximizing rewards, and adapts the classic receding-horizon control to human-like learning as a tractable and efficient implementation. To achieve this, we introduce Macro Action Quantization (MAQ), a human-like RL framework that distills human demonstrations into macro actions via Vector-Quantized VAE. Experiments on D4RL Adroit benchmarks show that MAQ significantly improves human-likeness, increasing trajectory similarity scores, and achieving the highest human-likeness rankings among all RL agents in the human evaluation study. Our results also demonstrate that MAQ can be easily integrated into various off-the-shelf RL algorithms, opening a promising direction for learning human-like RL agents. Our code is available at https://rlg.iis.sinica.edu.tw/papers/MAQ.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15048v1" target="_blank"><h2>Oversampling techniques for predicting COVID-19 patient length of stay <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Zachariah Farahany, Jiawei Wu, K M Sajjadul Islam, Praveen Madiraju<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 10 pages, 2022 IEEE International Conference on Big Data (Big Data)<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> COVID-19 is a respiratory disease that caused a global pandemic in 2019. It is highly infectious and has the following symptoms: fever or chills, cough, shortness of breath, fatigue, muscle or body aches, headache, the new loss of taste or smell, sore throat, congestion or runny nose, nausea or vomiting, and diarrhea. These symptoms vary in severity; some people with many risk factors have been known to have lengthy hospital stays or die from the disease. In this paper, we analyze patients' electronic health records (EHR) to predict the severity of their COVID-19 infection using the length of stay (LOS) as our measurement of severity. This is an imbalanced classification problem, as many people have a shorter LOS rather than a longer one. To combat this problem, we synthetically create alternate oversampled training data sets. Once we have this oversampled data, we run it through an Artificial Neural Network (ANN), which during training has its hyperparameters tuned using Bayesian optimization. We select the model with the best F1 score and then evaluate it and discuss it.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15046v1" target="_blank"><h2>UniHOI: Unified Human-Object Interaction Understanding via Unified Token Space <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Panqi Yang, Haodong Jing, Nanning Zheng, Yongqiang Ma<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> Accepted by AAAI 2026,9 pages, 4 figures<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> In the field of human-object interaction (HOI), detection and generation are two dual tasks that have traditionally been addressed separately, hindering the development of comprehensive interaction understanding. To address this, we propose UniHOI, which jointly models HOI detection and generation via a unified token space, thereby effectively promoting knowledge sharing and enhancing generalization. Specifically, we introduce a symmetric interaction-aware attention module and a unified semi-supervised learning paradigm, enabling effective bidirectional mapping between images and interaction semantics even under limited annotations. Extensive experiments demonstrate that UniHOI achieves state-of-the-art performance in both HOI detection and generation. Specifically, UniHOI improves accuracy by 4.9% on long-tailed HOI detection and boosts interaction metrics by 42.0% on open-vocabulary generation tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15750v1" target="_blank"><h2>Writing With Machines and Peers: Designing for Critical Engagement with Generative AI <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Xinran Zhu, Cong Wang, Duane Searsmith<br><strong><u>Categories:</u></strong> cs.CY, cs.AI, cs.ET, cs.HC<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> literature review (abstract)<br><p><strong><u>Abstract:</u></strong> The growing integration of generative AI in higher education is transforming how students write, learn, and engage with knowledge. As AI tools become more integrated into classrooms, there is an urgent need for pedagogical approaches that help students use them critically and reflectively. This study proposes a pedagogical design that integrates AI and peer feedback in a graduate-level academic writing activity. Over eight weeks, students developed literature review projects through multiple writing and revision stages, receiving feedback from both a custom-built AI reviewer and human peers. We examine two questions: (1) How did students interact with and incorporate AI and peer feedback during the writing process? and (2) How did they reflect on and build relationships with both human and AI reviewers? Data sources include student writing artifacts, AI and peer feedback, AI chat logs, and student reflections. Findings show that students engaged differently with each feedback source-relying on AI for rubric alignment and surface-level edits, and on peer feedback for conceptual development and disciplinary relevance. Reflections revealed evolving relationships with AI, characterized by increasing confidence, strategic use, and critical awareness of its limitations. The pedagogical design supported writing development, AI literacy, and disciplinary understanding. This study offers a scalable pedagogical model for integrating AI into writing instruction and contributes insights for system-level approaches to fostering meaningful human-AI collaboration in higher education.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15010v1" target="_blank"><h2>Latent space analysis and generalization to out-of-distribution data <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Katie Rainey, Erin Hausmann, Donald Waagen, David Gray, Donald Hulsey<br><strong><u>Categories:</u></strong> stat.ML, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> latent space (title, abstract)<br><p><strong><u>Abstract:</u></strong> Understanding the relationships between data points in the latent decision space derived by the deep learning system is critical to evaluating and interpreting the performance of the system on real world data. Detecting \textit{out-of-distribution} (OOD) data for deep learning systems continues to be an active research topic. We investigate the connection between latent space OOD detection and classification accuracy of the model. Using open source simulated and measured Synthetic Aperture RADAR (SAR) datasets, we empirically demonstrate that the OOD detection cannot be used as a proxy measure for model performance. We hope to inspire additional research into the geometric properties of the latent space that may yield future insights into deep learning robustness and generalizability.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15006v1" target="_blank"><h2>Toward Complete Merger Identification at Cosmic Noon with Deep Learning <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Aimee Schechter, Aleksandra Ciprijanovic, Rebecca Nevin, Julie Comerford, Xuejian Shen, Aaron Stemo, Laura Blecha<br><strong><u>Categories:</u></strong> astro-ph.GA<br><strong><u>Comments:</u></strong> 5 pages, accepted to the NeurIPS 2025 Machine Learning for the Physical Sciences Workshop<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> latent space (abstract)<br><p><strong><u>Abstract:</u></strong> As we enter the era of large imaging surveys such as $\textit{Roman}$, Rubin, and $\textit{Euclid}$, a deeper understanding of potential biases and selection effects in optical astronomical catalogs created with the use of ML-based methods is paramount. This work focuses on a deeper understanding of the performance and limitations of deep learning-based classifiers as tools for galaxy merger identification. We train a ResNet18 model on mock Hubble Space Telescope CANDELS images from the IllustrisTNG50 simulation. Our focus is on a more challenging classification of galaxy mergers and nonmergers at higher redshifts $1<z<1.5$, including minor mergers and lower mass galaxies down to the stellar mass of $10^8 M_\odot$. We demonstrate, for the first time, that a deep learning model, such as the one developed in this work, can successfully identify even minor and low mass mergers even at these redshifts. Our model achieves overall accuracy, purity, and completeness of 73%. We show that some galaxy mergers can only be identified from certain observation angles, leading to a potential upper limit in overall accuracy. Using Grad-CAMs and UMAPs, we more deeply examine the performance and observe a visible gradient in the latent space with stellar mass and specific star formation rate, but no visible gradient with merger mass ratio or merger stage.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15003v1" target="_blank"><h2>Resource-Based Time and Cost Prediction in Project Networks: From Statistical Modeling to Graph Neural Networks <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Reza Mirjalili, Behrad Braghi, Shahram Shadrokh Sikari<br><strong><u>Categories:</u></strong> stat.AP, cs.LG<br><strong><u>Comments:</u></strong> 52 pages, 12 figures<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> explainable (abstract), neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> Accurate prediction of project duration and cost remains one of the most challenging aspects of project management, particularly in resource-constrained and interdependent task networks. Traditional analytical techniques such as the Critical Path Method (CPM) and Program Evaluation and Review Technique (PERT) rely on simplified and often static assumptions regarding task interdependencies and resource performance. This study proposes a novel resource-based predictive framework that integrates network representations of project activities with graph neural networks (GNNs) to capture structural and contextual relationships among tasks, resources, and time-cost dynamics. The model represents the project as a heterogeneous activity-resource graph in which nodes denote activities and resources, and edges encode temporal and resource dependencies.
  We evaluate multiple learning paradigms, including GraphSAGE and Temporal Graph Networks, on both synthetic and benchmark project datasets. Experimental results show that the proposed GNN framework achieves an average 23 to 31 percent reduction in mean absolute error compared to traditional regression and tree-based methods, while improving the coefficient of determination R2 from approximately 0.78 to 0.91 for large and complex project networks. Furthermore, the learned embeddings provide interpretable insights into resource bottlenecks and critical dependencies, enabling more explainable and adaptive scheduling decisions.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14997v1" target="_blank"><h2>A Neural Network Approach to Preferred Event Selection for Low-Latency Gravitational-Wave Alerts <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Pratyusava Baral, Cody Messick, Patrick Brady<br><strong><u>Categories:</u></strong> astro-ph.IM, astro-ph.HE, gr-qc<br><strong><u>Comments:</u></strong> 24 pages, 7 figures<br><strong><u>Published:</u></strong> 2025-11-19<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> The LIGO-Virgo-KAGRA collaboration uses multiple independent search pipelines to detect gravitational waves, often resulting in multiple triggers (g-events) for a single astrophysical source. These triggers are grouped into superevents, raising a critical question for multimessenger astronomy: which g-event provides the most accurate sky localization for electromagnetic follow-up? Currently, the g-event with the highest signal-to-noise ratio (SNR) is selected, under the assumption that it should provide the best estimators of the source's parameters, including its location on the sky. Analysis of simulated signals reveals systematic deviations from this expectation. In particular, a false-alarm rate (FAR)-based selector performs slightly better than the SNR-based method, but introduces pipeline biases. We present a neural network-based selector trained on simulated signals to identify the g-event with the minimum searched area -- a metric quantifying localization accuracy. The network uses information (detector SNRs, FAR, and chirp mass) from all of the triggers associated with each astrophysical source and is designed to be pipeline-agnostic. Our results show that the neural network outperforms both traditional selectors, achieving a mean searched area ~2% smaller than the SNR-based selector. Unlike FAR-based selection, the neural network preserves the underlying distribution of pipeline contributions, avoiding systematic biases toward specific pipelines. The network can be trained in approximately one minute on a few thousand events and performs event selection instantaneously, making it suitable for low-latency applications. These results demonstrate that machine learning can enhance multimessenger astronomy capabilities while maintaining fairness across detection pipelines. We recommend implementing this approach for future observing runs.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14981v1" target="_blank"><h2>Logit-Based Losses Limit the Effectiveness of Feature Knowledge Distillation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Nicholas Cooper, Lijun Chen, Sailesh Dwivedy, Danna Gurari<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> NeurIPS Workshop on Symmetry and Geometry in Neural Representations (NeurReps), December 2025<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract), transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Knowledge distillation (KD) methods can transfer knowledge of a parameter-heavy teacher model to a light-weight student model. The status quo for feature KD methods is to utilize loss functions based on logits (i.e., pre-softmax class scores) and intermediate layer features (i.e., latent representations). Unlike previous approaches, we propose a feature KD framework for training the student's backbone using feature-based losses exclusively (i.e., without logit-based losses such as cross entropy). Leveraging recent discoveries about the geometry of latent representations, we introduce a knowledge quality metric for identifying which teacher layers provide the most effective knowledge for distillation. Experiments on three image classification datasets with four diverse student-teacher pairs, spanning convolutional neural networks and vision transformers, demonstrate our KD method achieves state-of-the-art performance, delivering top-1 accuracy boosts of up to 15% over standard approaches. We publically share our code to facilitate future work at https://github.com/Thegolfingocto/KD_wo_CE.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14970v1" target="_blank"><h2>EGSA-PT:Edge-Guided Spatial Attention with Progressive Training for Monocular Depth Estimation and Segmentation of Transparent Objects <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Gbenga Omotara, Ramy Farag, Seyed Mohamad Ali Tousi, G. N. DeSouza<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.RO<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> multi-modal (abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Transparent object perception remains a major challenge in computer vision research, as transparency confounds both depth estimation and semantic segmentation. Recent work has explored multi-task learning frameworks to improve robustness, yet negative cross-task interactions often hinder performance. In this work, we introduce Edge-Guided Spatial Attention (EGSA), a fusion mechanism designed to mitigate destructive interactions by incorporating boundary information into the fusion between semantic and geometric features. On both Syn-TODD and ClearPose benchmarks, EGSA consistently improved depth accuracy over the current state of the art method (MODEST), while preserving competitive segmentation performance, with the largest improvements appearing in transparent regions. Besides our fusion design, our second contribution is a multi-modal progressive training strategy, where learning transitions from edges derived from RGB images to edges derived from predicted depth images. This approach allows the system to bootstrap learning from the rich textures contained in RGB images, and then switch to more relevant geometric content in depth maps, while it eliminates the need for ground-truth depth at training time. Together, these contributions highlight edge-guided fusion as a robust approach capable of improving transparent object perception.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14969v1" target="_blank"><h2>Quality-Controlled Multimodal Emotion Recognition in Conversations with Identity-Based Transfer Learning and MAMBA Fusion <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zanxu Wang, Homayoon Beigi<br><strong><u>Categories:</u></strong> eess.AS, cs.AI, cs.LG, eess.IV, eess.SP<br><strong><u>Comments:</u></strong> 8 pages, 14 images, 3 tables, Recognition Technologies, Inc. Technical Report RTI-20251118-01<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract), transfer learning (title, abstract)<br><p><strong><u>Abstract:</u></strong> This paper addresses data quality issues in multimodal emotion recognition in conversation (MERC) through systematic quality control and multi-stage transfer learning. We implement a quality control pipeline for MELD and IEMOCAP datasets that validates speaker identity, audio-text alignment, and face detection. We leverage transfer learning from speaker and face recognition, assuming that identity-discriminative embeddings capture not only stable acoustic and Facial traits but also person-specific patterns of emotional expression. We employ RecoMadeEasy(R) engines for extracting 512-dimensional speaker and face embeddings, fine-tune MPNet-v2 for emotion-aware text representations, and adapt these features through emotion-specific MLPs trained on unimodal datasets. MAMBA-based trimodal fusion achieves 64.8% accuracy on MELD and 74.3% on IEMOCAP. These results show that combining identity-based audio and visual embeddings with emotion-tuned text representations on a quality-controlled subset of data yields consistent competitive performance for multimodal emotion recognition in conversation and provides a basis for further improvement on challenging, low-frequency emotion classes.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14962v1" target="_blank"><h2>Reconstruction of three-dimensional shapes of normal and disease-related erythrocytes from partial observations using multi-fidelity neural networks <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Haizhou Wen, He Li, Zhen Li<br><strong><u>Categories:</u></strong> physics.comp-ph, cs.LG, eess.IV, physics.bio-ph, q-bio.QM<br><strong><u>Comments:</u></strong> 29 pages, 10 figures, 3 appendices<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> Reconstruction of 3D erythrocyte or red blood cell (RBC) morphology from partial observations, such as microscope images, is essential for understanding the physiology of RBC aging and the pathology of various RBC disorders. In this study, we propose a multi-fidelity neural network (MFNN) approach to fuse high-fidelity cross-sections of an RBC, with a morphologically similar low-fidelity reference 3D RBC shape to recover its full 3D surface. The MFNN predictor combines a convolutional neural network trained on low-fidelity reference RBC data with a feedforward neural network that captures nonlinear morphological correlations, and augments training with surface area and volume constraints for regularization in the low-fidelity branch. This approach is theoretically grounded by a topological homeomorphism between a sphere and 3D RBC surfaces, with training data generated by dissipative particle dynamics simulations of stomatocyte-discocyte-echinocyte transformation. Benchmarking across diverse RBC shapes observed in normal and aged populations, our results show that the MFNN predictor can reconstruct complex RBC morphologies with over 95% coordinate accuracy when provided with at least two orthogonal cross-sections. It is observed that informative oblique cross-sections intersecting spicule tips of echinocytes improve both local and global feature reconstruction, highlighting the value of feature-aware sampling. Our study further evaluates the influence of sampling strategies, shape dissimilarity, and noise, showing enhanced robustness under physically constrained training. Altogether, these results demonstrate the capability of MFNN to reconstruct the 3D shape of normal and aged RBCs from partial cross-sections as observed in conventional microscope images, which could facilitate the quantitative analysis of RBC morphological parameters in normal and disease-related RBC samples.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14961v1" target="_blank"><h2>Knowledge Graphs as Structured Memory for Embedding Spaces: From Training Clusters to Explainable Inference <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Artur A. Oliveira, Mateus Espadoto, Roberto M. Cesar, Roberto Hirata<br><strong><u>Categories:</u></strong> cs.LG, cs.CV<br><strong><u>Comments:</u></strong> Submitted to GRIVAPP 2026 (21st International Conference on Computer Graphics, Interaction, Visualization Theory and Applications), Marbella, Spain, March 9-11 2026<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> explainable (title)<br><p><strong><u>Abstract:</u></strong> We introduce Graph Memory (GM), a structured non-parametric framework that augments embedding-based inference with a compact, relational memory over region-level prototypes. Rather than treating each training instance in isolation, GM summarizes the embedding space into prototype nodes annotated with reliability indicators and connected by edges that encode geometric and contextual relations. This design unifies instance retrieval, prototype-based reasoning, and graph-based label propagation within a single inductive model that supports both efficient inference and faithful explanation. Experiments on synthetic and real datasets including breast histopathology (IDC) show that GM achieves accuracy competitive with $k$NN and Label Spreading while offering substantially better calibration and smoother decision boundaries, all with an order of magnitude fewer samples. By explicitly modeling reliability and relational structure, GM provides a principled bridge between local evidence and global consistency in non-parametric learning.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14953v1" target="_blank"><h2>Compiling to recurrent neurons <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Joey Velez-Ginorio, Nada Amin, Konrad Kording, Steve Zdancewic<br><strong><u>Categories:</u></strong> cs.PL, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Discrete structures are currently second-class in differentiable programming. Since functions over discrete structures lack overt derivatives, differentiable programs do not differentiate through them and limit where they can be used. For example, when programming a neural network, conditionals and iteration cannot be used everywhere; they can break the derivatives necessary for gradient-based learning to work. This limits the class of differentiable algorithms we can directly express, imposing restraints on how we build neural networks and differentiable programs more generally. However, these restraints are not fundamental. Recent work shows conditionals can be first-class, by compiling them into differentiable form as linear neurons. Similarly, this work shows iteration can be first-class -- by compiling to linear recurrent neurons. We present a minimal typed, higher-order and linear programming language with iteration called $\textsf{Cajal}\scriptstyle(\mathbb{\multimap}, \mathbb{2}, \mathbb{N})$. We prove its programs compile correctly to recurrent neurons, allowing discrete algorithms to be expressed in a differentiable form compatible with gradient-based learning. With our implementation, we conduct two experiments where we link these recurrent neurons against a neural network solving an iterative image transformation task. This determines part of its function prior to learning. As a result, the network learns faster and with greater data-efficiency relative to a neural network programmed without first-class iteration. A key lesson is that recurrent neurons enable a rich interplay between learning and the discrete structures of ordinary programming.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14952v1" target="_blank"><h2>Artificial intelligence approaches for energy-efficient laser cutting machines <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Mohamed Abdallah Salem, Hamdy Ahmed Ashour, Ahmed Elshenawy<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.LG, cs.RO<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract), transfer learning (abstract)<br><p><strong><u>Abstract:</u></strong> This research addresses the significant challenges of energy consumption and environmental impact in laser cutting by proposing novel deep learning (DL) methodologies to achieve energy reduction. Recognizing the current lack of adaptive control and the open-loop nature of CO2 laser suction pumps, this study utilizes closed-loop configurations that dynamically adjust pump power based on both the material being cut and the smoke level generated. To implement this adaptive system, diverse material classification methods are introduced, including techniques leveraging lens-less speckle sensing with a customized Convolutional Neural Network (CNN) and an approach using a USB camera with transfer learning via the pre-trained VGG16 CNN model. Furthermore, a separate DL model for smoke level detection is employed to simultaneously refine the pump's power output. This integration prompts the exhaust suction pump to automatically halt during inactive times and dynamically adjust power during operation, leading to experimentally proven and remarkable energy savings, with results showing a 20% to 50% reduction in the smoke suction pump's energy consumption, thereby contributing substantially to sustainable development in the manufacturing sector.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14922v1" target="_blank"><h2>Integrating Causal Inference with Graph Neural Networks for Alzheimer's Disease Analysis <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Pranay Kumar Peddi, Dhrubajyoti Ghosh<br><strong><u>Categories:</u></strong> cs.LG, stat.ME<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (title)<br><p><strong><u>Abstract:</u></strong> Deep graph learning has advanced Alzheimer's (AD) disease classification from MRI, but most models remain correlational, confounding demographic and genetic factors with disease specific features. We present Causal-GCN, an interventional graph convolutional framework that integrates do-calculus-based back-door adjustment to identify brain regions exerting stable causal influence on AD progression. Each subject's MRI is represented as a structural connectome where nodes denote cortical and subcortical regions and edges encode anatomical connectivity. Confounders such as age, sec, and APOE4 genotype are summarized via principal components and included in the causal adjustment set. After training, interventions on individual regions are simulated by serving their incoming edges and altering node features to estimate average causal effects on disease probability. Applied to 484 subjects from the ADNI cohort, Causal-GCN achieves performance comparable to baseline GNNs while providing interpretable causal effect rankings that highlight posterior, cingulate, and insular hubs consistent with established AD neuropathology.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14920v1" target="_blank"><h2>Structured Contrastive Learning for Interpretable Latent Representations <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Zhengyang Shen, Hua Tu, Mayue Shi<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> Comments: 10 pages, 6 figures. Applications to medical signal retrieval and activity recognition. Correspondence: m.shi16@imperial.this http URL<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> latent space (abstract), neural network (abstract), data augmentation (abstract)<br><p><strong><u>Abstract:</u></strong> Neural networks exhibit severe brittleness to semantically irrelevant transformations. A mere 75ms electrocardiogram (ECG) phase shift degrades latent cosine similarity from 1.0 to 0.2, while sensor rotations collapse activity recognition performance with inertial measurement units (IMUs). We identify the root cause as "laissez-faire" representation learning, where latent spaces evolve unconstrained provided task performance is satisfied. We propose Structured Contrastive Learning (SCL), a framework that partitions latent space representations into three semantic groups: invariant features that remain consistent under given transformations (e.g., phase shifts or rotations), variant features that actively differentiate transformations via a novel variant mechanism, and free features that preserve task flexibility. This creates controllable push-pull dynamics where different latent dimensions serve distinct, interpretable purposes. The variant mechanism enhances contrastive learning by encouraging variant features to differentiate within positive pairs, enabling simultaneous robustness and interpretability. Our approach requires no architectural modifications and integrates seamlessly into existing training pipelines. Experiments on ECG phase invariance and IMU rotation robustness demonstrate superior performance: ECG similarity improves from 0.25 to 0.91 under phase shifts, while WISDM activity recognition achieves 86.65% accuracy with 95.38% rotation consistency, consistently outperforming traditional data augmentation. This work represents a paradigm shift from reactive data augmentation to proactive structural learning, enabling interpretable latent representations in neural networks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14916v1" target="_blank"><h2>SN 2023taz: Implications for the UV Diversity of Superluminous Supernovae <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Aysha Aamer, Matt Nicholl, Charlotte Angus, Shubham Srivastav, Jeff Cooke, Natasha Van Bemmel, Frédérick Poidevin, Stefan Geier, Joseph P. Anderson, Thomas de Boer, Kenneth C. Chambers, Ting-Wan Chen, Mariusz Gromadzki, Claudia P. Gutiérrez, Erkki Kankare, Réka Könyves-Tóth, Chien-Cheng Lin, Thomas B. Lowe, Eugene Magnier, Paolo Mazzali, Kyle Medler, Paloma Minguez, Tomás E. Müller-Bravo, Ben Warwick<br><strong><u>Categories:</u></strong> astro-ph.HE<br><strong><u>Comments:</u></strong> Submitted to ApJ<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> VAE (title, abstract)<br><p><strong><u>Abstract:</u></strong> Superluminous supernovae (SLSNe) are some of the brightest explosions in the Universe representing the extremes of stellar deaths. At the upper end of their distribution is SN\,2023taz, one of the most luminous SLSNe discovered to date with a peak absolute magnitude of $M_{g,\rm{peak}}=-22.75 \pm 0.03$ and a lower limit for energy radiated of $E=2.9 \times 10^{51}$\,erg. Magnetar model fits reveal individual parameter values typical of the SLSN population, but the combination of a low $B$-field and ejecta mass with a short spin period places SN\,2023taz in a unusual region of parameter space, accounting for its extreme luminosity. The optical data around peak are consistent with a temperature of $\sim$17\,000\,K but SN\,2023taz shows a surprising deficit in the UV compared to other events in this temperature range. We find no indication of dust extinction that could plausibly explain the UV deficit. The lower level of UV flux is reminiscent of the absorption seen in lower-luminosity events like SN\,2017dwh, where Fe-group elements are responsible for the effect. However, in the case of SN\,2023taz, there is no evidence for a larger amount of Fe-group elements which could contribute to line blanketing. Comparing to SLSNe with well-observed UV spectra, an underlying temperature of $8000-9000$\,K would match the UV spectral slope, but is not consistent with the optical colour temperatures of these events. The most likely explanation is enhanced absorption by intermediate-mass elements, challenging previous findings that SLSNe exhibit similar UV absorption line equivalent widths. This highlights the need for expanded UV spectroscopic coverage of SLSNe, especially at early times, to build a framework for interpreting their diversity and to enable classification at higher redshifts where optical observations will exclusively probe rest-frame UV emission.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14889v1" target="_blank"><h2>Bringing Federated Learning to Space <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Grace Kim, Filip Svoboda, Nicholas Lane<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 15 pages, 9 figures, 3 tables accepted to IEEE Aeroconf 2026<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> As Low Earth Orbit (LEO) satellite constellations rapidly expand to hundreds and thousands of spacecraft, the need for distributed on-board machine learning becomes critical to address downlink bandwidth limitations. Federated learning (FL) offers a promising framework to conduct collaborative model training across satellite networks. Realizing its benefits in space naturally requires addressing space-specific constraints, from intermittent connectivity to dynamics imposed by orbital motion. This work presents the first systematic feasibility analysis of adapting off-the-shelf FL algorithms for satellite constellation deployment. We introduce a comprehensive "space-ification" framework that adapts terrestrial algorithms (FedAvg, FedProx, FedBuff) to operate under orbital constraints, producing an orbital-ready suite of FL algorithms. We then evaluate these space-ified methods through extensive parameter sweeps across 768 constellation configurations that vary cluster sizes (1-10), satellites per cluster (1-10), and ground station networks (1-13). Our analysis demonstrates that space-adapted FL algorithms efficiently scale to constellations of up to 100 satellites, achieving performance close to the centralized ideal. Multi-month training cycles can be reduced to days, corresponding to a 9x speedup through orbital scheduling and local coordination within satellite clusters. These results provide actionable insights for future mission designers, enabling distributed on-board learning for more autonomous, resilient, and data-driven satellite operations.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15743v1" target="_blank"><h2>Connecting the Dots: A Machine Learning Ready Dataset for Ionospheric Forecasting Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Linnea M. Wolniewicz, Halil S. Kelebek, Simone Mestici, Michael D. Vergalla, Giacomo Acciarini, Bala Poduval, Olga Verkhoglyadova, Madhulika Guhathakurta, Thomas E. Berger, Atılım Güneş Baydin, Frank Soboczenski<br><strong><u>Categories:</u></strong> cs.LG, astro-ph.EP, astro-ph.IM<br><strong><u>Comments:</u></strong> 8 pages, 2 figures, 2 tables. Accepted as a poster presentation in the Machine Learning for the Physical Sciences workshop at NeurIPS 2025<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> Operational forecasting of the ionosphere remains a critical space weather challenge due to sparse observations, complex coupling across geospatial layers, and a growing need for timely, accurate predictions that support Global Navigation Satellite System (GNSS), communications, aviation safety, as well as satellite operations. As part of the 2025 NASA Heliolab, we present a curated, open-access dataset that integrates diverse ionospheric and heliospheric measurements into a coherent, machine learning-ready structure, designed specifically to support next-generation forecasting models and address gaps in current operational frameworks. Our workflow integrates a large selection of data sources comprising Solar Dynamic Observatory data, solar irradiance indices (F10.7), solar wind parameters (velocity and interplanetary magnetic field), geomagnetic activity indices (Kp, AE, SYM-H), and NASA JPL's Global Ionospheric Maps of Total Electron Content (GIM-TEC). We also implement geospatially sparse data such as the TEC derived from the World-Wide GNSS Receiver Network and crowdsourced Android smartphone measurements. This novel heterogeneous dataset is temporally and spatially aligned into a single, modular data structure that supports both physical and data-driven modeling. Leveraging this dataset, we train and benchmark several spatiotemporal machine learning architectures for forecasting vertical TEC under both quiet and geomagnetically active conditions. This work presents an extensive dataset and modeling pipeline that enables exploration of not only ionospheric dynamics but also broader Sun-Earth interactions, supporting both scientific inquiry and operational forecasting efforts.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14887v1" target="_blank"><h2>Transformer-Guided Deep Reinforcement Learning for Optimal Takeoff Trajectory Design of an eVTOL Drone <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Nathan M. Roberts, Xiaosong Du<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> Conference version with 12 pages and 2 figures<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> The rapid advancement of electric vertical take-off and landing (eVTOL) aircraft offers a promising opportunity to alleviate urban traffic congestion. Thus, developing optimal takeoff trajectories for minimum energy consumption becomes essential for broader eVTOL aircraft applications. Conventional optimal control methods (such as dynamic programming and linear quadratic regulator) provide highly efficient and well-established solutions but are limited by problem dimensionality and complexity. Deep reinforcement learning (DRL) emerges as a special type of artificial intelligence tackling complex, nonlinear systems; however, the training difficulty is a key bottleneck that limits DRL applications. To address these challenges, we propose the transformer-guided DRL to alleviate the training difficulty by exploring a realistic state space at each time step using a transformer. The proposed transformer-guided DRL was demonstrated on an optimal takeoff trajectory design of an eVTOL drone for minimal energy consumption while meeting takeoff conditions (i.e., minimum vertical displacement and minimum horizontal velocity) by varying control variables (i.e., power and wing angle to the vertical). Results presented that the transformer-guided DRL agent learned to take off with $4.57\times10^6$ time steps, representing 25% of the $19.79\times10^6$ time steps needed by a vanilla DRL agent. In addition, the transformer-guided DRL achieved 97.2% accuracy on the optimal energy consumption compared against the simulation-based optimal reference while the vanilla DRL achieved 96.3% accuracy. Therefore, the proposed transformer-guided DRL outperformed vanilla DRL in terms of both training efficiency as well as optimal design verification.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14868v1" target="_blank"><h2>Hierarchical Token Prepending: Enhancing Information Flow in Decoder-based LLM Embeddings <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Xueying Ding, Xingyue Huang, Mingxuan Ju, Liam Collins, Yozen Liu, Leman Akoglu, Neil Shah, Tong Zhao<br><strong><u>Categories:</u></strong> cs.CL, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Large language models produce powerful text embeddings, but their causal attention mechanism restricts the flow of information from later to earlier tokens, degrading representation quality. While recent methods attempt to solve this by prepending a single summary token, they over-compress information, hence harming performance on long documents. We propose Hierarchical Token Prepending (HTP), a method that resolves two critical bottlenecks. To mitigate attention-level compression, HTP partitions the input into blocks and prepends block-level summary tokens to subsequent blocks, creating multiple pathways for backward information flow. To address readout-level over-squashing, we replace last-token pooling with mean-pooling, a choice supported by theoretical analysis. HTP achieves consistent performance gains across 11 retrieval datasets and 30 general embedding benchmarks, especially in long-context settings. As a simple, architecture-agnostic method, HTP enhances both zero-shot and finetuned models, offering a scalable route to superior long-document embeddings.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14865v1" target="_blank"><h2>FinTRec: Transformer Based Unified Contextual Ads Targeting and Personalization for Financial Applications <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Dwipam Katariya, Snehita Varma, Akshat Shreemali, Benjamin Wu, Kalanand Mishra, Pranab Mohanty<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 10 pages, 7 figures, Accepted at CARS @ RecSys 2025<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> explainability (abstract), transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> Transformer-based architectures are widely adopted in sequential recommendation systems, yet their application in Financial Services (FS) presents distinct practical and modeling challenges for real-time recommendation. These include:a) long-range user interactions (implicit and explicit) spanning both digital and physical channels generating temporally heterogeneous context, b) the presence of multiple interrelated products require coordinated models to support varied ad placements and personalized feeds, while balancing competing business goals. We propose FinTRec, a transformer-based framework that addresses these challenges and its operational objectives in FS. While tree-based models have traditionally been preferred in FS due to their explainability and alignment with regulatory requirements, our study demonstrate that FinTRec offers a viable and effective shift toward transformer-based architectures. Through historic simulation and live A/B test correlations, we show FinTRec consistently outperforms the production-grade tree-based baseline. The unified architecture, when fine-tuned for product adaptation, enables cross-product signal sharing, reduces training cost and technical debt, while improving offline performance across all products. To our knowledge, this is the first comprehensive study of unified sequential recommendation modeling in FS that addresses both technical and business considerations.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14860v1" target="_blank"><h2>When CNNs Outperform Transformers and Mambas: Revisiting Deep Architectures for Dental Caries Segmentation <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Aashish Ghimire, Jun Zeng, Roshan Paudel, Nikhil Kumar Tomar, Deepak Ranjan Nayak, Harshith Reddy Nalla, Vivek Jha, Glenda Reynolds, Debesh Jha<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> 8 pages, 4 figures<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract), transformer (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Accurate identification and segmentation of dental caries in panoramic radiographs are critical for early diagnosis and effective treatment planning. Automated segmentation remains challenging due to low lesion contrast, morphological variability, and limited annotated data. In this study, we present the first comprehensive benchmarking of convolutional neural networks, vision transformers and state-space mamba architectures for automated dental caries segmentation on panoramic radiographs through a DC1000 dataset. Twelve state-of-the-art architectures, including VMUnet, MambaUNet, VMUNetv2, RMAMamba-S, TransNetR, PVTFormer, DoubleU-Net, and ResUNet++, were trained under identical configurations. Results reveal that, contrary to the growing trend toward complex attention based architectures, the CNN-based DoubleU-Net achieved the highest dice coefficient of 0.7345, mIoU of 0.5978, and precision of 0.8145, outperforming all transformer and Mamba variants. In the study, the top 3 results across all performance metrics were achieved by CNN-based architectures. Here, Mamba and transformer-based methods, despite their theoretical advantage in global context modeling, underperformed due to limited data and weaker spatial priors. These findings underscore the importance of architecture-task alignment in domain-specific medical image segmentation more than model complexity. Our code is available at: https://github.com/JunZengz/dental-caries-segmentation.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14858v1" target="_blank"><h2>Large Language Model Driven Analysis of General Coordinates Network (GCN) Circulars <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Vidushi Sharma, Ronit Agarwala, Judith L. Racusin, Leo P. Singer, Tyler Barna, Eric Burns, Michael W. Coughlin, Dakota Dutko, Courey Elliott, Rahul Gupta, Ashish Mahabal, Nikhil Mukund<br><strong><u>Categories:</u></strong> astro-ph.HE, astro-ph.IM<br><strong><u>Comments:</u></strong> 61 pages, 11 figures, 7 tables. Accepted for publication in ApJS<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> time-domain (abstract)<br><p><strong><u>Abstract:</u></strong> The General Coordinates Network (GCN) is NASA's time-domain and multi-messenger alert system. GCN distributes two data products - automated ``Notices,'' and human-generated ``Circulars,'' that report the observations of high-energy and multi-messenger astronomical transients. The flexible and non-structured format of GCN Circulars, comprising of more than 40500 Circulars accumulated over three decades, makes it challenging to manually extract observational information, such as redshift or observed wavebands. In this work, we employ large language models (LLMs) to facilitate the automated parsing of transient reports. We develop a neural topic modeling pipeline with open-source tools for the automatic clustering and summarization of astrophysical topics in the Circulars database. Using neural topic modeling and contrastive fine-tuning, we classify Circulars based on their observation wavebands and messengers. Additionally, we separate gravitational wave (GW) event clusters and their electromagnetic (EM) counterparts from the Circulars database. Finally, using the open-source Mistral model, we implement a system to automatically extract gamma-ray burst (GRB) redshift information from the Circulars archive, without the need for any training. Evaluation against the manually curated Neil Gehrels Swift Observatory GRB table shows that our simple system, with the help of prompt-tuning, output parsing, and retrieval augmented generation (RAG), can achieve an accuracy of 97.2 % for redshift-containing Circulars. Our neural search enhanced RAG pipeline accurately retrieved 96.8 % of redshift circulars from the manually curated database. Our study demonstrates the potential of LLMs, to automate and enhance astronomical text mining, and provides a foundation work for future advances in transient alert analysis.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14857v1" target="_blank"><h2>511 keV Gamma Ray Echo from Particle Decays in Supernovae <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Garv Chauhan, Cecilia Lunardini<br><strong><u>Categories:</u></strong> hep-ph, astro-ph.HE<br><strong><u>Comments:</u></strong> 5+3 pages, 1 table, 2+2 figures<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> VAE (title)<br><p><strong><u>Abstract:</u></strong> The formation of a hot and dense core in a core-collapse supernova (SN) can produce massive Beyond Standard Model (BSM) particles. These particles can decay in the stellar envelope, generating positrons either directly or through secondary processes involving neutrinos or photons. We show for the first time that such positrons regardless of their production channel, can thermalize and annihilate at rest with ambient electrons in the outer SN envelope, producing a characteristic echo of 511 keV gamma rays. For axion-like particles (ALPs), we derive bounds on the ALP-photon coupling ($G_{a γ}$) using Pioneer Venus Orbiter observations of SN 1987A. We also evaluate the sensitivity of upcoming MeV gap gamma-ray telescopes in the 511 keV range, such as COSI and AMEGO, for future Galactic SNe, which can improve existing constraints or enable ALP discovery. The echo signal is a generic prediction for any particle species that efficiently produces positrons near the stellar surface.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14844v1" target="_blank"><h2>J-HERTz: J-PLUS Heritage Exploration of Radio Targets at z $<$ 5 <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> D. Fernández Gil, J. A. Fernández-Ontiveros, C. López-Sanjuan, F. Arizo-Borillo, A. del Pino, A. Hernán-Caballero, A. Lumbreras-Calle, Rahna P. T., David Sobral, H. Vázquez Ramió, A. J. Cenarro, A. Marín-Franch, R. E. Angulo, A. Ederoclite, D. Cristóbal-Hornillos, R. A. Dupke, C. Hernández-Monteagudo, M. Moles, L. Sodré, J. Varela<br><strong><u>Categories:</u></strong> astro-ph.GA<br><strong><u>Comments:</u></strong> 27 pages, 15 figures, 4 tables. Accepted for publication in ApJS. DOI:https://doi.org/10.3847/1538-4365/ae2016<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> We introduce J-HERTz (J-PLUS Heritage Exploration of Radio Targets at $z < 5$), a new multi-wavelength catalog that combines optical narrow-band photometry from J-PLUS, infrared observations from WISE, and deep low-frequency radio data from LoTSS for nearly half a million sources across 2,100 deg$^2$ of the northern sky. Key innovations of J-HERTz include Bayesian neural network classifications for 390,000 galaxies, 31,000 quasars, and 20,000 stars, along with significantly improved photometric redshifts for 235,000 galaxies compared to previous J-PLUS DR3 and LoTSS DR2 estimates. We identify 831 candidate Galactic radio stars, which, if confirmed, would constitute a significant addition to the number of radio-emitting stars identified to date. Among radio-loud galaxies with spectroscopic observations, $\gtrsim$20% lack Seyfert or LINER signatures, indicating a substantial population of optically quiescent radio galaxies, in agreement with previous works. Spectral energy distribution fitting of their host galaxies using J-PLUS photospectra reveals systematically low specific star formation rates, consistent with quenched stellar populations. J-HERTz thus provides a powerful dataset to exploit radio-optical synergies, enabling studies that span from the origin of stellar radio emission to the AGN life cycle and the role of jet activity in shaping host galaxy evolution.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14832v1" target="_blank"><h2>How to pick the best anomaly detector? <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Marie Hein, Gregor Kasieczka, Michael Krämer, Louis Moureaux, Alexander Mück, David Shih<br><strong><u>Categories:</u></strong> hep-ph, cs.LG, hep-ex, physics.data-an<br><strong><u>Comments:</u></strong> 12 pages, 7 figures<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), anomaly detection (abstract)<br><p><strong><u>Abstract:</u></strong> Anomaly detection has the potential to discover new physics in unexplored regions of the data. However, choosing the best anomaly detector for a given data set in a model-agnostic way is an important challenge which has hitherto largely been neglected. In this paper, we introduce the data-driven ARGOS metric, which has a sound theoretical foundation and is empirically shown to robustly select the most sensitive anomaly detection model given the data. Focusing on weakly-supervised, classifier-based anomaly detection methods, we show that the ARGOS metric outperforms other model selection metrics previously used in the literature, in particular the binary cross-entropy loss. We explore several realistic applications, including hyperparameter tuning as well as architecture and feature selection, and in all cases we demonstrate that ARGOS is robust to the noisy conditions of anomaly detection.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14761v1" target="_blank"><h2>ARC Is a Vision Problem! <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Keya Hu, Ali Cy, Linlu Qiu, Xiaoman Delores Ding, Runqian Wang, Yeyin Eva Zhu, Jacob Andreas, Kaiming He<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> Technical Report. Project webpage:this https URL<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> The Abstraction and Reasoning Corpus (ARC) is designed to promote research on abstract reasoning, a fundamental aspect of human intelligence. Common approaches to ARC treat it as a language-oriented problem, addressed by large language models (LLMs) or recurrent reasoning models. However, although the puzzle-like tasks in ARC are inherently visual, existing research has rarely approached the problem from a vision-centric perspective. In this work, we formulate ARC within a vision paradigm, framing it as an image-to-image translation problem. To incorporate visual priors, we represent the inputs on a "canvas" that can be processed like natural images. It is then natural for us to apply standard vision architectures, such as a vanilla Vision Transformer (ViT), to perform image-to-image mapping. Our model is trained from scratch solely on ARC data and generalizes to unseen tasks through test-time training. Our framework, termed Vision ARC (VARC), achieves 60.4% accuracy on the ARC-1 benchmark, substantially outperforming existing methods that are also trained from scratch. Our results are competitive with those of leading LLMs and close the gap to average human performance.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14753v1" target="_blank"><h2>SparseST: Exploiting Data Sparsity in Spatiotemporal Modeling and Prediction <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Junfeng Wu, Hadjer Benmeziane, Kaoutar El Maghraoui, Liu Liu, Yinan Wang<br><strong><u>Categories:</u></strong> cs.LG, cs.CE<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract)<br><p><strong><u>Abstract:</u></strong> Spatiotemporal data mining (STDM) has a wide range of applications in various complex physical systems (CPS), i.e., transportation, manufacturing, healthcare, etc. Among all the proposed methods, the Convolutional Long Short-Term Memory (ConvLSTM) has proved to be generalizable and extendable in different applications and has multiple variants achieving state-of-the-art performance in various STDM applications. However, ConvLSTM and its variants are computationally expensive, which makes them inapplicable in edge devices with limited computational resources. With the emerging need for edge computing in CPS, efficient AI is essential to reduce the computational cost while preserving the model performance. Common methods of efficient AI are developed to reduce redundancy in model capacity (i.e., model pruning, compression, etc.). However, spatiotemporal data mining naturally requires extensive model capacity, as the embedded dependencies in spatiotemporal data are complex and hard to capture, which limits the model redundancy. Instead, there is a fairly high level of data and feature redundancy that introduces an unnecessary computational burden, which has been largely overlooked in existing research. Therefore, we developed a novel framework SparseST, that pioneered in exploiting data sparsity to develop an efficient spatiotemporal model. In addition, we explore and approximate the Pareto front between model performance and computational efficiency by designing a multi-objective composite loss function, which provides a practical guide for practitioners to adjust the model according to computational resource constraints and the performance requirements of downstream tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14744v1" target="_blank"><h2>Measuring AI Progress in Drug Discovery: A Reproducible Leaderboard for the Tox21 Challenge <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Antonia Ebner, Christoph Bartmann, Sonja Topf, Sohvi Luukkonen, Johannes Schimunek, Günter Klambauer<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Deep learning's rise since the early 2010s has transformed fields like computer vision and natural language processing and strongly influenced biomedical research. For drug discovery specifically, a key inflection - akin to vision's "ImageNet moment" - arrived in 2015, when deep neural networks surpassed traditional approaches on the Tox21 Data Challenge. This milestone accelerated the adoption of deep learning across the pharmaceutical industry, and today most major companies have integrated these methods into their research pipelines. After the Tox21 Challenge concluded, its dataset was included in several established benchmarks, such as MoleculeNet and the Open Graph Benchmark. However, during these integrations, the dataset was altered and labels were imputed or manufactured, resulting in a loss of comparability across studies. Consequently, the extent to which bioactivity and toxicity prediction methods have improved over the past decade remains unclear. To this end, we introduce a reproducible leaderboard, hosted on Hugging Face with the original Tox21 Challenge dataset, together with a set of baseline and representative methods. The current version of the leaderboard indicates that the original Tox21 winner - the ensemble-based DeepTox method - and the descriptor-based self-normalizing neural networks introduced in 2017, continue to perform competitively and rank among the top methods for toxicity prediction, leaving it unclear whether substantial progress in toxicity prediction has been achieved over the past decade. As part of this work, we make all baselines and evaluated models publicly accessible for inference via standardized API calls to Hugging Face Spaces.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14727v1" target="_blank"><h2>The first data-driven bounds on the quantum decoherence of inflationary gravitational waves <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jessie de Kruijf, Giacomo Galloni, Nicola Bartolo<br><strong><u>Categories:</u></strong> astro-ph.CO<br><strong><u>Comments:</u></strong> Prepared for submission to JCAP<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> data-driven (title)<br><p><strong><u>Abstract:</u></strong> The (large-scale) structures we observe in the Universe are classical, but within the inflationary scenario they do originate from quantum fluctuations. This leads to the question: ''How did this quantum-to-classical transition occur?''. A potential explanation is quantum decoherence due to interactions between different fields present during inflation. The tensor modes (i.e. primordial gravitational waves) can interact with a scalar sector, causing their quantum decoherence to occur and inducing a change in the gravitational wave (GW) background. The power spectrum of these GWs can be constrained using the upper bounds found by Planck, BICEP/Keck Array, LIGO-Virgo-KAGRA, Big Bang Nucleosynthesis, and the Pulsar Timing Array detections. These impose constraints on the interaction between the fields. We find that the observational upper bounds mainly constrain scenarios with a strong interaction, especially if the interaction is also strongly time dependent. Furthermore, we find which observationally allowed scenarios have not completed decoherence by the end of inflation, thus possibly leaving quantum signatures in the GW background. Lastly, we show that, interestingly enough, there are decoherence scenarios corresponding to the signal observed by PTA experiments. This highlights the importance of the quantum decoherence effect on GWs.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14721v1" target="_blank"><h2>AdamHD: Decoupled Huber Decay Regularization for Language Model Pre-Training <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Fu-Ming Guo, Yingfang Fan<br><strong><u>Categories:</u></strong> cs.LG, math.OC<br><strong><u>Comments:</u></strong> 39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: GPU-Accelerated and Scalable Optimization (ScaleOpt)<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Adaptive optimizers with decoupled weight decay, such as AdamW, are the de facto standard for pre-training large transformer-based generative models. Yet the quadratic nature of the $\ell_2$ penalty embedded in weight decay drives all parameters toward the origin at the same rate, making the update vulnerable to rare but extreme gradient directions and often over-penalizing well-conditioned coordinates. We propose AdamHuberDecay, a drop-in replacement for AdamW that substitutes the $\ell_2$ penalty with a decoupled smooth Huber regularizer. The resulting update decays parameters quadratically while their magnitude remains below a threshold $δ$, and linearly ($\ell_1$-like) once they exceed $δ$, yielding (i) bounded regularization gradients, (ii) invariance to per-coordinate second-moment rescaling, and (iii) stronger sparsity pressure on overgrown weights.
  We derive the closed-form decoupled Huber decay step and show how to integrate it with any Adam-family optimizer at $O(1)$ extra cost. Extensive experiments on GPT-2 and GPT-3 pre-training demonstrate that AdamHuberDecay (a) converges 10-15% faster in wall-clock time, (b) reduces validation perplexity by up to 4 points, (c) delivers performance improvements of 2.5-4.7% across downstream tasks, and (d) yields visibly sparser weight histograms that translate into 20-30% memory savings after magnitude pruning, without tuning the decay coefficient beyond the default grid used for AdamW. Ablations confirm robustness to outlier gradients and large-batch regimes, together with theoretical analyses that bound the expected parameter norm under noisy updates. AdamHuberDecay therefore provides a simple, principled path toward more efficient and resilient training of next-generation foundational generative transformers.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14710v1" target="_blank"><h2>Towards a Unified Analysis of Neural Networks in Nonparametric Instrumental Variable Regression: Optimization and Generalization <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zonghao Chen, Atsushi Nitanda, Arthur Gretton, Taiji Suzuki<br><strong><u>Categories:</u></strong> stat.ML, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> We establish the first global convergence result of neural networks for two stage least squares (2SLS) approach in nonparametric instrumental variable regression (NPIV). This is achieved by adopting a lifted perspective through mean-field Langevin dynamics (MFLD), unlike standard MFLD, however, our setting of 2SLS entails a \emph{bilevel} optimization problem in the space of probability measures. To address this challenge, we leverage the penalty gradient approach recently developed for bilevel optimization which formulates bilevel optimization as a Lagrangian problem. This leads to a novel fully first-order algorithm, termed \texttt{F$^2$BMLD}. Apart from the convergence bound, we further provide a generalization bound, revealing an inherent trade-off in the choice of the Lagrange multiplier between optimization and statistical guarantees. Finally, we empirically validate the effectiveness of the proposed method on an offline reinforcement learning benchmark.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14702v1" target="_blank"><h2>Seeing Beyond the Image: ECG and Anatomical Knowledge-Guided Myocardial Scar Segmentation from Late Gadolinium-Enhanced Images <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Farheen Ramzan, Yusuf Kiberu, Nikesh Jathanna, Meryem Jabrane, Vicente Grau, Shahnaz Jamil-Copley, Richard H. Clayton, Chen, Chen<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Accurate segmentation of myocardial scar from late gadolinium enhanced (LGE) cardiac MRI is essential for evaluating tissue viability, yet remains challenging due to variable contrast and imaging artifacts. Electrocardiogram (ECG) signals provide complementary physiological information, as conduction abnormalities can help localize or suggest scarred myocardial regions. In this work, we propose a novel multimodal framework that integrates ECG-derived electrophysiological information with anatomical priors from the AHA-17 atlas for physiologically consistent LGE-based scar segmentation. As ECGs and LGE-MRIs are not acquired simultaneously, we introduce a Temporal Aware Feature Fusion (TAFF) mechanism that dynamically weights and fuses features based on their acquisition time difference. Our method was evaluated on a clinical dataset and achieved substantial gains over the state-of-the-art image-only baseline (nnU-Net), increasing the average Dice score for scars from 0.6149 to 0.8463 and achieving high performance in both precision (0.9115) and sensitivity (0.9043). These results show that integrating physiological and anatomical knowledge allows the model to "see beyond the image", setting a new direction for robust and physiologically grounded cardiac scar segmentation.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14698v1" target="_blank"><h2>HyMAD: A Hybrid Multi-Activity Detection Approach for Border Surveillance and Monitoring <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Sriram Srinivasan, Srinivasan Aruchamy, Siva Ram Krisha Vadali<br><strong><u>Categories:</u></strong> cs.CV, cs.LG, eess.SP<br><strong><u>Comments:</u></strong> Multi-label seismic signal classification using novel attention-based feature fusion. Submitting to cs.CV due to relevance to general pattern recognition and time-frequency (spectrogram) analysis<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Seismic sensing has emerged as a promising solution for border surveillance and monitoring; the seismic sensors that are often buried underground are small and cannot be noticed easily, making them difficult for intruders to detect, avoid, or vandalize. This significantly enhances their effectiveness compared to highly visible cameras or fences. However, accurately detecting and distinguishing between overlapping activities that are happening simultaneously, such as human intrusions, animal movements, and vehicle rumbling, remains a major challenge due to the complex and noisy nature of seismic signals. Correctly identifying simultaneous activities is critical because failing to separate them can lead to misclassification, missed detections, and an incomplete understanding of the situation, thereby reducing the reliability of surveillance systems. To tackle this problem, we propose HyMAD (Hybrid Multi-Activity Detection), a deep neural architecture based on spatio-temporal feature fusion. The framework integrates spectral features extracted with SincNet and temporal dependencies modeled by a recurrent neural network (RNN). In addition, HyMAD employs self-attention layers to strengthen intra-modal representations and a cross-modal fusion module to achieve robust multi-label classification of seismic events. e evaluate our approach on a dataset constructed from real-world field recordings collected in the context of border surveillance and monitoring, demonstrating its ability to generalize to complex, simultaneous activity scenarios involving humans, animals, and vehicles. Our method achieves competitive performance and offers a modular framework for extending seismic-based activity recognition in real-world security applications.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14694v1" target="_blank"><h2>Near-Lossless Model Compression Enables Longer Context Inference in DNA Large Language Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Rui Zhu, Xiaopu Zhou, Haixu Tang, Stephen W. Scherer, Lucila Ohno-Machado<br><strong><u>Categories:</u></strong> q-bio.GN, cs.AI, cs.LG, q-bio.PE<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Trained on massive cross-species DNA corpora, DNA large language models (LLMs) learn the fundamental "grammar" and evolutionary patterns of genomic sequences. This makes them powerful priors for DNA sequence modeling, particularly over long ranges. However, two major constraints hinder their use in practice: the quadratic computational cost of self-attention and the growing memory required for key-value (KV) caches during autoregressive decoding. These constraints force the use of heuristics such as fixed-window truncation or sliding windows, which compromise fidelity on ultra-long sequences by discarding distant information. We introduce FOCUS (Feature-Oriented Compression for Ultra-long Self-attention), a progressive context-compression module that can be plugged into pretrained DNA LLMs. FOCUS combines the established k-mer representation in genomics with learnable hierarchical compression: it inserts summary tokens at k-mer granularity and progressively compresses attention key and value activations across multiple Transformer layers, retaining only the summary KV states across windows while discarding ordinary-token KV. A shared-boundary windowing scheme yields a stationary cross-window interface that propagates long-range information with minimal loss. We validate FOCUS on an Evo-2-based DNA LLM fine-tuned on GRCh38 chromosome 1 with self-supervised training and randomized compression schedules to promote robustness across compression ratios. On held-out human chromosomes, FOCUS achieves near-lossless fidelity: compressing a 1 kb context into only 10 summary tokens (about 100x) shifts the average per-nucleotide probability by only about 0.0004. Compared to a baseline without compression, FOCUS reduces KV-cache memory and converts effective inference scaling from O(N^2) to near-linear O(N), enabling about 100x longer inference windows on commodity GPUs with near-lossless fidelity.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14691v1" target="_blank"><h2>Attention via Synaptic Plasticity is All You Need: A Biologically Inspired Spiking Neuromorphic Transformer <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Kallol Mondal, Ankush Kumar<br><strong><u>Categories:</u></strong> cs.NE, cs.AI, cs.CV, cs.ET, stat.ML<br><strong><u>Comments:</u></strong> 21 Pages, 5 Figures, 3 Table<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> explainable (abstract), neural network (abstract), transformer (title, abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Attention is the brain's ability to selectively focus on a few specific aspects while ignoring irrelevant ones. This biological principle inspired the attention mechanism in modern Transformers. Transformers now underpin large language models (LLMs) such as GPT, but at the cost of massive training and inference energy, leading to a large carbon footprint. While brain attention emerges from neural circuits, Transformer attention relies on dot-product similarity to weight elements in the input sequence. Neuromorphic computing, especially spiking neural networks (SNNs), offers a brain-inspired path to energy-efficient intelligence. Despite recent work on attention-based spiking Transformers, the core attention layer remains non-neuromorphic. Current spiking attention (i) relies on dot-product or element-wise similarity suited to floating-point operations, not event-driven spikes; (ii) keeps attention matrices that suffer from the von Neumann bottleneck, limiting in-memory computing; and (iii) still diverges from brain-like computation. To address these issues, we propose the Spiking STDP Transformer (S$^{2}$TDPT), a neuromorphic Transformer that implements self-attention through spike-timing-dependent plasticity (STDP), embedding query--key correlations in synaptic weights. STDP, a core mechanism of memory and learning in the brain and widely studied in neuromorphic devices, naturally enables in-memory computing and supports non-von Neumann hardware. On CIFAR-10 and CIFAR-100, our model achieves 94.35\% and 78.08\% accuracy with only four timesteps and 0.49 mJ on CIFAR-100, an 88.47\% energy reduction compared to a standard ANN Transformer. Grad-CAM shows that the model attends to semantically relevant regions, enhancing interpretability. Overall, S$^{2}$TDPT illustrates how biologically inspired attention can yield energy-efficient, hardware-friendly, and explainable neuromorphic models.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14654v1" target="_blank"><h2>Improving segmentation of retinal arteries and veins using cardiac signal in doppler holograms <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Marius Dubosc, Yann Fischer, Zacharie Auray, Nicolas Boutry, Edwin Carlinet, Michael Atlan, Thierry Geraud<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> 5 pages, 3 figures, 1 table. Submitted to ISBI2026<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Doppler holography is an emerging retinal imaging technique that captures the dynamic behavior of blood flow with high temporal resolution, enabling quantitative assessment of retinal hemodynamics. This requires accurate segmentation of retinal arteries and veins, but traditional segmentation methods focus solely on spatial information and overlook the temporal richness of holographic data. In this work, we propose a simple yet effective approach for artery-vein segmentation in temporal Doppler holograms using standard segmentation architectures. By incorporating features derived from a dedicated pulse analysis pipeline, our method allows conventional U-Nets to exploit temporal dynamics and achieve performance comparable to more complex attention- or iteration-based models. These findings demonstrate that time-resolved preprocessing can unlock the full potential of deep learning for Doppler holography, opening new perspectives for quantitative exploration of retinal hemodynamics. The dataset is publicly available at https://huggingface.co/datasets/DigitalHolography/</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14640v1" target="_blank"><h2>Doppler Invariant CNN for Signal Classification <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Avi Bagchi, Dwight Hutchenson<br><strong><u>Categories:</u></strong> eess.SP, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Radio spectrum monitoring in contested environments motivates the need for reliable automatic signal classification technology. Prior work highlights deep learning as a promising approach, but existing models depend on brute-force Doppler augmentation to achieve real-world generalization, which undermines both training efficiency and interpretability. In this paper, we propose a convolutional neural network (CNN) architecture with complex-valued layers that exploits convolutional shift equivariance in the frequency domain. To establish provable frequency bin shift invariance, we use adaptive polyphase sampling (APS) as pooling layers followed by a global average pooling layer at the end of the network. Using a synthetic dataset of common interference signals, experimental results demonstrate that unlike a vanilla CNN, our model maintains consistent classification accuracy with and without random Doppler shifts despite being trained on no Doppler-shifted examples. Overall, our method establishes an invariance-driven framework for signal classification that offers provable robustness against real-world effects.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14632v1" target="_blank"><h2>Adapformer: Adaptive Channel Management for Multivariate Time Series Forecasting <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Yuchen Luo, Xinyu Li, Liuhua Peng, Mingming Gong<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> In multivariate time series forecasting (MTSF), accurately modeling the intricate dependencies among multiple variables remains a significant challenge due to the inherent limitations of traditional approaches. Most existing models adopt either \textbf{channel-independent} (CI) or \textbf{channel-dependent} (CD) strategies, each presenting distinct drawbacks. CI methods fail to leverage the potential insights from inter-channel interactions, resulting in models that may not fully exploit the underlying statistical dependencies present in the data. Conversely, CD approaches often incorporate too much extraneous information, risking model overfitting and predictive inefficiency. To address these issues, we introduce the Adaptive Forecasting Transformer (\textbf{Adapformer}), an advanced Transformer-based framework that merges the benefits of CI and CD methodologies through effective channel management. The core of Adapformer lies in its dual-stage encoder-decoder architecture, which includes the \textbf{A}daptive \textbf{C}hannel \textbf{E}nhancer (\textbf{ACE}) for enriching embedding processes and the \textbf{A}daptive \textbf{C}hannel \textbf{F}orecaster (\textbf{ACF}) for refining the predictions. ACE enhances token representations by selectively incorporating essential dependencies, while ACF streamlines the decoding process by focusing on the most relevant covariates, substantially reducing noise and redundancy. Our rigorous testing on diverse datasets shows that Adapformer achieves superior performance over existing models, enhancing both predictive accuracy and computational efficiency, thus making it state-of-the-art in MTSF.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14631v1" target="_blank"><h2>Enhancing Agentic Autonomous Scientific Discovery with Vision-Language Model Capabilities <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Kahaan Gandhi, Boris Bolliet, Inigo Zubeldia<br><strong><u>Categories:</u></strong> cs.CL, cs.AI, cs.CV, cs.MA<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> We show that multi-agent systems guided by vision-language models (VLMs) improve end-to-end autonomous scientific discovery. By treating plots as verifiable checkpoints, a VLM-as-a-judge evaluates figures against dynamically generated domain-specific rubrics, enabling agents to correct their own errors and steer exploratory data analysis in real-time. Case studies in cosmology and astrochemistry demonstrate recovery from faulty reasoning paths and adaptation to new datasets without human intervention. On a 10-task benchmark for data-driven discovery, VLM-augmented systems achieve pass at 1 scores of 0.7-0.8, compared to 0.2-0.3 for code-only and 0.4-0.5 for code-and-text baselines, while also providing auditable reasoning traces that improve interpretability. Code available here: https://github.com/CMBAgents/cmbagent</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14606v1" target="_blank"><h2>Bridging Human and Model Perspectives: A Comparative Analysis of Political Bias Detection in News Media Using Large Language Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Shreya Adrita Banik, Niaz Nafi Rahman, Tahsina Moiukh, Farig Sadeque<br><strong><u>Categories:</u></strong> cs.CL, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Detecting political bias in news media is a complex task that requires interpreting subtle linguistic and contextual cues. Although recent advances in Natural Language Processing (NLP) have enabled automatic bias classification, the extent to which large language models (LLMs) align with human judgment still remains relatively underexplored and not yet well understood. This study aims to present a comparative framework for evaluating the detection of political bias across human annotations and multiple LLMs, including GPT, BERT, RoBERTa, and FLAN. We construct a manually annotated dataset of news articles and assess annotation consistency, bias polarity, and inter-model agreement to quantify divergence between human and model perceptions of bias. Experimental results show that among traditional transformer-based models, RoBERTa achieves the highest alignment with human labels, whereas generative models such as GPT demonstrate the strongest overall agreement with human annotations in a zero-shot setting. Among all transformer-based baselines, our fine-tuned RoBERTa model acquired the highest accuracy and the strongest alignment with human-annotated labels. Our findings highlight systematic differences in how humans and LLMs perceive political slant, underscoring the need for hybrid evaluation frameworks that combine human interpretability with model scalability in automated media bias detection.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14603v1" target="_blank"><h2>A Method for Characterizing Disease Progression from Acute Kidney Injury to Chronic Kidney Disease <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yilu Fang, Jordan G. Nestor, Casey N. Ta, Jerard Z. Kneifati-Hayek, Chunhua Weng<br><strong><u>Categories:</u></strong> cs.CL, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> Patients with acute kidney injury (AKI) are at high risk of developing chronic kidney disease (CKD), but identifying those at greatest risk remains challenging. We used electronic health record (EHR) data to dynamically track AKI patients' clinical evolution and characterize AKI-to-CKD progression. Post-AKI clinical states were identified by clustering patient vectors derived from longitudinal medical codes and creatinine measurements. Transition probabilities between states and progression to CKD were estimated using multi-state modeling. After identifying common post-AKI trajectories, CKD risk factors in AKI subpopulations were identified through survival analysis. Of 20,699 patients with AKI at admission, 3,491 (17%) developed CKD. We identified fifteen distinct post-AKI states, each with different probabilities of CKD development. Most patients (75%, n=15,607) remained in a single state or made only one transition during the study period. Both established (e.g., AKI severity, diabetes, hypertension, heart failure, liver disease) and novel CKD risk factors, with their impact varying across these clinical states. This study demonstrates a data-driven approach for identifying high-risk AKI patients, supporting the development of decision-support tools for early CKD detection and intervention.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14601v1" target="_blank"><h2>MRI Embeddings Complement Clinical Predictors for Cognitive Decline Modeling in Alzheimer's Disease Cohorts <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Nathaniel Putera, Daniel Vilet Rodríguez, Noah Videcrantz, Julia Machnio, Mostafa Mehdipour Ghazi<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> Accepted at SPIE - Medical Imaging Conference 2026<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), multimodal (abstract), transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Accurate modeling of cognitive decline in Alzheimer's disease is essential for early stratification and personalized management. While tabular predictors provide robust markers of global risk, their ability to capture subtle brain changes remains limited. In this study, we evaluate the predictive contributions of tabular and imaging-based representations, with a focus on transformer-derived Magnetic Resonance Imaging (MRI) embeddings. We introduce a trajectory-aware labeling strategy based on Dynamic Time Warping clustering to capture heterogeneous patterns of cognitive change, and train a 3D Vision Transformer (ViT) via unsupervised reconstruction on harmonized and augmented MRI data to obtain anatomy-preserving embeddings without progression labels. The pretrained encoder embeddings are subsequently assessed using both traditional machine learning classifiers and deep learning heads, and compared against tabular representations and convolutional network baselines. Results highlight complementary strengths across modalities. Clinical and volumetric features achieved the highest AUCs of around 0.70 for predicting mild and severe progression, underscoring their utility in capturing global decline trajectories. In contrast, MRI embeddings from the ViT model were most effective in distinguishing cognitively stable individuals with an AUC of 0.71. However, all approaches struggled in the heterogeneous moderate group. These findings indicate that clinical features excel in identifying high-risk extremes, whereas transformer-based MRI embeddings are more sensitive to subtle markers of stability, motivating multimodal fusion strategies for AD progression modeling.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14599v1" target="_blank"><h2>CCSD: Cross-Modal Compositional Self-Distillation for Robust Brain Tumor Segmentation with Missing Modalities <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Dongqing Xie, Yonghuang Wu, Zisheng Ai, Jun Min, Zhencun Jiang, Shaojin Geng, Lei Wang<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> 9 pages, 5 figures<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> multi-modal (abstract)<br><p><strong><u>Abstract:</u></strong> The accurate segmentation of brain tumors from multi-modal MRI is critical for clinical diagnosis and treatment planning. While integrating complementary information from various MRI sequences is a common practice, the frequent absence of one or more modalities in real-world clinical settings poses a significant challenge, severely compromising the performance and generalizability of deep learning-based segmentation models. To address this challenge, we propose a novel Cross-Modal Compositional Self-Distillation (CCSD) framework that can flexibly handle arbitrary combinations of input modalities. CCSD adopts a shared-specific encoder-decoder architecture and incorporates two self-distillation strategies: (i) a hierarchical modality self-distillation mechanism that transfers knowledge across modality hierarchies to reduce semantic discrepancies, and (ii) a progressive modality combination distillation approach that enhances robustness to missing modalities by simulating gradual modality dropout during training. Extensive experiments on public brain tumor segmentation benchmarks demonstrate that CCSD achieves state-of-the-art performance across various missing-modality scenarios, with strong generalization and stability.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15741v1" target="_blank"><h2>Uncertainty-Resilient Multimodal Learning via Consistency-Guided Cross-Modal Transfer <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Hyo-Jeong Jang<br><strong><u>Categories:</u></strong> cs.AI, cs.HC, cs.LG<br><strong><u>Comments:</u></strong> Master's thesis, Korea University, 2025<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> latent space (abstract), multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Multimodal learning systems often face substantial uncertainty due to noisy data, low-quality labels, and heterogeneous modality characteristics. These issues become especially critical in human-computer interaction settings, where data quality, semantic reliability, and annotation consistency vary across users and recording conditions. This thesis tackles these challenges by exploring uncertainty-resilient multimodal learning through consistency-guided cross-modal transfer. The central idea is to use cross-modal semantic consistency as a basis for robust representation learning. By projecting heterogeneous modalities into a shared latent space, the proposed framework mitigates modality gaps and uncovers structural relations that support uncertainty estimation and stable feature learning. Building on this foundation, the thesis investigates strategies to enhance semantic robustness, improve data efficiency, and reduce the impact of noise and imperfect supervision without relying on large, high-quality annotations. Experiments on multimodal affect-recognition benchmarks demonstrate that consistency-guided cross-modal transfer significantly improves model stability, discriminative ability, and robustness to noisy or incomplete supervision. Latent space analyses further show that the framework captures reliable cross-modal structure even under challenging conditions. Overall, this thesis offers a unified perspective on resilient multimodal learning by integrating uncertainty modeling, semantic alignment, and data-efficient supervision, providing practical insights for developing reliable and adaptive brain-computer interface systems.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14577v1" target="_blank"><h2>Estimating differential pistons for the Extremely Large Telescope using focal plane imaging and a residual network <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> P. Janin-Potiron, M. Gray, B. Neichel, M. Dumont, J. -F. Sauvage, C. T. Heritier, P. Jouve, R. Fetick, T. Fusco<br><strong><u>Categories:</u></strong> astro-ph.IM<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> As the Extremely Large Telescope (ELT) approaches operational status, optimising its imaging performance is critical. A differential piston, arising from either the adaptive optics (AO) control loop, thermomechanical effects, or other sources, significantly degrades the image quality and is detrimental to the telescope's overall performance. In a numerical simulation set-up, we propose a method for estimating the differential piston between the petals of the ELT's M4 mirror using images from a 2x2 Shack-Hartmann wavefront sensor (SH-WFS), commonly used in the ELT's tomographic AO mode. We aim to identify the limitations of this approach by evaluating its sensitivity to various observing conditions and sources of noise. Using a deep learning model based on a ResNet architecture, we trained a neural network (NN) on simulated datasets to estimate the differential piston. We assessed the robustness of the method under various conditions, including variations in Strehl ratio, polychromaticity, and detector noise. The performance was quantified using the root mean square error (RMSE) of the estimated differential piston aberration. This method demonstrates the ability to extract differential piston information from 2x2 SH-WFS images. Temporal averaging of frames makes the differential piston signal emerge from the turbulence-induced speckle field and leads to a significant improvement in the RMSE calculation. As expected, better seeing conditions result in improved accuracy. Polychromaticity only degrades the performance by less than 5% compared to the monochromatic case. In a realistic scenario, detector noise is not a limiting factor, as the primary limitation rather arises from the need for sufficient speckle averaging. The network was also shown to be applicable to input images other than the 2x2 SH-WFS data.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14569v1" target="_blank"><h2>Task Addition and Weight Disentanglement in Closed-Vocabulary Models <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Adam Hazimeh, Alessandro Favero, Pascal Frossard<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Task arithmetic has recently emerged as a promising method for editing pre-trained \textit{open-vocabulary} models, offering a cost-effective alternative to standard multi-task fine-tuning. However, despite the abundance of \textit{closed-vocabulary} models that are not pre-trained with language supervision, applying task arithmetic to these models remains unexplored. In this paper, we deploy and study task addition in closed-vocabulary image classification models. We consider different pre-training schemes and find that \textit{weight disentanglement} -- the property enabling task arithmetic -- is a general consequence of pre-training, as it appears in different pre-trained closed-vocabulary models. In fact, we find that pre-trained closed-vocabulary vision transformers can also be edited with task arithmetic, achieving high task addition performance and enabling the efficient deployment of multi-task models. Finally, we demonstrate that simple linear probing is a competitive baseline to task addition. Overall, our findings expand the applicability of task arithmetic to a broader class of pre-trained models and open the way for more efficient use of pre-trained models in diverse settings.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14566v1" target="_blank"><h2>Examining the Metrics for Document-Level Claim Extraction in Czech and Slovak <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Lucia Makaiová, Martin Fajčík, Antonín Jarolím<br><strong><u>Categories:</u></strong> cs.CL, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Document-level claim extraction remains an open challenge in the field of fact-checking, and subsequently, methods for evaluating extracted claims have received limited attention. In this work, we explore approaches to aligning two sets of claims pertaining to the same source document and computing their similarity through an alignment score. We investigate techniques to identify the best possible alignment and evaluation method between claim sets, with the aim of providing a reliable evaluation framework. Our approach enables comparison between model-extracted and human-annotated claim sets, serving as a metric for assessing the extraction performance of models and also as a possible measure of inter-annotator agreement. We conduct experiments on newly collected dataset-claims extracted from comments under Czech and Slovak news articles-domains that pose additional challenges due to the informal language, strong local context, and subtleties of these closely related languages. The results draw attention to the limitations of current evaluation approaches when applied to document-level claim extraction and highlight the need for more advanced methods-ones able to correctly capture semantic similarity and evaluate essential claim properties such as atomicity, checkworthiness, and decontextualization.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14554v1" target="_blank"><h2>ForensicFlow: A Tri-Modal Adaptive Network for Robust Deepfake Detection <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Mohammad Romani<br><strong><u>Categories:</u></strong> cs.CV, cs.CR, cs.LG<br><strong><u>Comments:</u></strong> 11 pages, 4 figures, 2 tables. Preprint. Submitted on November 18, 2025<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Deepfakes generated by advanced GANs and autoencoders severely threaten information integrity and societal stability. Single-stream CNNs fail to capture multi-scale forgery artifacts across spatial, texture, and frequency domains, limiting robustness and generalization. We introduce the ForensicFlow, a tri-modal forensic framework that synergistically fuses RGB, texture, and frequency evidence for video Deepfake detection. The RGB branch (ConvNeXt-tiny) extracts global visual inconsistencies; the texture branch (Swin Transformer-tiny) detects fine-grained blending artifacts; the frequency branch (CNN + SE) identifies periodic spectral noise. Attention-based temporal pooling dynamically prioritizes high-evidence frames, while adaptive attention fusion balances branch contributions.Trained on Celeb-DF (v2) with Focal Loss, ForensicFlow achieves AUC 0.9752, F1-Score 0.9408, and accuracy 0.9208, outperforming single-stream baselines. Ablation validates branch synergy; Grad-CAM confirms forensic focus. This comprehensive feature fusion provides superior resilience against subtle forgeries.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14545v1" target="_blank"><h2>DeepBlip: Estimating Conditional Average Treatment Effects Over Time <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Haorui Ma, Dennis Frauen, Stefan Feuerriegel<br><strong><u>Categories:</u></strong> stat.ML, cs.LG, stat.ME<br><strong><u>Comments:</u></strong> 42 pages<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Structural nested mean models (SNMMs) are a principled approach to estimate the treatment effects over time. A particular strength of SNMMs is to break the joint effect of treatment sequences over time into localized, time-specific ``blip effects''. This decomposition promotes interpretability through the incremental effects and enables the efficient offline evaluation of optimal treatment policies without re-computation. However, neural frameworks for SNMMs are lacking, as their inherently sequential g-estimation scheme prevents end-to-end, gradient-based training. Here, we propose DeepBlip, the first neural framework for SNMMs, which overcomes this limitation with a novel double optimization trick to enable simultaneous learning of all blip functions. Our DeepBlip seamlessly integrates sequential neural networks like LSTMs or transformers to capture complex temporal dependencies. By design, our method correctly adjusts for time-varying confounding to produce unbiased estimates, and its Neyman-orthogonal loss function ensures robustness to nuisance model misspecification. Finally, we evaluate our DeepBlip across various clinical datasets, where it achieves state-of-the-art performance.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14544v1" target="_blank"><h2>Mind the Gaps: Measuring Visual Artifacts in Dimensionality Reduction <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Jaume Ros, Alessio Arleo, Fernando Paulovich<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> dimensionality reduction (title, abstract)<br><p><strong><u>Abstract:</u></strong> Dimensionality Reduction (DR) techniques are commonly used for the visual exploration and analysis of high-dimensional data due to their ability to project datasets of high-dimensional points onto the 2D plane. However, projecting datasets in lower dimensions often entails some distortion, which is not necessarily easy to recognize but can lead users to misleading conclusions. Several Projection Quality Metrics (PQMs) have been developed as tools to quantify the goodness-of-fit of a DR projection; however, they mostly focus on measuring how well the projection captures the global or local structure of the data, without taking into account the visual distortion of the resulting plots, thus often ignoring the presence of outliers or artifacts that can mislead a visual analysis of the projection. In this work, we introduce the Warping Index (WI), a new metric for measuring the quality of DR projections onto the 2D plane, based on the assumption that the correct preservation of empty regions between points is of crucial importance towards a faithful visual representation of the data.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14533v1" target="_blank"><h2>A Neuro-Symbolic Framework for Reasoning under Perceptual Uncertainty: Bridging Continuous Perception and Discrete Symbolic Planning <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Jiahao Wu, Shengwen Yu<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> 29 pages, 10 figures, 12 tables<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Bridging continuous perceptual signals and discrete symbolic reasoning is a fundamental challenge in AI systems that must operate under uncertainty. We present a neuro-symbolic framework that explicitly models and propagates uncertainty from perception to planning, providing a principled connection between these two abstraction levels. Our approach couples a transformer-based perceptual front-end with graph neural network (GNN) relational reasoning to extract probabilistic symbolic states from visual observations, and an uncertainty-aware symbolic planner that actively gathers information when confidence is low. We demonstrate the framework's effectiveness on tabletop robotic manipulation as a concrete application: the translator processes 10,047 PyBullet-generated scenes (3--10 objects) and outputs probabilistic predicates with calibrated confidences (overall F1=0.68). When embedded in the planner, the system achieves 94\%/90\%/88\% success on Simple Stack, Deep Stack, and Clear+Stack benchmarks (90.7\% average), exceeding the strongest POMDP baseline by 10--14 points while planning within 15\,ms. A probabilistic graphical-model analysis establishes a quantitative link between calibrated uncertainty and planning convergence, providing theoretical guarantees that are validated empirically. The framework is general-purpose and can be applied to any domain requiring uncertainty-aware reasoning from perceptual input to symbolic planning.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14530v1" target="_blank"><h2>DeCo-VAE: Learning Compact Latents for Video Reconstruction via Decoupled Representation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Xiangchen Yin, Jiahui Yuan, Zhangchi Hu, Wenzhang Sun, Jie Chen, Xiaozhen Qiao, Hao Li, Xiaoyan Sun<br><strong><u>Categories:</u></strong> cs.CV, cs.LG, cs.MM<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> variational autoencoder (abstract), VAE (title, abstract)<br><p><strong><u>Abstract:</u></strong> Existing video Variational Autoencoders (VAEs) generally overlook the similarity between frame contents, leading to redundant latent modeling. In this paper, we propose decoupled VAE (DeCo-VAE) to achieve compact latent representation. Instead of encoding RGB pixels directly, we decompose video content into distinct components via explicit decoupling: keyframe, motion and residual, and learn dedicated latent representation for each. To avoid cross-component interference, we design dedicated encoders for each decoupled component and adopt a shared 3D decoder to maintain spatiotemporal consistency during reconstruction. We further utilize a decoupled adaptation strategy that freezes partial encoders while training the others sequentially, ensuring stable training and accurate learning of both static and dynamic features. Extensive quantitative and qualitative experiments demonstrate that DeCo-VAE achieves superior video reconstruction performance.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14516v2" target="_blank"><h2>Full-Atom Peptide Design via Riemannian-Euclidean Bayesian Flow Networks <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Hao Qian, Shikui Tu, Lei Xu<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> AAAI2026<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Diffusion and flow matching models have recently emerged as promising approaches for peptide binder design. Despite their progress, these models still face two major challenges. First, categorical sampling of discrete residue types collapses their continuous parameters into onehot assignments, while continuous variables (e.g., atom positions) evolve smoothly throughout the generation process. This mismatch disrupts the update dynamics and results in suboptimal performance. Second, current models assume unimodal distributions for side-chain torsion angles, which conflicts with the inherently multimodal nature of side chain rotameric states and limits prediction accuracy. To address these limitations, we introduce PepBFN, the first Bayesian flow network for full atom peptide design that directly models parameter distributions in fully continuous space. Specifically, PepBFN models discrete residue types by learning their continuous parameter distributions, enabling joint and smooth Bayesian updates with other continuous structural parameters. It further employs a novel Gaussian mixture based Bayesian flow to capture the multimodal side chain rotameric states and a Matrix Fisher based Riemannian flow to directly model residue orientations on the $\mathrm{SO}(3)$ manifold. Together, these parameter distributions are progressively refined via Bayesian updates, yielding smooth and coherent peptide generation. Experiments on side chain packing, reverse folding, and binder design tasks demonstrate the strong potential of PepBFN in computational peptide design.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14515v1" target="_blank"><h2>IMSE: Efficient U-Net-based Speech Enhancement using Inception Depthwise Convolution and Amplitude-Aware Linear Attention <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Xinxin Tang, Bin Qin, Yufang Li<br><strong><u>Categories:</u></strong> cs.SD, cs.AI, cs.CV<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Achieving a balance between lightweight design and high performance remains a significant challenge for speech enhancement (SE) tasks on resource-constrained devices. Existing state-of-the-art methods, such as MUSE, have established a strong baseline with only 0.51M parameters by introducing a Multi-path Enhanced Taylor (MET) transformer and Deformable Embedding (DE). However, an in-depth analysis reveals that MUSE still suffers from efficiency bottlenecks: the MET module relies on a complex "approximate-compensate" mechanism to mitigate the limitations of Taylor-expansion-based attention, while the offset calculation for deformable embedding introduces additional computational burden. This paper proposes IMSE, a systematically optimized and ultra-lightweight network. We introduce two core innovations: 1) Replacing the MET module with Amplitude-Aware Linear Attention (MALA). MALA fundamentally rectifies the "amplitude-ignoring" problem in linear attention by explicitly preserving the norm information of query vectors in the attention calculation, achieving efficient global modeling without an auxiliary compensation branch. 2) Replacing the DE module with Inception Depthwise Convolution (IDConv). IDConv borrows the Inception concept, decomposing large-kernel operations into efficient parallel branches (square, horizontal, and vertical strips), thereby capturing spectrogram features with extremely low parameter redundancy. Extensive experiments on the VoiceBank+DEMAND dataset demonstrate that, compared to the MUSE baseline, IMSE significantly reduces the parameter count by 16.8\% (from 0.513M to 0.427M) while achieving competitive performance comparable to the state-of-the-art on the PESQ metric (3.373). This study sets a new benchmark for the trade-off between model size and speech quality in ultra-lightweight speech enhancement.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14510v1" target="_blank"><h2>CLO: Efficient LLM Inference System with CPU-Light KVCache Offloading via Algorithm-System Co-Design <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Jiawei Yi, Ping Gong, Youhui Bai, Jiaqi Ruan, Shengnan Wang, Pengcheng Wang, Haibo Wang, Weiguang Wang, Xia Zhu, Feng Wu, Cheng Li<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> The growth of million-token LLMs exposes the scalability limits of inference systems, where the KVCache dominates memory usage and data transfer overhead. Recent offloading systems migrate the KVCache to CPU memory and incorporate top-k attention to reduce the volume of data transferred from the CPU, while further applying system-level optimizations such as on-GPU caching and prefetching to lower transfer overhead. However, they overlook the CPU bottleneck in three aspects: (1) substantial overhead of fine-grained dynamic cache management performed on the CPU side, (2) significant transfer overhead from poor PCIe bandwidth utilization caused by heavy gathering operations at the CPU side, and (3) GPU runtime bubbles introduced by coarse-grained CPU-centric synchronization. To address these challenges, we propose CLO, a CPU-light KVCache offloading system via algorithm-system co-design. CLO features: (1) a coarse-grained head-wise approximate on-GPU caching strategy with negligible cache management cost, (2) seamless combination of data prefetching and on-GPU persistent caching for lower transfer overhead, (3) a zero-copy transfer engine to fully exploit PCIe bandwidth, and a GPU-centric synchronization method to eliminate GPU stalls. Evaluation on two widely-used LLMs demonstrates that CLO achieves comparable accuracy to state-of-the-art systems, while substantially minimizing CPU overhead, fully utilizing PCIe bandwidth, thus improving decoding throughput by 9.3%-66.6%. Our results highlight that algorithm-system co-design is essential for memory-constrained LLM inference on modern GPU platforms. We open source CLO at https://github.com/CommediaJW/CLO.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14501v1" target="_blank"><h2>Improved Convergence in Parameter-Agnostic Error Feedback through Momentum <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Abdurakhmon Sadiev, Yury Demidovich, Igor Sokolov, Grigory Malinovsky, Sarit Khirirat, Peter Richtárik<br><strong><u>Categories:</u></strong> math.OC, cs.LG<br><strong><u>Comments:</u></strong> 50 pages, 12 figures<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Communication compression is essential for scalable distributed training of modern machine learning models, but it often degrades convergence due to the noise it introduces. Error Feedback (EF) mechanisms are widely adopted to mitigate this issue of distributed compression algorithms. Despite their popularity and training efficiency, existing distributed EF algorithms often require prior knowledge of problem parameters (e.g., smoothness constants) to fine-tune stepsizes. This limits their practical applicability especially in large-scale neural network training. In this paper, we study normalized error feedback algorithms that combine EF with normalized updates, various momentum variants, and parameter-agnostic, time-varying stepsizes, thus eliminating the need for problem-dependent tuning. We analyze the convergence of these algorithms for minimizing smooth functions, and establish parameter-agnostic complexity bounds that are close to the best-known bounds with carefully-tuned problem-dependent stepsizes. Specifically, we show that normalized EF21 achieve the convergence rate of near ${O}(1/T^{1/4})$ for Polyak's heavy-ball momentum, ${O}(1/T^{2/7})$ for Iterative Gradient Transport (IGT), and ${O}(1/T^{1/3})$ for STORM and Hessian-corrected momentum. Our results hold with decreasing stepsizes and small mini-batches. Finally, our empirical experiments confirm our theoretical insights.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14482v1" target="_blank"><h2>Gradient-Based Join Ordering <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Tim Schwabe, Maribel Acosta<br><strong><u>Categories:</u></strong> cs.DB, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Join ordering is the NP-hard problem of selecting the most efficient sequence in which to evaluate joins (conjunctive, binary operators) in a database query. As the performance of query execution critically depends on this choice, join ordering lies at the core of query optimization. Traditional approaches cast this problem as a discrete combinatorial search over binary trees guided by a cost model, but they often suffer from high computational complexity and limited scalability. We show that, when the cost model is differentiable, the query plans can be continuously relaxed into a soft adjacency matrix representing a superposition of plans. This continuous relaxation, together with a Gumbel-Softmax parameterization of the adjacency matrix and differentiable constraints enforcing plan validity, enables gradient-based search for plans within this relaxed space. Using a learned Graph Neural Network as the cost model, we demonstrate that this gradient-based approach can find comparable and even lower-cost plans compared to traditional discrete local search methods on two different graph datasets. Furthermore, we empirically show that the runtime of this approach scales linearly with query size, in contrast to quadratic or exponential runtimes of classical approaches. We believe this first step towards gradient-based join ordering can lead to more effective and efficient query optimizers in the future.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14465v1" target="_blank"><h2>nnterp: A Standardized Interface for Mechanistic Interpretability of Transformers <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Clément Dumas<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 7 pages, 1 figure, accepted at the mechanistic interpretability workshop of NeurIPS 2025<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Mechanistic interpretability research requires reliable tools for analyzing transformer internals across diverse architectures. Current approaches face a fundamental tradeoff: custom implementations like TransformerLens ensure consistent interfaces but require coding a manual adaptation for each architecture, introducing numerical mismatch with the original models, while direct HuggingFace access through NNsight preserves exact behavior but lacks standardization across models. To bridge this gap, we develop nnterp, a lightweight wrapper around NNsight that provides a unified interface for transformer analysis while preserving original HuggingFace implementations. Through automatic module renaming and comprehensive validation testing, nnterp enables researchers to write intervention code once and deploy it across 50+ model variants spanning 16 architecture families. The library includes built-in implementations of common interpretability methods (logit lens, patchscope, activation steering) and provides direct access to attention probabilities for models that support it. By packaging validation tests with the library, researchers can verify compatibility with custom models locally. nnterp bridges the gap between correctness and usability in mechanistic interpretability tooling.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14456v1" target="_blank"><h2>Analyzing the Impact of Participant Failures in Cross-Silo Federated Learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Fabian Stricker, David Bermbach, Christian Zirpins<br><strong><u>Categories:</u></strong> cs.DC, cs.AI<br><strong><u>Comments:</u></strong> Accepted for publication in 3rd IEEE International Conference on Federated Learning Applications and Technologies (FLTA2025)<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Federated learning (FL) is a new paradigm for training machine learning (ML) models without sharing data. While applying FL in cross-silo scenarios, where organizations collaborate, it is necessary that the FL system is reliable; however, participants can fail due to various reasons (e.g., communication issues or misconfigurations). In order to provide a reliable system, it is necessary to analyze the impact of participant failures. While this problem received attention in cross-device FL where mobile devices with limited resources participate, there is comparatively little research in cross-silo FL.
  Therefore, we conduct an extensive study for analyzing the impact of participant failures on the model quality in the context of inter-organizational cross-silo FL with few participants. In our study, we focus on analyzing generally influential factors such as the impact of the timing and the data as well as the impact on the evaluation, which is important for deciding, if the model should be deployed. We show that under high skews the evaluation is optimistic and hides the real impact. Furthermore, we demonstrate that the timing impacts the quality of the trained model. Our results offer insights for researchers and software architects aiming to build robust FL systems.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14455v2" target="_blank"><h2>Nonparametric estimation of conditional probability distributions using a generative approach based on conditional push-forward neural networks <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Nicola Rares Franco, Lorenzo Tedesco<br><strong><u>Categories:</u></strong> cs.LG, stat.ME<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> We introduce conditional push-forward neural networks (CPFN), a generative framework for conditional distribution estimation. Instead of directly modeling the conditional density $f_{Y|X}$, CPFN learns a stochastic map $\varphi=\varphi(x,u)$ such that $\varphi(x,U)$ and $Y|X=x$ follow approximately the same law, with $U$ a suitable random vector of pre-defined latent variables. This enables efficient conditional sampling and straightforward estimation of conditional statistics through Monte Carlo methods. The model is trained via an objective function derived from a Kullback-Leibler formulation, without requiring invertibility or adversarial training. We establish a near-asymptotic consistency result and demonstrate experimentally that CPFN can achieve performance competitive with, or even superior to, state-of-the-art methods, including kernel estimators, tree-based algorithms, and popular deep learning techniques, all while remaining lightweight and easy to train.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14452v1" target="_blank"><h2>Hybrid Modeling of Photoplethysmography for Non-invasive Monitoring of Cardiovascular Parameters <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Emanuele Palumbo, Sorawit Saengkyongam, Maria R. Cervera, Jens Behrmann, Andrew C. Miller, Guillermo Sapiro, Christina Heinze-Deml, Antoine Wehenkel<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> variational autoencoder (abstract)<br><p><strong><u>Abstract:</u></strong> Continuous cardiovascular monitoring can play a key role in precision health. However, some fundamental cardiac biomarkers of interest, including stroke volume and cardiac output, require invasive measurements, e.g., arterial pressure waveforms (APW). As a non-invasive alternative, photoplethysmography (PPG) measurements are routinely collected in hospital settings. Unfortunately, the prediction of key cardiac biomarkers from PPG instead of APW remains an open challenge, further complicated by the scarcity of annotated PPG measurements. As a solution, we propose a hybrid approach that uses hemodynamic simulations and unlabeled clinical data to estimate cardiovascular biomarkers directly from PPG signals. Our hybrid model combines a conditional variational autoencoder trained on paired PPG-APW data with a conditional density estimator of cardiac biomarkers trained on labeled simulated APW segments. As a key result, our experiments demonstrate that the proposed approach can detect fluctuations of cardiac output and stroke volume and outperform a supervised baseline in monitoring temporal changes in these biomarkers.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14445v1" target="_blank"><h2>Tell Me: An LLM-powered Mental Well-being Assistant with RAG, Synthetic Dialogue Generation, and Agentic Planning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Trishala Jayesh Ahalpara<br><strong><u>Categories:</u></strong> cs.CL, cs.AI, cs.HC, cs.LG<br><strong><u>Comments:</u></strong> 8 pages, 2 figures, 1 Table. Submitted to the Computation and Language (cs.CL) category. Uses the ACL-style template. Code and demo will be released at:this https URL<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> data augmentation (abstract)<br><p><strong><u>Abstract:</u></strong> We present Tell Me, a mental well-being system that leverages advances in large language models to provide accessible, context-aware support for users and researchers. The system integrates three components: (i) a retrieval-augmented generation (RAG) assistant for personalized, knowledge-grounded dialogue; (ii) a synthetic client-therapist dialogue generator conditioned on client profiles to facilitate research on therapeutic language and data augmentation; and (iii) a Well-being AI crew, implemented with CrewAI, that produces weekly self-care plans and guided meditation audio. The system is designed as a reflective space for emotional processing rather than a substitute for professional therapy. It illustrates how conversational assistants can lower barriers to support, complement existing care, and broaden access to mental health resources. To address the shortage of confidential therapeutic data, we introduce synthetic client-therapist dialogue generation conditioned on client profiles. Finally, the planner demonstrates an innovative agentic workflow for dynamically adaptive, personalized self-care, bridging the limitations of static well-being tools. We describe the architecture, demonstrate its functionalities, and report evaluation of the RAG assistant in curated well-being scenarios using both automatic LLM-based judgments and a human-user study. This work highlights opportunities for interdisciplinary collaboration between NLP researchers and mental health professionals to advance responsible innovation in human-AI interaction for well-being.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14428v1" target="_blank"><h2>Context-aware, Ante-hoc Explanations of Driving Behaviour <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Dominik Grundt, Ishan Saxena, Malte Petersen, Bernd Westphal, Eike Möhlmann<br><strong><u>Categories:</u></strong> cs.LO, cs.AI, cs.CY<br><strong><u>Comments:</u></strong> In Proceedings FMAS 2025,arXiv:2511.13245<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> explainability (abstract)<br><p><strong><u>Abstract:</u></strong> Autonomous vehicles (AVs) must be both safe and trustworthy to gain social acceptance and become a viable option for everyday public transportation. Explanations about the system behaviour can increase safety and trust in AVs. Unfortunately, explaining the system behaviour of AI-based driving functions is particularly challenging, as decision-making processes are often opaque. The field of Explainability Engineering tackles this challenge by developing explanation models at design time. These models are designed from system design artefacts and stakeholder needs to develop correct and good explanations. To support this field, we propose an approach that enables context-aware, ante-hoc explanations of (un)expectable driving manoeuvres at runtime. The visual yet formal language Traffic Sequence Charts is used to formalise explanation contexts, as well as corresponding (un)expectable driving manoeuvres. A dedicated runtime monitoring enables context-recognition and ante-hoc presentation of explanations at runtime. In combination, we aim to support the bridging of correct and good explanations. Our method is demonstrated in a simulated overtaking.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14427v1" target="_blank"><h2>Self-Supervised Multisensory Pretraining for Contact-Rich Robot Reinforcement Learning <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Rickmer Krohn, Vignesh Prasad, Gabriele Tiboni, Georgia Chalvatzaki<br><strong><u>Categories:</u></strong> cs.RO, cs.LG<br><strong><u>Comments:</u></strong> 9 pages, 10 figures, preprint<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Effective contact-rich manipulation requires robots to synergistically leverage vision, force, and proprioception. However, Reinforcement Learning agents struggle to learn in such multisensory settings, especially amidst sensory noise and dynamic changes. We propose MultiSensory Dynamic Pretraining (MSDP), a novel framework for learning expressive multisensory representations tailored for task-oriented policy learning. MSDP is based on masked autoencoding and trains a transformer-based encoder by reconstructing multisensory observations from only a subset of sensor embeddings, leading to cross-modal prediction and sensor fusion. For downstream policy learning, we introduce a novel asymmetric architecture, where a cross-attention mechanism allows the critic to extract dynamic, task-specific features from the frozen embeddings, while the actor receives a stable pooled representation to guide its actions. Our method demonstrates accelerated learning and robust performance under diverse perturbations, including sensor noise, and changes in object dynamics. Evaluations in multiple challenging, contact-rich robot manipulation tasks in simulation and the real world showcase the effectiveness of MSDP. Our approach exhibits strong robustness to perturbations and achieves high success rates on the real robot with as few as 6,000 online interactions, offering a simple yet powerful solution for complex multisensory robotic control.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14422v1" target="_blank"><h2>Sigil: Server-Enforced Watermarking in U-Shaped Split Federated Learning via Gradient Injection <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zhengchunmin Dai, Jiaxiong Tang, Peng Sun, Honglong Chen, Liantao Wu<br><strong><u>Categories:</u></strong> cs.CR, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> 18 pages,8 figures<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> anomaly detection (abstract)<br><p><strong><u>Abstract:</u></strong> In decentralized machine learning paradigms such as Split Federated Learning (SFL) and its variant U-shaped SFL, the server's capabilities are severely restricted. Although this enhances client-side privacy, it also leaves the server highly vulnerable to model theft by malicious clients. Ensuring intellectual property protection for such capability-limited servers presents a dual challenge: watermarking schemes that depend on client cooperation are unreliable in adversarial settings, whereas traditional server-side watermarking schemes are technically infeasible because the server lacks access to critical elements such as model parameters or labels.
  To address this challenge, this paper proposes Sigil, a mandatory watermarking framework designed specifically for capability-limited servers. Sigil defines the watermark as a statistical constraint on the server-visible activation space and embeds the watermark into the client model via gradient injection, without requiring any knowledge of the data. Besides, we design an adaptive gradient clipping mechanism to ensure that our watermarking process remains both mandatory and stealthy, effectively countering existing gradient anomaly detection methods and a specifically designed adaptive subspace removal attack. Extensive experiments on multiple datasets and models demonstrate Sigil's fidelity, robustness, and stealthiness.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14396v1" target="_blank"><h2>Continuous Vision-Language-Action Co-Learning with Semantic-Physical Alignment for Behavioral Cloning <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Xiuxiu Qi, Yu Yang, Jiannong Cao, Luyao Bai, Chongshan Fan, Chengtai Cao, Hongpeng Wang<br><strong><u>Categories:</u></strong> cs.RO, cs.AI, cs.CV<br><strong><u>Comments:</u></strong> Accepted at AAAI 2026, the Project website is available atthis https URL<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> attention (abstract), data augmentation (abstract)<br><p><strong><u>Abstract:</u></strong> Language-conditioned manipulation facilitates human-robot interaction via behavioral cloning (BC), which learns control policies from human demonstrations and serves as a cornerstone of embodied AI. Overcoming compounding errors in sequential action decisions remains a central challenge to improving BC performance. Existing approaches mitigate compounding errors through data augmentation, expressive representation, or temporal abstraction. However, they suffer from physical discontinuities and semantic-physical misalignment, leading to inaccurate action cloning and intermittent execution. In this paper, we present Continuous vision-language-action Co-Learning with Semantic-Physical Alignment (CCoL), a novel BC framework that ensures temporally consistent execution and fine-grained semantic grounding. It generates robust and smooth action execution trajectories through continuous co-learning across vision, language, and proprioceptive inputs (e.g., robot internal states). Meanwhile, we anchor language semantics to visuomotor representations by a bidirectional cross-attention to learn contextual information for action generation, successfully overcoming the problem of semantic-physical misalignment. Extensive experiments show that CCoL achieves an average 8.0% relative improvement across three simulation suites, with up to 19.2% relative gain in human-demonstrated bimanual insertion tasks. Real-world tests on a 7-DoF robot further confirm CCoL's generalization under unseen and noisy object states.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14348v1" target="_blank"><h2>Enforcing hidden physics in physics-informed neural networks <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Nanxi Chen, Sifan Wang, Rujin Ma, Airong Chen, Chuanjie Cui<br><strong><u>Categories:</u></strong> cs.LG, physics.comp-ph<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> Physics-informed neural networks (PINNs) represent a new paradigm for solving partial differential equations (PDEs) by integrating physical laws into the learning process of neural networks. However, despite their foundational role, the hidden irreversibility implied by the Second Law of Thermodynamics is often neglected during training, leading to unphysical solutions or even training failures in conventional PINNs. In this paper, we identify this critical gap and introduce a simple, generalized, yet robust irreversibility-regularized strategy that enforces hidden physical laws as soft constraints during training. This approach ensures that the learned solutions consistently respect the intrinsic one-way nature of irreversible physical processes. Across a wide range of benchmarks spanning traveling wave propagation, steady combustion, ice melting, corrosion evolution, and crack propagation, we demonstrate that our regularization scheme reduces predictive errors by more than an order of magnitude, while requiring only minimal modification to existing PINN frameworks. We believe that the proposed framework is broadly applicable to a wide class of PDE-governed physical systems and will have significant impact within the scientific machine learning community.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14341v1" target="_blank"><h2>Going Places: Place Recognition in Artificial and Natural Systems <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Michael Milford, Tobias Fischer<br><strong><u>Categories:</u></strong> cs.RO, cs.AI, cs.CV<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Place recognition, the ability to identify previously visited locations, is critical for both biological navigation and autonomous systems. This review synthesizes findings from robotic systems, animal studies, and human research to explore how different systems encode and recall place. We examine the computational and representational strategies employed across artificial systems, animals, and humans, highlighting convergent solutions such as topological mapping, cue integration, and memory management. Animal systems reveal evolved mechanisms for multimodal navigation and environmental adaptation, while human studies provide unique insights into semantic place concepts, cultural influences, and introspective capabilities. Artificial systems showcase scalable architectures and data-driven models. We propose a unifying set of concepts by which to consider and develop place recognition mechanisms and identify key challenges such as generalization, robustness, and environmental variability. This review aims to foster innovations in artificial localization by connecting future developments in artificial place recognition systems to insights from both animal navigation research and human spatial cognition studies.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14322v1" target="_blank"><h2>LSP-YOLO: A Lightweight Single-Stage Network for Sitting Posture Recognition on Embedded Devices <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Nanjun Li, Ziyue Hao, Quanqiang Wang, Xuanyin Wang<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> Submitted to Engineering Applications of Artificial Intelligence (EAAI)<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> With the rise in sedentary behavior, health problems caused by poor sitting posture have drawn increasing attention. Most existing methods, whether using invasive sensors or computer vision, rely on two-stage pipelines, which result in high intrusiveness, intensive computation, and poor real-time performance on embedded edge devices. Inspired by YOLOv11-Pose, a lightweight single-stage network for sitting posture recognition on embedded edge devices termed LSP-YOLO was proposed. By integrating partial convolution(PConv) and Similarity-Aware Activation Module(SimAM), a lightweight module, Light-C3k2, was designed to reduce computational cost while maintaining feature extraction capability. In the recognition head, keypoints were directly mapped to posture classes through pointwise convolution, and intermediate supervision was employed to enable efficient fusion of pose estimation and classification. Furthermore, a dataset containing 5,000 images across six posture categories was constructed for model training and testing. The smallest trained model, LSP-YOLO-n, achieved 94.2% accuracy and 251 Fps on personal computer(PC) with a model size of only 1.9 MB. Meanwhile, real-time and high-accuracy inference under constrained computational resources was demonstrated on the SV830C + GC030A platform. The proposed approach is characterized by high efficiency, lightweight design and deployability, making it suitable for smart classrooms, rehabilitation, and human-computer interaction applications.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14312v1" target="_blank"><h2>H-LDM: Hierarchical Latent Diffusion Models for Controllable and Interpretable PCG Synthesis from Clinical Metadata <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Chenyang Xu, Siming Li, Hao Wang<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> This paper was accepted by IEEE BIBM 2025 conference<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> VAE (abstract), latent space (abstract), attention (abstract), data augmentation (abstract)<br><p><strong><u>Abstract:</u></strong> Phonocardiogram (PCG) analysis is vital for cardiovascular disease diagnosis, yet the scarcity of labeled pathological data hinders the capability of AI systems. To bridge this, we introduce H-LDM, a Hierarchical Latent Diffusion Model for generating clinically accurate and controllable PCG signals from structured metadata. Our approach features: (1) a multi-scale VAE that learns a physiologically-disentangled latent space, separating rhythm, heart sounds, and murmurs; (2) a hierarchical text-to-biosignal pipeline that leverages rich clinical metadata for fine-grained control over 17 distinct conditions; and (3) an interpretable diffusion process guided by a novel Medical Attention module. Experiments on the PhysioNet CirCor dataset demonstrate state-of-the-art performance, achieving a Fréchet Audio Distance of 9.7, a 92% attribute disentanglement score, and 87.1% clinical validity confirmed by cardiologists. Augmenting diagnostic models with our synthetic data improves the accuracy of rare disease classification by 11.3\%. H-LDM establishes a new direction for data augmentation in cardiac diagnostics, bridging data scarcity with interpretable clinical insights.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14301v1" target="_blank"><h2>Steganographic Backdoor Attacks in NLP: Ultra-Low Poisoning and Defense Evasion <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Eric Xue, Ruiyi Zhang, Zijun Zhang, Pengtao Xie<br><strong><u>Categories:</u></strong> cs.CR, cs.CL, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Transformer models are foundational to natural language processing (NLP) applications, yet remain vulnerable to backdoor attacks introduced through poisoned data, which implant hidden behaviors during training. To strengthen the ability to prevent such compromises, recent research has focused on designing increasingly stealthy attacks to stress-test existing defenses, pairing backdoor behaviors with stylized artifact or token-level perturbation triggers. However, this trend diverts attention from the harder and more realistic case: making the model respond to semantic triggers such as specific names or entities, where a successful backdoor could manipulate outputs tied to real people or events in deployed systems. Motivated by this growing disconnect, we introduce SteganoBackdoor, bringing stealth techniques back into line with practical threat models. Leveraging innocuous properties from natural-language steganography, SteganoBackdoor applies a gradient-guided data optimization process to transform semantic trigger seeds into steganographic carriers that embed a high backdoor payload, remain fluent, and exhibit no representational resemblance to the trigger. Across diverse experimental settings, SteganoBackdoor achieves over 99% attack success at an order-of-magnitude lower data-poisoning rate than prior approaches while maintaining unparalleled evasion against a comprehensive suite of data-level defenses. By revealing this practical and covert attack, SteganoBackdoor highlights an urgent blind spot in current defenses and demands immediate attention to adversarial data defenses and real-world threat modeling.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14299v1" target="_blank"><h2>DataSage: Multi-agent Collaboration for Insight Discovery with External Knowledge Retrieval, Multi-role Debating, and Multi-path Reasoning <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Xiaochuan Liu, Yuanfeng Song, Xiaoming Yin, Xing Chen<br><strong><u>Categories:</u></strong> cs.AI, cs.CL, cs.MA<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> In today's data-driven era, fully automated end-to-end data analytics, particularly insight discovery, is critical for discovering actionable insights that assist organizations in making effective decisions. With the rapid advancement of large language models (LLMs), LLM-driven agents have emerged as a promising paradigm for automating data analysis and insight discovery. However, existing data insight agents remain limited in several key aspects, often failing to deliver satisfactory results due to: (1) insufficient utilization of domain knowledge, (2) shallow analytical depth, and (3) error-prone code generation during insight generation. To address these issues, we propose DataSage, a novel multi-agent framework that incorporates three innovative features including external knowledge retrieval to enrich the analytical context, a multi-role debating mechanism to simulate diverse analytical perspectives and deepen analytical depth, and multi-path reasoning to improve the accuracy of the generated code and insights. Extensive experiments on InsightBench demonstrate that DataSage consistently outperforms existing data insight agents across all difficulty levels, offering an effective solution for automated data insight discovery.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14283v1" target="_blank"><h2>NeuralSSD: A Neural Solver for Signed Distance Surface Reconstruction <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zi-Chen Xi, Jiahui Huang, Hao-Xiang Chen, Francis Williams, Qun-Ce Xu, Tai-Jiang Mu, Shi-Min Hu<br><strong><u>Categories:</u></strong> cs.CV, cs.GR, cs.LG<br><strong><u>Comments:</u></strong> Under review<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract)<br><p><strong><u>Abstract:</u></strong> We proposed a generalized method, NeuralSSD, for reconstructing a 3D implicit surface from the widely-available point cloud data. NeuralSSD is a solver-based on the neural Galerkin method, aimed at reconstructing higher-quality and accurate surfaces from input point clouds. Implicit method is preferred due to its ability to accurately represent shapes and its robustness in handling topological changes. However, existing parameterizations of implicit fields lack explicit mechanisms to ensure a tight fit between the surface and input data. To address this, we propose a novel energy equation that balances the reliability of point cloud information. Additionally, we introduce a new convolutional network that learns three-dimensional information to achieve superior optimization results. This approach ensures that the reconstructed surface closely adheres to the raw input points and infers valuable inductive biases from point clouds, resulting in a highly accurate and stable surface reconstruction. NeuralSSD is evaluated on a variety of challenging datasets, including the ShapeNet and Matterport datasets, and achieves state-of-the-art results in terms of both surface reconstruction accuracy and generalizability.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14282v1" target="_blank"><h2>Weight Variance Amplifier Improves Accuracy in High-Sparsity One-Shot Pruning <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Vincent-Daniel Yun, Junhyuk Jo, Sunwoo Lee<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Deep neural networks achieve outstanding performance in visual recognition tasks, yet their large number of parameters makes them less practical for real-world applications. Recently, one-shot pruning has emerged as an effective strategy for reducing model size without additional training. However, models trained with standard objective functions often suffer a significant drop in accuracy after aggressive pruning. Some existing pruning-robust optimizers, such as SAM, and CrAM, mitigate this accuracy drop by guiding the model toward flatter regions of the parameter space, but they inevitably incur non-negligible additional computations. We propose a Variance Amplifying Regularizer (VAR) that deliberately increases the variance of model parameters during training. Our study reveals an intriguing finding that parameters with higher variance exhibit greater pruning robustness. VAR exploits this property by promoting such variance in the weight distribution, thereby mitigating the adverse effects of pruning. We further provide a theoretical analysis of its convergence behavior, supported by extensive empirical results demonstrating the superior pruning robustness of VAR.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14268v1" target="_blank"><h2>Statistically controllable microstructure reconstruction framework for heterogeneous materials using sliced-Wasserstein metric and neural networks <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Zhenchuan Ma, Qizhi Teng, Pengcheng Yan, Lindong Li, Kirill M. Gerke, Marina V. Karsanina, Xiaohai He<br><strong><u>Categories:</u></strong> physics.comp-ph, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> Heterogeneous porous materials play a crucial role in various engineering systems. Microstructure characterization and reconstruction provide effective means for modeling these materials, which are critical for conducting physical property simulations, structure-property linkage studies, and enhancing their performance across different applications. To achieve superior controllability and applicability with small sample sizes, we propose a statistically controllable microstructure reconstruction framework that integrates neural networks with sliced-Wasserstein metric. Specifically, our approach leverages local pattern distribution for microstructure characterization and employs a controlled sampling strategy to generate target distributions that satisfy given conditional parameters. A neural network-based model establishes the mapping from the input distribution to the target local pattern distribution, enabling microstructure reconstruction. Combinations of sliced-Wasserstein metric and gradient optimization techniques minimize the distance between these distributions, leading to a stable and reliable model. Our method can perform stochastic and controllable reconstruction tasks even with small sample sizes. Additionally, it can generate large-size (e.g. 512 and 1024) 3D microstructures using a chunking strategy. By introducing spatial location masks, our method excels at generating spatially heterogeneous and complex microstructures. We conducted experiments on stochastic reconstruction, controllable reconstruction, heterogeneous reconstruction, and large-size microstructure reconstruction across various materials. Comparative analysis through visualization, statistical measures, and physical property simulations demonstrates the effectiveness, providing new insights and possibilities for research on structure-property linkage and material inverse design.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14265v1" target="_blank"><h2>Unified Multimodal Vessel Trajectory Prediction with Explainable Navigation Intention <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Rui Zhang, Chao Li, Kezhong Liu, Chen Wang, Bolong Zheng, Hongbo Jiang<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> variational autoencoder (abstract), VAE (abstract), explainability (abstract), explainable (title, abstract), multimodal (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Vessel trajectory prediction is fundamental to intelligent maritime systems. Within this domain, short-term prediction of rapid behavioral changes in complex maritime environments has established multimodal trajectory prediction (MTP) as a promising research area. However, existing vessel MTP methods suffer from limited scenario applicability and insufficient explainability. To address these challenges, we propose a unified MTP framework incorporating explainable navigation intentions, which we classify into sustained and transient categories. Our method constructs sustained intention trees from historical trajectories and models dynamic transient intentions using a Conditional Variational Autoencoder (CVAE), while using a non-local attention mechanism to maintain global scenario consistency. Experiments on real Automatic Identification System (AIS) datasets demonstrates our method's broad applicability across diverse scenarios, achieving significant improvements in both ADE and FDE. Furthermore, our method improves explainability by explicitly revealing the navigational intentions underlying each predicted trajectory.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14263v1" target="_blank"><h2>Algebraformer: A Neural Approach to Linear Systems <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Pietro Sittoni, Francesco Tudisco<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Recent work in deep learning has opened new possibilities for solving classical algorithmic tasks using end-to-end learned models. In this work, we investigate the fundamental task of solving linear systems, particularly those that are ill-conditioned. Existing numerical methods for ill-conditioned systems often require careful parameter tuning, preconditioning, or domain-specific expertise to ensure accuracy and stability. In this work, we propose Algebraformer, a Transformer-based architecture that learns to solve linear systems end-to-end, even in the presence of severe ill-conditioning. Our model leverages a novel encoding scheme that enables efficient representation of matrix and vector inputs, with a memory complexity of $O(n^2)$, supporting scalable inference. We demonstrate its effectiveness on application-driven linear problems, including interpolation tasks from spectral methods for boundary value problems and acceleration of the Newton method. Algebraformer achieves competitive accuracy with significantly lower computational overhead at test time, demonstrating that general-purpose neural architectures can effectively reduce complexity in traditional scientific computing pipelines.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14262v1" target="_blank"><h2>Object-Centric World Models for Causality-Aware Reinforcement Learning <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Yosuke Nishimoto, Takashi Matsubara<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> Accepted by AAAI-26<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (abstract), causality (title, abstract)<br><p><strong><u>Abstract:</u></strong> World models have been developed to support sample-efficient deep reinforcement learning agents. However, it remains challenging for world models to accurately replicate environments that are high-dimensional, non-stationary, and composed of multiple objects with rich interactions since most world models learn holistic representations of all environmental components. By contrast, humans perceive the environment by decomposing it into discrete objects, facilitating efficient decision-making. Motivated by this insight, we propose \emph{Slot Transformer Imagination with CAusality-aware reinforcement learning} (STICA), a unified framework in which object-centric Transformers serve as the world model and causality-aware policy and value networks. STICA represents each observation as a set of object-centric tokens, together with tokens for the agent action and the resulting reward, enabling the world model to predict token-level dynamics and interactions. The policy and value networks then estimate token-level cause--effect relations and use them in the attention layers, yielding causality-guided decision-making. Experiments on object-rich benchmarks demonstrate that STICA consistently outperforms state-of-the-art agents in both sample efficiency and final performance.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14250v1" target="_blank"><h2>Count The Notes: Histogram-Based Supervision for Automatic Music Transcription <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Jonathan Yaffe, Ben Maman, Meinard Müller, Amit H. Bermano<br><strong><u>Categories:</u></strong> cs.SD, cs.LG<br><strong><u>Comments:</u></strong> ISMIR 2025<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Automatic Music Transcription (AMT) converts audio recordings into symbolic musical representations. Training deep neural networks (DNNs) for AMT typically requires strongly aligned training pairs with precise frame-level annotations. Since creating such datasets is costly and impractical for many musical contexts, weakly aligned approaches using segment-level annotations have gained traction. However, existing methods often rely on Dynamic Time Warping (DTW) or soft alignment loss functions, both of which still require local semantic correspondences, making them error-prone and computationally expensive. In this article, we introduce CountEM, a novel AMT framework that eliminates the need for explicit local alignment by leveraging note event histograms as supervision, enabling lighter computations and greater flexibility. Using an Expectation-Maximization (EM) approach, CountEM iteratively refines predictions based solely on note occurrence counts, significantly reducing annotation efforts while maintaining high transcription accuracy. Experiments on piano, guitar, and multi-instrument datasets demonstrate that CountEM matches or surpasses existing weakly supervised methods, improving AMT's robustness, scalability, and efficiency. Our project page is available at https://yoni-yaffe.github.io/count-the-notes.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14248v1" target="_blank"><h2>Enhancing Regional Airbnb Trend Forecasting Using LLM-Based Embeddings of Accessibility and Human Mobility <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Hongju Lee, Youngjun Park, Jisun An, Dongman Lee<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> Accepted at ASONAM 2025<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), transformer (abstract)<br><p><strong><u>Abstract:</u></strong> The expansion of short-term rental platforms, such as Airbnb, has significantly disrupted local housing markets, often leading to increased rental prices and housing affordability issues. Accurately forecasting regional Airbnb market trends can thus offer critical insights for policymakers and urban planners aiming to mitigate these impacts. This study proposes a novel time-series forecasting framework to predict three key Airbnb indicators -- Revenue, Reservation Days, and Number of Reservations -- at the regional level. Using a sliding-window approach, the model forecasts trends 1 to 3 months ahead. Unlike prior studies that focus on individual listings at fixed time points, our approach constructs regional representations by integrating listing features with external contextual factors such as urban accessibility and human mobility. We convert structured tabular data into prompt-based inputs for a Large Language Model (LLM), producing comprehensive regional embeddings. These embeddings are then fed into advanced time-series models (RNN, LSTM, Transformer) to better capture complex spatio-temporal dynamics. Experiments on Seoul's Airbnb dataset show that our method reduces both average RMSE and MAE by approximately 48% compared to conventional baselines, including traditional statistical and machine learning models. Our framework not only improves forecasting accuracy but also offers practical insights for detecting oversupplied regions and supporting data-driven urban policy decisions.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14235v1" target="_blank"><h2>Spontaneous Symmetry Breaking as a Late-Time Trigger for Interacting Dark Energy <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Pradosh Keshav MV, NS Kavya, Kenath Arun<br><strong><u>Categories:</u></strong> astro-ph.CO, gr-qc<br><strong><u>Comments:</u></strong> Prepared for submission to JCAP<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> Persistent tensions in the Hubble constant (H0) and the matter clustering parameter (S8) motivate late-time new physics that suppresses structure growth without significantly altering the background expansion history of the LambdaCDM model. We study a class of dark-sector dynamics in which a scalar dark energy field, governed by a Z2-symmetric quartic potential, interacts with dark matter through Yukawa and portal couplings. When the matter density drops below a critical threshold, a cosmological spontaneous symmetry breaking mechanism generates a time-dependent vacuum expectation value v(a) and activates an effective coupling eta(a). This creates a symmetric phase (a <= ac) identical to LambdaCDM at early times, and a broken phase (a > ac) in which eta(a) > 0 transfers energy from dark matter to dark energy, suppressing linear structure growth. Using RSD, BAO, cosmic chronometers, Pantheon+SH0ES supernovae, and compressed Planck distance priors, we compare a fixed LambdaCDM background with a self-consistent coupled-scalar evolution. The RSD-only analysis shows a strong shift: the dynamical background gives Omega_m ~ 0.31 +/- 0.10 and sigma8,0 ~ 0.59 +/- 0.01, while the fixed-background case gives Omega_m ~ 0.20 +/- 0.09 and sigma8,0 ~ 0.75 +/- 0.05. In the full joint fit, we obtain Omega_m = 0.29 +/- 0.01, H0 = 69.7 +/- 0.6 km s^-1 Mpc^-1, and sigma8,0 = 0.78 +/- 0.01. A late-time interaction triggered by spontaneous symmetry breaking can therefore damp structure growth and ease the S8 tension while leaving the expansion history and the inferred H0 essentially unchanged, suggesting distinct physical origins for the two tensions.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14229v1" target="_blank"><h2>EBind: a practical approach to space binding <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jim Broadbent, Felix Cohen, Frederik Hvilshøj, Eric Landau, Eren Sasoglu<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> We simplify space binding by focusing on two core components, a single encoder per modality and high-quality data; enabling training state-of-the-art models on a single GPU in a few hours as opposed to multiple days. We present EBind, an Easy, data-centric, and parameter-efficient method to Bind the embedding spaces of multiple contrastive models. We demonstrate that a simple 1.8B-parameter image-text-video-audio-3D model can outperform models 4 to 17x the size. The key to achieving this is a carefully curated dataset of three complementary data sources: i) 6.7M fully-automated multimodal quintuples sourced via SOTA retrieval models, ii) 1M diverse, semi-automated triples annotated by humans as negative, partial, or positive matches, and iii) 3.4M pre-existing captioned data items. We use 13 different evaluations to demonstrate the value of each data source. Due to limitations with existing benchmarks, we further introduce the first high-quality, consensus-annotated zero-shot classification benchmark between audio and PCs. In contrast to related work, we will open-source our code, model weights, and datasets.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14219v1" target="_blank"><h2>Listen Like a Teacher: Mitigating Whisper Hallucinations using Adaptive Layer Attention and Knowledge Distillation <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Kumud Tripathi, Aditya Srinivas Menon, Aman Gaurav, Raj Prakash Gohil, Pankaj Wasnik<br><strong><u>Categories:</u></strong> cs.AI, cs.SD<br><strong><u>Comments:</u></strong> Accepted at AAAI 2026 - Main Technical Track<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> The Whisper model, an open-source automatic speech recognition system, is widely adopted for its strong performance across multilingual and zero-shot settings. However, it frequently suffers from hallucination errors, especially under noisy acoustic conditions. Previous works to reduce hallucinations in Whisper-style ASR systems have primarily focused on audio preprocessing or post-processing of transcriptions to filter out erroneous content. However, modifications to the Whisper model itself remain largely unexplored to mitigate hallucinations directly. To address this challenge, we present a two-stage architecture that first enhances encoder robustness through Adaptive Layer Attention (ALA) and further suppresses hallucinations using a multi-objective knowledge distillation (KD) framework. In the first stage, ALA groups encoder layers into semantically coherent blocks via inter-layer correlation analysis. A learnable multi-head attention module then fuses these block representations, enabling the model to jointly exploit low- and high-level features for more robust encoding. In the second stage, our KD framework trains the student model on noisy audio to align its semantic and attention distributions with a teacher model processing clean inputs. Our experiments on noisy speech benchmarks show notable reductions in hallucinations and word error rates, while preserving performance on clean speech. Together, ALA and KD offer a principled strategy to improve Whisper's reliability under real-world noisy conditions.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14210v2" target="_blank"><h2>Orion: A Unified Visual Agent for Multimodal Perception, Advanced Visual Reasoning and Execution <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> N Dinesh Reddy, Dylan Snyder, Lona Kiragu, Mirajul Mohin, Shahrear Bin Amin, Sudeep Pillai<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> multimodal (title)<br><p><strong><u>Abstract:</u></strong> We introduce Orion, a visual agent that integrates vision-based reasoning with tool-augmented execution to achieve powerful, precise, multi-step visual intelligence across images, video, and documents. Unlike traditional vision-language models that generate descriptive outputs, Orion orchestrates a suite of specialized computer vision tools, including object detection, keypoint localization, panoptic segmentation, Optical Character Recognition (OCR), and geometric analysis, to execute complex multi-step visual workflows. The system achieves competitive performance across MMMU, MMBench, DocVQA, and MMLongBench while extending monolithic VLM capabilities to production-grade visual intelligence. Through its agentic, tool-augmented approach, Orion enables autonomous visual reasoning that bridges neural perception with symbolic execution, marking the transition from passive visual understanding to active, tool-driven visual intelligence.
  Try Orion for free at: https://chat.vlm.run
  Learn more at: https://www.vlm.run/orion</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14206v1" target="_blank"><h2>Causal Discovery on Higher-Order Interactions <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Alessio Zanga, Marco Scutari, Fabio Stella<br><strong><u>Categories:</u></strong> stat.ML, cs.LG<br><strong><u>Comments:</u></strong> 16 pages, 2 figures<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Causal discovery combines data with knowledge provided by experts to learn the DAG representing the causal relationships between a given set of variables. When data are scarce, bagging is used to measure our confidence in an average DAG obtained by aggregating bootstrapped DAGs. However, the aggregation step has received little attention from the specialized literature: the average DAG is constructed using only the confidence in the individual edges of the bootstrapped DAGs, thus disregarding complex higher-order edge structures. In this paper, we introduce a novel theoretical framework based on higher-order structures and describe a new DAG aggregation algorithm. We perform a simulation study, discussing the advantages and limitations of the proposed approach. Our proposal is both computationally efficient and effective, outperforming state-of-the-art solutions, especially in low sample size regimes and under high dimensionality settings.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14203v1" target="_blank"><h2>Multi-Scale Correlation-Aware Transformer for Maritime Vessel Re-Identification <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Yunhe Liu<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> Maritime vessel re-identification (Re-ID) plays a crucial role in advancing maritime monitoring and intelligent situational awareness systems. However, some existing vessel Re-ID methods are directly adapted from pedestrian-focused algorithms, making them ill-suited for mitigating the unique problems present in vessel images, particularly the greater intra-identity variations and more severe missing of local parts, which lead to the emergence of outlier samples within the same identity. To address these challenges, we propose the Multi-scale Correlation-aware Transformer Network (MCFormer), which explicitly models multi-scale correlations across the entire input set to suppress the adverse effects of outlier samples with intra-identity variations or local missing, incorporating two novel modules, the Global Correlation Module (GCM), and the Local Correlation Module (LCM). Specifically, GCM constructs a global similarity affinity matrix across all input images to model global correlations through feature aggregation based on inter-image consistency, rather than solely learning features from individual images as in most existing approaches. Simultaneously, LCM mines and aligns local features of positive samples with contextual similarity to extract local correlations by maintaining a dynamic memory bank, effectively compensating for missing or occluded regions in individual images. To further enhance feature robustness, MCFormer integrates global and local features that have been respectively correlated across multiple scales, effectively capturing latent relationships among image features. Experiments on three benchmarks demonstrate that MCFormer achieves state-of-the-art performance.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14186v1" target="_blank"><h2>Few-Shot Precise Event Spotting via Unified Multi-Entity Graph and Distillation <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Zhaoyu Liu, Kan Jiang, Murong Ma, Zhe Hou, Yun Lin, Jin Song Dong<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> The 40th Annual AAAI Conference on Artificial Intelligence (AAAI 2026)<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Precise event spotting (PES) aims to recognize fine-grained events at exact moments and has become a key component of sports analytics. This task is particularly challenging due to rapid succession, motion blur, and subtle visual differences. Consequently, most existing methods rely on domain-specific, end-to-end training with large labeled datasets and often struggle in few-shot conditions due to their dependence on pixel- or pose-based inputs alone. However, obtaining large labeled datasets is practically hard. We propose a Unified Multi-Entity Graph Network (UMEG-Net) for few-shot PES. UMEG-Net integrates human skeletons and sport-specific object keypoints into a unified graph and features an efficient spatio-temporal extraction module based on advanced GCN and multi-scale temporal shift. To further enhance performance, we employ multimodal distillation to transfer knowledge from keypoint-based graphs to visual representations. Our approach achieves robust performance with limited labeled data and significantly outperforms baseline models in few-shot settings, providing a scalable and effective solution for few-shot PES. Code is publicly available at https://github.com/LZYAndy/UMEG-Net.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14172v1" target="_blank"><h2>SymLoc: Symbolic Localization of Hallucination across HaluEval and TruthfulQA <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Naveen Lamba, Sanju Tiwari, Manas Gaur<br><strong><u>Categories:</u></strong> cs.CL, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> LLMs still struggle with hallucination, especially when confronted with symbolic triggers like modifiers, negation, numbers, exceptions, and named entities. Yet, we lack a clear understanding of where these symbolic hallucinations originate, making it crucial to systematically handle such triggers and localize the emergence of hallucination inside the model. While prior work explored localization using statistical techniques like LSC and activation variance analysis, these methods treat all tokens equally and overlook the role symbolic linguistic knowledge plays in triggering hallucinations. So far, no approach has investigated how symbolic elements specifically drive hallucination failures across model layers, nor has symbolic linguistic knowledge been used as the foundation for a localization framework. We propose the first symbolic localization framework that leverages symbolic linguistic and semantic knowledge to meaningfully trace the development of hallucinations across all model layers. By focusing on how models process symbolic triggers, we analyze five models using HaluEval and TruthfulQA. Our symbolic knowledge approach reveals that attention variance for these linguistic elements explodes to critical instability in early layers (2-4), with negation triggering catastrophic variance levels, demonstrating that symbolic semantic processing breaks down from the very beginning. Through the lens of symbolic linguistic knowledge, despite larger model sizes, hallucination rates remain consistently high (78.3%-83.7% across Gemma variants), with steep attention drops for symbolic semantic triggers throughout deeper layers. Our findings demonstrate that hallucination is fundamentally a symbolic linguistic processing failure, not a general generation problem, revealing that symbolic semantic knowledge provides the key to understanding and localizing hallucination mechanisms in LLMs.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14169v1" target="_blank"><h2>AdaTok: Adaptive Token Compression with Object-Aware Representations for Efficient Multimodal LLMs <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Xinliang Zhang, Lei Zhu, Hangzhou He, Shuang Zeng, Ourui Fu, Jiakui Hu, Zhengjian Yao, Yanye Lu<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Multimodal Large Language Models (MLLMs) have demonstrated substantial value in unified text-image understanding and reasoning, primarily by converting images into sequences of patch-level tokens that align with their architectural paradigm. However, patch-level tokenization leads to a quadratic growth in image tokens, burdening MLLMs' understanding and reasoning with enormous computation and memory. Additionally, the traditional patch-wise scanning tokenization workflow misaligns with the human vision cognition system, further leading to hallucination and computational redundancy. To address this issue, we propose an object-level token merging strategy for Adaptive Token compression, revealing the consistency with human vision system. The experiments are conducted on multiple comprehensive benchmarks, which show that our approach averagely, utilizes only 10% tokens while achieving almost 96% of the vanilla model's performance. More extensive experimental results in comparison with relevant works demonstrate the superiority of our method in balancing compression ratio and performance. Our code will be available.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14168v1" target="_blank"><h2>Certified Signed Graph Unlearning <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Junpeng Zhao, Lin Li, Kaixi Hu, Kaize Shi, Jingling Yuan<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Signed graphs model complex relationships through positive and negative edges, with widespread real-world applications. Given the sensitive nature of such data, selective removal mechanisms have become essential for privacy protection. While graph unlearning enables the removal of specific data influences from Graph Neural Networks (GNNs), existing methods are designed for conventional GNNs and overlook the unique heterogeneous properties of signed graphs. When applied to Signed Graph Neural Networks (SGNNs), these methods lose critical sign information, degrading both model utility and unlearning effectiveness. To address these challenges, we propose Certified Signed Graph Unlearning (CSGU), which provides provable privacy guarantees while preserving the sociological principles underlying SGNNs. CSGU employs a three-stage method: (1) efficiently identifying minimal influenced neighborhoods via triangular structures, (2) applying sociological theories to quantify node importance for optimal privacy budget allocation, and (3) performing importance-weighted parameter updates to achieve certified modifications with minimal utility degradation. Extensive experiments demonstrate that CSGU outperforms existing methods, achieving superior performance in both utility preservation and unlearning effectiveness on SGNNs.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14153v1" target="_blank"><h2>A Comprehensive Study of Implicit and Explicit Biases in Large Language Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Fatima Kazi, Alex Young, Yash Inani, Setareh Rafatirad<br><strong><u>Categories:</u></strong> cs.LG, cs.CY<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> data augmentation (abstract)<br><p><strong><u>Abstract:</u></strong> Large Language Models (LLMs) inherit explicit and implicit biases from their training datasets. Identifying and mitigating biases in LLMs is crucial to ensure fair outputs, as they can perpetuate harmful stereotypes and misinformation. This study highlights the need to address biases in LLMs amid growing generative AI. We studied bias-specific benchmarks such as StereoSet and CrowSPairs to evaluate the existence of various biases in multiple generative models such as BERT and GPT 3.5. We proposed an automated Bias-Identification Framework to recognize various social biases in LLMs such as gender, race, profession, and religion. We adopted a two-pronged approach to detect explicit and implicit biases in text data. Results indicated fine-tuned models struggle with gender biases but excelled at identifying and avoiding racial biases. Our findings illustrated that despite having some success, LLMs often over-relied on keywords. To illuminate the capability of the analyzed LLMs in detecting implicit biases, we employed Bag-of-Words analysis and unveiled indications of implicit stereotyping within the vocabulary. To bolster the model performance, we applied an enhancement strategy involving fine-tuning models using prompting techniques and data augmentation of the bias benchmarks. The fine-tuned models exhibited promising adaptability during cross-dataset testing and significantly enhanced performance on implicit bias benchmarks, with performance gains of up to 20%.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14143v1" target="_blank"><h2>SMART: Shot-Aware Multimodal Video Moment Retrieval with Audio-Enhanced MLLM <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> An Yu, Weiheng Lu, Jian Li, Zhenfei Zhang, Yunhang Shen, Felix X. -F. Ye, Ming-Ching Chang<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Video Moment Retrieval is a task in video understanding that aims to localize a specific temporal segment in an untrimmed video based on a natural language query. Despite recent progress in moment retrieval from videos using both traditional techniques and Multimodal Large Language Models (MLLM), most existing methods still rely on coarse temporal understanding and a single visual modality, limiting performance on complex videos. To address this, we introduce \textit{S}hot-aware \textit{M}ultimodal \textit{A}udio-enhanced \textit{R}etrieval of \textit{T}emporal \textit{S}egments (SMART), an MLLM-based framework that integrates audio cues and leverages shot-level temporal structure. SMART enriches multimodal representations by combining audio and visual features while applying \textbf{Shot-aware Token Compression}, which selectively retains high-information tokens within each shot to reduce redundancy and preserve fine-grained temporal details. We also refine prompt design to better utilize audio-visual cues. Evaluations on Charades-STA and QVHighlights show that SMART achieves significant improvements over state-of-the-art methods, including a 1.61\% increase in R1@0.5 and 2.59\% gain in R1@0.7 on Charades-STA.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14131v1" target="_blank"><h2>Run, Ruminate, and Regulate: A Dual-process Thinking System for Vision-and-Language Navigation <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Yu Zhong, Zihao Zhang, Rui Zhang, Lingdong Huang, Haihan Gao, Shuo Wang, Da Li, Ruijian Han, Jiaming Guo, Shaohui Peng, Di Huang, Yunji Chen<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract), transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Vision-and-Language Navigation (VLN) requires an agent to dynamically explore complex 3D environments following human instructions. Recent research underscores the potential of harnessing large language models (LLMs) for VLN, given their commonsense knowledge and general reasoning capabilities. Despite their strengths, a substantial gap in task completion performance persists between LLM-based approaches and domain experts, as LLMs inherently struggle to comprehend real-world spatial correlations precisely. Additionally, introducing LLMs is accompanied with substantial computational cost and inference latency. To address these issues, we propose a novel dual-process thinking framework dubbed R3, integrating LLMs' generalization capabilities with VLN-specific expertise in a zero-shot manner. The framework comprises three core modules: Runner, Ruminator, and Regulator. The Runner is a lightweight transformer-based expert model that ensures efficient and accurate navigation under regular circumstances. The Ruminator employs a powerful multimodal LLM as the backbone and adopts chain-of-thought (CoT) prompting to elicit structured reasoning. The Regulator monitors the navigation progress and controls the appropriate thinking mode according to three criteria, integrating Runner and Ruminator harmoniously. Experimental results illustrate that R3 significantly outperforms other state-of-the-art methods, exceeding 3.28% and 3.30% in SPL and RGSPL respectively on the REVERIE benchmark. This pronounced enhancement highlights the effectiveness of our method in handling challenging VLN tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14119v1" target="_blank"><h2>Real-Time Mobile Video Analytics for Pre-arrival Emergency Medical Services <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Liuyi Jin, Amran Haroon, Radu Stoleru, Pasan Gunawardena, Michael Middleton, Jeeeun Kim<br><strong><u>Categories:</u></strong> cs.MM, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Timely and accurate pre-arrival video streaming and analytics are critical for emergency medical services (EMS) to deliver life-saving interventions. Yet, current-generation EMS infrastructure remains constrained by one-to-one video streaming and limited analytics capabilities, leaving dispatchers and EMTs to manually interpret overwhelming, often noisy or redundant information in high-stress environments. We present TeleEMS, a mobile live video analytics system that enables pre-arrival multimodal inference by fusing audio and video into a unified decision-making pipeline before EMTs arrive on scene.
  TeleEMS comprises two key components: TeleEMS Client and TeleEMS Server. The TeleEMS Client runs across phones, smart glasses, and desktops to support bystanders, EMTs en route, and 911 dispatchers. The TeleEMS Server, deployed at the edge, integrates EMS-Stream, a communication backbone that enables smooth multi-party video streaming. On top of EMSStream, the server hosts three real-time analytics modules: (1) audio-to-symptom analytics via EMSLlama, a domain-specialized LLM for robust symptom extraction and normalization; (2) video-to-vital analytics using state-of-the-art rPPG methods for heart rate estimation; and (3) joint text-vital analytics via PreNet, a multimodal multitask model predicting EMS protocols, medication types, medication quantities, and procedures.
  Evaluation shows that EMSLlama outperforms GPT-4o (exact-match 0.89 vs. 0.57) and that text-vital fusion improves inference robustness, enabling reliable pre-arrival intervention recommendations. TeleEMS demonstrates the potential of mobile live video analytics to transform EMS operations, bridging the gap between bystanders, dispatchers, and EMTs, and paving the way for next-generation intelligent EMS infrastructure.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14112v1" target="_blank"><h2>Synthetic Clinical Notes for Rare ICD Codes: A Data-Centric Framework for Long-Tail Medical Coding <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Truong Vo, Weiyi Wu, Kaize Ding<br><strong><u>Categories:</u></strong> cs.CL, cs.AI<br><strong><u>Comments:</u></strong> 4 page-short paper<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Automatic ICD coding from clinical text is a critical task in medical NLP but remains hindered by the extreme long-tail distribution of diagnostic codes. Thousands of rare and zero-shot ICD codes are severely underrepresented in datasets like MIMIC-III, leading to low macro-F1 scores. In this work, we propose a data-centric framework that generates high-quality synthetic discharge summaries to mitigate this imbalance. Our method constructs realistic multi-label code sets anchored on rare codes by leveraging real-world co-occurrence patterns, ICD descriptions, synonyms, taxonomy, and similar clinical notes. Using these structured prompts, we generate 90,000 synthetic notes covering 7,902 ICD codes, significantly expanding the training distribution. We fine-tune two state-of-the-art transformer-based models, PLM-ICD and GKI-ICD, on both the original and extended datasets. Experiments show that our approach modestly improves macro-F1 while maintaining strong micro-F1, outperforming prior SOTA. While the gain may seem marginal relative to the computational cost, our results demonstrate that carefully crafted synthetic data can enhance equity in long-tail ICD code prediction.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14111v1" target="_blank"><h2>CascadedViT: Cascaded Chunk-FeedForward and Cascaded Group Attention Vision Transformer <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Srivathsan Sivakumar, Faisal Z. Qureshi<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract), attention (title)<br><p><strong><u>Abstract:</u></strong> Vision Transformers (ViTs) have demonstrated remarkable performance across a range of computer vision tasks; however, their high computational, memory, and energy demands hinder deployment on resource-constrained platforms. In this paper, we propose \emph{Cascaded-ViT (CViT)}, a lightweight and compute-efficient vision transformer architecture featuring a novel feedforward network design called \emph{Cascaded-Chunk Feed Forward Network (CCFFN)}. By splitting input features, CCFFN improves parameter and FLOP efficiency without sacrificing accuracy. Experiments on ImageNet-1K show that our \emph{CViT-XL} model achieves 75.5\% Top-1 accuracy while reducing FLOPs by 15\% and energy consumption by 3.3\% compared to EfficientViT-M5. Across various model sizes, the CViT family consistently exhibits the lowest energy consumption, making it suitable for deployment on battery-constrained devices such as mobile phones and drones. Furthermore, when evaluated using a new metric called \emph{Accuracy-Per-FLOP (APF)}, which quantifies compute efficiency relative to accuracy, CViT models consistently achieve top-ranking efficiency. Particularly, CViT-L is 2.2\% more accurate than EfficientViT-M2 while having comparable APF scores.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14110v1" target="_blank"><h2>A Patient-Independent Neonatal Seizure Prediction Model Using Reduced Montage EEG and ECG <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Sithmini Ranasingha, Agasthi Haputhanthri, Hansa Marasinghe, Nima Wickramasinghe, Kithmin Wickremasinghe, Jithangi Wanigasinghe, Chamira U. S. Edussooriya, Joshua P. Kulasingham<br><strong><u>Categories:</u></strong> eess.SP, cs.LG<br><strong><u>Comments:</u></strong> 10 pages, 4 figures<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), explainable (abstract), neural network (abstract), transfer learning (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Neonates are highly susceptible to seizures, often leading to short or long-term neurological impairments. However, clinical manifestations of neonatal seizures are subtle and often lead to misdiagnoses. This increases the risk of prolonged, untreated seizure activity and subsequent brain injury. Continuous video electroencephalogram (cEEG) monitoring is the gold standard for seizure detection. However, this is an expensive evaluation that requires expertise and time. In this study, we propose a convolutional neural network-based model for early prediction of neonatal seizures by distinguishing between interictal and preictal states of the EEG. Our model is patient-independent, enabling generalization across multiple subjects, and utilizes mel-frequency cepstral coefficient matrices extracted from multichannel EEG and electrocardiogram (ECG) signals as input features. Trained and validated on the Helsinki neonatal EEG dataset with 10-fold cross-validation, the proposed model achieved an average accuracy of 97.52%, sensitivity of 98.31%, specificity of 96.39%, and F1-score of 97.95%, enabling accurate seizure prediction up to 30 minutes before onset. The inclusion of ECG alongside EEG improved the F1-score by 1.42%, while the incorporation of an attention mechanism yielded an additional 0.5% improvement. To enhance transparency, we incorporated SHapley Additive exPlanations (SHAP) as an explainable artificial intelligence method to interpret the model and provided localization of seizure focus using scalp plots. The overall results demonstrate the model's potential for minimally supervised deployment in neonatal intensive care units, enabling timely and reliable prediction of neonatal seizures, while demonstrating strong generalization capability across unseen subjects through transfer learning.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14099v1" target="_blank"><h2>FAPE-IR: Frequency-Aware Planning and Execution Framework for All-in-One Image Restoration <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Jingren Liu, Shuning Xu, Qirui Yang, Yun Wang, Xiangyu Chen, Zhong Ji<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> All-in-One Image Restoration (AIO-IR) aims to develop a unified model that can handle multiple degradations under complex conditions. However, existing methods often rely on task-specific designs or latent routing strategies, making it hard to adapt to real-world scenarios with various degradations. We propose FAPE-IR, a Frequency-Aware Planning and Execution framework for image restoration. It uses a frozen Multimodal Large Language Model (MLLM) as a planner to analyze degraded images and generate concise, frequency-aware restoration plans. These plans guide a LoRA-based Mixture-of-Experts (LoRA-MoE) module within a diffusion-based executor, which dynamically selects high- or low-frequency experts, complemented by frequency features of the input image. To further improve restoration quality and reduce artifacts, we introduce adversarial training and a frequency regularization loss. By coupling semantic planning with frequency-based restoration, FAPE-IR offers a unified and interpretable solution for all-in-one image restoration. Extensive experiments show that FAPE-IR achieves state-of-the-art performance across seven restoration tasks and exhibits strong zero-shot generalization under mixed degradations.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14087v1" target="_blank"><h2>GCA-ResUNet:Image segmentation in medical images using grouped coordinate attention <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jun Ding, Shang Gao<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract), transformer (abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Medical image segmentation underpins computer-aided diagnosis and therapy by supporting clinical diagnosis, preoperative planning, and disease monitoring. While U-Net style convolutional neural networks perform well due to their encoder-decoder structures with skip connections, they struggle to capture long-range dependencies. Transformer-based variants address global context but often require heavy computation and large training datasets. This paper proposes GCA-ResUNet, an efficient segmentation network that integrates Grouped Coordinate Attention (GCA) into ResNet-50 residual blocks. GCA uses grouped coordinate modeling to jointly encode global dependencies across channels and spatial locations, strengthening feature representation and boundary delineation while adding minimal parameter and FLOP overhead compared with self-attention. On the Synapse dataset, GCA-ResUNet achieves a Dice score of 86.11%, and on the ACDC dataset, it reaches 92.64%, surpassing several state-of-the-art baselines while maintaining fast inference and favorable computational efficiency. These results indicate that GCA offers a practical way to enhance convolutional architectures with global modeling capability, enabling high-accuracy and resource-efficient medical image segmentation.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14082v1" target="_blank"><h2>Zero-Training Task-Specific Model Synthesis for Few-Shot Medical Image Classification <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yao Qin, Yangyang Yan, YuanChao Yang, Jinhua Pang, Huanyong Bi, Yuan Liu, HaiHua Wang<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> multi-modal (abstract)<br><p><strong><u>Abstract:</u></strong> Deep learning models have achieved remarkable success in medical image analysis but are fundamentally constrained by the requirement for large-scale, meticulously annotated datasets. This dependency on "big data" is a critical bottleneck in the medical domain, where patient data is inherently difficult to acquire and expert annotation is expensive, particularly for rare diseases where samples are scarce by definition. To overcome this fundamental challenge, we propose a novel paradigm: Zero-Training Task-Specific Model Synthesis (ZS-TMS). Instead of adapting a pre-existing model or training a new one, our approach leverages a large-scale, pre-trained generative engine to directly synthesize the entire set of parameters for a task-specific classifier. Our framework, the Semantic-Guided Parameter Synthesizer (SGPS), takes as input minimal, multi-modal task information as little as a single example image (1-shot) and a corresponding clinical text description to directly synthesize the entire set of parameters for a task-specific classifier.
  The generative engine interprets these inputs to generate the weights for a lightweight, efficient classifier (e.g., an EfficientNet-V2), which can be deployed for inference immediately without any task-specific training or fine-tuning. We conduct extensive evaluations on challenging few-shot classification benchmarks derived from the ISIC 2018 skin lesion dataset and a custom rare disease dataset. Our results demonstrate that SGPS establishes a new state-of-the-art, significantly outperforming advanced few-shot and zero-shot learning methods, especially in the ultra-low data regimes of 1-shot and 5-shot classification. This work paves the way for the rapid development and deployment of AI-powered diagnostic tools, particularly for the long tail of rare diseases where data is critically limited.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14076v1" target="_blank"><h2>Meta-SimGNN: Adaptive and Robust WiFi Localization Across Dynamic Configurations and Diverse Scenarios <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Qiqi Xiao, Ziqi Ye, Yinghui He, Jianwei Liu, Guanding Yu<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> To promote the practicality of deep learning-based localization, existing studies aim to address the issue of scenario dependence through meta-learning. However, these studies primarily focus on variations in environmental layouts while overlooking the impact of changes in device configurations, such as bandwidth, the number of access points (APs), and the number of antennas used. Unlike environmental changes, variations in device configurations affect the dimensionality of channel state information (CSI), thereby compromising neural network usability. To address this issue, we propose Meta-SimGNN, a novel WiFi localization system that integrates graph neural networks with meta-learning to improve localization generalization and robustness. First, we introduce a fine-grained CSI graph construction scheme, where each AP is treated as a graph node, allowing for adaptability to changes in the number of APs. To structure the features of each node, we propose an amplitude-phase fusion method and a feature extraction method. The former utilizes both amplitude and phase to construct CSI images, enhancing data reliability, while the latter extracts dimension-consistent features to address variations in bandwidth and the number of antennas. Second, a similarity-guided meta-learning strategy is developed to enhance adaptability in diverse scenarios. The initial model parameters for the fine-tuning stage are determined by comparing the similarity between the new scenario and historical scenarios, facilitating rapid adaptation of the model to the new localization scenario. Extensive experimental results over commodity WiFi devices in different scenarios show that Meta-SimGNN outperforms the baseline methods in terms of localization generalization and accuracy.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14064v1" target="_blank"><h2>CafeMed: Causal Attention Fusion Enhanced Medication Recommendation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Kelin Ren, Chan-Yang Ju, Dong-Ho Lee<br><strong><u>Categories:</u></strong> cs.LG, cs.AI, stat.ME<br><strong><u>Comments:</u></strong> Accepted by BIBM 2025<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Medication recommendation systems play a crucial role in assisting clinicians with personalized treatment decisions. While existing approaches have made significant progress in learning medication representations, they suffer from two fundamental limitations: (i) treating medical entities as independent features without modeling their synergistic effects on medication selection; (ii) employing static causal relationships that fail to adapt to patient-specific contexts and health states. To address these challenges, we propose CafeMed, a framework that integrates dynamic causal reasoning with cross-modal attention for safe and accurate medication recommendation. CafeMed introduces two key components: the Causal Weight Generator (CWG) that transforms static causal effects into dynamic modulation weights based on individual patient states, and the Channel Harmonized Attention Refinement Module (CHARM) that captures complex interdependencies between diagnoses and procedures. This design enables CafeMed to model how different medical conditions jointly influence treatment decisions while maintaining medication safety constraints. Extensive experiments on MIMIC-III and MIMIC-IV datasets demonstrate that CafeMed significantly outperforms state-of-the-art baselines, achieving superior accuracy in medication prediction while maintaining the lower drug--drug interaction rates. Our results indicate that incorporating dynamic causal relationships and cross-modal synergies leads to more clinically-aligned and personalized medication recommendations. Our code is released publicly at https://github.com/rkl71/CafeMed.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14062v1" target="_blank"><h2>LogPurge: Log Data Purification for Anomaly Detection via Rule-Enhanced Filtering <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Shenglin Zhang, Ziang Chen, Zijing Que, Yilun Liu, Yongqian Sun, Sicheng Wei, Dan Pei, Hailin Li<br><strong><u>Categories:</u></strong> cs.SE, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> anomaly detection (title, abstract)<br><p><strong><u>Abstract:</u></strong> Log anomaly detection, which is critical for identifying system failures and preempting security breaches, detects irregular patterns within large volumes of log data, and impacts domains such as service reliability, performance optimization, and database log analysis. Modern log anomaly detection methods rely on training deep learning models on clean, anomaly-free log sequences. However, obtaining such clean log data requires costly and tedious human labeling, and existing automatic cleaning methods fail to fully integrate the specific characteristics and actual semantics of logs in their purification process. In this paper, we propose a cost-aware, rule-enhanced purification framework, LogPurge, that automatically selects a sufficient subset of normal log sequences from contamination log sequences to train a anomaly detection model. Our approach involves a two-stage filtering algorithm: In the first stage, we use a large language model (LLM) to remove clustered anomalous patterns and enhance system rules to improve LLM's understanding of system logs; in the second stage, we utilize a divide-and-conquer strategy that decomposes the remaining contaminated regions into smaller subproblems, allowing each to be effectively purified through the first stage procedure. Our experiments, conducted on two public datasets and one industrial dataset, show that our method significantly removes an average of 98.74% of anomalies while retaining 82.39% of normal samples. Compared to the latest unsupervised log sample selection algorithms, our method achieves F-1 score improvements of 35.7% and 84.11% on the public datasets, and an impressive 149.72% F-1 improvement on the private dataset, demonstrating the effectiveness of our approach.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14057v1" target="_blank"><h2>A Machine Learning-Based Multimodal Framework for Wearable Sensor-Based Archery Action Recognition and Stress Estimation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Xianghe Liu, Jiajia Liu, Chuxian Xu, Minghan Wang, Hongbo Peng, Tao Sun, Jiaqi Xu<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> In precision sports such as archery, athletes' performance depends on both biomechanical stability and psychological resilience. Traditional motion analysis systems are often expensive and intrusive, limiting their use in natural training environments. To address this limitation, we propose a machine learning-based multimodal framework that integrates wearable sensor data for simultaneous action recognition and stress estimation. Using a self-developed wrist-worn device equipped with an accelerometer and photoplethysmography (PPG) sensor, we collected synchronized motion and physiological data during real archery sessions. For motion recognition, we introduce a novel feature--Smoothed Differential Acceleration (SmoothDiff)--and employ a Long Short-Term Memory (LSTM) model to identify motion phases, achieving 96.8% accuracy and 95.9% F1-score. For stress estimation, we extract heart rate variability (HRV) features from PPG signals and apply a Multi-Layer Perceptron (MLP) classifier, achieving 80% accuracy in distinguishing high- and low-stress levels. The proposed framework demonstrates that integrating motion and physiological sensing can provide meaningful insights into athletes' technical and mental states. This approach offers a foundation for developing intelligent, real-time feedback systems for training optimization in archery and other precision sports.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14056v1" target="_blank"><h2>Radial Compensation: Stable and Semantically Decoupled Generative Models on Riemannian Manifolds <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Marios Papamichals, Regina Ruane<br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.IT, math.DG, stat.ML<br><strong><u>Comments:</u></strong> This is the first version of the paper<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> Generative models on curved spaces rely on charts to map Euclidean spaces to manifolds. Exponential maps preserve geodesics but have stiff, radius-dependent Jacobians, while volume-preserving charts maintain densities but distort geodesic distances. Both approaches entangle curvature with model parameters, inflating gradient variance. In high-dimensional latent normalizing flows, the wrapped exponential prior can stretch radii far beyond the curvature scale, leading to poor test likelihoods and stiff solvers. We introduce Radial Compensation (RC), an information-geometric method that selects the base density in the tangent space so that the likelihood depends only on geodesic distance from a pole, decoupling parameter semantics from curvature. RC lets radial parameters retain their usual meaning in geodesic units, while the chart can be tuned as a numerical preconditioner. We extend RC to manifolds with known geodesic polar volume and show that RC is the only construction for geodesic-radial likelihoods with curvature-invariant Fisher information. We derive the Balanced-Exponential (bExp) chart family, balancing volume distortion and geodesic error. Under RC, all bExp settings preserve the same manifold density and Fisher information, with smaller dial values reducing gradient variance and flow cost. Empirically, RC yields stable generative models across densities, VAEs, flows on images and graphs, and protein models. RC improves likelihoods, restores clean geodesic radii, and prevents radius blow-ups in high-dimensional flows, making RC-bExp a robust default for likelihood-trained generative models on manifolds.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14052v1" target="_blank"><h2>Making Evidence Actionable in Adaptive Learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Amirreza Mehrabi, Jason W. Morphew, Breejha Quezada, N. Sanjay Rebello<br><strong><u>Categories:</u></strong> cs.AI, cs.CE, stat.AP, stat.OT<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Adaptive learning often diagnoses precisely yet intervenes weakly, yielding help that is mistimed or misaligned. This study presents evidence supporting an instructor-governed feedback loop that converts concept-level assessment evidence into vetted micro-interventions. The adaptive learning algorithm contains three safeguards: adequacy as a hard guarantee of gap closure, attention as a budgeted constraint for time and redundancy, and diversity as protection against overfitting to a single resource. We formalize intervention assignment as a binary integer program with constraints for coverage, time, difficulty windows informed by ability estimates, prerequisites encoded by a concept matrix, and anti-redundancy enforced through diversity. Greedy selection serves low-richness and tight-latency regimes, gradient-based relaxation serves rich repositories, and a hybrid method transitions along a richness-latency frontier. In simulation and in an introductory physics deployment with one thousand two hundred four students, both solvers achieved full skill coverage for essentially all learners within bounded watch time. The gradient-based method reduced redundant coverage by approximately twelve percentage points relative to greedy and harmonized difficulty across slates, while greedy delivered comparable adequacy with lower computational cost in scarce settings. Slack variables localized missing content and supported targeted curation, sustaining sufficiency across subgroups. The result is a tractable and auditable controller that closes the diagnostic-pedagogical loop and delivers equitable, load-aware personalization at classroom scale.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14049v1" target="_blank"><h2>SmallML: Bayesian Transfer Learning for Small-Data Predictive Analytics <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Semen Leontev<br><strong><u>Categories:</u></strong> cs.LG, stat.ML<br><strong><u>Comments:</u></strong> 64 pages, 5 figures, 15 tables<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> transfer learning (title, abstract)<br><p><strong><u>Abstract:</u></strong> Small and medium-sized enterprises (SMEs) represent 99.9% of U.S. businesses yet remain systematically excluded from AI due to a mismatch between their operational scale and modern machine learning's data requirements. This paper introduces SmallML, a Bayesian transfer learning framework achieving enterprise-level prediction accuracy with datasets as small as 50-200 observations.
  We develop a three-layer architecture integrating transfer learning, hierarchical Bayesian modeling, and conformal prediction. Layer 1 extracts informative priors from 22,673 public records using a SHAP-based procedure transferring knowledge from gradient boosting to logistic regression. Layer 2 implements hierarchical pooling across J=5-50 SMEs with adaptive shrinkage, balancing population patterns with entity-specific characteristics. Layer 3 provides conformal sets with finite-sample coverage guarantees P(y in C(x)) >= 1-alpha for distribution-free uncertainty quantification.
  Validation on customer churn data demonstrates 96.7% +/- 4.2% AUC with 100 observations per business -- a +24.2 point improvement over independent logistic regression (72.5% +/- 8.1%), with p < 0.000001. Conformal prediction achieves 92% empirical coverage at 90% target. Training completes in 33 minutes on standard CPU hardware. By enabling enterprise-grade predictions for 33 million U.S. SMEs previously excluded from machine learning, SmallML addresses a critical gap in AI democratization.
  Keywords: Bayesian transfer learning, hierarchical models, conformal prediction, small-data analytics, SME machine learning</p><br><hr><br><a href="https://arxiv.org/pdf/2511.15732v1" target="_blank"><h2>Just Asking Questions: Doing Our Own Research on Conspiratorial Ideation by Generative AI Chatbots <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Katherine M. FitzGerald, Michelle Riedlinger, Axel Bruns, Stephen Harrington, Timothy Graham, Daniel Angus<br><strong><u>Categories:</u></strong> cs.CY, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Interactive chat systems that build on artificial intelligence frameworks are increasingly ubiquitous and embedded into search engines, Web browsers, and operating systems, or are available on websites and apps. Researcher efforts have sought to understand the limitations and potential for harm of generative AI, which we contribute to here. Conducting a systematic review of six AI-powered chat systems (ChatGPT 3.5; ChatGPT 4 Mini; Microsoft Copilot in Bing; Google Search AI; Perplexity; and Grok in Twitter/X), this study examines how these leading products respond to questions related to conspiracy theories. This follows the platform policy implementation audit approach established by Glazunova et al. (2023). We select five well-known and comprehensively debunked conspiracy theories and four emerging conspiracy theories that relate to breaking news events at the time of data collection. Our findings demonstrate that the extent of safety guardrails against conspiratorial ideation in generative AI chatbots differs markedly, depending on chatbot model and conspiracy theory. Our observations indicate that safety guardrails in AI chatbots are often very selectively designed: generative AI companies appear to focus especially on ensuring that their products are not seen to be racist; they also appear to pay particular attention to conspiracy theories that address topics of substantial national trauma such as 9/11 or relate to well-established political issues. Future work should include an ongoing effort extended to further platforms, multiple languages, and a range of conspiracy theories extending well beyond the United States.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14044v1" target="_blank"><h2>The CHASM-SWPC Dataset for Coronal Hole Detection & Analysis <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Cutter Beck, Evan Smith, Khagendra Katuwal, Rudra Kafle, Jacob Whitehill<br><strong><u>Categories:</u></strong> astro-ph.IM, cs.CV, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Coronal holes (CHs) are low-activity, low-density solar coronal regions with open magnetic field lines (Cranmer 2009). In the extreme ultraviolet (EUV) spectrum, CHs appear as dark patches. Using daily hand-drawn maps from the Space Weather Prediction Center (SWPC), we developed a semi-automated pipeline to digitize the SWPC maps into binary segmentation masks. The resulting masks constitute the CHASM-SWPC dataset, a high-quality dataset to train and test automated CH detection models, which is released with this paper. We developed CHASM (Coronal Hole Annotation using Semi-automatic Methods), a software tool for semi-automatic annotation that enables users to rapidly and accurately annotate SWPC maps. The CHASM tool enabled us to annotate 1,111 CH masks, comprising the CHASM-SWPC-1111 dataset. We then trained multiple CHRONNOS (Coronal Hole RecOgnition Neural Network Over multi-Spectral-data) architecture (Jarolim et al. 2021) neural networks using the CHASM-SWPC dataset and compared their performance. Training the CHRONNOS neural network on these data achieved an accuracy of 0.9805, a True Skill Statistic (TSS) of 0.6807, and an intersection-over-union (IoU) of 0.5668, which is higher than the original pretrained CHRONNOS model Jarolim et al. (2021) achieved an accuracy of 0.9708, a TSS of 0.6749, and an IoU of 0.4805, when evaluated on the CHASM-SWPC-1111 test set.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14023v1" target="_blank"><h2>Syn-STARTS: Synthesized START Triage Scenario Generation Framework for Scalable LLM Evaluation <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Chiharu Hagiwara, Naoki Nonaka, Yuhta Hashimoto, Ryu Uchimido, Jun Seita<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> Introducing an open dataset<br><strong><u>Published:</u></strong> 2025-11-18<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Triage is a critically important decision-making process in mass casualty incidents (MCIs) to maximize victim survival rates. While the role of AI in such situations is gaining attention for making optimal decisions within limited resources and time, its development and performance evaluation require benchmark datasets of sufficient quantity and quality. However, MCIs occur infrequently, and sufficient records are difficult to accumulate at the scene, making it challenging to collect large-scale realworld data for research use. Therefore, we developed Syn-STARTS, a framework that uses LLMs to generate triage cases, and verified its effectiveness. The results showed that the triage cases generated by Syn-STARTS were qualitatively indistinguishable from the TRIAGE open dataset generated by manual curation from training materials. Furthermore, when evaluating the LLM accuracy using hundreds of cases each from the green, yellow, red, and black categories defined by the standard triage method START, the results were found to be highly stable. This strongly indicates the possibility of synthetic data in developing high-performance AI models for severe and critical medical situations.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13987v1" target="_blank"><h2>Artificial Intelligence Agents in Music Analysis: An Integrative Perspective Based on Two Use Cases <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Antonio Manuel Martínez-Heredia, Dolores Godrid Rodríguez, Andrés Ortiz García<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> Extended version of the conference paper presented at SATMUS 2025<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> explainable (abstract)<br><p><strong><u>Abstract:</u></strong> This paper presents an integrative review and experimental validation of artificial intelligence (AI) agents applied to music analysis and education. We synthesize the historical evolution from rule-based models to contemporary approaches involving deep learning, multi-agent architectures, and retrieval-augmented generation (RAG) frameworks. The pedagogical implications are evaluated through a dual-case methodology: (1) the use of generative AI platforms in secondary education to foster analytical and creative skills; (2) the design of a multiagent system for symbolic music analysis, enabling modular, scalable, and explainable workflows.
  Experimental results demonstrate that AI agents effectively enhance musical pattern recognition, compositional parameterization, and educational feedback, outperforming traditional automated methods in terms of interpretability and adaptability. The findings highlight key challenges concerning transparency, cultural bias, and the definition of hybrid evaluation metrics, emphasizing the need for responsible deployment of AI in educational environments.
  This research contributes to a unified framework that bridges technical, pedagogical, and ethical considerations, offering evidence-based guidance for the design and application of intelligent agents in computational musicology and music education.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13981v1" target="_blank"><h2>Data Whitening Improves Sparse Autoencoder Learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Ashwin Saraswatula, David Klindt<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> Accepted to the AAAI 2026 XAI4Science Workshop<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Sparse autoencoders (SAEs) have emerged as a promising approach for learning interpretable features from neural network activations. However, the optimization landscape for SAE training can be challenging due to correlations in the input data. We demonstrate that applying PCA Whitening to input activations -- a standard preprocessing technique in classical sparse coding -- improves SAE performance across multiple metrics. Through theoretical analysis and simulation, we show that whitening transforms the optimization landscape, making it more convex and easier to navigate. We evaluate both ReLU and Top-K SAEs across diverse model architectures, widths, and sparsity regimes. Empirical evaluation on SAEBench, a comprehensive benchmark for sparse autoencoders, reveals that whitening consistently improves interpretability metrics, including sparse probing accuracy and feature disentanglement, despite minor drops in reconstruction quality. Our results challenge the assumption that interpretability aligns with an optimal sparsity--fidelity trade-off and suggest that whitening should be considered as a default preprocessing step for SAE training, particularly when interpretability is prioritized over perfect reconstruction.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13977v1" target="_blank"><h2>Efficient reconstruction of multidimensional random field models with heterogeneous data using stochastic neural networks <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Mingtao Xia, Qijing Shen<br><strong><u>Categories:</u></strong> cs.LG, math.NA, math.PR<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> In this paper, we analyze the scalability of a recent Wasserstein-distance approach for training stochastic neural networks (SNNs) to reconstruct multidimensional random field models. We prove a generalization error bound for reconstructing multidimensional random field models on training stochastic neural networks with a limited number of training data. Our results indicate that when noise is heterogeneous across dimensions, the convergence rate of the generalization error may not depend explicitly on the model's dimensionality, partially alleviating the "curse of dimensionality" for learning multidimensional random field models from a finite number of data points. Additionally, we improve the previous Wasserstein-distance SNN training approach and showcase the robustness of the SNN. Through numerical experiments on different multidimensional uncertainty quantification tasks, we show that our Wasserstein-distance approach can successfully train stochastic neural networks to learn multidimensional uncertainty models.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13954v1" target="_blank"><h2>A Brain Wave Encodes a Thousand Tokens: Modeling Inter-Cortical Neural Interactions for Effective EEG-based Emotion Recognition <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Nilay Kumar, Priyansh Bhandari, G. Maragatham<br><strong><u>Categories:</u></strong> q-bio.NC, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> latent space (abstract), neural network (abstract), transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Human emotions are difficult to convey through words and are often abstracted in the process; however, electroencephalogram (EEG) signals can offer a more direct lens into emotional brain activity. Recent studies show that deep learning models can process these signals to perform emotion recognition with high accuracy. However, many existing approaches overlook the dynamic interplay between distinct brain regions, which can be crucial to understanding how emotions unfold and evolve over time, potentially aiding in more accurate emotion recognition. To address this, we propose RBTransformer, a Transformer-based neural network architecture that models inter-cortical neural dynamics of the brain in latent space to better capture structured neural interactions for effective EEG-based emotion recognition. First, the EEG signals are converted into Band Differential Entropy (BDE) tokens, which are then passed through Electrode Identity embeddings to retain spatial provenance. These tokens are processed through successive inter-cortical multi-head attention blocks that construct an electrode x electrode attention matrix, allowing the model to learn the inter-cortical neural dependencies. The resulting features are then passed through a classification head to obtain the final prediction. We conducted extensive experiments, specifically under subject-dependent settings, on the SEED, DEAP, and DREAMER datasets, over all three dimensions, Valence, Arousal, and Dominance (for DEAP and DREAMER), under both binary and multi-class classification settings. The results demonstrate that the proposed RBTransformer outperforms all previous state-of-the-art methods across all three datasets, over all three dimensions under both classification settings. The source code is available at: https://github.com/nnilayy/RBTransformer.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13937v1" target="_blank"><h2>Complex-Weighted Convolutional Networks: Provable Expressiveness via Complex Diffusion <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Cristina López Amado, Tassilo Schwarz, Yu Tian, Renaud Lambiotte<br><strong><u>Categories:</u></strong> cs.LG, cs.SI, math.DS, physics.soc-ph<br><strong><u>Comments:</u></strong> 19 pages, 6 figures. Learning on Graphs Conference 2025<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> convolutional (title, abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Graph Neural Networks (GNNs) have achieved remarkable success across diverse applications, yet they remain limited by oversmoothing and poor performance on heterophilic graphs. To address these challenges, we introduce a novel framework that equips graphs with a complex-weighted structure, assigning each edge a complex number to drive a diffusion process that extends random walks into the complex domain. We prove that this diffusion is highly expressive: with appropriately chosen complex weights, any node-classification task can be solved in the steady state of a complex random walk. Building on this insight, we propose the Complex-Weighted Convolutional Network (CWCN), which learns suitable complex-weighted structures directly from data while enriching diffusion with learnable matrices and nonlinear activations. CWCN is simple to implement, requires no additional hyperparameters beyond those of standard GNNs, and achieves competitive performance on benchmark datasets. Our results demonstrate that complex-weighted diffusion provides a principled and general mechanism for enhancing GNN expressiveness, opening new avenues for models that are both theoretically grounded and practically effective.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13935v2" target="_blank"><h2>Weather Maps as Tokens: Transformers for Renewable Energy Forecasting <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Federico Battini<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract), transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> Accurate renewable energy forecasting is essential to reduce dependence on fossil fuels and enabling grid decarbonization. However, current approaches fail to effectively integrate the rich spatial context of weather patterns with their temporal evolution. This work introduces a novel approach that treats weather maps as tokens in transformer sequences to predict renewable energy. Hourly weather maps are encoded as spatial tokens using a lightweight convolutional neural network, and then processed by a transformer to capture temporal dynamics across a 45-hour forecast horizon. Despite disadvantages in input initialization, evaluation against ENTSO-E operational forecasts shows a reduction in RMSE of about 60% and 20% for wind and solar respectively. A live dashboard showing daily forecasts is available at: https://www.sardiniaforecast.ifabfoundation.it.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13912v1" target="_blank"><h2>Compute-in-Memory Implementation of State Space Models for Event Sequence Processing <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Xiaoyu Zhang, Mingtao Hu, Sen Lu, Soohyeon Kim, Eric Yeu-Jer Lee, Yuyang Liu, Wei D. Lu<br><strong><u>Categories:</u></strong> eess.SP, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> Xiaoyu Zhang and Mingtao Hu contributed equally to this work<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract)<br><p><strong><u>Abstract:</u></strong> State space models (SSMs) have recently emerged as a powerful framework for long sequence processing, outperforming traditional methods on diverse benchmarks. Fundamentally, SSMs can generalize both recurrent and convolutional networks and have been shown to even capture key functions of biological systems. Here we report an approach to implement SSMs in energy-efficient compute-in-memory (CIM) hardware to achieve real-time, event-driven processing. Our work re-parameterizes the model to function with real-valued coefficients and shared decay constants, reducing the complexity of model mapping onto practical hardware systems. By leveraging device dynamics and diagonalized state transition parameters, the state evolution can be natively implemented in crossbar-based CIM systems combined with memristors exhibiting short-term memory effects. Through this algorithm and hardware co-design, we show the proposed system offers both high accuracy and high energy efficiency while supporting fully asynchronous processing for event-based vision and audio tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13899v1" target="_blank"><h2>A Disentangled Low-Rank RNN Framework for Uncovering Neural Connectivity and Dynamics <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Chengrui Li, Yunmiao Wang, Yule Wang, Weihan Li, Dieter Jaeger, Anqi Wu<br><strong><u>Categories:</u></strong> q-bio.NC, cs.CE, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> variational autoencoder (abstract), VAE (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Low-rank recurrent neural networks (lrRNNs) are a class of models that uncover low-dimensional latent dynamics underlying neural population activity. Although their functional connectivity is low-rank, it lacks disentanglement interpretations, making it difficult to assign distinct computational roles to different latent dimensions. To address this, we propose the Disentangled Recurrent Neural Network (DisRNN), a generative lrRNN framework that assumes group-wise independence among latent dynamics while allowing flexible within-group entanglement. These independent latent groups allow latent dynamics to evolve separately, but are internally rich for complex computation. We reformulate the lrRNN under a variational autoencoder (VAE) framework, enabling us to introduce a partial correlation penalty that encourages disentanglement between groups of latent dimensions. Experiments on synthetic, monkey M1, and mouse voltage imaging data show that DisRNN consistently improves the disentanglement and interpretability of learned neural latent trajectories in low-dimensional space and low-rank connectivity over baseline lrRNNs that do not encourage partial disentanglement.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13893v1" target="_blank"><h2>Beyond One-Size-Fits-All: Neural Networks for Differentially Private Tabular Data Synthesis <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Kai Chen, Chen Gong, Tianhao Wang<br><strong><u>Categories:</u></strong> cs.LG, cs.CR<br><strong><u>Comments:</u></strong> 18 pages. Github Link provided:this https URL<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> In differentially private (DP) tabular data synthesis, the consensus is that statistical models are better than neural network (NN)-based methods. However, we argue that this conclusion is incomplete and overlooks the challenge of densely correlated datasets, where intricate dependencies can overwhelm statistical models. In such complex scenarios, neural networks are more suitable due to their capacity to fit complex distributions by learning directly from samples. Despite this potential, existing NN-based algorithms still suffer from significant limitations. We therefore propose MargNet, incorporating successful algorithmic designs of statistical models into neural networks. MargNet applies an adaptive marginal selection strategy and trains the neural networks to generate data that conforms to the selected marginals. On sparsely correlated datasets, our approach achieves utility close to the best statistical method while offering an average 7$\times$ speedup over it. More importantly, on densely correlated datasets, MargNet establishes a new state-of-the-art, reducing fidelity error by up to 26\% compared to the previous best. We release our code on GitHub.\footnote{https://github.com/KaiChen9909/margnet}</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13892v1" target="_blank"><h2>Jailbreaking Large Vision Language Models in Intelligent Transportation Systems <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Badhan Chandra Das, Md Tasnim Jawad, Md Jueal Mia, M. Hadi Amini, Yanzhao Wu<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Large Vision Language Models (LVLMs) demonstrate strong capabilities in multimodal reasoning and many real-world applications, such as visual question answering. However, LVLMs are highly vulnerable to jailbreaking attacks. This paper systematically analyzes the vulnerabilities of LVLMs integrated in Intelligent Transportation Systems (ITS) under carefully crafted jailbreaking attacks. First, we carefully construct a dataset with harmful queries relevant to transportation, following OpenAI's prohibited categories to which the LVLMs should not respond. Second, we introduce a novel jailbreaking attack that exploits the vulnerabilities of LVLMs through image typography manipulation and multi-turn prompting. Third, we propose a multi-layered response filtering defense technique to prevent the model from generating inappropriate responses. We perform extensive experiments with the proposed attack and defense on the state-of-the-art LVLMs (both open-source and closed-source). To evaluate the attack method and defense technique, we use GPT-4's judgment to determine the toxicity score of the generated responses, as well as manual verification. Further, we compare our proposed jailbreaking method with existing jailbreaking techniques and highlight severe security risks involved with jailbreaking attacks with image typography manipulation and multi-turn prompting in the LVLMs integrated in ITS.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13889v2" target="_blank"><h2>Uni-Hema: Unified Model for Digital Hematopathology <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Abdul Rehman, Iqra Rasool, Ayisha Imran, Mohsen Ali, Waqas Sultani<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract), multi-modal (abstract)<br><p><strong><u>Abstract:</u></strong> Digital hematopathology requires cell-level analysis across diverse disease categories, including malignant disorders (e.g., leukemia), infectious conditions (e.g., malaria), and non-malignant red blood cell disorders (e.g., sickle cell disease). Whether single-task, vision-language, WSI-optimized, or single-cell hematology models, these approaches share a key limitation, they cannot provide unified, multi-task, multi-modal reasoning across the complexities of digital hematopathology. To overcome these limitations, we propose Uni-Hema, a multi-task, unified model for digital hematopathology integrating detection, classification, segmentation, morphology prediction, and reasoning across multiple diseases. Uni-Hema leverages 46 publicly available datasets, encompassing over 700K images and 21K question-answer pairs, and is built upon Hema-Former, a multimodal module that bridges visual and textual representations at the hierarchy level for the different tasks (detection, classification, segmentation, morphology, mask language modeling and visual question answer) at different granularity. Extensive experiments demonstrate that Uni-Hema achieves comparable or superior performance to train on a single-task and single dataset models, across diverse hematological tasks, while providing interpretable, morphologically relevant insights at the single-cell level. Our framework establishes a new standard for multi-task and multi-modal digital hematopathology. The code will be made publicly available.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13877v1" target="_blank"><h2>Hybrid Convolution Neural Network Integrated with Pseudo-Newton Boosting for Lumbar Spine Degeneration Detection <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Pandiyaraju V, Abishek Karthik, Jaspin K, Kannan A, Jaime Lloret<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (title), transfer learning (abstract)<br><p><strong><u>Abstract:</u></strong> This paper proposes a new enhanced model architecture to perform classification of lumbar spine degeneration with DICOM images while using a hybrid approach, integrating EfficientNet and VGG19 together with custom-designed components. The proposed model is differentiated from traditional transfer learning methods as it incorporates a Pseudo-Newton Boosting layer along with a Sparsity-Induced Feature Reduction Layer that forms a multi-tiered framework, further improving feature selection and representation. The Pseudo-Newton Boosting layer makes smart variations of feature weights, with more detailed anatomical features, which are mostly left out in a transfer learning setup. In addition, the Sparsity-Induced Layer removes redundancy for learned features, producing lean yet robust representations for pathology in the lumbar spine. This architecture is novel as it overcomes the constraints in the traditional transfer learning approach, especially in the high-dimensional context of medical images, and achieves a significant performance boost, reaching a precision of 0.9, recall of 0.861, F1 score of 0.88, loss of 0.18, and an accuracy of 88.1%, compared to the baseline model, EfficientNet. This work will present the architectures, preprocessing pipeline, and experimental results. The results contribute to the development of automated diagnostic tools for medical images.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13869v2" target="_blank"><h2>H-CNN-ViT: A Hierarchical Gated Attention Multi-Branch Model for Bladder Cancer Recurrence Prediction <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Xueyang Li, Zongren Wang, Yuliang Zhang, Zixuan Pan, Yu-Jen Chen, Nishchal Sapkota, Gelei Xu, Danny Z. Chen, Yiyu Shi<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multi-modal (abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Bladder cancer is one of the most prevalent malignancies worldwide, with a recurrence rate of up to 78%, necessitating accurate post-operative monitoring for effective patient management. Multi-sequence contrast-enhanced MRI is commonly used for recurrence detection; however, interpreting these scans remains challenging, even for experienced radiologists, due to post-surgical alterations such as scarring, swelling, and tissue remodeling. AI-assisted diagnostic tools have shown promise in improving bladder cancer recurrence prediction, yet progress in this field is hindered by the lack of dedicated multi-sequence MRI datasets for recurrence assessment study. In this work, we first introduce a curated multi-sequence, multi-modal MRI dataset specifically designed for bladder cancer recurrence prediction, establishing a valuable benchmark for future research. We then propose H-CNN-ViT, a new Hierarchical Gated Attention Multi-Branch model that enables selective weighting of features from the global (ViT) and local (CNN) paths based on contextual demands, achieving a balanced and targeted feature fusion. Our multi-branch architecture processes each modality independently, ensuring that the unique properties of each imaging channel are optimally captured and integrated. Evaluated on our dataset, H-CNN-ViT achieves an AUC of 78.6%, surpassing state-of-the-art models. Our model is publicly available at https://github.com/XLIAaron/H-CNN-ViT.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14808v1" target="_blank"><h2>Transformer Injectivity & Geometric Robustness - Analytic Margins and Bi-Lipschitz Uniformity of Sequence-Level Hidden States <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Mikael von Strauss<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 22 pages, 5 figures<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> Under real-analytic assumptions on decoder-only Transformers, recent work shows that the map from discrete prompts to last-token hidden states is generically injective on finite prompt sets. We refine this picture: for each layer $\ell$ we define a collision discriminant $Δ^\ell \subset Θ$ and injective stratum $U^\ell = Θ\setminus Δ^\ell$, and prove a dichotomy -- either the model is nowhere injective on the set, or $U^\ell$ is open and dense and every $F^\ell_θ$ is injective. Under mild non-singularity assumptions on the optimizer and an absolutely continuous initialization, generic injectivity persists along smooth training trajectories over any fixed horizon. We also treat symmetry groups $G$, showing that discriminants and injective strata descend to the quotient $Θ/G$, so injectivity is naturally a property of functional equivalence classes.
  We complement these results with an empirical study of layerwise geometric diagnostics. We define a separation margin and a co-Lipschitz (lower Lipschitz) constant between prompt space and last-token representation space, estimated via nearest-neighbor statistics on large prompt sets. Applying these diagnostics to pretrained LLaMA-3 and Qwen models, we study behavior across layers, sequence lengths, model scales, and 8- and 4-bit activation quantization. On our sampled prompts we see no collisions in full precision or at 8 bits, while 4-bit quantization induces a small number of collisions and markedly shrinks co-Lipschitz estimates. For a small GPT-2 trained from scratch, normalized metrics remain stable over training. Overall, the results suggest that Transformer representations are generically and persistently injective in the continuous-parameter idealization, while their practical invertibility can be probed using simple geometric diagnostics.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.14806v1" target="_blank"><h2>MergeDNA: Context-aware Genome Modeling with Dynamic Tokenization through Token Merging <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Siyuan Li, Kai Yu, Anna Wang, Zicheng Liu, Chang Yu, Jingbo Zhou, Qirong Yang, Yucheng Guo, Xiaoming Zhang, Stan Z. Li<br><strong><u>Categories:</u></strong> q-bio.GN, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> AAAI 2026 (Oral Presentation) Preprint<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Modeling genomic sequences faces two unsolved challenges: the information density varies widely across different regions, while there is no clearly defined minimum vocabulary unit. Relying on either four primitive bases or independently designed DNA tokenizers, existing approaches with naive masked language modeling pre-training often fail to adapt to the varying complexities of genomic sequences. Leveraging Token Merging techniques, this paper introduces a hierarchical architecture that jointly optimizes a dynamic genomic tokenizer and latent Transformers with context-aware pre-training tasks. As for network structures, the tokenization module automatically chunks adjacent bases into words by stacking multiple layers of the differentiable token merging blocks with local-window constraints, then a Latent Encoder captures the global context of these merged words by full-attention blocks. Symmetrically employing a Latent Decoder and a Local Decoder, MergeDNA learns with two pre-training tasks: Merged Token Reconstruction simultaneously trains the dynamic tokenization module and adaptively filters important tokens, while Adaptive Masked Token Modeling learns to predict these filtered tokens to capture informative contents. Extensive experiments show that MergeDNA achieves superior performance on three popular DNA benchmarks and several multi-omics tasks with fine-tuning or zero-shot evaluation, outperforming typical tokenization methods and large-scale DNA foundation models.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13860v1" target="_blank"><h2>Randomized Controlled Trials for Phishing Triage Agent <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> James Bono<br><strong><u>Categories:</u></strong> econ.GN, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Security operations centers (SOCs) face a persistent challenge: efficiently triaging a high volume of user-reported phishing emails while maintaining robust protection against threats. This paper presents the first randomized controlled trial (RCT) evaluating the impact of a domain-specific AI agent - the Microsoft Security Copilot Phishing Triage Agent - on analyst productivity and accuracy. Our results demonstrate that agent-augmented analysts achieved up to 6.5 times as many true positives per analyst minute and a 77% improvement in verdict accuracy compared to a control group. The agent's queue prioritization and verdict explanations were both significant drivers of efficiency. Behavioral analysis revealed that agent-augmented analysts reallocated their attention, spending 53% more time on malicious emails, and were not prone to rubber-stamping the agent's malicious verdicts. These findings offer actionable insights for SOC leaders considering AI adoption, including the potential for agents to fundamentally change the optimal allocation of SOC resources.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13840v1" target="_blank"><h2>Inferring Planet and Disk Parameters from Protoplanetary Disk Images Using a Variational Autoencoder <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Sayed Shafaat Mahmud, Sayantan Auddy, Neal Turner, Jeffrey S. Bary<br><strong><u>Categories:</u></strong> astro-ph.EP, astro-ph.IM<br><strong><u>Comments:</u></strong> 29 Pages, 20 Figures, Accepted at the Astrophysical Journal<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> variational autoencoder (title, abstract), VAE (abstract)<br><p><strong><u>Abstract:</u></strong> Dust-continuum observations of many protoplanetary disks reveal rings and gaps that are widely interpreted as evidence of ongoing planet formation. Here we present the first framework for inferring planet and disk parameters from such images using variational autoencoder (VAE) based generative machine learning (ML). The new framework is called VADER (Variational Autoencoder for Disks with Embedded Rings). We train VADER on synthetic images of dust continuum emission, generated from \texttt{FARGO3D} hydrodynamic simulations post-processed with Monte Carlo radiative transfer calculations. VADER infers the masses of up to three embedded planets as well as the disk parameters viscous $α$, dust-to-gas ratio, Stokes number, and flaring index. VADER returns a full posterior distribution for each of these quantities. We demonstrate that VADER reconstructs disk morphologies with high structural similarity (index $>$ 0.99), accurately recovers planet parameters with $R^2 > 0.9$ across planet masses, and reliably predicts disk parameters. Applied to ALMA dust continuum images of 23 protoplanetary disks, our model returns mass estimates for embedded planets of 0.3-2~$M_{\mathrm{Jup}}$ that agree to within $1σ$ of published values in most cases, and infers disk parameters consistent with current literature. Once trained, the VAE performs full posterior parameter inference in a matter of minutes, offering statistical rigor with enough computational speed for application to large-scale ALMA surveys. These results establish VAE-based models as powerful tools for inferring from disk structure the masses of embedded planets and the global disk parameters, with their associated uncertainties.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13815v1" target="_blank"><h2>Stripped-Envelope Supernovae for QCD Axion Detection <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Francisco R. Candón, Damiano F. G. Fiorillo, Ángel Gil Muyor, Hans-Thomas Janka, Georg G. Raffelt, Edoardo Vitagliano<br><strong><u>Categories:</u></strong> hep-ph, astro-ph.CO, astro-ph.HE<br><strong><u>Comments:</u></strong> 7 pages, 4 figures<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> VAE (title)<br><p><strong><u>Abstract:</u></strong> QCD axions would be copiously produced in the proto-neutron star formed in a core-collapse supernova (SN). After escaping, they would convert into gamma rays in the Galactic magnetic field and, as recently shown, in that of the progenitor star itself. Here, we show that Type Ibc SNe -- whose progenitors have lost their hydrogen or even helium envelopes -- are the optimal targets for this search. The stripped progenitors are much more compact, and show larger magnetic fields than both red and blue supergiants, the progenitors of Type IIP/L SNe. If the next galactic SN is of Type Ibc, Fermi-LAT or a similar gamma-ray satellite might be able to discover the QCD axion down to masses as small as $m_a\simeq 10^{-4}\,\rm eV$ (Peccei-Quinn scale $f_a\simeq 10^{11} \,\rm GeV$).</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13719v1" target="_blank"><h2>Scaling Spatial Intelligence with Multimodal Foundation Models <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Zhongang Cai, Ruisi Wang, Chenyang Gu, Fanyi Pu, Junxiang Xu, Yubo Wang, Wanqi Yin, Zhitao Yang, Chen Wei, Qingping Sun, Tongxi Zhou, Jiaqi Li, Hui En Pang, Oscar Qian, Yukun Wei, Zhiqian Lin, Xuanke Shi, Kewang Deng, Xiaoyang Han, Zukai Chen, Xiangyu Fan, Hanming Deng, Lewei Lu, Liang Pan, Bo Li, Ziwei Liu, Quan Wang, Dahua Lin, Lei Yang<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.LG, cs.MM, cs.RO<br><strong><u>Comments:</u></strong> Model:this https URLCode:this https URL<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Despite remarkable progress, multimodal foundation models still exhibit surprising deficiencies in spatial intelligence. In this work, we explore scaling up multimodal foundation models to cultivate spatial intelligence within the SenseNova-SI family, built upon established multimodal foundations including visual understanding models (i.e., Qwen3-VL and InternVL3) and unified understanding and generation models (i.e., Bagel). We take a principled approach to constructing high-performing and robust spatial intelligence by systematically curating SenseNova-SI-8M: eight million diverse data samples under a rigorous taxonomy of spatial capabilities. SenseNova-SI demonstrates unprecedented performance across a broad range of spatial intelligence benchmarks: 68.7% on VSI-Bench, 43.3% on MMSI, 85.6% on MindCube, 54.6% on ViewSpatial, and 50.1% on SITE, while maintaining strong general multimodal understanding (e.g., 84.9% on MMBench-En). More importantly, we analyze the impact of data scaling, discuss early signs of emergent generalization capabilities enabled by diverse data training, analyze the risk of overfitting and language shortcuts, present a preliminary study on spatial chain-of-thought reasoning, and validate the potential downstream application. SenseNova-SI is an ongoing project, and this report will be updated continuously. All newly trained multimodal foundation models are publicly released to facilitate further research in this direction.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13712v1" target="_blank"><h2>From Black Box to Insight: Explainable AI for Extreme Event Preparedness <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Kiana Vu, İsmet Selçuk Özer, Phung Lai, Zheng Wu, Thilanka Munasinghe, Jennifer Wei<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> explainability (abstract), explainable (title, abstract)<br><p><strong><u>Abstract:</u></strong> As climate change accelerates the frequency and severity of extreme events such as wildfires, the need for accurate, explainable, and actionable forecasting becomes increasingly urgent. While artificial intelligence (AI) models have shown promise in predicting such events, their adoption in real-world decision-making remains limited due to their black-box nature, which limits trust, explainability, and operational readiness. This paper investigates the role of explainable AI (XAI) in bridging the gap between predictive accuracy and actionable insight for extreme event forecasting. Using wildfire prediction as a case study, we evaluate various AI models and employ SHapley Additive exPlanations (SHAP) to uncover key features, decision pathways, and potential biases in model behavior. Our analysis demonstrates how XAI not only clarifies model reasoning but also supports critical decision-making by domain experts and response teams. In addition, we provide supporting visualizations that enhance the interpretability of XAI outputs by contextualizing feature importance and temporal patterns in seasonality and geospatial characteristics. This approach enhances the usability of AI explanations for practitioners and policymakers. Our findings highlight the need for AI systems that are not only accurate but also interpretable, accessible, and trustworthy, essential for effective use in disaster preparedness, risk mitigation, and climate resilience planning.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13705v1" target="_blank"><h2>Rare Genomic Subtype Discovery from RNA-seq via Autoencoder Embeddings and Stability-Aware Clustering <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Alaa Mezghiche<br><strong><u>Categories:</u></strong> cs.LG, q-bio.GN<br><strong><u>Comments:</u></strong> 16 pages<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> latent space (abstract)<br><p><strong><u>Abstract:</u></strong> Unsupervised learning on high-dimensional RNA-seq data can reveal molecular subtypes beyond standard labels. We combine an autoencoder-based representation with clustering and stability analysis to search for rare but reproducible genomic subtypes. On the UCI "Gene Expression Cancer RNA-Seq" dataset (801 samples, 20,531 genes; BRCA, COAD, KIRC, LUAD, PRAD), a pan-cancer analysis shows clusters aligning almost perfectly with tissue of origin (Cramer's V = 0.887), serving as a negative control. We therefore reframe the problem within KIRC (n = 146): we select the top 2,000 highly variable genes, standardize them, train a feed-forward autoencoder (128-dimensional latent space), and run k-means for k = 2-10. While global indices favor small k, scanning k with a pre-specified discovery rule (rare < 10 percent and stable with Jaccard >= 0.60 across 20 seeds after Hungarian alignment) yields a simple solution at k = 5 (silhouette = 0.129, DBI = 2.045) with a rare cluster C0 (6.85 percent of patients) that is highly stable (Jaccard = 0.787). Cluster-vs-rest differential expression (Welch's t-test, Benjamini-Hochberg FDR) identifies coherent markers. Overall, pan-cancer clustering is dominated by tissue of origin, whereas a stability-aware within-cancer approach reveals a rare, reproducible KIRC subtype.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13699v1" target="_blank"><h2>Efficient Calibration for Decision Making <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Parikshit Gopalan, Konstantinos Stavropoulos, Kunal Talwar, Pranay Tankala<br><strong><u>Categories:</u></strong> cs.LG, cs.DS, stat.ML<br><strong><u>Comments:</u></strong> 50 pages, 3 figures<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> A decision-theoretic characterization of perfect calibration is that an agent seeking to minimize a proper loss in expectation cannot improve their outcome by post-processing a perfectly calibrated predictor. Hu and Wu (FOCS'24) use this to define an approximate calibration measure called calibration decision loss ($\mathsf{CDL}$), which measures the maximal improvement achievable by any post-processing over any proper loss. Unfortunately, $\mathsf{CDL}$ turns out to be intractable to even weakly approximate in the offline setting, given black-box access to the predictions and labels.
  We suggest circumventing this by restricting attention to structured families of post-processing functions $K$. We define the calibration decision loss relative to $K$, denoted $\mathsf{CDL}_K$ where we consider all proper losses but restrict post-processings to a structured family $K$. We develop a comprehensive theory of when $\mathsf{CDL}_K$ is information-theoretically and computationally tractable, and use it to prove both upper and lower bounds for natural classes $K$. In addition to introducing new definitions and algorithmic techniques to the theory of calibration for decision making, our results give rigorous guarantees for some widely used recalibration procedures in machine learning.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13685v1" target="_blank"><h2>Protein Secondary Structure Prediction Using 3D Graphs and Relation-Aware Message Passing Transformers <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Disha Varshney, Samarth Garg, Sarthak Tyagi, Deeksha Varshney, Nayan Deep, Asif Ekbal<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 40 pages<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract), transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> In this study, we tackle the challenging task of predicting secondary structures from protein primary sequences, a pivotal initial stride towards predicting tertiary structures, while yielding crucial insights into protein activity, relationships, and functions. Existing methods often utilize extensive sets of unlabeled amino acid sequences. However, these approaches neither explicitly capture nor harness the accessible protein 3D structural data, which is recognized as a decisive factor in dictating protein functions. To address this, we utilize protein residue graphs and introduce various forms of sequential or structural connections to capture enhanced spatial information. We adeptly combine Graph Neural Networks (GNNs) and Language Models (LMs), specifically utilizing a pre-trained transformer-based protein language model to encode amino acid sequences and employing message-passing mechanisms like GCN and R-GCN to capture geometric characteristics of protein structures. Employing convolution within a specific node's nearby region, including relations, we stack multiple convolutional layers to efficiently learn combined insights from the protein's spatial graph, revealing intricate interconnections and dependencies in its structural arrangement. To assess our model's performance, we employed the training dataset provided by NetSurfP-2.0, which outlines secondary structure in 3-and 8-states. Extensive experiments show that our proposed model, SSRGNet surpasses the baseline on f1-scores.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13679v1" target="_blank"><h2>QUILL: An Algorithm-Architecture Co-Design for Cache-Local Deformable Attention <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Hyunwoo Oh, Hanning Chen, Sanggeon Yun, Yang Ni, Wenjun Huang, Tamoghno Das, Suyeon Jang, Mohsen Imani<br><strong><u>Categories:</u></strong> cs.AR, cs.CV, cs.LG<br><strong><u>Comments:</u></strong> Accepted to DATE 2026<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Deformable transformers deliver state-of-the-art detection but map poorly to hardware due to irregular memory access and low arithmetic intensity. We introduce QUILL, a schedule-aware accelerator that turns deformable attention into cache-friendly, single-pass work. At its core, Distance-based Out-of-Order Querying (DOOQ) orders queries by spatial proximity; the look-ahead drives a region prefetch into an alternate buffer--forming a schedule-aware prefetch loop that overlaps memory and compute. A fused MSDeformAttn engine executes interpolation, Softmax, aggregation, and the final projection (W''m) in one pass without spilling intermediates, while small tensors are kept on-chip and surrounding dense layers run on integrated GEMMs. Implemented as RTL and evaluated end-to-end, QUILL achieves up to 7.29x higher throughput and 47.3x better energy efficiency than an RTX 4090, and exceeds prior accelerators by 3.26-9.82x in throughput and 2.01-6.07x in energy efficiency. With mixed-precision quantization, accuracy tracks FP32 within <=0.9 AP across Deformable and Sparse DETR variants. By converting sparsity into locality--and locality into utilization--QUILL delivers consistent, end-to-end speedups.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13666v1" target="_blank"><h2>A model-independent assessment of the late-time dark energy density evolution <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Rayff de Souza, Agripino Sousa-Neto, Javier E. González, Jailson Alcaniz<br><strong><u>Categories:</u></strong> astro-ph.CO, gr-qc, hep-ph, hep-th<br><strong><u>Comments:</u></strong> 13 pages, 5 figures, 3 tables<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> Combined measurements of Baryon Acoustic Oscillations (BAO) from the Dark Energy Spectroscopic Survey (DESI), the Cosmic Microwave Background (CMB) and Type Ia Supernovae (SN Ia), have recently challenged the $Λ$-Cold Dark Matter ($Λ$CDM) paradigm, indicating potential evidence for a dynamical dark energy component. These results are usually obtained in the context of the dark energy equation-of-state (EoS) parameterizations, generally implying in phantom-crossing at intermediate redshifts. However, a general mapping between these parameterizations that yields approximately the same background observables clouds the inference of the true nature of dark energy in the context of these parametric methods. In this work, we propose a model-independent reconstruction of the dark energy density, which is more directly constrained than its EoS, based on the Gaussian Process (GP) regression method with the use of DESI DR2 BAO data and the Pantheon+, Union3 and DESY5 SN Ia samples. In addition, we perform a statistical comparison between the energy densities of $Λ$, a non-phantom thawing quintessence-type dark energy, and the Chevallier-Polarski-Linder parameterization with the reconstructed function. We find that all models agree with the GP reconstruction at 95\% C.L., with the largest discrepancy coming from $Λ$CDM with DESY5 at low redshifts. Even in this case, our findings suggest that it may be premature to claim statistically significant evidence for evolving or phantom dark energy with current DESI and SN Ia measurements.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13663v1" target="_blank"><h2>Cost-Driven Synthesis of Sound Abstract Interpreters <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Qiuhan Gu, Avaljot Singh, Gagandeep Singh<br><strong><u>Categories:</u></strong> cs.PL, cs.LG<br><strong><u>Comments:</u></strong> 37 pages, 20 figures<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Constructing abstract interpreters that provide global soundness guarantees remains a major obstacle in abstract interpretation. We investigate whether modern LLMs can reduce this burden by leveraging them to synthesize sound, non-trivial abstract interpreters across multiple abstract domains in the setting of neural network verification. We formulate synthesis as a constrained optimization problem and introduce a novel mathematically grounded cost function for measuring unsoundness under strict syntactic and semantic constraints. Based on this formulation, we develop a unified framework that unifies LLM-based generation with syntactic and semantic validation and a quantitative cost-guided feedback mechanism. Empirical results demonstrate that our framework not only matches the quality of handcrafted transformers, but more importantly, discovers sound, high-precision transformers for complex nonlinear operators that are absent from existing literature.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13809v1" target="_blank"><h2>ScoresActivation: A New Activation Function for Model Agnostic Global Explainability by Design <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Emanuel Covaci, Fabian Galis, Radu Balan, Daniela Zaharie, Darian Onchis<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> Paper submitted to ECAI 2025 Conference<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> explainability (title, abstract), explainable (abstract)<br><p><strong><u>Abstract:</u></strong> Understanding the decision of large deep learning models is a critical challenge for building transparent and trustworthy systems. Although the current post hoc explanation methods offer valuable insights into feature importance, they are inherently disconnected from the model training process, limiting their faithfulness and utility. In this work, we introduce a novel differentiable approach to global explainability by design, integrating feature importance estimation directly into model training. Central to our method is the ScoresActivation function, a feature-ranking mechanism embedded within the learning pipeline. This integration enables models to prioritize features according to their contribution to predictive performance in a differentiable and end-to-end trainable manner. Evaluations across benchmark datasets show that our approach yields globally faithful, stable feature rankings aligned with SHAP values and ground-truth feature importance, while maintaining high predictive performance. Moreover, feature scoring is 150 times faster than the classical SHAP method, requiring only 2 seconds during training compared to SHAP's 300 seconds for feature ranking in the same configuration. Our method also improves classification accuracy by 11.24% with 10 features (5 relevant) and 29.33% with 16 features (5 relevant, 11 irrelevant), demonstrating robustness to irrelevant inputs. This work bridges the gap between model accuracy and interpretability, offering a scalable framework for inherently explainable machine learning.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13655v1" target="_blank"><h2>OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Henry Herzog, Favyen Bastani, Yawen Zhang, Gabriel Tseng, Joseph Redmon, Hadrien Sablon, Ryan Park, Jacob Morrison, Alexandra Buraczynski, Karen Farley, Joshua Hansen, Andrew Howe, Patrick Alan Johnson, Mark Otterlee, Ted Schmitt, Hunter Pitelka, Stephen Daspit, Rachel Ratner, Christopher Wilhelm, Sebastian Wood, Mike Jacobi, Hannah Kerner, Evan Shelhamer, Ali Farhadi, Ranjay Krishna, Patrick Beukema<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Earth observation data presents a unique challenge: it is spatial like images, sequential like video or text, and highly multimodal. We present OlmoEarth: a multimodal, spatio-temporal foundation model that employs a novel self-supervised learning formulation, masking strategy, and loss all designed for the Earth observation domain. OlmoEarth achieves state-of-the-art performance compared to 12 other foundation models across a variety of research benchmarks and real-world tasks from external partners. When evaluating embeddings OlmoEarth achieves the best performance on 15 out of 24 tasks, and with full fine-tuning it is the best on 19 of 29 tasks. We deploy OlmoEarth as the backbone of an end-to-end platform for data collection, labeling, training, and inference of Earth observation models. The OlmoEarth Platform puts frontier foundation models and powerful data management tools into the hands of non-profits and NGOs working to solve the world's biggest problems. OlmoEarth source code, training data, and pre-trained weights are available at $\href{https://github.com/allenai/olmoearth_pretrain}{\text{https://github.com/allenai/olmoearth_pretrain}}$.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13653v1" target="_blank"><h2>Weight-sparse transformers have interpretable circuits <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Leo Gao, Achyuta Rajaram, Jacob Coxon, Soham V. Govande, Bowen Baker, Dan Mossing<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> transformer (title)<br><p><strong><u>Abstract:</u></strong> Finding human-understandable circuits in language models is a central goal of the field of mechanistic interpretability. We train models to have more understandable circuits by constraining most of their weights to be zeros, so that each neuron only has a few connections. To recover fine-grained circuits underlying each of several hand-crafted tasks, we prune the models to isolate the part responsible for the task. These circuits often contain neurons and residual channels that correspond to natural concepts, with a small number of straightforwardly interpretable connections between them. We study how these models scale and find that making weights sparser trades off capability for interpretability, and scaling model size improves the capability-interpretability frontier. However, scaling sparse models beyond tens of millions of nonzero parameters while preserving interpretability remains a challenge. In addition to training weight-sparse models de novo, we show preliminary results suggesting our method can also be adapted to explain existing dense models. Our work produces circuits that achieve an unprecedented level of human understandability and validates them with considerable rigor.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13637v1" target="_blank"><h2>Towards Multimodal Representation Learning in Paediatric Kidney Disease <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Ana Durica, John Booth, Ivana Drobnjak<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> 4 pages, 3 figures. EurIPS 2025 Multimodal Representation Learning for Healthcare (MMRL4H) workshop paper<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Paediatric kidney disease varies widely in its presentation and progression, which calls for continuous monitoring of renal function. Using electronic health records collected between 2019 and 2025 at Great Ormond Street Hospital, a leading UK paediatric hospital, we explored a temporal modelling approach that integrates longitudinal laboratory sequences with demographic information. A recurrent neural model trained on these data was used to predict whether a child would record an abnormal serum creatinine value within the following thirty days. Framed as a pilot study, this work provides an initial demonstration that simple temporal representations can capture useful patterns in routine paediatric data and lays the groundwork for future multimodal extensions using additional clinical signals and more detailed renal outcomes.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13626v1" target="_blank"><h2>CreBench: Human-Aligned Creativity Evaluation from Idea to Process to Product <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Kaiwen Xue, Chenglong Li, Zhonghong Ou, Guoxin Zhang, Kaoyan Lu, Shuai Lyu, Yifan Zhu, Ping Zong Junpeng Ding, Xinyu Liu, Qunlin Chen, Weiwei Qin, Yiran Shen, Jiayi Cen<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> 13 pages, 3 figures,The 40th Annual AAAI Conference on Artificial Intelligence(AAAI 2026),Paper has been accepted for a poster presentation<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Human-defined creativity is highly abstract, posing a challenge for multimodal large language models (MLLMs) to comprehend and assess creativity that aligns with human judgments. The absence of an existing benchmark further exacerbates this dilemma. To this end, we propose CreBench, which consists of two key components: 1) an evaluation benchmark covering the multiple dimensions from creative idea to process to products; 2) CreMIT (Creativity Multimodal Instruction Tuning dataset), a multimodal creativity evaluation dataset, consisting of 2.2K diverse-sourced multimodal data, 79.2K human feedbacks and 4.7M multi-typed instructions. Specifically, to ensure MLLMs can handle diverse creativity-related queries, we prompt GPT to refine these human feedbacks to activate stronger creativity assessment capabilities. CreBench serves as a foundation for building MLLMs that understand human-aligned creativity. Based on the CreBench, we fine-tune open-source general MLLMs, resulting in CreExpert, a multimodal creativity evaluation expert model. Extensive experiments demonstrate that the proposed CreExpert models achieve significantly better alignment with human creativity evaluation compared to state-of-the-art MLLMs, including the most advanced GPT-4V and Gemini-Pro-Vision.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13609v1" target="_blank"><h2>AtlasMorph: Learning conditional deformable templates for brain MRI <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Marianne Rakic, Andrew Hoopes, S. Mazdak Abulnaga, Mert R. Sabuncu, John V. Guttag, Adrian V. Dalca<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Deformable templates, or atlases, are images that represent a prototypical anatomy for a population, and are often enhanced with probabilistic anatomical label maps. They are commonly used in medical image analysis for population studies and computational anatomy tasks such as registration and segmentation. Because developing a template is a computationally expensive process, relatively few templates are available. As a result, analysis is often conducted with sub-optimal templates that are not truly representative of the study population, especially when there are large variations within this population. We propose a machine learning framework that uses convolutional registration neural networks to efficiently learn a function that outputs templates conditioned on subject-specific attributes, such as age and sex. We also leverage segmentations, when available, to produce anatomical segmentation maps for the resulting templates. The learned network can also be used to register subject images to the templates. We demonstrate our method on a compilation of 3D brain MRI datasets, and show that it can learn high-quality templates that are representative of populations. We find that annotated conditional templates enable better registration than their unlabeled unconditional counterparts, and outperform other templates construction methods.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13595v2" target="_blank"><h2>Physics-Informed Neural Networks for Nonlinear Output Regulation <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Sebastiano Mengozzi, Giovanni B. Esposito, Michelangelo Bin, Andrea Acquaviva, Andrea Bartolini, Lorenzo Marconi<br><strong><u>Categories:</u></strong> eess.SY, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> This work addresses the full-information output regulation problem for nonlinear systems, assuming the states of both the plant and the exosystem are known. In this setting, perfect tracking or rejection is achieved by constructing a zero-regulation-error manifold $π(w)$ and a feedforward input $c(w)$ that render such manifold invariant. The pair $(π(w), c(w))$ is characterized by the regulator equations, i.e., a system of PDEs with an algebraic constraint. We focus on accurately solving the regulator equations introducing a physics-informed neural network (PINN) approach that directly approximates $π(w)$ and $c(w)$ by minimizing the residuals under boundary and feasibility conditions, without requiring precomputed trajectories or labeled data. The learned operator maps exosystem states to steady state plant states and inputs, enables real-time inference and, critically, generalizes across families of the exosystem with varying initial conditions and parameters. The framework is validated on a regulation task that synchronizes a helicopter's vertical dynamics with a harmonically oscillating platform. The resulting PINN-based solver reconstructs the zero-error manifold with high fidelity and sustains regulation performance under exosystem variations, highlighting the potential of learning-enabled solvers for nonlinear output regulation. The proposed approach is broadly applicable to nonlinear systems that admit a solution to the output regulation problem.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13591v1" target="_blank"><h2>A Fleeting GLIMPSE of N/O Enrichment at Cosmic Dawn: Evidence for Wolf Rayet N Stars in a z = 6.1 Galaxy <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Danielle A. Berg, Rohan P. Naidu, John Chisholm, Hakim Atek, Seiji Fujimoto, Vasily Kokorev, Lukas J. Furtak, Chiaki Kobayashi, Daniel Schaerer, Angela Adamo, Qinyue Fei, Damien Korber, Jorryt Matthee, Rui Marques-Chaves, Zorayda Martinez, Kristen B. W. Mcquinn, Julian B. Muñoz, Pascal A. Oesch, Daniel P. Stark, Mabel G. Stephenson, Tiger Yu-Yang Hsiao<br><strong><u>Categories:</u></strong> astro-ph.GA<br><strong><u>Comments:</u></strong> 18 pages, 6 figures, submitted to ApJ<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> We present the discovery of extreme nitrogen enrichment by Wolf Rayet nitrogen stars (WN) in the metal-poor ($\sim10\%Z_\odot$), lensed, compact ($R_{\rm eff}\sim20$ pc) galaxy RXCJ2248 at $z=6.1$, revealed by unprecedentedly deep JWST/NIRSpec medium-resolution spectroscopy from the GLIMPSE-D Survey. The exquisite S/N reveals multiple high-ionization nebular lines and broad Balmer and [OIII] components (FWHM$\sim700-3000$ km s$^{-1}$). We detect broadened HeII $λ$1640 and $λ$4687 (FWHM$\sim530$ km s$^{-1}$) and strong NIII] $λ$4642 emission consistent with a population of WN stars, making RXCJ2248 the most distant galaxy with confirmed WR features to date. We measure the multi-phase nebular density across five ions, the direct-method metallicity ($12+\log(\rm O/H)= 7.749\pm0.023$), and a non-uniform elemental enrichment pattern of extreme N/O enhancement ($\log(\rm N/O)=-0.390\pm0.035$ from N$^+$, N$^{+2}$, and N$^{+3}$) and suppressed C/O relative to empirical C/N trends. We show that this abundance pattern can be explained by enrichment from a dual-burst with a low WC/WN ratio, as expected at low metallicities. Crucially, these signatures can only arise during a brief, rare evolutionary window shortly after a burst ($\sim3-6$ Myr), when WN stars dominate chemical feedback but before dilution by later yields (e.g., supernovae). The observed frequency of strong N emitters at high$-z$ implies a $\sim50$ Myr burst duty cycle, suggesting that N/O outliers may represent a brief but ubiquitous phase in the evolution of highly star-forming early galaxies. The detection in RXCJ2248, therefore, provides the first direct evidence of WN-driven chemical enrichment in the early Universe and a novel timing argument for the bursty star formation cycles that shaped galaxies at cosmic dawn.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13588v1" target="_blank"><h2>Data-driven Acceleration of MPC with Guarantees <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Agustin Castellano, Shijie Pan, Enrique Mallada<br><strong><u>Categories:</u></strong> eess.SY, cs.AI, math.DS<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> data-driven (title, abstract)<br><p><strong><u>Abstract:</u></strong> Model Predictive Control (MPC) is a powerful framework for optimal control but can be too slow for low-latency applications. We present a data-driven framework to accelerate MPC by replacing online optimization with a nonparametric policy constructed from offline MPC solutions. Our policy is greedy with respect to a constructed upper bound on the optimal cost-to-go, and can be implemented as a nonparametric lookup rule that is orders of magnitude faster than solving MPC online. Our analysis shows that under sufficient coverage condition of the offline data, the policy is recursively feasible and admits provable, bounded optimality gap. These conditions establish an explicit trade-off between the amount of data collected and the tightness of the bounds. Our experiments show that this policy is between 100 and 1000 times faster than standard MPC, with only a modest hit to optimality, showing potential for real-time control tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13575v1" target="_blank"><h2>Hierarchical Prompt Learning for Image- and Text-Based Person Re-Identification <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Linhan Zhou, Shuang Li, Neng Dong, Yonghang Tai, Yafei Zhang, Huafeng Li<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> 9 pages, 4 figures, accepted by AAAI 2026<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Person re-identification (ReID) aims to retrieve target pedestrian images given either visual queries (image-to-image, I2I) or textual descriptions (text-to-image, T2I). Although both tasks share a common retrieval objective, they pose distinct challenges: I2I emphasizes discriminative identity learning, while T2I requires accurate cross-modal semantic alignment. Existing methods often treat these tasks separately, which may lead to representation entanglement and suboptimal performance. To address this, we propose a unified framework named Hierarchical Prompt Learning (HPL), which leverages task-aware prompt modeling to jointly optimize both tasks. Specifically, we first introduce a Task-Routed Transformer, which incorporates dual classification tokens into a shared visual encoder to route features for I2I and T2I branches respectively. On top of this, we develop a hierarchical prompt generation scheme that integrates identity-level learnable tokens with instance-level pseudo-text tokens. These pseudo-tokens are derived from image or text features via modality-specific inversion networks, injecting fine-grained, instance-specific semantics into the prompts. Furthermore, we propose a Cross-Modal Prompt Regularization strategy to enforce semantic alignment in the prompt token space, ensuring that pseudo-prompts preserve source-modality characteristics while enhancing cross-modal transferability. Extensive experiments on multiple ReID benchmarks validate the effectiveness of our method, achieving state-of-the-art performance on both I2I and T2I tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13565v1" target="_blank"><h2>Artificial Intelligence-driven Intelligent Wearable Systems: A full-stack Integration from Material Design to Personalized Interaction <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jingyi Zhao, Daqian Shi, Zhengda Wang, Xiongfeng Tang, Yanguo Qin<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> 5 pages, l figure, l table. Accepted at AI4RWC@WI-IAT 2025<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multi-modal (abstract)<br><p><strong><u>Abstract:</u></strong> Intelligent wearable systems are at the forefront of precision medicine and play a crucial role in enhancing human-machine interaction. Traditional devices often encounter limitations due to their dependence on empirical material design and basic signal processing techniques. To overcome these issues, we introduce the concept of Human-Symbiotic Health Intelligence (HSHI), which is a framework that integrates multi-modal sensor networks with edge-cloud collaborative computing and a hybrid approach to data and knowledge modeling. HSHI is designed to adapt dynamically to both inter-individual and intra-individual variability, transitioning health management from passive monitoring to an active collaborative evolution. The framework incorporates AI-driven optimization of materials and micro-structures, provides robust interpretation of multi-modal signals, and utilizes a dual mechanism that merges population-level insights with personalized adaptations. Moreover, the integration of closed-loop optimization through reinforcement learning and digital twins facilitates customized interventions and feedback. In general, HSHI represents a significant shift in healthcare, moving towards a model that emphasizes prevention, adaptability, and a harmonious relationship between technology and health management.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13561v1" target="_blank"><h2>RAC-DMVC: Reliability-Aware Contrastive Deep Multi-View Clustering under Multi-Source Noise <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Shihao Dong, Yue Liu, Xiaotong Zhou, Yuhui Zheng, Huiying Xu, Xinzhong Zhu<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Multi-view clustering (MVC), which aims to separate the multi-view data into distinct clusters in an unsupervised manner, is a fundamental yet challenging task. To enhance its applicability in real-world scenarios, this paper addresses a more challenging task: MVC under multi-source noises, including missing noise and observation noise. To this end, we propose a novel framework, Reliability-Aware Contrastive Deep Multi-View Clustering (RAC-DMVC), which constructs a reliability graph to guide robust representation learning under noisy environments. Specifically, to address observation noise, we introduce a cross-view reconstruction to enhances robustness at the data level, and a reliability-aware noise contrastive learning to mitigates bias in positive and negative pairs selection caused by noisy representations. To handle missing noise, we design a dual-attention imputation to capture shared information across views while preserving view-specific features. In addition, a self-supervised cluster distillation module further refines the learned representations and improves the clustering performance. Extensive experiments on five benchmark datasets demonstrate that RAC-DMVC outperforms SOTA methods on multiple evaluation metrics and maintains excellent performance under varying ratios of noise.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13545v1" target="_blank"><h2>Robust Defense Strategies for Multimodal Contrastive Learning: Efficient Fine-tuning Against Backdoor Attacks <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Md. Iqbal Hossain, Afia Sajeeda, Neeresh Kumar Perla, Ming Shao<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> The advent of multimodal deep learning models, such as CLIP, has unlocked new frontiers in a wide range of applications, from image-text understanding to classification tasks. However, these models are not safe for adversarial attacks, particularly backdoor attacks, which can subtly manipulate model behavior. Moreover, existing defense methods typically involve training from scratch or fine-tuning using a large dataset without pinpointing the specific labels that are affected. In this study, we introduce an innovative strategy to enhance the robustness of multimodal contrastive learning models against such attacks. In particular, given a poisoned CLIP model, our approach can identify the backdoor trigger and pinpoint the victim samples and labels in an efficient manner. To that end, an image segmentation ``oracle'' is introduced as the supervisor for the output of the poisoned CLIP. We develop two algorithms to rectify the poisoned model: (1) differentiating between CLIP and Oracle's knowledge to identify potential triggers; (2) pinpointing affected labels and victim samples, and curating a compact fine-tuning dataset. With this knowledge, we are allowed to rectify the poisoned CLIP model to negate backdoor effects. Extensive experiments on visual recognition benchmarks demonstrate our strategy is effective in CLIP-based backdoor defense.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13542v2" target="_blank"><h2>Making Evidence Actionable in Adaptive Learning Closing the Diagnostic Pedagogical Loop <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Amirreza Mehrabi, Jason Wade Morphew, Breejha Quezada, N. Sanjay Rebello<br><strong><u>Categories:</u></strong> cs.CE, cs.AI, cs.CY, stat.AP<br><strong><u>Comments:</u></strong> We have submitted the same article with another title: Making Evidence Actionable in Adaptive Learning (arXiv:2511.14052)<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Adaptive learning often diagnoses precisely yet intervenes weakly, producing help that is mistimed or misaligned. This study presents evidence supporting an instructor-governed feedback loop that converts concept-level assessment evidence into vetted microinterventions. The adaptive learning algorithm includes three safeguards: adequacy as a hard guarantee of gap closure, attention as a budgeted limit for time and redundancy, and diversity as protection against overfitting to a single resource. We formulate intervention assignment as a binary integer program with constraints for coverage, time, difficulty windows derived from ability estimates, prerequisites encoded by a concept matrix, and anti-redundancy with diversity. Greedy selection serves low-richness and tight-latency settings, gradient-based relaxation serves rich repositories, and a hybrid switches along a richness-latency frontier. In simulation and in an introductory physics deployment with 1204 students, both solvers achieved full skill coverage for nearly all learners within bounded watch time. The gradient-based method reduced redundant coverage by about 12 percentage points relative to greedy and produced more consistent difficulty alignment, while greedy delivered comparable adequacy at lower computational cost in resource-scarce environments. Slack variables localized missing content and guided targeted curation, sustaining sufficiency across student subgroups. The result is a tractable and auditable controller that closes the diagnostic pedagogical loop and enables equitable, load-aware personalization at the classroom scale.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13541v1" target="_blank"><h2>Graph Out-of-Distribution Detection via Test-Time Calibration with Dual Dynamic Dictionaries <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Yue Hou, Ruomei Liu, Yingke Su, Junran Wu, Ke Xu<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> Accepted by AAAI 2026 (The 40th Annual AAAI Conference on Artificial Intelligence)<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> A key challenge in graph out-of-distribution (OOD) detection lies in the absence of ground-truth OOD samples during training. Existing methods are typically optimized to capture features within the in-distribution (ID) data and calculate OOD scores, which often limits pre-trained models from representing distributional boundaries, leading to unreliable OOD detection. Moreover, the latent structure of graph data is often governed by multiple underlying factors, which remains less explored. To address these challenges, we propose a novel test-time graph OOD detection method, termed BaCa, that calibrates OOD scores using dual dynamically updated dictionaries without requiring fine-tuning the pre-trained model. Specifically, BaCa estimates graphons and applies a mix-up strategy solely with test samples to generate diverse boundary-aware discriminative topologies, eliminating the need for exposing auxiliary datasets as outliers. We construct dual dynamic dictionaries via priority queues and attention mechanisms to adaptively capture latent ID and OOD representations, which are then utilized for boundary-aware OOD score calibration. To the best of our knowledge, extensive experiments on real-world datasets show that BaCa significantly outperforms existing state-of-the-art methods in OOD detection.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13540v2" target="_blank"><h2>Fairness-Aware Graph Representation Learning with Limited Demographic Information <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zichong Wang, Zhipeng Yin, Liping Yang, Jun Zhuang, Rui Yu, Qingzhao Kong, Wenbin Zhang<br><strong><u>Categories:</u></strong> cs.LG, cs.CY<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Ensuring fairness in Graph Neural Networks is fundamental to promoting trustworthy and socially responsible machine learning systems. In response, numerous fair graph learning methods have been proposed in recent years. However, most of them assume full access to demographic information, a requirement rarely met in practice due to privacy, legal, or regulatory restrictions. To this end, this paper introduces a novel fair graph learning framework that mitigates bias in graph learning under limited demographic information. Specifically, we propose a mechanism guided by partial demographic data to generate proxies for demographic information and design a strategy that enforces consistent node embeddings across demographic groups. In addition, we develop an adaptive confidence strategy that dynamically adjusts each node's contribution to fairness and utility based on prediction confidence. We further provide theoretical analysis demonstrating that our framework, FairGLite, achieves provable upper bounds on group fairness metrics, offering formal guarantees for bias mitigation. Through extensive experiments on multiple datasets and fair graph learning frameworks, we demonstrate the framework's effectiveness in both mitigating bias and maintaining model utility.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13530v1" target="_blank"><h2>Towards Affect-Adaptive Human-Robot Interaction: A Protocol for Multimodal Dataset Collection on Social Anxiety <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Vesna Poprcova, Iulia Lefter, Matthias Wieser, Martijn Warnier, Frances Brazier<br><strong><u>Categories:</u></strong> cs.RO, cs.AI<br><strong><u>Comments:</u></strong> Accepted at the Workshop on Benefits of pErsonalization and behAvioral adaptation in assistive Robots (BEAR 2025), held at the IEEE RO-MAN Conference 2025<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Social anxiety is a prevalent condition that affects interpersonal interactions and social functioning. Recent advances in artificial intelligence and social robotics offer new opportunities to examine social anxiety in the human-robot interaction context. Accurate detection of affective states and behaviours associated with social anxiety requires multimodal datasets, where each signal modality provides complementary insights into its manifestations. However, such datasets remain scarce, limiting progress in both research and applications. To address this, this paper presents a protocol for multimodal dataset collection designed to reflect social anxiety in a human-robot interaction context. The dataset will consist of synchronised audio, video, and physiological recordings acquired from at least 70 participants, grouped according to their level of social anxiety, as they engage in approximately 10-minute interactive Wizard-of-Oz role-play scenarios with the Furhat social robot under controlled experimental conditions. In addition to multimodal data, the dataset will be enriched with contextual data providing deeper insight into individual variability in social anxiety responses. This work can contribute to research on affect-adaptive human-robot interaction by providing support for robust multimodal detection of social anxiety.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13527v1" target="_blank"><h2>Mitigating Spurious Correlations in Patch-wise Tumor Classification on High-Resolution Multimodal Images <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Ihab Asaad, Maha Shadaydeh, Joachim Denzler<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> Accepted at EurIPS 2025 Workshop: Unifying Perspectives on Learning Biases (UPLB)<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Patch-wise multi-label classification provides an efficient alternative to full pixel-wise segmentation on high-resolution images, particularly when the objective is to determine the presence or absence of target objects within a patch rather than their precise spatial extent. This formulation substantially reduces annotation cost, simplifies training, and allows flexible patch sizing aligned with the desired level of decision granularity. In this work, we focus on a special case, patch-wise binary classification, applied to the detection of a single class of interest (tumor) on high-resolution multimodal nonlinear microscopy images. We show that, although this simplified formulation enables efficient model development, it can introduce spurious correlations between patch composition and labels: tumor patches tend to contain larger tissue regions, whereas non-tumor patches often consist mostly of background with small tissue areas. We further quantify the bias in model predictions caused by this spurious correlation, and propose to use a debiasing strategy to mitigate its effect. Specifically, we apply GERNE, a debiasing method that can be adapted to maximize worst-group accuracy (WGA). Our results show an improvement in WGA by approximately 7% compared to ERM for two different thresholds used to binarize the spurious feature. This enhancement boosts model performance on critical minority cases, such as tumor patches with small tissues and non-tumor patches with large tissues, and underscores the importance of spurious correlation-aware learning in patch-wise classification problems.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13514v1" target="_blank"><h2>A Quantum Tensor Network-Based Viewpoint for Modeling and Analysis of Time Series Data <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Pragatheeswaran Vipulananthan, Kamal Premaratne, Dilip Sarkar, Manohar N. Murthi<br><strong><u>Categories:</u></strong> cs.LG, cs.IT<br><strong><u>Comments:</u></strong> IEEE International Conference on Knowledge Graph (ICKG), 378-387, 2024<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Accurate uncertainty quantification is a critical challenge in machine learning. While neural networks are highly versatile and capable of learning complex patterns, they often lack interpretability due to their ``black box'' nature. On the other hand, probabilistic ``white box'' models, though interpretable, often suffer from a significant performance gap when compared to neural networks. To address this, we propose a novel quantum physics-based ``white box'' method that offers both accurate uncertainty quantification and enhanced interpretability. By mapping the kernel mean embedding (KME) of a time series data vector to a reproducing kernel Hilbert space (RKHS), we construct a tensor network-inspired 1D spin chain Hamiltonian, with the KME as one of its eigen-functions or eigen-modes. We then solve the associated Schr{ö}dinger equation and apply perturbation theory to quantify uncertainty, thereby improving the interpretability of tasks performed with the quantum tensor network-based model. We demonstrate the effectiveness of this methodology, compared to state-of-the-art ``white box" models, in change point detection and time series clustering, providing insights into the uncertainties associated with decision-making throughout the process.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13487v2" target="_blank"><h2>Systematic Evaluation of Time-Frequency Features for Binaural Sound Source Localization <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Davoud Shariat Panah, Alessandro Ragano, Dan Barry, Jan Skoglund, Andrew Hines<br><strong><u>Categories:</u></strong> eess.AS, cs.LG, cs.SD<br><strong><u>Comments:</u></strong> Submitted to ICASSP 2026<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> This study presents a systematic evaluation of time-frequency feature design for binaural sound source localization (SSL), focusing on how feature selection influences model performance across diverse conditions. We investigate the performance of a convolutional neural network (CNN) model using various combinations of amplitude-based features (magnitude spectrogram, interaural level difference - ILD) and phase-based features (phase spectrogram, interaural phase difference - IPD). Evaluations on in-domain and out-of-domain data with mismatched head-related transfer functions (HRTFs) reveal that carefully chosen feature combinations often outperform increases in model complexity. While two-feature sets such as ILD + IPD are sufficient for in-domain SSL, generalization to diverse content requires richer inputs combining channel spectrograms with both ILD and IPD. Using the optimal feature sets, our low-complexity CNN model achieves competitive performance. Our findings underscore the importance of feature design in binaural SSL and provide practical guidance for both domain-specific and general-purpose localization.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13476v1" target="_blank"><h2>Multi-Agent Multimodal Large Language Model Framework for Automated Interpretation of Fuel Efficiency Analytics in Public Transportation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zhipeng Ma, Ali Rida Bahja, Andreas Burgdorf, André Pomp, Tobias Meisen, Bo Nørregaard Jørgensen, Zheng Grace Ma<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Enhancing fuel efficiency in public transportation requires the integration of complex multimodal data into interpretable, decision-relevant insights. However, traditional analytics and visualization methods often yield fragmented outputs that demand extensive human interpretation, limiting scalability and consistency. This study presents a multi-agent framework that leverages multimodal large language models (LLMs) to automate data narration and energy insight generation. The framework coordinates three specialized agents, including a data narration agent, an LLM-as-a-judge agent, and an optional human-in-the-loop evaluator, to iteratively transform analytical artifacts into coherent, stakeholder-oriented reports. The system is validated through a real-world case study on public bus transportation in Northern Jutland, Denmark, where fuel efficiency data from 4006 trips are analyzed using Gaussian Mixture Model clustering. Comparative experiments across five state-of-the-art LLMs and three prompting paradigms identify GPT-4.1 mini with Chain-of-Thought prompting as the optimal configuration, achieving 97.3% narrative accuracy while balancing interpretability and computational cost. The findings demonstrate that multi-agent orchestration significantly enhances factual precision, coherence, and scalability in LLM-based reporting. The proposed framework establishes a replicable and domain-adaptive methodology for AI-driven narrative generation and decision support in energy informatics.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13469v1" target="_blank"><h2>GREAT: Generalizable Representation Enhancement via Auxiliary Transformations for Zero-Shot Environmental Prediction <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Shiyuan Luo, Chonghao Qiu, Runlong Yu, Yiqun Xie, Xiaowei Jia<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Environmental modeling faces critical challenges in predicting ecosystem dynamics across unmonitored regions due to limited and geographically imbalanced observation data. This challenge is compounded by spatial heterogeneity, causing models to learn spurious patterns that fit only local data. Unlike conventional domain generalization, environmental modeling must preserve invariant physical relationships and temporal coherence during augmentation. In this paper, we introduce Generalizable Representation Enhancement via Auxiliary Transformations (GREAT), a framework that effectively augments available datasets to improve predictions in completely unseen regions. GREAT guides the augmentation process to ensure that the original governing processes can be recovered from the augmented data, and the inclusion of the augmented data leads to improved model generalization. Specifically, GREAT learns transformation functions at multiple layers of neural networks to augment both raw environmental features and temporal influence. They are refined through a novel bi-level training process that constrains augmented data to preserve key patterns of the original source data. We demonstrate GREAT's effectiveness on stream temperature prediction across six ecologically diverse watersheds in the eastern U.S., each containing multiple stream segments. Experimental results show that GREAT significantly outperforms existing methods in zero-shot scenarios. This work provides a practical solution for environmental applications where comprehensive monitoring is infeasible.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13463v1" target="_blank"><h2>Multi-task GINN-LP for Multi-target Symbolic Regression <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Hussein Rajabu, Lijun Qian, Xishuang Dong<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> explainable (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> In the area of explainable artificial intelligence, Symbolic Regression (SR) has emerged as a promising approach by discovering interpretable mathematical expressions that fit data. However, SR faces two main challenges: most methods are evaluated on scientific datasets with well-understood relationships, limiting generalization, and SR primarily targets single-output regression, whereas many real-world problems involve multi-target outputs with interdependent variables. To address these issues, we propose multi-task regression GINN-LP (MTRGINN-LP), an interpretable neural network for multi-target symbolic regression. By integrating GINN-LP with a multi-task deep learning, the model combines a shared backbone including multiple power-term approximator blocks with task-specific output layers, capturing inter-target dependencies while preserving interpretability. We validate multi-task GINN-LP on practical multi-target applications, including energy efficiency prediction and sustainable agriculture. Experimental results demonstrate competitive predictive performance alongside high interpretability, effectively extending symbolic regression to broader real-world multi-output tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13457v1" target="_blank"><h2>Artificial Intelligence-Enabled Spirometry for Early Detection of Right Heart Failure <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Bin Liu, Qinghao Zhao, Yuxi Zhou, Zhejun Sun, Kaijie Lei, Deyun Zhang, Shijia Geng, Shenda Hong<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 19 pages, 5 figures<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> variational autoencoder (abstract), VAE (abstract)<br><p><strong><u>Abstract:</u></strong> Right heart failure (RHF) is a disease characterized by abnormalities in the structure or function of the right ventricle (RV), which is associated with high morbidity and mortality. Lung disease often causes increased right ventricular load, leading to RHF. Therefore, it is very important to screen out patients with cor pulmonale who develop RHF from people with underlying lung diseases. In this work, we propose a self-supervised representation learning method to early detecting RHF from patients with cor pulmonale, which uses spirogram time series to predict patients with RHF at an early stage. The proposed model is divided into two stages. The first stage is the self-supervised representation learning-based spirogram embedding (SLSE) network training process, where the encoder of the Variational autoencoder (VAE-encoder) learns a robust low-dimensional representation of the spirogram time series from the data-augmented unlabeled data. Second, this low-dimensional representation is fused with demographic information and fed into a CatBoost classifier for the downstream RHF prediction task. Trained and tested on a carefully selected subset of 26,617 individuals from the UK Biobank, our model achieved an AUROC of 0.7501 in detecting RHF, demonstrating strong population-level distinction ability. We further evaluated the model on high-risk clinical subgroups, achieving AUROC values of 0.8194 on a test set of 74 patients with chronic kidney disease (CKD) and 0.8413 on a set of 64 patients with valvular heart disease (VHD). These results highlight the model's potential utility in predicting RHF among clinically elevated-risk populations. In conclusion, this study presents a self-supervised representation learning approach combining spirogram time series and demographic data, demonstrating promising potential for early RHF detection in clinical practice.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13444v1" target="_blank"><h2>Discovering Operational Patterns Using Image-Based Convolutional Clustering and Composite Evaluation: A Case Study in Foundry Melting Processes <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Zhipeng Ma, Bo Nørregaard Jørgensen, Zheng Grace Ma<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), convolutional (title, abstract), explainability (abstract), explainable (abstract)<br><p><strong><u>Abstract:</u></strong> Industrial process monitoring increasingly relies on sensor-generated time-series data, yet the lack of labels, high variability, and operational noise make it difficult to extract meaningful patterns using conventional methods. Existing clustering techniques either rely on fixed distance metrics or deep models designed for static data, limiting their ability to handle dynamic, unstructured industrial sequences. Addressing this gap, this paper proposes a novel framework for unsupervised discovery of operational modes in univariate time-series data using image-based convolutional clustering with composite internal evaluation. The proposed framework improves upon existing approaches in three ways: (1) raw time-series sequences are transformed into grayscale matrix representations via overlapping sliding windows, allowing effective feature extraction using a deep convolutional autoencoder; (2) the framework integrates both soft and hard clustering outputs and refines the selection through a two-stage strategy; and (3) clustering performance is objectively evaluated by a newly developed composite score, S_eva, which combines normalized Silhouette, Calinski-Harabasz, and Davies-Bouldin indices. Applied to over 3900 furnace melting operations from a Nordic foundry, the method identifies seven explainable operational patterns, revealing significant differences in energy consumption, thermal dynamics, and production duration. Compared to classical and deep clustering baselines, the proposed approach achieves superior overall performance, greater robustness, and domain-aligned explainability. The framework addresses key challenges in unsupervised time-series analysis, such as sequence irregularity, overlapping modes, and metric inconsistency, and provides a generalizable solution for data-driven diagnostics and energy optimization in industrial systems.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13442v2" target="_blank"><h2>Unlocking the Forgery Detection Potential of Vanilla MLLMs: A Novel Training-Free Pipeline <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Rui Zuo, Qinyue Tong, Zhe-Ming Lu, Ziqian Lu<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> With the rapid advancement of artificial intelligence-generated content (AIGC) technologies, including multimodal large language models (MLLMs) and diffusion models, image generation and manipulation have become remarkably effortless. Existing image forgery detection and localization (IFDL) methods often struggle to generalize across diverse datasets and offer limited interpretability. Nowadays, MLLMs demonstrate strong generalization potential across diverse vision-language tasks, and some studies introduce this capability to IFDL via large-scale training. However, such approaches cost considerable computational resources, while failing to reveal the inherent generalization potential of vanilla MLLMs to address this problem. Inspired by this observation, we propose Foresee, a training-free MLLM-based pipeline tailored for image forgery analysis. It eliminates the need for additional training and enables a lightweight inference process, while surpassing existing MLLM-based methods in both tamper localization accuracy and the richness of textual explanations. Foresee employs a type-prior-driven strategy and utilizes a Flexible Feature Detector (FFD) module to specifically handle copy-move manipulations, thereby effectively unleashing the potential of vanilla MLLMs in the forensic domain. Extensive experiments demonstrate that our approach simultaneously achieves superior localization accuracy and provides more comprehensive textual explanations. Moreover, Foresee exhibits stronger generalization capability, outperforming existing IFDL methods across various tampering types, including copy-move, splicing, removal, local enhancement, deepfake, and AIGC-based editing. The code will be released in the final version.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13419v1" target="_blank"><h2>MMWSTM-ADRAN+: A Novel Hybrid Deep Learning Architecture for Enhanced Climate Time Series Forecasting and Extreme Event Prediction <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Shaheen Mohammed Saleh Ahmed, Hakan Hakan Guneyli<br><strong><u>Categories:</u></strong> cs.LG, physics.ao-ph<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multi-modal (abstract), attention (abstract), data augmentation (abstract)<br><p><strong><u>Abstract:</u></strong> Accurate short-range prediction of extreme air temperature events remains a fundamental challenge in operational climate-risk management. We present Multi-Modal Weather State Transition Model with Anomaly-Driven Recurrent Attention Network Plus (MMWSTM-ADRAN+), a dual-stream deep learning architecture that couples a regime-aware dynamics model with an anomaly-focused attention mechanism to forecast daily maximum temperature and its extremes. The first stream, MMWSTM, combines bidirectional Long Short-Term Memory (BiLSTM) units with a learnable Markov state transition matrix to capture synoptic-scale weather regime changes. The second stream, ADRAN, integrates bidirectional Gated Recurrent Units (BiGRUs), multi-head self-attention, and a novel anomaly amplification layer to enhance sensitivity to low-probability signals. A lightweight attentive fusion gate adaptively determines the contribution of each stream to the final prediction. Model optimization employs a custom ExtremeWeatherLoss function that up-weights errors on the upper 5% and lower 5% of the temperature distribution, and a time-series data augmentation suite (jittering, scaling, time/magnitude warping) that effectively quadruples the training data</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13394v1" target="_blank"><h2>Fast and Robust Simulation-Based Inference With Optimization Monte Carlo <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Vasilis Gkolemis, Christos Diou, Michael Gutmann<br><strong><u>Categories:</u></strong> cs.LG, stat.ML<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Bayesian parameter inference for complex stochastic simulators is challenging due to intractable likelihood functions. Existing simulation-based inference methods often require large number of simulations and become costly to use in high-dimensional parameter spaces or in problems with partially uninformative outputs. We propose a new method for differentiable simulators that delivers accurate posterior inference with substantially reduced runtimes. Building on the Optimization Monte Carlo framework, our approach reformulates stochastic simulation as deterministic optimization problems. Gradient-based methods are then applied to efficiently navigate toward high-density posterior regions and avoid wasteful simulations in low-probability areas. A JAX-based implementation further enhances the performance through vectorization of key method components. Extensive experiments, including high-dimensional parameter spaces, uninformative outputs, multiple observations and multimodal posteriors show that our method consistently matches, and often exceeds, the accuracy of state-of-the-art approaches, while reducing the runtime by a substantial margin.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13393v1" target="_blank"><h2>Learning Cosmology from Nearest Neighbour Statistics <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Atrideb Chatterjee, Arka Banerjee, Francisco Villaescusa-Navarro, Tom Abel<br><strong><u>Categories:</u></strong> astro-ph.CO<br><strong><u>Comments:</u></strong> Submitted for publication to A&A<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Extracting cosmological parameters from galaxy/halo catalogues with sub-percent level accuracy is an important aspect of modern cosmology, especially in view of ongoing and upcoming surveys such as Euclid, DESI, and LSST. While traditional two-point statistics have been known to be suboptimal for this task, recently proposed k-Nearest Neighbour (kNN) based summary statistics have demonstrated tighter constraining power. Building on the kNN statistics, we introduce a new field-level representation of discrete halo catalogues - NN distance maps. We employ this technique on the halo catalogues obtained from Quijote N-body simulation suites. By combining these maps with kNN-based summary statistics, we train a hybrid neural network to infer cosmological parameters, showing that the resulting constraints achieve state-of-the-art, if not the best, accuracy. In addition, our hybrid framework is 5-10 times more computationally efficient than some of the existing point-cloud-based ML methods.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13392v1" target="_blank"><h2>Cosmic Expansion Driven by Gravitational Particle Production: Toward a Complete Cosmological Scenario <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> P. W. R. Lima, J. A. S. Lima<br><strong><u>Categories:</u></strong> astro-ph.CO<br><strong><u>Comments:</u></strong> 23 pages and 5 figures<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> A dark-energy-free cosmological model ($Ω_{DE} \equiv 0$) based on gravitationally induced adiabatic particle creation is proposed. The thermodynamics of particle production yields an effective negative pressure that drives both primordial inflation and late-time cosmic acceleration. The model, characterized by four components and two free parameters ($α$, $β$), reproduces a $Λ$CDM-like expansion for suitable $α$, while $β$ introduces small but testable deviations from the cosmic concordance model. Constraints from type Ia Supernovae (Pantheon+SH0ES) and H(z) data indicate $β\simeq 0.13$, suggesting a mild departure from standard cosmology and possible relief of the $H_0$ and $S_8$ tensions. The resulting classical cosmology evolves smoothly between two extreme de Sitter phases, offering a singularity-free, unified scenario that beyond solving old cosmological puzzles opens a new perspective to handle the tensions plaguing the current cosmic concordance model.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13373v1" target="_blank"><h2>A Novel Hierarchical Integration Method for Efficient Model Merging in Medical LLMs <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Prakrit Timilsina, Anuj Nepal, Rajan Kadel, Robin Doss<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Large Language Models (LLMs) face significant challenges in distributed healthcare, including consolidating specialized domain knowledge across institutions while maintaining privacy, reducing computational overhead, and preventing catastrophic forgetting during model updates.This paper presents a systematic evaluation of six parameter-space merging techniques applied to two architecturally compatible medical LLMs derived from the Mistral-7B base model. We introduce a novel hierarchical method that combines selective Optimal Transport (OT) alignment for attention layers with cosine similarity-weighted interpolation, designed to address permutation variance while minimizing computational overhead for edge deployment scenarios. Our study evaluates Task Arithmetic, Linear Averaging, DARE-TIES, DELLA, Breadcrumbs, and our Hierarchical approach across five medical benchmarks. Results demonstrate that architecturally compatible models benefit significantly from simple averaging methods, with Task Arithmetic achieving 45.80% accuracy on MedQA, outperforming complex pruning-based approaches. These findings offer critical insights for the deployment of distributed medical AI in resource-constrained IoT environments, where computational efficiency and model compatibility are paramount. Our work establishes that for architecturally compatible models, simple averaging provides a robust and computationally efficient baseline for knowledge consolidation, offering a pragmatic path forward for scalable medical AI systems.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13372v1" target="_blank"><h2>Constraining r-process nucleosynthesis with multi-objective Galactic chemical evolution models <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> M. Molero, A. Arcones, F. Montes, C. J. Hansen<br><strong><u>Categories:</u></strong> astro-ph.GA, astro-ph.HE, astro-ph.SR<br><strong><u>Comments:</u></strong> 21 pages, 16 figures, submitted to Astronomy&Astrophysics<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract), VAE (abstract)<br><p><strong><u>Abstract:</u></strong> The astrophysical site(s) of the r-process are uncertain, with candidates such as neutron star mergers and magneto-rotational supernovae predicting different event rates, delay times, and heavy-element yields. Galactic chemical evolution models constrain these properties by comparing model predictions with observed abundances. We explore, in a systematic and data-driven way, the astrophysical conditions under which r-process enrichment can reproduce the observed trends of multiple neutron-capture elements in the Milky Way. Rather than assuming a fixed site, we adopt a flexible, parametric approach to test whether a common set of r-process parameters can explain the chemical evolution of several heavy elements. We compute a grid of one-infall, homogeneous models varying: Eu yield per event, r-process event rate, enrichment delay time, and progenitor mass range. For each of the $\sim 1.5 \times 10^5$ models, we predict [X/Fe] vs. [Fe/H] trends by scaling Eu yields with the solar r-process pattern. A multi-objective optimisation based on Pareto fronts identifies models that best reproduce the abundance trends. Best-fitting models favour short delay times ($\leq 30\ \rm Myr$), low-mass progenitors ($\sim 20-25\ \rm M_\odot$), and an effective Eu injection of $\sim 2 \times 10^{-7}\ \rm M_\odot$ per event. Stars more massive than $\sim 80\ \rm M_\odot$ are too rare to dominate the enrichment. While heavy elements can be reproduced, lighter ones show stronger conflicts with Eu, reflecting that the solar r-process scaling relation becomes less valid toward lighter elements. No single class of r-process events, under solar-scaled yields, can explain light and heavy neutron-capture elements; at least two components are required: a main r-process consistent with solar and r-rich stars, and a weaker component producing enhanced light r-process elements, similar to that observed in r-poor stars.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13371v1" target="_blank"><h2>Cognitive Maps in Language Models: A Mechanistic Analysis of Spatial Planning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Caroline Baumgartner, Eleanor Spens, Neil Burgess, Petru Manescu<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> How do large language models solve spatial navigation tasks? We investigate this by training GPT-2 models on three spatial learning paradigms in grid environments: passive exploration (Foraging Model- predicting steps in random walks), goal-directed planning (generating optimal shortest paths) on structured Hamiltonian paths (SP-Hamiltonian), and a hybrid model fine-tuned with exploratory data (SP-Random Walk). Using behavioural, representational and mechanistic analyses, we uncover two fundamentally different learned algorithms. The Foraging model develops a robust, map-like representation of space, akin to a 'cognitive map'. Causal interventions reveal that it learns to consolidate spatial information into a self-sufficient coordinate system, evidenced by a sharp phase transition where its reliance on historical direction tokens vanishes by the middle layers of the network. The model also adopts an adaptive, hierarchical reasoning system, switching between a low-level heuristic for short contexts and map-based inference for longer ones. In contrast, the goal-directed models learn a path-dependent algorithm, remaining reliant on explicit directional inputs throughout all layers. The hybrid model, despite demonstrating improved generalisation over its parent, retains the same path-dependent strategy. These findings suggest that the nature of spatial intelligence in transformers may lie on a spectrum, ranging from generalisable world models shaped by exploratory data to heuristics optimised for goal-directed tasks. We provide a mechanistic account of this generalisation-optimisation trade-off and highlight how the choice of training regime influences the strategies that emerge.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13351v1" target="_blank"><h2>Dual-LoRA and Quality-Enhanced Pseudo Replay for Multimodal Continual Food Learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Xinlan Wu, Bin Zhu, Feng Han, Pengkun Jiao, Jingjing Chen<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Food analysis has become increasingly critical for health-related tasks such as personalized nutrition and chronic disease prevention. However, existing large multimodal models (LMMs) in food analysis suffer from catastrophic forgetting when learning new tasks, requiring costly retraining from scratch. To address this, we propose a novel continual learning framework for multimodal food learning, integrating a Dual-LoRA architecture with Quality-Enhanced Pseudo Replay. We introduce two complementary low-rank adapters for each task: a specialized LoRA that learns task-specific knowledge with orthogonal constraints to previous tasks' subspaces, and a cooperative LoRA that consolidates shared knowledge across tasks via pseudo replay. To improve the reliability of replay data, our Quality-Enhanced Pseudo Replay strategy leverages self-consistency and semantic similarity to reduce hallucinations in generated samples. Experiments on the comprehensive Uni-Food dataset show superior performance in mitigating forgetting, representing the first effective continual learning approach for complex food tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13339v1" target="_blank"><h2>Statistically Accurate and Robust Generative Prediction of Rock Discontinuities with A Tabular Foundation Model <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Han Meng, Gang Mei, Hong Tian, Nengxiong Xu, Jianbing Peng<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> Rock discontinuities critically govern the mechanical behavior and stability of rock masses. Their internal distributions remain largely unobservable and are typically inferred from surface-exposed discontinuities using generative prediction approaches. However, surface-exposed observations are inherently sparse, and existing generative prediction approaches either fail to capture the underlying complex distribution patterns or lack robustness under data-sparse conditions. Here, we proposed a simple yet robust approach for statistically accurate generative prediction of rock discontinuities by utilizing a tabular foundation model. By leveraging the powerful sample learning capability of the foundation model specifically designed for small data, our approach can effectively capture the underlying complex distribution patterns within limited measured discontinuities. Comparative experiments on ten datasets with diverse scales and distribution patterns of discontinuities demonstrate superior accuracy and robustness over conventional statistical models and deep generative approaches. This work advances quantitative characterization of rock mass structures, supporting safer and more reliable data-driven geotechnical design.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13338v1" target="_blank"><h2>Tab-PET: Graph-Based Positional Encodings for Tabular Transformers <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yunze Leng, Rohan Ghosh, Mehul Motani<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract), attention (abstract), causality (abstract)<br><p><strong><u>Abstract:</u></strong> Supervised learning with tabular data presents unique challenges, including low data sizes, the absence of structural cues, and heterogeneous features spanning both categorical and continuous domains. Unlike vision and language tasks, where models can exploit inductive biases in the data, tabular data lacks inherent positional structure, hindering the effectiveness of self-attention mechanisms. While recent transformer-based models like TabTransformer, SAINT, and FT-Transformer (which we refer to as 3T) have shown promise on tabular data, they typically operate without leveraging structural cues such as positional encodings (PEs), as no prior structural information is usually available. In this work, we find both theoretically and empirically that structural cues, specifically PEs can be a useful tool to improve generalization performance for tabular transformers. We find that PEs impart the ability to reduce the effective rank (a form of intrinsic dimensionality) of the features, effectively simplifying the task by reducing the dimensionality of the problem, yielding improved generalization. To that end, we propose Tab-PET (PEs for Tabular Transformers), a graph-based framework for estimating and inculcating PEs into embeddings. Inspired by approaches that derive PEs from graph topology, we explore two paradigms for graph estimation: association-based and causality-based. We empirically demonstrate that graph-derived PEs significantly improve performance across 50 classification and regression datasets for 3T. Notably, association-based graphs consistently yield more stable and pronounced gains compared to causality-driven ones. Our work highlights an unexpected role of PEs in tabular transformers, revealing how they can be harnessed to improve generalization.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13326v2" target="_blank"><h2>TacEleven: generative tactic discovery for football open play <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Siyao Zhao, Hao Ma, Zhiqiang Pu, Jingjing Huang, Yi Pan, Shijie Wang, Zhi Ming<br><strong><u>Categories:</u></strong> stat.AP, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Creating offensive advantages during open play is fundamental to football success. However, due to the highly dynamic and long-sequence nature of open play, the potential tactic space grows exponentially as the sequence progresses, making automated tactic discovery extremely challenging. To address this, we propose TacEleven, a generative framework for football open-play tactic discovery developed in close collaboration with domain experts from AJ Auxerre, designed to assist coaches and analysts in tactical decision-making. TacEleven consists of two core components: a language-controlled tactical generator that produces diverse tactical proposals, and a multimodal large language model-based tactical critic that selects the optimal proposal aligned with a high-level stylistic tactical instruction. The two components enables rapid exploration of tactical proposals and discovery of alternative open-play offensive tactics. We evaluate TacEleven across three tasks with progressive tactical complexity: counterfactual exploration, single-step discovery, and multi-step discovery, through both quantitative metrics and a questionnaire-based qualitative assessment. The results show that the TacEleven-discovered tactics exhibit strong realism and tactical creativity, with 52.50% of the multi-step tactical alternatives rated adoptable in real-world elite football scenarios, highlighting the framework's ability to rapidly generate numerous high-quality tactics for complex long-sequence open-play situations. TacEleven demonstrates the potential of creatively leveraging domain data and generative models to advance tactical analysis in sports.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13322v1" target="_blank"><h2>Explainable RL Policies by Distilling to Locally-Specialized Linear Policies with Voronoi State Partitioning <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Senne Deproost, Dennis Steckelmacher, Ann Nowé<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> Accepted for BNAIC/BeNeLearn 2025<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> explainable (title, abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Deep Reinforcement Learning is one of the state-of-the-art methods for producing near-optimal system controllers. However, deep RL algorithms train a deep neural network, that lacks transparency, which poses challenges when the controller has to meet regulations, or foster trust. To alleviate this, one could transfer the learned behaviour into a model that is human-readable by design using knowledge distilla- tion. Often this is done with a single model which mimics the original model on average but could struggle in more dynamic situations. A key challenge is that this simpler model should have the right balance be- tween flexibility and complexity or right balance between balance bias and accuracy. We propose a new model-agnostic method to divide the state space into regions where a simplified, human-understandable model can operate in. In this paper, we use Voronoi partitioning to find regions where linear models can achieve similar performance to the original con- troller. We evaluate our approach on a gridworld environment and a classic control task. We observe that our proposed distillation to locally- specialized linear models produces policies that are explainable and show that the distillation matches or even slightly outperforms the black-box policy they are distilled from.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13315v1" target="_blank"><h2>Computer Vision based group activity detection and action spotting <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Narthana Sivalingam, Santhirarajah Sivasthigan, Thamayanthi Mahendranathan, G. M. R. I. Godaliyadda, M. P. B. Ekanayake, H. M. V. R. Herath<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Group activity detection in multi-person scenes is challenging due to complex human interactions, occlusions, and variations in appearance over time. This work presents a computer vision based framework for group activity recognition and action spotting using a combination of deep learning models and graph based relational reasoning. The system first applies Mask R-CNN to obtain accurate actor localization through bounding boxes and instance masks. Multiple backbone networks, including Inception V3, MobileNet, and VGG16, are used to extract feature maps, and RoIAlign is applied to preserve spatial alignment when generating actor specific features. The mask information is then fused with the feature maps to obtain refined masked feature representations for each actor. To model interactions between individuals, we construct Actor Relation Graphs that encode appearance similarity and positional relations using methods such as normalized cross correlation, sum of absolute differences, and dot product. Graph Convolutional Networks operate on these graphs to reason about relationships and predict both individual actions and group level activities. Experiments on the Collective Activity dataset demonstrate that the combination of mask based feature refinement, robust similarity search, and graph neural network reasoning leads to improved recognition performance across both crowded and non crowded scenarios. This approach highlights the potential of integrating segmentation, feature extraction, and relational graph reasoning for complex video understanding tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13314v1" target="_blank"><h2>Unveiling the nature of the Einstein Probe transient EP 241021a <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> J. Quirola-Vásquez, P. G. Jonker, A. J. Levan, D. B. Malesani, F. E. Bauer, N. Sarin, G. P. Lamb, A. Martin-Carrillo, J. Sánchez-Sierras, M. Fraser, L. Izzo, M. E. Ravasio, D. Mata Sánchez, M. A. P. Torres, J. N. D. van Dalen, A. P. C. van Hoof, J. A. Chacón, S. Littlefair, V. S. Dhillon, L. Cotter, G. Corcoran, R. A. J. Eyles-Ferris, P. T. O'Brien, D. Stern, V. D'Elia, D. H. Hartmann<br><strong><u>Categories:</u></strong> astro-ph.HE<br><strong><u>Comments:</u></strong> 21 pages, 15 Figures (+3 in an Appendix), accepted for publication in MNRAS<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> We present a multi-wavelength analysis of the fast X-ray transient EP 241021a, discovered by the Wide-field X-ray Telescope aboard the \emph{Einstein Probe} satellite on 2024 October 21. The event was not detected in gamma-rays. Follow-up observations from $\sim$1.5 to 100 days post-trigger were obtained across X-ray, UV, optical, near-infrared, and radio bands with ground- and space-based facilities. The redshift is constrained to $z = 0.7485$ from prominent optical spectral features. The optical light curve shows complex evolution: an initial $\sim t^{-0.7}$ decay, followed by a rapid re-brightening peaking at day 7.7 with $\sim t^{-1.7}$ decay, and a third phase peaking near day 19 with $\sim t^{-1.3}$ decay. The spectral energy distribution (SED) and its temporal evolution are consistent with a mix of non-thermal and thermal components. Early optical-to-X-ray spectral indices agree with optically thin synchrotron emission, while steepening of the optical SED after $\sim$20 days indicates either a shift in emission mechanism or the emergence of an additional component. Although broad-lined absorption features are absent, comparisons with type Ic-BL supernovae suggest a SN contribution at late times, suggesting a collapsar origin for EP 241021a. The likely SN in EP 241021a appears to require an additional energy source beyond $^{56}$Ni decay. These results support the view that some fast X-ray transients detected by the \emph{Einstein Probe} arise from massive stellar explosions.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13295v1" target="_blank"><h2>Causal Inference, Biomarker Discovery, Graph Neural Network, Feature Selection <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Chaowang Lan, Jingxin Wu, Yulong Yuan, Chuxun Liu, Huangyi Kang, Caihua Liu<br><strong><u>Categories:</u></strong> q-bio.QM, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> Biomarker discovery from high-throughput transcriptomic data is crucial for advancing precision medicine. However, existing methods often neglect gene-gene regulatory relationships and lack stability across datasets, leading to conflation of spurious correlations with genuine causal effects. To address these issues, we develop a causal graph neural network (Causal-GNN) method that integrates causal inference with multi-layer graph neural networks (GNNs). The key innovation is the incorporation of causal effect estimation for identifying stable biomarkers, coupled with a GNN-based propensity scoring mechanism that leverages cross-gene regulatory networks. Experimental results demonstrate that our method achieves consistently high predictive accuracy across four distinct datasets and four independent classifiers. Moreover, it enables the identification of more stable biomarkers compared to traditional methods. Our work provides a robust, efficient, and biologically interpretable tool for biomarker discovery, demonstrating strong potential for broad application across medical disciplines.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13265v1" target="_blank"><h2>TransFit-CSM: A Fast, Physically Consistent Framework for Interaction-Powered Transients <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Yu-Hao Zhang, Liang-Duan Liu, Ze-Xin Du, Guang-Lei Wu, Jing-Yao Li, Yun-Wei Yu<br><strong><u>Categories:</u></strong> astro-ph.HE<br><strong><u>Comments:</u></strong> 19 pages 10 figures<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> time-domain (abstract)<br><p><strong><u>Abstract:</u></strong> We present TransFit-CSM, a fast and physically consistent framework for modeling interaction-powered transients. The method self-consistently couples the ejecta circumstellar medium (CSM) shock dynamics to radiative diffusion from a moving heating boundary tied to the shocks, so that both the photon escape path and the effective diffusion time evolve with radius and time. We solve the mass and momentum equations for the forward and reverse shocks together with the diffusion equation in the unshocked CSM. TransFit-CSM reproduces the canonical sequence of an early dark phase, a diffusion-mediated rise and peak, and a post-interaction cooling tail, and it clarifies why Arnett-like peak scalings break down in optically thick CSM. The framework is well suited for Bayesian inference and constrains physical parameters of the ejecta and CSM from bolometric or joint multi-band light curves. Applications to SN 2006gy and SN 2010jl yield accurate fits and physically interpretable posteriors, highlighting the dominant role of pre-supernova mass loss in shaping the observables. Because it is both computationally efficient and physically grounded, TransFit-CSM bridges simple analytic prescriptions and radiation-hydrodynamic simulations, enabling population-level inference for current and future time-domain surveys.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13259v1" target="_blank"><h2>GeoX-Bench: Benchmarking Cross-View Geo-Localization and Pose Estimation Capabilities of Large Multimodal Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yushuo Zheng, Jiangyong Ying, Huiyu Duan, Chunyi Li, Zicheng Zhang, Jing Liu, Xiaohong Liu, Guangtao Zhai<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Large multimodal models (LMMs) have demonstrated remarkable capabilities across a wide range of tasks, however their knowledge and abilities in the cross-view geo-localization and pose estimation domains remain unexplored, despite potential benefits for navigation, autonomous driving, outdoor robotics, \textit{etc}. To bridge this gap, we introduce \textbf{GeoX-Bench}, a comprehensive \underline{Bench}mark designed to explore and evaluate the capabilities of LMMs in \underline{cross}-view \underline{Geo}-localization and pose estimation. Specifically, GeoX-Bench contains 10,859 panoramic-satellite image pairs spanning 128 cities in 49 countries, along with corresponding 755,976 question-answering (QA) pairs. Among these, 42,900 QA pairs are designated for benchmarking, while the remaining are intended to enhance the capabilities of LMMs. Based on GeoX-Bench, we evaluate the capabilities of 25 state-of-the-art LMMs on cross-view geo-localization and pose estimation tasks, and further explore the empowered capabilities of instruction-tuning. Our benchmark demonstrate that while current LMMs achieve impressive performance in geo-localization tasks, their effectiveness declines significantly on the more complex pose estimation tasks, highlighting a critical area for future improvement, and instruction-tuning LMMs on the training data of GeoX-Bench can significantly improve the cross-view geo-sense abilities. The GeoX-Bench is available at \textcolor{magenta}{https://github.com/IntMeGroup/GeoX-Bench}.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13243v1" target="_blank"><h2>Uncovering and Mitigating Transient Blindness in Multimodal Model Editing <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Xiaoqi Han, Ru Li, Ran Yi, Hongye Tan, Zhuomin Liang, Víctor Gutiérrez-Basulto, Jeff Z. Pan<br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.CV<br><strong><u>Comments:</u></strong> Accepted at AAAI'26<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Multimodal Model Editing (MMED) aims to correct erroneous knowledge in multimodal models. Existing evaluation methods, adapted from textual model editing, overstate success by relying on low-similarity or random inputs, obscure overfitting. We propose a comprehensive locality evaluation framework, covering three key dimensions: random-image locality, no-image locality, and consistent-image locality, operationalized through seven distinct data types, enabling a detailed and structured analysis of multimodal edits. We introduce De-VQA, a dynamic evaluation for visual question answering, uncovering a phenomenon we term transient blindness, overfitting to edit-similar text while ignoring visuals. Token analysis shows edits disproportionately affect textual tokens. We propose locality-aware adversarial losses to balance cross-modal representations. Empirical results demonstrate that our approach consistently outperforms existing baselines, reducing transient blindness and improving locality by 17% on average.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13238v1" target="_blank"><h2>Computational Measurement of Political Positions: A Review of Text-Based Ideal Point Estimation Algorithms <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Patrick Parschan, Charlott Jakob<br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.CL, cs.CY<br><strong><u>Comments:</u></strong> 46 pages, 8 figures, 2 tables, accepted for publication in Quality & Quantity<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> literature review (abstract)<br><p><strong><u>Abstract:</u></strong> This article presents the first systematic review of unsupervised and semi-supervised computational text-based ideal point estimation (CT-IPE) algorithms, methods designed to infer latent political positions from textual data. These algorithms are widely used in political science, communication, computational social science, and computer science to estimate ideological preferences from parliamentary speeches, party manifestos, and social media. Over the past two decades, their development has closely followed broader NLP trends -- beginning with word-frequency models and most recently turning to large language models (LLMs). While this trajectory has greatly expanded the methodological toolkit, it has also produced a fragmented field that lacks systematic comparison and clear guidance for applied use. To address this gap, we identified 25 CT-IPE algorithms through a systematic literature review and conducted a manual content analysis of their modeling assumptions and development contexts. To compare them meaningfully, we introduce a conceptual framework that distinguishes how algorithms generate, capture, and aggregate textual variance. On this basis, we identify four methodological families -- word-frequency, topic modeling, word embedding, and LLM-based approaches -- and critically assess their assumptions, interpretability, scalability, and limitations. Our review offers three contributions. First, it provides a structured synthesis of two decades of algorithm development, clarifying how diverse methods relate to one another. Second, it translates these insights into practical guidance for applied researchers, highlighting trade-offs in transparency, technical requirements, and validation strategies that shape algorithm choice. Third, it emphasizes that differences in estimation outcomes across algorithms are themselves informative, underscoring the need for systematic benchmarking.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13237v1" target="_blank"><h2>Counterfactual Explainable AI (XAI) Method for Deep Learning-Based Multivariate Time Series Classification <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Alan G. Paredes Cetina, Kaouther Benguessoum, Raoni Lourenço, Sylvain Kubler<br><strong><u>Categories:</u></strong> cs.LG, stat.ML<br><strong><u>Comments:</u></strong> Accepted in AAAI 2026 Technical Main Track<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> explainable (title, abstract)<br><p><strong><u>Abstract:</u></strong> Recent advances in deep learning have improved multivariate time series (MTS) classification and regression by capturing complex patterns, but their lack of transparency hinders decision-making. Explainable AI (XAI) methods offer partial insights, yet often fall short of conveying the full decision space. Counterfactual Explanations (CE) provide a promising alternative, but current approaches typically prioritize either accuracy, proximity or sparsity -- rarely all -- limiting their practical value. To address this, we propose CONFETTI, a novel multi-objective CE method for MTS. CONFETTI identifies key MTS subsequences, locates a counterfactual target, and optimally modifies the time series to balance prediction confidence, proximity and sparsity. This method provides actionable insights with minimal changes, improving interpretability, and decision support. CONFETTI is evaluated on seven MTS datasets from the UEA archive, demonstrating its effectiveness in various domains. CONFETTI consistently outperforms state-of-the-art CE methods in its optimization objectives, and in six other metrics from the literature, achieving $\geq10\%$ higher confidence while improving sparsity in $\geq40\%$.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13224v1" target="_blank"><h2>Natural gradient descent for improving variational inference based classification of radio galaxies <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Devina Mohan, Anna M. M. Scaife<br><strong><u>Categories:</u></strong> astro-ph.IM<br><strong><u>Comments:</u></strong> Accepted to Machine Learning and the Physical Sciences Workshop, NeurIPS 2025<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Bayesian neural networks (BNNs) are most commonly optimised with first-order optimisers such as stochastic gradient descent. However, when optimising for parameters of probabilistic models, incorporating second order information during optimisation can lead to a more direct path in the distribution space and faster convergence. In this work we examine whether using natural gradient descent can improve the performance of variational inference based classification of radio galaxies. We use the Improved Variational Online Newton (iVON) algorithm and compare its performance against a recent benchmark for BNNs for radio galaxy classification. We find that iVON results in better uncertainty calibration out of all the methods previously considered while providing similar predictive performance to the best performing inference methods such as Hamiltonian Monte Carlo and Bayes by Backprop based variational inference. Models trained with iVON can distinguish far out-of-distribution optical galaxy data, but they cannot reliably detect radio galaxy images from a telescope with different resolution and sensitivity. We find that the cold posterior effect persists in the models trained with iVON. Our results suggest that the choice of the optimiser can lead to qualitatively different solutions and future work using probabilistic neural network models should carefully consider the inductive biases being encoded through the optimisation process, in addition to the data, architecture and inference method.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13221v1" target="_blank"><h2>Likelihood-guided Regularization in Attention Based Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Mohamed Salem, Inyoung Kim<br><strong><u>Categories:</u></strong> stat.ML, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> The transformer architecture has demonstrated strong performance in classification tasks involving structured and high-dimensional data. However, its success often hinges on large- scale training data and careful regularization to prevent overfitting. In this paper, we intro- duce a novel likelihood-guided variational Ising-based regularization framework for Vision Transformers (ViTs), which simultaneously enhances model generalization and dynamically prunes redundant parameters. The proposed variational Ising-based regularization approach leverages Bayesian sparsification techniques to impose structured sparsity on model weights, allowing for adaptive architecture search during training. Unlike traditional dropout-based methods, which enforce fixed sparsity patterns, the variational Ising-based regularization method learns task-adaptive regularization, improving both efficiency and interpretability. We evaluate our approach on benchmark vision datasets, including MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100, demonstrating improved generalization under sparse, complex data and allowing for principled uncertainty quantification on both weights and selection parameters. Additionally, we show that the Ising regularizer leads to better-calibrated probability estimates and structured feature selection through uncertainty-aware attention mechanisms. Our results highlight the effectiveness of structured Bayesian sparsification in enhancing transformer-based architectures, offering a principled alternative to standard regularization techniques.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13214v1" target="_blank"><h2>Learning to Solve Resource-Constrained Project Scheduling Problems with Duration Uncertainty using Graph Neural Networks <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Guillaume Infantes, Stéphanie Roussel, Antoine Jacquet, Emmanuel Benazera<br><strong><u>Categories:</u></strong> cs.AI, cs.LG<br><strong><u>Comments:</u></strong> Accepted at ICTAI 2025 Conference<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> The Resource-Constrained Project Scheduling Problem (RCPSP) is a classical scheduling problem that has received significant attention due to of its numerous applications in industry. However, in practice, task durations are subject to uncertainty that must be considered in order to propose resilient scheduling. In this paper, we address the RCPSP variant with uncertain tasks duration (modeled using known probabilities) and aim to minimize the overall expected project duration. Our objective is to produce a baseline schedule that can be reused multiple times in an industrial setting regardless of the actual duration scenario. We leverage Graph Neural Networks in conjunction with Deep Reinforcement Learning (DRL) to develop an effective policy for task scheduling. This policy operates similarly to a priority dispatch rule and is paired with a Serial Schedule Generation Scheme to produce a schedule. Our empirical evaluation on standard benchmarks demonstrates the approach's superiority in terms of performance and its ability to generalize. The developed framework, Wheatley, is made publicly available online to facilitate further research and reproducibility.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13198v1" target="_blank"><h2>ParaDySe: A Parallel-Strategy Switching Framework for Dynamic Sequence Lengths in Transformer <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zhixin Ou, Peng Liang, Jianchen Han, Baihui Liu, Linbo Qiao<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> Dynamic sequences with varying lengths have been widely used in the training of Transformer-based large language models (LLMs). However, current training frameworks adopt a pre-defined static parallel strategy for these sequences, causing neither communication-parallelization cancellation on short sequences nor out-of-memory on long sequences. To mitigate these issues, we propose ParaDySe, a novel adaptive Parallel strategy switching framework for Dynamic Sequences. ParaDySe enables on-the-fly optimal strategy adoption according to the immediate input sequence. It first implements the modular function libraries for parallel strategies with unified tensor layout specifications, and then builds sequence-aware memory and time cost models with hybrid methods. Guided by cost models, ParaDySe selects optimal layer-wise strategies for dynamic sequences via an efficient heuristic algorithm. By integrating these techniques together, ParaDySe achieves seamless hot-switching of optimal strategies through its well-designed function libraries. We compare ParaDySe with baselines on representative LLMs under datasets with sequence lengths up to 624K. Experimental results indicate that ParaDySe addresses OOM and CPC bottlenecks in LLM training by systematically integrating long-sequence optimizations with existing frameworks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13186v1" target="_blank"><h2>DiffFP: Learning Behaviors from Scratch via Diffusion-based Fictitious Play <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Akash Karthikeyan, Yash Vardhan Pant<br><strong><u>Categories:</u></strong> cs.LG, eess.SY<br><strong><u>Comments:</u></strong> Initial results presented at the IJCAI 2025 Workshop on User-Aligned Assessment of Adaptive AI Systems. Project page:this https URL<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Self-play reinforcement learning has demonstrated significant success in learning complex strategic and interactive behaviors in competitive multi-agent games. However, achieving such behaviors in continuous decision spaces remains challenging. Ensuring adaptability and generalization in self-play settings is critical for achieving competitive performance in dynamic multi-agent environments. These challenges often cause methods to converge slowly or fail to converge at all to a Nash equilibrium, making agents vulnerable to strategic exploitation by unseen opponents. To address these challenges, we propose DiffFP, a fictitious play (FP) framework that estimates the best response to unseen opponents while learning a robust and multimodal behavioral policy. Specifically, we approximate the best response using a diffusion policy that leverages generative modeling to learn adaptive and diverse strategies. Through empirical evaluation, we demonstrate that the proposed FP framework converges towards $ε$-Nash equilibria in continuous- space zero-sum games. We validate our method on complex multi-agent environments, including racing and multi-particle zero-sum games. Simulation results show that the learned policies are robust against diverse opponents and outperform baseline reinforcement learning policies. Our approach achieves up to 3$\times$ faster convergence and 30$\times$ higher success rates on average against RL-based baselines, demonstrating its robustness to opponent strategies and stability across training iterations</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13185v1" target="_blank"><h2>Uncertainty-aware Physics-informed Neural Networks for Robust CARS-to-Raman Signal Reconstruction <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Aishwarya Venkataramanan, Sai Karthikeya Vemuri, Adithya Ashok Chalain Valapil, Joachim Denzler<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> EurIPS DiffSys workshop 2025<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (title)<br><p><strong><u>Abstract:</u></strong> Coherent anti-Stokes Raman scattering (CARS) spectroscopy is a powerful and rapid technique widely used in medicine, material science, and chemical analyses. However, its effectiveness is hindered by the presence of a non-resonant background that interferes with and distorts the true Raman signal. Deep learning methods have been employed to reconstruct the true Raman spectrum from measured CARS data using labeled datasets. A more recent development integrates the domain knowledge of Kramers-Kronig relationships and smoothness constraints in the form of physics-informed loss functions. However, these deterministic models lack the ability to quantify uncertainty, an essential feature for reliable deployment in high-stakes scientific and biomedical applications. In this work, we evaluate and compare various uncertainty quantification (UQ) techniques within the context of CARS-to-Raman signal reconstruction. Furthermore, we demonstrate that incorporating physics-informed constraints into these models improves their calibration, offering a promising path toward more trustworthy CARS data analysis.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13178v1" target="_blank"><h2>Real-time distortion prediction in metallic additive manufacturing via a physics-informed neural operator approach <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Mingxuan Tian, Haochen Mu, Donghong Ding, Mengjiao Li, Yuhan Ding, Jianping Zhao<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> With the development of digital twins and smart manufacturing systems, there is an urgent need for real-time distortion field prediction to control defects in metal Additive Manufacturing (AM). However, numerical simulation methods suffer from high computational cost, long run-times that prevent real-time use, while conventional Machine learning (ML) models struggle to extract spatiotemporal features for long-horizon prediction and fail to decouple thermo-mechanical fields. This paper proposes a Physics-informed Neural Operator (PINO) to predict z and y-direction distortion for the future 15 s. Our method, Physics-informed Deep Operator Network-Recurrent Neural Network (PIDeepONet-RNN) employs trunk and branch network to process temperature history and encode distortion fields, respectively, enabling decoupling of thermo-mechanical responses. By incorporating the heat conduction equation as a soft constraint, the model ensures physical consistency and suppresses unphysical artifacts, thereby establishing a more physically consistent mapping between the thermal history and distortion. This is important because such a basis function, grounded in physical laws, provides a robust and interpretable foundation for predictions. The proposed models are trained and tested using datasets generated from experimentally validated Finite Element Method (FEM). Evaluation shows that the model achieves high accuracy, low error accumulation, time efficiency. The max absolute errors in the z and y-directions are as low as 0.9733 mm and 0.2049 mm, respectively. The error distribution shows high errors in the molten pool but low gradient norms in the deposited and key areas. The performance of PINO surrogate model highlights its potential for real-time long-horizon physics field prediction in controlling defects.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13174v1" target="_blank"><h2>Warm-starting active-set solvers using graph neural networks <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Ella J. Schmidtobreick, Daniel Arnström, Paul Häusner, Jens Sjölund<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> Under review, 15 pages, 8 figures<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> Quadratic programming (QP) solvers are widely used in real-time control and optimization, but their computational cost often limits applicability in time-critical settings. We propose a learning-to-optimize approach using graph neural networks (GNNs) to predict active sets in the dual active-set solver DAQP. The method exploits the structural properties of QPs by representing them as bipartite graphs and learning to identify the optimal active set for efficiently warm-starting the solver. Across varying problem sizes, the GNN consistently reduces the number of solver iterations compared to cold-starting, while performance is comparable to a multilayer perceptron (MLP) baseline. Furthermore, a GNN trained on varying problem sizes generalizes effectively to unseen dimensions, demonstrating flexibility and scalability. These results highlight the potential of structure-aware learning to accelerate optimization in real-time applications such as model predictive control.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13168v1" target="_blank"><h2>SOMA: Feature Gradient Enhanced Affine-Flow Matching for SAR-Optical Registration <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Haodong Wang, Tao Zhuo, Xiuwei Zhang, Hanlin Yin, Wencong Wu, Yanning Zhang<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Achieving pixel-level registration between SAR and optical images remains a challenging task due to their fundamentally different imaging mechanisms and visual characteristics. Although deep learning has achieved great success in many cross-modal tasks, its performance on SAR-Optical registration tasks is still unsatisfactory. Gradient-based information has traditionally played a crucial role in handcrafted descriptors by highlighting structural differences. However, such gradient cues have not been effectively leveraged in deep learning frameworks for SAR-Optical image matching. To address this gap, we propose SOMA, a dense registration framework that integrates structural gradient priors into deep features and refines alignment through a hybrid matching strategy. Specifically, we introduce the Feature Gradient Enhancer (FGE), which embeds multi-scale, multi-directional gradient filters into the feature space using attention and reconstruction mechanisms to boost feature distinctiveness. Furthermore, we propose the Global-Local Affine-Flow Matcher (GLAM), which combines affine transformation and flow-based refinement within a coarse-to-fine architecture to ensure both structural consistency and local accuracy. Experimental results demonstrate that SOMA significantly improves registration precision, increasing the CMR@1px by 12.29% on the SEN1-2 dataset and 18.50% on the GFGE_SO dataset. In addition, SOMA exhibits strong robustness and generalizes well across diverse scenes and resolutions.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13160v1" target="_blank"><h2>InteractiveGNNExplainer: A Visual Analytics Framework for Multi-Faceted Understanding and Probing of Graph Neural Network Predictions <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> TC Singh, Sougata Mukherjea<br><strong><u>Categories:</u></strong> cs.AI, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> explainability (abstract), neural network (title, abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Graph Neural Networks (GNNs) excel in graph-based learning tasks, but their complex, non-linear operations often render them as opaque "black boxes". This opacity hinders user trust, complicates debugging, bias detection, and adoption in critical domains requiring explainability. This paper introduces InteractiveGNNExplainer, a visual analytics framework to enhance GNN explainability, focusing on node classification. Our system uniquely integrates coordinated interactive views (dynamic graph layouts, embedding projections, feature inspection, neighborhood analysis) with established post-hoc (GNNExplainer) and intrinsic (GAT attention) explanation techniques. Crucially, it incorporates interactive graph editing, allowing users to perform a "what-if" analysis by perturbing graph structures and observing immediate impacts on GNN predictions and explanations. We detail the system architecture and, through case studies on Cora and CiteSeer datasets, demonstrate how InteractiveGNNExplainer facilitates in-depth misclassification diagnosis, comparative analysis of GCN versus GAT behaviors, and rigorous probing of model sensitivity. These capabilities foster a deeper, multifaceted understanding of GNN predictions, contributing to more transparent, trustworthy, and robust graph analysis.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13154v1" target="_blank"><h2>Towards an anomaly detection pipeline for gravitational waves at the Einstein Telescope <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Gianluca Inguglia, Huw Haigh, Kristyna Vitulova, Ulyana Dupletsa<br><strong><u>Categories:</u></strong> gr-qc, astro-ph.IM<br><strong><u>Comments:</u></strong> 10 pages, 12 figures<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), anomaly detection (title, abstract)<br><p><strong><u>Abstract:</u></strong> We present the implementation of an anomaly-detection algorithm based on a deep convolutional autoencoder for the search for gravitational waves (GWs) in time-frequency spectrograms. Our method targets short-duration ($\lesssim 2\,\text{s}$) GW signals, exemplified by mergers of compact objects forming or involving an intermediate-mass black hole (IMBH). Such short signals are difficult to distinguish from background noise; yet their brevity makes them well-suited to machine-learning analyses with modest computational requirements. Using the data from the Einstein Telescope Mock Data Challenge as a benchmark, we demonstrate that the approach can successfully flag GW-like transients as anomalies in interferometer data of a single detector, achieving an initial detection efficiency of 23% for injected signals corresponding to IMBH-forming mergers. After introducing weak supervision, the model exhibits excellent generalisation and recovers all injected IMBH-forming mergers, independent of their total mass or signal-to-noise ratio, with a false-alarm rate due to statistical noise fluctuations of approximately 4.5 events per year for a single interferometer operating with a 100% duty cycle. The method also successfully identifies lower-mass mergers leading to the formation of black holes with mass larger than $\simeq 20\,M_\odot$. Our pipeline does not yet classify anomalies, distinguishing between actual GW signals and noise artefacts; however, it highlights any deviation from the learned background noise distribution for further scrutiny. These results demonstrate that anomaly detection offers a powerful, model-independent framework for future GW searches, paving the way toward fully automated and adaptive analysis pipelines.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13145v1" target="_blank"><h2>Automated Road Distress Detection Using Vision Transformersand Generative Adversarial Networks <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Cesar Portocarrero Rodriguez, Laura Vandeweyen, Yosuke Yamamoto<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> convolutional (abstract), neural network (abstract), transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> The American Society of Civil Engineers has graded Americas infrastructure condition as a C, with the road system receiving a dismal D. Roads are vital to regional economic viability, yet their management, maintenance, and repair processes remain inefficient, relying on outdated manual or laser-based inspection methods that are both costly and time-consuming. With the increasing availability of real-time visual data from autonomous vehicles, there is an opportunity to apply computer vision (CV) methods for advanced road monitoring, providing insights to guide infrastructure rehabilitation efforts. This project explores the use of state-of-the-art CV techniques for road distress segmentation. It begins by evaluating synthetic data generated with Generative Adversarial Networks (GANs) to assess its usefulness for model training. The study then applies Convolutional Neural Networks (CNNs) for road distress segmentation and subsequently examines the transformer-based model MaskFormer. Results show that GAN-generated data improves model performance and that MaskFormer outperforms the CNN model in two metrics: mAP50 and IoU.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13143v1" target="_blank"><h2>SoK: The Last Line of Defense: On Backdoor Defense Evaluation <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Gorka Abad, Marina Krček, Stefanos Koffas, Behrad Tajalli, Marco Arazzi, Roberto Riaño, Xiaoyun Xu, Zhuoran Liu, Antonino Nocera, Stjepan Picek<br><strong><u>Categories:</u></strong> cs.CR, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> literature review (abstract)<br><p><strong><u>Abstract:</u></strong> Backdoor attacks pose a significant threat to deep learning models by implanting hidden vulnerabilities that can be activated by malicious inputs. While numerous defenses have been proposed to mitigate these attacks, the heterogeneous landscape of evaluation methodologies hinders fair comparison between defenses. This work presents a systematic (meta-)analysis of backdoor defenses through a comprehensive literature review and empirical evaluation. We analyzed 183 backdoor defense papers published between 2018 and 2025 across major AI and security venues, examining the properties and evaluation methodologies of these defenses.
  Our analysis reveals significant inconsistencies in experimental setups, evaluation metrics, and threat model assumptions in the literature. Through extensive experiments involving three datasets (MNIST, CIFAR-100, ImageNet-1K), four model architectures (ResNet-18, VGG-19, ViT-B/16, DenseNet-121), 16 representative defenses, and five commonly used attacks, totaling over 3\,000 experiments, we demonstrate that defense effectiveness varies substantially across different evaluation setups. We identify critical gaps in current evaluation practices, including insufficient reporting of computational overhead and behavior under benign conditions, bias in hyperparameter selection, and incomplete experimentation. Based on our findings, we provide concrete challenges and well-motivated recommendations to standardize and improve future defense evaluations. Our work aims to equip researchers and industry practitioners with actionable insights for developing, assessing, and deploying defenses to different systems.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13137v1" target="_blank"><h2>Conditional Diffusion Model for Multi-Agent Dynamic Task Decomposition <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yanda Zhu, Yuanyang Zhu, Daoyi Dong, Caihua Chen, Chunlin Chen<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> AAAI 2026<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Task decomposition has shown promise in complex cooperative multi-agent reinforcement learning (MARL) tasks, which enables efficient hierarchical learning for long-horizon tasks in dynamic and uncertain environments. However, learning dynamic task decomposition from scratch generally requires a large number of training samples, especially exploring the large joint action space under partial observability. In this paper, we present the Conditional Diffusion Model for Dynamic Task Decomposition (C$\text{D}^\text{3}$T), a novel two-level hierarchical MARL framework designed to automatically infer subtask and coordination patterns. The high-level policy learns subtask representation to generate a subtask selection strategy based on subtask effects. To capture the effects of subtasks on the environment, C$\text{D}^\text{3}$T predicts the next observation and reward using a conditional diffusion model. At the low level, agents collaboratively learn and share specialized skills within their assigned subtasks. Moreover, the learned subtask representation is also used as additional semantic information in a multi-head attention mixing network to enhance value decomposition and provide an efficient reasoning bridge between individual and joint value functions. Experimental results on various benchmarks demonstrate that C$\text{D}^\text{3}$T achieves better performance than existing baselines.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13133v1" target="_blank"><h2>Soft Conflict-Resolution Decision Transformer for Offline Multi-Task Reinforcement Learning <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Shudong Wang, Xinfei Wang, Chenhao Zhang, Shanchen Pang, Haiyuan Gui, Wenhao Ji, Xiaojian Liao<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> transformer (title)<br><p><strong><u>Abstract:</u></strong> Multi-task reinforcement learning (MTRL) seeks to learn a unified policy for diverse tasks, but often suffers from gradient conflicts across tasks. Existing masking-based methods attempt to mitigate such conflicts by assigning task-specific parameter masks. However, our empirical study shows that coarse-grained binary masks have the problem of over-suppressing key conflicting parameters, hindering knowledge sharing across tasks. Moreover, different tasks exhibit varying conflict levels, yet existing methods use a one-size-fits-all fixed sparsity strategy to keep training stability and performance, which proves inadequate. These limitations hinder the model's generalization and learning efficiency.
  To address these issues, we propose SoCo-DT, a Soft Conflict-resolution method based by parameter importance. By leveraging Fisher information, mask values are dynamically adjusted to retain important parameters while suppressing conflicting ones. In addition, we introduce a dynamic sparsity adjustment strategy based on the Interquartile Range (IQR), which constructs task-specific thresholding schemes using the distribution of conflict and harmony scores during training. To enable adaptive sparsity evolution throughout training, we further incorporate an asymmetric cosine annealing schedule to continuously update the threshold. Experimental results on the Meta-World benchmark show that SoCo-DT outperforms the state-of-the-art method by 7.6% on MT50 and by 10.5% on the suboptimal dataset, demonstrating its effectiveness in mitigating gradient conflicts and improving overall multi-task performance.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13800v1" target="_blank"><h2>Synergizing Multigrid Algorithms with Vision Transformer: A Novel Approach to Enhance the Seismic Foundation Model <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Huiwen Wu, Shuo Zhang, Yi Liu, Hongbin Ye<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, math.NA<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> Due to the emergency and homogenization of Artificial Intelligence (AI) technology development, transformer-based foundation models have revolutionized scientific applications, such as drug discovery, materials research, and astronomy. However, seismic data presents unique characteristics that require specialized processing techniques for pretraining foundation models in seismic contexts with high- and low-frequency features playing crucial roles. Existing vision transformers (ViTs) with sequential tokenization ignore the intrinsic pattern and fail to grasp both the high- and low-frequency seismic information efficiently and effectively. This work introduces a novel adaptive two-grid foundation model training strategy (ADATG) with Hilbert encoding specifically tailored for seismogram data, leveraging the hierarchical structures inherent in seismic data. Specifically, our approach employs spectrum decomposition to separate high- and low-frequency components and utilizes hierarchical Hilbert encoding to represent the data effectively. Moreover, observing the frequency principle observed in ViTs, we propose an adaptive training strategy that initially emphasizes coarse-level information and then progressively refines the model's focus on fine-level features. Our extensive experiments demonstrate the effectiveness and efficiency of our training methods. This research highlights the importance of data encoding and training strategies informed by the distinct characteristics of high- and low-frequency features in seismic images, ultimately contributing to the enhancement of visual seismic foundation models pretraining.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13131v1" target="_blank"><h2>MM-Telco: Benchmarks and Multimodal Large Language Models for Telecom Applications <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Gagan Raj Gupta, Anshul Kumar, Manish Rai, Apu Chakraborty, Ashutosh Modi, Abdelaali Chaoub, Soumajit Pramanik, Moyank Giri, Yashwanth Holla, Sunny Kumar, M. V. Kiran Sooraj<br><strong><u>Categories:</u></strong> cs.AI, cs.CV, cs.ET, cs.NI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Large Language Models (LLMs) have emerged as powerful tools for automating complex reasoning and decision-making tasks. In telecommunications, they hold the potential to transform network optimization, automate troubleshooting, enhance customer support, and ensure regulatory compliance. However, their deployment in telecom is hindered by domain-specific challenges that demand specialized adaptation. To overcome these challenges and to accelerate the adaptation of LLMs for telecom, we propose MM-Telco, a comprehensive suite of multimodal benchmarks and models tailored for the telecom domain. The benchmark introduces various tasks (both text based and image based) that address various practical real-life use cases such as network operations, network management, improving documentation quality, and retrieval of relevant text and images. Further, we perform baseline experiments with various LLMs and VLMs. The models fine-tuned on our dataset exhibit a significant boost in performance. Our experiments also help analyze the weak areas in the working of current state-of-art multimodal LLMs, thus guiding towards further development and research.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13125v1" target="_blank"><h2>Region-Point Joint Representation for Effective Trajectory Similarity Learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Hao Long, Silin Zhou, Lisi Chen, Shuo Shang<br><strong><u>Categories:</u></strong> cs.CV, cs.IR, cs.LG<br><strong><u>Comments:</u></strong> This paper is accepted by AAAI2026<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> attention (abstract)<br><p><strong><u>Abstract:</u></strong> Recent learning-based methods have reduced the computational complexity of traditional trajectory similarity computation, but state-of-the-art (SOTA) methods still fail to leverage the comprehensive spectrum of trajectory information for similarity modeling. To tackle this problem, we propose \textbf{RePo}, a novel method that jointly encodes \textbf{Re}gion-wise and \textbf{Po}int-wise features to capture both spatial context and fine-grained moving patterns. For region-wise representation, the GPS trajectories are first mapped to grid sequences, and spatial context are captured by structural features and semantic context enriched by visual features. For point-wise representation, three lightweight expert networks extract local, correlation, and continuous movement patterns from dense GPS sequences. Then, a router network adaptively fuses the learned point-wise features, which are subsequently combined with region-wise features using cross-attention to produce the final trajectory embedding. To train RePo, we adopt a contrastive loss with hard negative samples to provide similarity ranking supervision. Experiment results show that RePo achieves an average accuracy improvement of 22.2\% over SOTA baselines across all evaluation metrics.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13103v1" target="_blank"><h2>Transformer-Based Scalable Multi-Agent Reinforcement Learning for Networked Systems with Long-Range Interactions <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Vidur Sinha, Muhammed Ustaomeroglu, Guannan Qu<br><strong><u>Categories:</u></strong> cs.LG, cs.MA, eess.SY<br><strong><u>Comments:</u></strong> 8 pages, 7 figures, submitted for review<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> Multi-agent reinforcement learning (MARL) has shown promise for large-scale network control, yet existing methods face two major limitations. First, they typically rely on assumptions leading to decay properties of local agent interactions, limiting their ability to capture long-range dependencies such as cascading power failures or epidemic outbreaks. Second, most approaches lack generalizability across network topologies, requiring retraining when applied to new graphs. We introduce STACCA (Shared Transformer Actor-Critic with Counterfactual Advantage), a unified transformer-based MARL framework that addresses both challenges. STACCA employs a centralized Graph Transformer Critic to model long-range dependencies and provide system-level feedback, while its shared Graph Transformer Actor learns a generalizable policy capable of adapting across diverse network structures. Further, to improve credit assignment during training, STACCA integrates a novel counterfactual advantage estimator that is compatible with state-value critic estimates. We evaluate STACCA on epidemic containment and rumor-spreading network control tasks, demonstrating improved performance, network generalization, and scalability. These results highlight the potential of transformer-based MARL architectures to achieve scalable and generalizable control in large-scale networked systems.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13082v1" target="_blank"><h2>Real-time prediction of breast cancer sites using deformation-aware graph neural network <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Kyunghyun Lee, Yong-Min Shin, Minwoo Shin, Jihun Kim, Sunghwan Lim, Won-Yong Shin, Kyungho Yoon<br><strong><u>Categories:</u></strong> cs.LG, cs.CV<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (title, abstract)<br><p><strong><u>Abstract:</u></strong> Early diagnosis of breast cancer is crucial, enabling the establishment of appropriate treatment plans and markedly enhancing patient prognosis. While direct magnetic resonance imaging-guided biopsy demonstrates promising performance in detecting cancer lesions, its practical application is limited by prolonged procedure times and high costs. To overcome these issues, an indirect MRI-guided biopsy that allows the procedure to be performed outside of the MRI room has been proposed, but it still faces challenges in creating an accurate real-time deformable breast model. In our study, we tackled this issue by developing a graph neural network (GNN)-based model capable of accurately predicting deformed breast cancer sites in real time during biopsy procedures. An individual-specific finite element (FE) model was developed by incorporating magnetic resonance (MR) image-derived structural information of the breast and tumor to simulate deformation behaviors. A GNN model was then employed, designed to process surface displacement and distance-based graph data, enabling accurate prediction of overall tissue displacement, including the deformation of the tumor region. The model was validated using phantom and real patient datasets, achieving an accuracy within 0.2 millimeters (mm) for cancer node displacement (RMSE) and a dice similarity coefficient (DSC) of 0.977 for spatial overlap with actual cancerous regions. Additionally, the model enabled real-time inference and achieved a speed-up of over 4,000 times in computational cost compared to conventional FE simulations. The proposed deformation-aware GNN model offers a promising solution for real-time tumor displacement prediction in breast biopsy, with high accuracy and real-time capability. Its integration with clinical procedures could significantly enhance the precision and efficiency of breast cancer diagnosis.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13078v1" target="_blank"><h2>A Smart-Glasses for Emergency Medical Services via Multimodal Multitask Learning <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Liuyi Jin, Pasan Gunawardena, Amran Haroon, Runzhi Wang, Sangwoo Lee, Radu Stoleru, Michael Middleton, Zepeng Huo, Jeeeun Kim, Jason Moats<br><strong><u>Categories:</u></strong> cs.LG, eess.AS, eess.IV<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Emergency Medical Technicians (EMTs) operate in high-pressure environments, making rapid, life-critical decisions under heavy cognitive and operational loads. We present EMSGlass, a smart-glasses system powered by EMSNet, the first multimodal multitask model for Emergency Medical Services (EMS), and EMSServe, a low-latency multimodal serving framework tailored to EMS scenarios. EMSNet integrates text, vital signs, and scene images to construct a unified real-time understanding of EMS incidents. Trained on real-world multimodal EMS datasets, EMSNet simultaneously supports up to five critical EMS tasks with superior accuracy compared to state-of-the-art unimodal baselines. Built on top of PyTorch, EMSServe introduces a modality-aware model splitter and a feature caching mechanism, achieving adaptive and efficient inference across heterogeneous hardware while addressing the challenge of asynchronous modality arrival in the field. By optimizing multimodal inference execution in EMS scenarios, EMSServe achieves 1.9x -- 11.7x speedup over direct PyTorch multimodal inference. A user study evaluation with six professional EMTs demonstrates that EMSGlass enhances real-time situational awareness, decision-making speed, and operational efficiency through intuitive on-glass interaction. In addition, qualitative insights from the user study provide actionable directions for extending EMSGlass toward next-generation AI-enabled EMS systems, bridging multimodal intelligence with real-world emergency response workflows.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13798v1" target="_blank"><h2>KANGURA: Kolmogorov-Arnold Network-Based Geometry-Aware Learning with Unified Representation Attention for 3D Modeling of Complex Structures <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Mohammad Reza Shafie, Morteza Hajiabadi, Hamed Khosravi, Mobina Noori, Imtiaz Ahmed<br><strong><u>Categories:</u></strong> cs.AI, cs.CV, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Microbial Fuel Cells (MFCs) offer a promising pathway for sustainable energy generation by converting organic matter into electricity through microbial processes. A key factor influencing MFC performance is the anode structure, where design and material properties play a crucial role. Existing predictive models struggle to capture the complex geometric dependencies necessary to optimize these structures. To solve this problem, we propose KANGURA: Kolmogorov-Arnold Network-Based Geometry-Aware Learning with Unified Representation Attention. KANGURA introduces a new approach to three-dimensional (3D) machine learning modeling. It formulates prediction as a function decomposition problem, where Kolmogorov-Arnold Network (KAN)- based representation learning reconstructs geometric relationships without a conventional multi- layer perceptron (MLP). To refine spatial understanding, geometry-disentangled representation learning separates structural variations into interpretable components, while unified attention mechanisms dynamically enhance critical geometric regions. Experimental results demonstrate that KANGURA outperforms over 15 state-of-the-art (SOTA) models on the ModelNet40 benchmark dataset, achieving 92.7% accuracy, and excels in a real-world MFC anode structure problem with 97% accuracy. This establishes KANGURA as a robust framework for 3D geometric modeling, unlocking new possibilities for optimizing complex structures in advanced manufacturing and quality-driven engineering applications.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13071v1" target="_blank"><h2>Orientation-Free Neural Network-Based Bias Estimation for Low-Cost Stationary Accelerometers <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Michal Levin, Itzik Klein<br><strong><u>Categories:</u></strong> cs.RO, cs.LG<br><strong><u>Comments:</u></strong> 22 pages, 10 figures<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (title)<br><p><strong><u>Abstract:</u></strong> Low-cost micro-electromechanical accelerometers are widely used in navigation, robotics, and consumer devices for motion sensing and position estimation. However, their performance is often degraded by bias errors. To eliminate deterministic bias terms a calibration procedure is applied under stationary conditions. It requires accelerom- eter leveling or complex orientation-dependent calibration procedures. To overcome those requirements, in this paper we present a model-free learning-based calibration method that estimates accelerometer bias under stationary conditions, without requiring knowledge of the sensor orientation and without the need to rotate the sensors. The proposed approach provides a fast, practical, and scalable solution suitable for rapid field deployment. Experimental validation on a 13.39-hour dataset collected from six accelerometers shows that the proposed method consistently achieves error levels more than 52% lower than traditional techniques. On a broader scale, this work contributes to the advancement of accurate calibration methods in orientation-free scenarios. As a consequence, it improves the reliability of low-cost inertial sensors in diverse scientific and industrial applications and eliminates the need for leveled calibration.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13062v1" target="_blank"><h2>Self-Adaptive Graph Mixture of Models <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Mohit Meena, Yash Punjabi, Abhishek A, Vishal Sharma, Mahesh Chandran<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 17 pages, 5 figures<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Graph Neural Networks (GNNs) have emerged as powerful tools for learning over graph-structured data, yet recent studies have shown that their performance gains are beginning to plateau. In many cases, well-established models such as GCN and GAT, when appropriately tuned, can match or even exceed the performance of more complex, state-of-the-art architectures. This trend highlights a key limitation in the current landscape: the difficulty of selecting the most suitable model for a given graph task or dataset. To address this, we propose Self-Adaptive Graph Mixture of Models (SAGMM), a modular and practical framework that learns to automatically select and combine the most appropriate GNN models from a diverse pool of architectures. Unlike prior mixture-of-experts approaches that rely on variations of a single base model, SAGMM leverages architectural diversity and a topology-aware attention gating mechanism to adaptively assign experts to each node based on the structure of the input graph. To improve efficiency, SAGMM includes a pruning mechanism that reduces the number of active experts during training and inference without compromising performance. We also explore a training-efficient variant in which expert models are pretrained and frozen, and only the gating and task-specific layers are trained. We evaluate SAGMM on 16 benchmark datasets covering node classification, graph classification, regression, and link prediction tasks, and demonstrate that it consistently outperforms or matches leading GNN baselines and prior mixture-based methods, offering a robust and adaptive solution for real-world graph learning.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13797v1" target="_blank"><h2>MAT-MPNN: A Mobility-Aware Transformer-MPNN Model for Dynamic Spatiotemporal Prediction of HIV Diagnoses in California, Florida, and New England <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zhaoxuan Wang, Weichen Kang, Yutian Han, Lingyuan Zhao, Bo Li<br><strong><u>Categories:</u></strong> q-bio.QM, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> 21 pages, 20 figures,1 table. Preprint<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> Human Immunodeficiency Virus (HIV) has posed a major global health challenge for decades, and forecasting HIV diagnoses continues to be a critical area of research. However, capturing the complex spatial and temporal dependencies of HIV transmission remains challenging. Conventional Message Passing Neural Network (MPNN) models rely on a fixed binary adjacency matrix that only encodes geographic adjacency, which is unable to represent interactions between non-contiguous counties. Our study proposes a deep learning architecture Mobility-Aware Transformer-Message Passing Neural Network (MAT-MPNN) framework to predict county-level HIV diagnosis rates across California, Florida, and the New England region. The model combines temporal features extracted by a Transformer encoder with spatial relationships captured through a Mobility Graph Generator (MGG). The MGG improves conventional adjacency matrices by combining geographic and demographic information. Compared with the best-performing hybrid baseline, the Transformer MPNN model, MAT-MPNN reduced the Mean Squared Prediction Error (MSPE) by 27.9% in Florida, 39.1% in California, and 12.5% in New England, and improved the Predictive Model Choice Criterion (PMCC) by 7.7%, 3.5%, and 3.9%, respectively. MAT-MPNN also achieved better results than the Spatially Varying Auto-Regressive (SVAR) model in Florida and New England, with comparable performance in California. These results demonstrate that applying mobility-aware dynamic spatial structures substantially enhances predictive accuracy and calibration in spatiotemporal epidemiological prediction.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13057v2" target="_blank"><h2>Dimension vs. Precision: A Comparative Analysis of Autoencoders and Quantization for Efficient Vector Retrieval on BEIR SciFact <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Satyanarayan Pati<br><strong><u>Categories:</u></strong> cs.IR, cs.AI<br><strong><u>Comments:</u></strong> 16 pages, 9 figures, 1 table<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> dimensionality reduction (abstract), latent space (abstract)<br><p><strong><u>Abstract:</u></strong> Dense retrieval models have become a standard for state-of-the-art information retrieval. However, their high-dimensional, high-precision (float32) vector embeddings create significant storage and memory challenges for real-world deployment. To address this, we conduct a rigorous empirical study on the BEIR SciFact benchmark, evaluating the trade-offs between two primary compression strategies: (1) Dimensionality Reduction via deep Autoencoders (AE), reducing original 384-dim vectors to latent spaces from 384 down to 12, and (2) Precision Reduction via Quantization (float16, int8, and binary). We systematically compare each method by measuring the "performance loss" (or gain) relative to a float32 baseline across a full suite of retrieval metrics (NDCG, MAP, MRR, Recall, Precision) at various k cutoffs. Our results show that int8 scalar quantization provides the most effective "sweet spot," achieving a 4x compression with a negligible [~1-2%] drop in nDCG@10. In contrast, Autoencoders show a graceful degradation but suffer a more significant performance loss at equivalent 4x compression ratios (AE-96). binary quantization was found to be unsuitable for this task due to catastrophic performance drops. This work provides a practical guide for deploying efficient, high-performance retrieval systems.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13052v1" target="_blank"><h2>Learning from the Undesirable: Robust Adaptation of Language Models without Forgetting <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yunhun Nam, Jaehyung Kim, Jongheon Jeong<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> 17 pages; AAAI 2026; Code is available atthis https URL<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> data augmentation (abstract)<br><p><strong><u>Abstract:</u></strong> Language models (LMs) are often adapted through supervised fine-tuning (SFT) to specialize their capabilities for downstream tasks. However, in typical scenarios where the fine-tuning data is limited, e.g., compared to pre-training, SFT can lead LMs to overfit, causing them to rely on spurious patterns within the target task or to compromise other broadly useful capabilities as a side effect of narrow specialization. In this paper, we propose Learning-from-the-Undesirable (LfU), a simple yet effective regularization scheme for SFT to mitigate overfitting issues when fine-tuning LMs with limited data. Specifically, we aim to regularize the fine-tuning process to favor solutions that are resilient to "undesirable" model updates, e.g., gradient ascent steps that steer the model toward undesirable behaviors. To this end, we propose a novel form of consistency regularization that directly aligns internal representations of the model with those after an undesirable update. By leveraging representation-level data augmentation through undesirable updates, LfU effectively promotes generalization under limited data. Our experiments on diverse LM downstream tasks show that LfU serves as an effective prior that enhances adaptability while preserving pretrained knowledge. For example, our LM from LfU achieves a 16.8% average improvement on math tasks compared to vanilla SFT on the same dataset, where the latter even leads to degraded performance on those tasks. Furthermore, LfU exhibits improved robustness to prompt variations, e.g., yielding a 92.1% lower standard deviation in output performances compared to SFT, highlighting its versatile effects.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13035v1" target="_blank"><h2>One-Step Generative Policies with Q-Learning: A Reformulation of MeanFlow <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zeyuan Wang, Da Li, Yulin Chen, Ye Shi, Liang Bai, Tianyuan Yu, Yanwei Fu<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> Accepted in AAAI 2026 Poster<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> We introduce a one-step generative policy for offline reinforcement learning that maps noise directly to actions via a residual reformulation of MeanFlow, making it compatible with Q-learning. While one-step Gaussian policies enable fast inference, they struggle to capture complex, multimodal action distributions. Existing flow-based methods improve expressivity but typically rely on distillation and two-stage training when trained with Q-learning. To overcome these limitations, we propose to reformulate MeanFlow to enable direct noise-to-action generation by integrating the velocity field and noise-to-action transformation into a single policy network-eliminating the need for separate velocity estimation. We explore several reformulation variants and identify an effective residual formulation that supports expressive and stable policy learning. Our method offers three key advantages: 1) efficient one-step noise-to-action generation, 2) expressive modelling of multimodal action distributions, and 3) efficient and stable policy learning via Q-learning in a single-stage training setup. Extensive experiments on 73 tasks across the OGBench and D4RL benchmarks demonstrate that our method achieves strong performance in both offline and offline-to-online reinforcement learning settings. Code is available at https://github.com/HiccupRL/MeanFlowQL.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13021v1" target="_blank"><h2>PragWorld: A Benchmark Evaluating LLMs' Local World Model under Minimal Linguistic Alterations and Conversational Dynamics <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Sachin Vashistha, Aryan Bibhuti, Atharva Naik, Martin Tutek, Somak Aditya<br><strong><u>Categories:</u></strong> cs.AI, cs.CL<br><strong><u>Comments:</u></strong> 23 pages, 15 tables, 10 figures; AAAI 2026 Conference Main Track (oral)<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Real-world conversations are rich with pragmatic elements, such as entity mentions, references, and implicatures. Understanding such nuances is a requirement for successful natural communication, and often requires building a local world model which encodes such elements and captures the dynamics of their evolving states. However, it is not well-understood whether language models (LMs) construct or maintain a robust implicit representation of conversations. In this work, we evaluate the ability of LMs to encode and update their internal world model in dyadic conversations and test their malleability under linguistic alterations. To facilitate this, we apply seven minimal linguistic alterations to conversations sourced from popular datasets and construct two benchmarks comprising yes-no questions. We evaluate a wide range of open and closed source LMs and observe that they struggle to maintain robust accuracy. Our analysis unveils that LMs struggle to memorize crucial details, such as tracking entities under linguistic alterations to conversations. We then propose a dual-perspective interpretability framework which identifies transformer layers that are useful or harmful and highlights linguistic alterations most influenced by harmful layers, typically due to encoding spurious signals or relying on shortcuts. Inspired by these insights, we propose two layer-regularization based fine-tuning strategies that suppress the effect of the harmful layers.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13020v1" target="_blank"><h2>SpectralAdapt: Semi-Supervised Domain Adaptation with Spectral Priors for Human-Centered Hyperspectral Image Reconstruction <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yufei Wen, Yuting Zhang, Jingdan Kang, Hao Ren, Weibin Cheng, Jintai Chen, Kaishun Wu<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> domain adaptation (title, abstract)<br><p><strong><u>Abstract:</u></strong> Hyperspectral imaging (HSI) holds great potential for healthcare due to its rich spectral information. However, acquiring HSI data remains costly and technically demanding. Hyperspectral image reconstruction offers a practical solution by recovering HSI data from accessible modalities, such as RGB. While general domain datasets are abundant, the scarcity of human HSI data limits progress in medical applications. To tackle this, we propose SpectralAdapt, a semi-supervised domain adaptation (SSDA) framework that bridges the domain gap between general and human-centered HSI datasets. To fully exploit limited labels and abundant unlabeled data, we enhance spectral reasoning by introducing Spectral Density Masking (SDM), which adaptively masks RGB channels based on their spectral complexity, encouraging recovery of informative regions from complementary cues during consistency training. Furthermore, we introduce Spectral Endmember Representation Alignment (SERA), which derives physically interpretable endmembers from valuable labeled pixels and employs them as domain-invariant anchors to guide unlabeled predictions, with momentum updates ensuring adaptability and stability. These components are seamlessly integrated into SpectralAdapt, a spectral prior-guided framework that effectively mitigates domain shift, spectral degradation, and data scarcity in HSI reconstruction. Experiments on benchmark datasets demonstrate consistent improvements in spectral fidelity, cross-domain generalization, and training stability, highlighting the promise of SSDA as an efficient solution for hyperspectral imaging in healthcare.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13019v1" target="_blank"><h2>MeanFlow Transformers with Representation Autoencoders <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zheyuan Hu, Chieh-Hsin Lai, Ge Wu, Yuki Mitsufuji, Stefano Ermon<br><strong><u>Categories:</u></strong> cs.CV, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> Code is available atthis https URL<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> variational autoencoder (abstract), VAE (abstract), latent space (abstract), transformer (title)<br><p><strong><u>Abstract:</u></strong> MeanFlow (MF) is a diffusion-motivated generative model that enables efficient few-step generation by learning long jumps directly from noise to data. In practice, it is often used as a latent MF by leveraging the pre-trained Stable Diffusion variational autoencoder (SD-VAE) for high-dimensional data modeling. However, MF training remains computationally demanding and is often unstable. During inference, the SD-VAE decoder dominates the generation cost, and MF depends on complex guidance hyperparameters for class-conditional generation. In this work, we develop an efficient training and sampling scheme for MF in the latent space of a Representation Autoencoder (RAE), where a pre-trained vision encoder (e.g., DINO) provides semantically rich latents paired with a lightweight decoder. We observe that naive MF training in the RAE latent space suffers from severe gradient explosion. To stabilize and accelerate training, we adopt Consistency Mid-Training for trajectory-aware initialization and use a two-stage scheme: distillation from a pre-trained flow matching teacher to speed convergence and reduce variance, followed by an optional bootstrapping stage with a one-point velocity estimator to further reduce deviation from the oracle mean flow. This design removes the need for guidance, simplifies training configurations, and reduces computation in both training and sampling. Empirically, our method achieves a 1-step FID of 2.03, outperforming vanilla MF's 3.43, while reducing sampling GFLOPS by 38% and total training cost by 83% on ImageNet 256. We further scale our approach to ImageNet 512, achieving a competitive 1-step FID of 3.23 with the lowest GFLOPS among all baselines. Code is available at https://github.com/sony/mf-rae.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13010v1" target="_blank"><h2>Are Graph Transformers Necessary? Efficient Long-Range Message Passing with Fractal Nodes in MPNNs <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Jeongwhan Choi, Seungjun Park, Sumin Park, Sung-Bae Cho, Noseong Park<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> Accepted in AAAI 2026 for Oral Representation. This is the extended version including the appendix<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (abstract), transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> Graph Neural Networks (GNNs) have emerged as powerful tools for learning on graph-structured data, but often struggle to balance local and global information. While graph Transformers aim to address this by enabling long-range interactions, they often overlook the inherent locality and efficiency of Message Passing Neural Networks (MPNNs). We propose a new concept called fractal nodes, inspired by the fractal structure observed in real-world networks. Our approach is based on the intuition that graph partitioning naturally induces fractal structure, where subgraphs often reflect the connectivity patterns of the full graph. Fractal nodes are designed to coexist with the original nodes and adaptively aggregate subgraph-level feature representations, thereby enforcing feature similarity within each subgraph. We show that fractal nodes alleviate the over-squashing problem by providing direct shortcut connections that enable long-range propagation of subgraph-level representations. Experiment results show that our method improves the expressive power of MPNNs and achieves comparable or better performance to graph Transformers while maintaining the computational efficiency of MPNN by improving the long-range dependencies of MPNN.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13005v1" target="_blank"><h2>SAGE: Spuriousness-Aware Guided Prompt Exploration for Mitigating Multimodal Bias <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Wenqian Ye, Di Wang, Guangtao Zheng, Bohan Liu, Aidong Zhang<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> Accepted at AAAI 2026<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Large vision-language models, such as CLIP, have shown strong zero-shot classification performance by aligning images and text in a shared embedding space. However, CLIP models often develop multimodal spurious biases, which is the undesirable tendency to rely on spurious features. For example, CLIP may infer object types in images based on frequently co-occurring backgrounds rather than the object's core features. This bias significantly impairs the robustness of pre-trained CLIP models on out-of-distribution data, where such cross-modal associations no longer hold. Existing methods for mitigating multimodal spurious bias typically require fine-tuning on downstream data or prior knowledge of the bias, which undermines the out-of-the-box usability of CLIP. In this paper, we first theoretically analyze the impact of multimodal spurious bias in zero-shot classification. Based on this insight, we propose Spuriousness-Aware Guided Exploration (SAGE), a simple and effective method that mitigates spurious bias through guided prompt selection. SAGE requires no training, fine-tuning, or external annotations. It explores a space of prompt templates and selects the prompts that induce the largest semantic separation between classes, thereby improving worst-group robustness. Extensive experiments on four real-world benchmark datasets and five popular backbone models demonstrate that SAGE consistently improves zero-shot performance and generalization, outperforming previous zero-shot approaches without any external knowledge or model updates.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12997v1" target="_blank"><h2>WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Genglin Liu, Shijie Geng, Sha Li, Hejie Cui, Sarah Zhang, Xin Liu, Tianyi Liu<br><strong><u>Categories:</u></strong> cs.AI, cs.CL<br><strong><u>Comments:</u></strong> 18 pages; work in progress<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Multimodal LLM-powered agents have recently demonstrated impressive capabilities in web navigation, enabling agents to complete complex browsing tasks across diverse domains. However, current agents struggle with repetitive errors and lack the ability to learn from past experiences across sessions, limiting their long-term robustness and sample efficiency. We introduce WebCoach, a model-agnostic self-evolving framework that equips web browsing agents with persistent cross-session memory, enabling improved long-term planning, reflection, and continual learning without retraining. WebCoach consists of three key components: (1) a WebCondenser, which standardizes raw navigation logs into concise summaries; (2) an External Memory Store, which organizes complete trajectories as episodic experiences; and (3) a Coach, which retrieves relevant experiences based on similarity and recency, and decides whether to inject task-specific advice into the agent via runtime hooks. This design empowers web agents to access long-term memory beyond their native context window, improving robustness in complex browsing tasks. Moreover, WebCoach achieves self-evolution by continuously curating episodic memory from new navigation trajectories, enabling agents to improve over time without retraining. Evaluations on the WebVoyager benchmark demonstrate that WebCoach consistently improves the performance of browser-use agents across three different LLM backbones. With a 38B model, it increases task success rates from 47% to 61% while reducing or maintaining the average number of steps. Notably, smaller base models with WebCoach achieve performance comparable to the same web agent using GPT-4o.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12986v1" target="_blank"><h2>Learning Branching Policies for MILPs with Proximal Policy Optimization <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Abdelouahed Ben Mhamed, Assia Kamal-Idrissi, Amal El Fallah Seghrouchni<br><strong><u>Categories:</u></strong> cs.LG, cs.AI, math.OC<br><strong><u>Comments:</u></strong> 11 pages, 3 figures, AAAI conference<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> data-driven (abstract)<br><p><strong><u>Abstract:</u></strong> Branch-and-Bound (B\&B) is the dominant exact solution method for Mixed Integer Linear Programs (MILP), yet its exponential time complexity poses significant challenges for large-scale instances. The growing capabilities of machine learning have spurred efforts to improve B\&B by learning data-driven branching policies. However, most existing approaches rely on Imitation Learning (IL), which tends to overfit to expert demonstrations and struggles to generalize to structurally diverse or unseen instances. In this work, we propose Tree-Gate Proximal Policy Optimization (TGPPO), a novel framework that employs Proximal Policy Optimization (PPO), a Reinforcement Learning (RL) algorithm, to train a branching policy aimed at improving generalization across heterogeneous MILP instances. Our approach builds on a parameterized state space representation that dynamically captures the evolving context of the search tree. Empirical evaluations show that TGPPO often outperforms existing learning-based policies in terms of reducing the number of nodes explored and improving p-Primal-Dual Integrals (PDI), particularly in out-of-distribution instances. These results highlight the potential of RL to develop robust and adaptable branching strategies for MILP solvers.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12985v1" target="_blank"><h2>Angular Gradient Sign Method: Uncovering Vulnerabilities in Hyperbolic Networks <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Minsoo Jo, Dongyoon Yang, Taesup Kim<br><strong><u>Categories:</u></strong> cs.LG, cs.CV<br><strong><u>Comments:</u></strong> Accepted by AAAI 2026<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Adversarial examples in neural networks have been extensively studied in Euclidean geometry, but recent advances in \textit{hyperbolic networks} call for a reevaluation of attack strategies in non-Euclidean geometries. Existing methods such as FGSM and PGD apply perturbations without regard to the underlying hyperbolic structure, potentially leading to inefficient or geometrically inconsistent attacks. In this work, we propose a novel adversarial attack that explicitly leverages the geometric properties of hyperbolic space. Specifically, we compute the gradient of the loss function in the tangent space of hyperbolic space, decompose it into a radial (depth) component and an angular (semantic) component, and apply perturbation derived solely from the angular direction. Our method generates adversarial examples by focusing perturbations in semantically sensitive directions encoded in angular movement within the hyperbolic geometry. Empirical results on image classification, cross-modal retrieval tasks and network architectures demonstrate that our attack achieves higher fooling rates than conventional adversarial attacks, while producing high-impact perturbations with deeper insights into vulnerabilities of hyperbolic embeddings. This work highlights the importance of geometry-aware adversarial strategies in curved representation spaces and provides a principled framework for attacking hierarchical embeddings.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12976v1" target="_blank"><h2>MCAQ-YOLO: Morphological Complexity-Aware Quantization for Efficient Object Detection with Curriculum Learning <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yoonjae Seo, Ermal Elbasani, Jaehong Lee<br><strong><u>Categories:</u></strong> cs.CV, cs.LG<br><strong><u>Comments:</u></strong> 9 pages, 2 figures, 7 tables. Preprint<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Most neural network quantization methods apply uniform bit precision across spatial regions, ignoring the heterogeneous structural and textural complexity of visual data. This paper introduces MCAQ-YOLO, a morphological complexity-aware quantization framework for object detection. The framework employs five morphological metrics - fractal dimension, texture entropy, gradient variance, edge density, and contour complexity - to characterize local visual morphology and guide spatially adaptive bit allocation. By correlating these metrics with quantization sensitivity, MCAQ-YOLO dynamically adjusts bit precision according to spatial complexity. In addition, a curriculum-based quantization-aware training scheme progressively increases quantization difficulty to stabilize optimization and accelerate convergence. Experimental results demonstrate a strong correlation between morphological complexity and quantization sensitivity and show that MCAQ-YOLO achieves superior detection accuracy and convergence efficiency compared with uniform quantization. On a safety equipment dataset, MCAQ-YOLO attains 85.6 percent mAP@0.5 with an average of 4.2 bits and a 7.6x compression ratio, yielding 3.5 percentage points higher mAP than uniform 4-bit quantization while introducing only 1.8 ms of additional runtime overhead per image. Cross-dataset validation on COCO and Pascal VOC further confirms consistent performance gains, indicating that morphology-driven spatial quantization can enhance efficiency and robustness for computationally constrained, safety-critical visual recognition tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12971v1" target="_blank"><h2>Esim: EVM Bytecode Similarity Detection Based on Stable-Semantic Graph <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Zhuo Chen, Gaoqiang Ji, Yiling He, Lei Wu, Yajin Zhou<br><strong><u>Categories:</u></strong> cs.CR, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Decentralized finance (DeFi) is experiencing rapid expansion. However, prevalent code reuse and limited open-source contributions have introduced significant challenges to the blockchain ecosystem, including plagiarism and the propagation of vulnerable code. Consequently, an effective and accurate similarity detection method for EVM bytecode is urgently needed to identify similar contracts. Traditional binary similarity detection methods are typically based on instruction stream or control flow graph (CFG), which have limitations on EVM bytecode due to specific features like low-level EVM bytecode and heavily-reused basic blocks. Moreover, the highly-diverse Solidity Compiler (Solc) versions further complicate accurate similarity detection.
  Motivated by these challenges, we propose a novel EVM bytecode representation called Stable-Semantic Graph (SSG), which captures relationships between 'stable instructions' (special instructions identified by our study). Moreover, we implement a prototype, Esim, which embeds SSG into matrices for similarity detection using a heterogeneous graph neural network. Esim demonstrates high accuracy in SSG construction, achieving F1-scores of 100% for control flow and 95.16% for data flow, and its similarity detection performance reaches 96.3% AUC, surpassing traditional approaches. Our large-scale study, analyzing 2,675,573 smart contracts on six EVM-compatible chains over a one-year period, also demonstrates that Esim outperforms the SOTA tool Etherscan in vulnerability search.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12964v1" target="_blank"><h2>CalibrateMix: Guided-Mixup Calibration of Image Semi-Supervised Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Mehrab Mustafy Rahman, Jayanth Mohan, Tiberiu Sosea, Cornelia Caragea<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> Semi-supervised learning (SSL) has demonstrated high performance in image classification tasks by effectively utilizing both labeled and unlabeled data. However, existing SSL methods often suffer from poor calibration, with models yielding overconfident predictions that misrepresent actual prediction likelihoods. Recently, neural networks trained with {\tt mixup} that linearly interpolates random examples from the training set have shown better calibration in supervised settings. However, calibration of neural models remains under-explored in semi-supervised settings. Although effective in supervised model calibration, random mixup of pseudolabels in SSL presents challenges due to the overconfidence and unreliability of pseudolabels. In this work, we introduce CalibrateMix, a targeted mixup-based approach that aims to improve the calibration of SSL models while maintaining or even improving their classification accuracy. Our method leverages training dynamics of labeled and unlabeled samples to identify ``easy-to-learn'' and ``hard-to-learn'' samples, which in turn are utilized in a targeted mixup of easy and hard samples. Experimental results across several benchmark image datasets show that our method achieves lower expected calibration error (ECE) and superior accuracy compared to existing SSL approaches.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12955v1" target="_blank"><h2>Global Cross-Time Attention Fusion for Enhanced Solar Flare Prediction from Multivariate Time Series <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Onur Vural, Shah Muhammad Hamdi, Soukaina Filali Boubrahimi<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> This work has been accepted at the 2025 IEEE International Conference on Big Data (IEEE BigData 2025) on October 23, 2025<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> transformer (abstract), attention (title, abstract)<br><p><strong><u>Abstract:</u></strong> Multivariate time series classification is increasingly investigated in space weather research as a means to predict intense solar flare events, which can cause widespread disruptions across modern technological systems. Magnetic field measurements of solar active regions are converted into structured multivariate time series, enabling predictive modeling across segmented observation windows. However, the inherently imbalanced nature of solar flare occurrences, where intense flares are rare compared to minor flare events, presents a significant barrier to effective learning. To address this challenge, we propose a novel Global Cross-Time Attention Fusion (GCTAF) architecture, a transformer-based model to enhance long-range temporal modeling. Unlike traditional self-attention mechanisms that rely solely on local interactions within time series, GCTAF injects a set of learnable cross-attentive global tokens that summarize salient temporal patterns across the entire sequence. These tokens are refined through cross-attention with the input sequence and fused back into the temporal representation, enabling the model to identify globally significant, non-contiguous time points that are critical for flare prediction. This mechanism functions as a dynamic attention-driven temporal summarizer that augments the model's capacity to capture discriminative flare-related dynamics. We evaluate our approach on the benchmark solar flare dataset and show that GCTAF effectively detects intense flares and improves predictive performance, demonstrating that refining transformer-based architectures presents a high-potential alternative for solar flare prediction tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12951v1" target="_blank"><h2>A FEDformer-Based Hybrid Framework for Anomaly Detection and Risk Forecasting in Financial Time Series <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Ziling Fan, Ruijia Liang, Yiwen Hu<br><strong><u>Categories:</u></strong> cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> anomaly detection (title, abstract), transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Financial markets are inherently volatile and prone to sudden disruptions such as market crashes, flash collapses, and liquidity crises. Accurate anomaly detection and early risk forecasting in financial time series are therefore crucial for preventing systemic instability and supporting informed investment decisions. Traditional deep learning models, such as LSTM and GRU, often fail to capture long-term dependencies and complex periodic patterns in highly nonstationary financial data. To address this limitation, this study proposes a FEDformer-Based Hybrid Framework for Anomaly Detection and Risk Forecasting in Financial Time Series, which integrates the Frequency Enhanced Decomposed Transformer (FEDformer) with a residual-based anomaly detector and a risk forecasting head. The FEDformer module models temporal dynamics in both time and frequency domains, decomposing signals into trend and seasonal components for improved interpretability. The residual-based detector identifies abnormal fluctuations by analyzing prediction errors, while the risk head predicts potential financial distress using learned latent embeddings. Experiments conducted on the S&P 500, NASDAQ Composite, and Brent Crude Oil datasets (2000-2024) demonstrate the superiority of the proposed model over benchmark methods, achieving a 15.7 percent reduction in RMSE and an 11.5 percent improvement in F1-score for anomaly detection. These results confirm the effectiveness of the model in capturing financial volatility, enabling reliable early-warning systems for market crash prediction and risk management.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12937v1" target="_blank"><h2>Yanyun-3: Enabling Cross-Platform Strategy Game Operation with Vision-Language Models <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Guoyan Wang, Yanyan Huang, Chunlin Chen, Lifeng Wang, Yuxiang Sun<br><strong><u>Categories:</u></strong> cs.AI, cs.CV<br><strong><u>Comments:</u></strong> 32 pages, 13 figures<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Automated operation in cross-platform strategy games demands agents with robust generalization across diverse user interfaces and dynamic battlefield conditions. While vision-language models (VLMs) have shown considerable promise in multimodal reasoning, their application to complex human-computer interaction scenarios--such as strategy gaming--remains largely unexplored. Here, we introduce Yanyun-3, a general-purpose agent framework that, for the first time, enables autonomous cross-platform operation across three heterogeneous strategy game environments. By integrating the vision-language reasoning of Qwen2.5-VL with the precise execution capabilities of UI-TARS, Yanyun-3 successfully performs core tasks including target localization, combat resource allocation, and area control. Through systematic ablation studies, we evaluate the effects of various multimodal data combinations--static images, multi-image sequences, and videos--and propose the concept of combination granularity to differentiate between intra-sample fusion and inter-sample mixing strategies. We find that a hybrid strategy, which fuses multi-image and video data while mixing in static images (MV+S), substantially outperforms full fusion: it reduces inference time by 63% and boosts the BLEU-4 score by a factor of 12 (from 4.81% to 62.41%, approximately 12.98x). Operating via a closed-loop pipeline of screen capture, model inference, and action execution, the agent demonstrates strong real-time performance and cross-platform generalization. Beyond providing an efficient solution for strategy game automation, our work establishes a general paradigm for enhancing VLM performance through structured multimodal data organization, offering new insights into the interplay between static perception and dynamic reasoning in embodied intelligence.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12934v2" target="_blank"><h2>AIF: Asynchronous Inference Framework for Cost-Effective Pre-Ranking <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Zhi Kou, Xiang-Rong Sheng, Shuguang Han, Zhishan Zhao, Yueyao Cheng, Han Zhu, Jian Xu, Bo Zheng<br><strong><u>Categories:</u></strong> cs.LG, cs.IR<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> In industrial recommendation systems, pre-ranking models based on deep neural networks (DNNs) commonly adopt a sequential execution framework: feature fetching and model forward computation are triggered only after receiving candidates from the upstream retrieval stage. This design introduces inherent bottlenecks, including redundant computations of identical users/items and increased latency due to strictly sequential operations, which jointly constrain the model's capacity and system efficiency. To address these limitations, we propose the Asynchronous Inference Framework (AIF), a cost-effective computational architecture that decouples interaction-independent components, those operating within a single user or item, from real-time prediction. AIF reorganizes the model inference process by performing user-side computations in parallel with the retrieval stage and conducting item-side computations in a nearline manner. This means that interaction-independent components are calculated just once and completed before the real-time prediction phase of the pre-ranking stage. As a result, AIF enhances computational efficiency and reduces latency, freeing up resources to significantly improve the feature set and model architecture of interaction-independent components. Moreover, we delve into model design within the AIF framework, employing approximated methods for interaction-dependent components in online real-time predictions. By co-designing both the framework and the model, our solution achieves notable performance gains without significantly increasing computational and latency costs. This has enabled the successful deployment of AIF in the Taobao display advertising system.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12922v1" target="_blank"><h2>Tokenize Once, Recommend Anywhere: Unified Item Tokenization for Multi-domain LLM-based Recommendation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yu Hou, Won-Yong Shin<br><strong><u>Categories:</u></strong> cs.IR, cs.AI, cs.LG, cs.NE, cs.SI<br><strong><u>Comments:</u></strong> 20 pages, 8 figures, 9 tables; Annual AAAI Conference on Artificial Intelligence (AAAI-26) (to appear) (Please cite our conference version.)<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> latent space (abstract)<br><p><strong><u>Abstract:</u></strong> Large language model (LLM)-based recommender systems have achieved high-quality performance by bridging the discrepancy between the item space and the language space through item tokenization. However, existing item tokenization methods typically require training separate models for each item domain, limiting generalization. Moreover, the diverse distributions and semantics across item domains make it difficult to construct a unified tokenization that preserves domain-specific information. To address these challenges, we propose UniTok, a Unified item Tokenization framework that integrates our own mixture-of-experts (MoE) architecture with a series of codebooks to convert items into discrete tokens, enabling scalable tokenization while preserving semantic information across multiple item domains. Specifically, items from different domains are first projected into a unified latent space through a shared encoder. They are then routed to domain-specific experts to capture the unique semantics, while a shared expert, which is always active, encodes common knowledge transferable across domains. Additionally, to mitigate semantic imbalance across domains, we present a mutual information calibration mechanism, which guides the model towards retaining similar levels of semantic information for each domain. Comprehensive experiments on wide-ranging real-world datasets demonstrate that the proposed UniTok framework is (a) highly effective: achieving up to 51.89% improvements over strong benchmarks, (b) theoretically sound: showing the analytical validity of our architectural design and optimization; and (c) highly generalizable: demonstrating robust performance across diverse domains without requiring per-domain retraining, a capability not supported by existing baselines.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12916v1" target="_blank"><h2>Fault2Flow: An AlphaEvolve-Optimized Human-in-the-Loop Multi-Agent System for Fault-to-Workflow Automation <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Yafang Wang, Yangjie Tian, Xiaoyu Shen, Gaoyang Zhang, Jiaze Sun, He Zhang, Ruohua Xu, Feng Zhao<br><strong><u>Categories:</u></strong> cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> transformer (abstract)<br><p><strong><u>Abstract:</u></strong> Power grid fault diagnosis is a critical process hindered by its reliance on manual, error-prone methods. Technicians must manually extract reasoning logic from dense regulations and attempt to combine it with tacit expert knowledge, which is inefficient, error-prone, and lacks maintainability as ragulations are updated and experience evolves. While Large Language Models (LLMs) have shown promise in parsing unstructured text, no existing framework integrates these two disparate knowledge sources into a single, verified, and executable workflow. To bridge this gap, we propose Fault2Flow, an LLM-based multi-agent system. Fault2Flow systematically: (1) extracts and structures regulatory logic into PASTA-formatted fault trees; (2) integrates expert knowledge via a human-in-the-loop interface for verification; (3) optimizes the reasoning logic using a novel AlphaEvolve module; and (4) synthesizes the final, verified logic into an n8n-executable workflow. Experimental validation on transformer fault diagnosis datasets confirms 100\% topological consistency and high semantic fidelity. Fault2Flow establishes a reproducible path from fault analysis to operational automation, substantially reducing expert workload.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12908v1" target="_blank"><h2>DeepSport: A Multimodal Large Language Model for Comprehensive Sports Video Reasoning via Agentic Reinforcement Learning <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Junbo Zou, Haotian Xia, Zhen Ye, Shengjie Zhang, Christopher Lai, Vicente Ordonez, Weining Shen, Hanjie Chen<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Sports video understanding presents unique challenges, requiring models to perceive high-speed dynamics, comprehend complex rules, and reason over long temporal contexts. While Multimodal Large Language Models (MLLMs) have shown promise in genral domains, the current state of research in sports remains narrowly focused: existing approaches are either single-sport centric, limited to specific tasks, or rely on training-free paradigms that lack robust, learned reasoning process. To address this gap, we introduce DeepSport, the first end-to-end trained MLLM framework designed for multi-task, multi-sport video understanding. DeepSport shifts the paradigm from passive frame processing to active, iterative reasoning, empowering the model to ``think with videos'' by dynamically interrogating content via a specialized frame-extraction tool. To enable this, we propose a data distillation pipeline that synthesizes high-quality Chain-of-Thought (CoT) trajectories from 10 diverse data source, creating a unified resource of 78k training data. We then employ a two-stage training strategy, Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) with a novel gated tool-use reward, to optimize the model's reasoning process. Extensive experiments on the testing benchmark of 6.7k questions demonstrate that DeepSport achieves state-of-the-art performance, significantly outperforming baselines of both proprietary model and open-source models. Our work establishes a new foundation for domain-specific video reasoning to address the complexities of diverse sports.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.13794v1" target="_blank"><h2>FusionFM: All-in-One Multi-Modal Image Fusion with Flow Matching <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Huayi Zhu, Xiu Shu, Youqiang Xiong, Qiao Liu, Rui Chen, Di Yuan, Xiaojun Chang, Zhenyu He<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multi-modal (title, abstract)<br><p><strong><u>Abstract:</u></strong> Current multi-modal image fusion methods typically rely on task-specific models, leading to high training costs and limited scalability. While generative methods provide a unified modeling perspective, they often suffer from slow inference due to the complex sampling trajectories from noise to image. To address this, we formulate image fusion as a direct probabilistic transport from source modalities to the fused image distribution, leveraging the flow matching paradigm to improve sampling efficiency and structural consistency. To mitigate the lack of high-quality fused images for supervision, we collect fusion results from multiple state-of-the-art models as priors, and employ a task-aware selection function to select the most reliable pseudo-labels for each task. We further introduce a Fusion Refiner module that employs a divide-and-conquer strategy to systematically identify, decompose, and enhance degraded components in selected pseudo-labels. For multi-task scenarios, we integrate elastic weight consolidation and experience replay mechanisms to preserve cross-task performance and enhance continual learning ability from both parameter stability and memory retention perspectives. Our approach achieves competitive performance across diverse fusion tasks, while significantly improving sampling efficiency and maintaining a lightweight model design. The code will be available at: https://github.com/Ist-Zhy/FusionFM.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12903v1" target="_blank"><h2>Contrastive Entropy Bounds for Density and Conditional Density Decomposition <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Bo Hu, Jose C. Principe<br><strong><u>Categories:</u></strong> cs.LG, cs.AI<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> neural network (abstract)<br><p><strong><u>Abstract:</u></strong> This paper studies the interpretability of neural network features from a Bayesian Gaussian view, where optimizing a cost is reaching a probabilistic bound; learning a model approximates a density that makes the bound tight and the cost optimal, often with a Gaussian mixture density. The two examples are Mixture Density Networks (MDNs) using the bound for the marginal and autoencoders using the conditional bound. It is a known result, not only for autoencoders, that minimizing the error between inputs and outputs maximizes the dependence between inputs and the middle.
  We use Hilbert space and decomposition to address cases where a multiple-output network produces multiple centers defining a Gaussian mixture. Our first finding is that an autoencoder's objective is equivalent to maximizing the trace of a Gaussian operator, the sum of eigenvalues under bases orthonormal w.r.t. the data and model distributions. This suggests that, when a one-to-one correspondence as needed in autoencoders is unnecessary, we can instead maximize the nuclear norm of this operator, the sum of singular values, to maximize overall rank rather than trace. Thus the trace of a Gaussian operator can be used to train autoencoders, and its nuclear norm can be used as divergence to train MDNs.
  Our second test uses inner products and norms in a Hilbert space to define bounds and costs. Such bounds often have an extra norm compared to KL-based bounds, which increases sample diversity and prevents the trivial solution where a multiple-output network produces the same constant, at the cost of requiring a sample batch to estimate and optimize. We propose an encoder-mixture-decoder architecture whose decoder is multiple-output, producing multiple centers per sample, potentially tightening the bound. Assuming the data are small-variance Gaussian mixtures, this upper bound can be tracked and analyzed quantitatively.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12891v1" target="_blank"><h2>Near-infrared [P II] and [Fe II] line mapping of Galactic supernova remnants <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Takuma Kokusho, Yuki Katsurada, Yong-Hyun Lee, Bon-Chul Koo, Takahiro Nagayama, Hidehiro Kaneda, Koji S. Kawabata, Tatsuya Nakaoka, Ho-Gyu Lee, Rommy L. S. E. Aliste Castillo<br><strong><u>Categories:</u></strong> astro-ph.GA<br><strong><u>Comments:</u></strong> 14 pages, 8 figures, accepted for publication in PASJ<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> VAE (abstract)<br><p><strong><u>Abstract:</u></strong> Phosphorus (P) is one of the key ingredients for life, yet its origins in galaxies remain poorly understood. In order to investigate the production of P by supernovae, we performed near-infrared (IR) [P II] and [Fe II] line mapping of 26 Galactic supernova remnants (SNRs) with the Infrared Survey Facility and Kanata telescopes, using the narrow-band filters tuned to these lines. By combining our data with archival [Fe II] maps from UKIRT, we detected both the [P II] and [Fe II] emissions in five SNRs, only the [Fe II] emission in 15 SNRs, and no line emissions in the remaining six. Using the observed [P II]/[Fe II] ratios and upper limits for non-detections, we derived the P/Fe abundance ratios, which vary by up to two orders of magnitude among our sample SNRs. This suggests that the production rate of P and/or the degree of dust destruction may differ from remnant to remnant, the latter being due to the fact that P is volatile while Fe is mostly locked in dust grains. We used the mid- and far-IR maps to examine the dust content for the five SNRs where both the line emissions are detected. As a result, we find that high P/Fe abundance ratios in the northern and southeastern regions of Cassiopeia A and the Crab Nebula, respectively, are not likely due to dust destruction but may reflect an asymmetric ejection of P during supernova explosions. In the Crab Nebula, it is also possible that near-IR [Ni II] emission contaminates the observed flux in the southeastern region, suggesting that the Ni/Fe abundance ratio, rather than the P/Fe abundance ratio, is relatively high in this part of the remnant.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12882v2" target="_blank"><h2>Towards High-Consistency Embodied World Model with Multi-View Trajectory Videos <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Taiyi Su, Jian Zhu, Yaxuan Li, Chong Ma, Zitai Huang, Hanli Wang, Yi Xu<br><strong><u>Categories:</u></strong> cs.RO, cs.AI<br><strong><u>Comments:</u></strong> 15 pages, 23 figures<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Embodied world models aim to predict and interact with the physical world through visual observations and actions. However, existing models struggle to accurately translate low-level actions (e.g., joint positions) into precise robotic movements in predicted frames, leading to inconsistencies with real-world physical interactions. To address these limitations, we propose MTV-World, an embodied world model that introduces Multi-view Trajectory-Video control for precise visuomotor prediction. Specifically, instead of directly using low-level actions for control, we employ trajectory videos obtained through camera intrinsic and extrinsic parameters and Cartesian-space transformation as control signals. However, projecting 3D raw actions onto 2D images inevitably causes a loss of spatial information, making a single view insufficient for accurate interaction modeling. To overcome this, we introduce a multi-view framework that compensates for spatial information loss and ensures high-consistency with physical world. MTV-World forecasts future frames based on multi-view trajectory videos as input and conditioning on an initial frame per view. Furthermore, to systematically evaluate both robotic motion precision and object interaction accuracy, we develop an auto-evaluation pipeline leveraging multimodal large models and referring video object segmentation models. To measure spatial consistency, we formulate it as an object location matching problem and adopt the Jaccard Index as the evaluation metric. Extensive experiments demonstrate that MTV-World achieves precise control execution and accurate physical interaction modeling in complex dual-arm scenarios.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12874v1" target="_blank"><h2>Classification of Hope in Textual Data using Transformer-Based Models <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Chukwuebuka Fortunate Ijezue, Tania-Amanda Fredrick Eneye, Maaz Amjad<br><strong><u>Categories:</u></strong> cs.CL, cs.AI, cs.LG<br><strong><u>Comments:</u></strong> No comments<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> transformer (title, abstract)<br><p><strong><u>Abstract:</u></strong> This paper presents a transformer-based approach for classifying hope expressions in text. We developed and compared three architectures (BERT, GPT-2, and DeBERTa) for both binary classification (Hope vs. Not Hope) and multiclass categorization (five hope-related categories). Our initial BERT implementation achieved 83.65% binary and 74.87% multiclass accuracy. In the extended comparison, BERT demonstrated superior performance (84.49% binary, 72.03% multiclass accuracy) while requiring significantly fewer computational resources (443s vs. 704s training time) than newer architectures. GPT-2 showed lowest overall accuracy (79.34% binary, 71.29% multiclass), while DeBERTa achieved moderate results (80.70% binary, 71.56% multiclass) but at substantially higher computational cost (947s for multiclass training). Error analysis revealed architecture-specific strengths in detecting nuanced hope expressions, with GPT-2 excelling at sarcasm detection (92.46% recall). This study provides a framework for computational analysis of hope, with applications in mental health and social media analysis, while demonstrating that architectural suitability may outweigh model size for specialized emotion detection tasks.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12869v1" target="_blank"><h2>On the Fundamental Limits of LLMs at Scale <span style="color: #45ABC2; font-size: 0.8em; font-weight: bold;">[NEW]</span></h2></a><strong><u>Authors:</u></strong> Muhammad Ahmed Mohsin, Muhammad Umer, Ahsan Bilal, Zeeshan Memon, Muhammad Ibtsaam Qadir, Sagnik Bhattacharya, Hassan Rizwan, Abhiram R. Gorle, Maahe Zehra Kazmi, Ayesha Mohsin, Muhammad Usman Rafique, Zihao He, Pulkit Mehta, Muhammad Ali Jamshed, John M. Cioffi<br><strong><u>Categories:</u></strong> cs.LG, cs.AI, cs.DC, cs.IT, cs.MA<br><strong><u>Comments:</u></strong> Submitted to TMLR 2025<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract), attention (abstract)<br><p><strong><u>Abstract:</u></strong> Large Language Models (LLMs) have benefited enormously from scaling, yet these gains are bounded by five fundamental limitations: (1) hallucination, (2) context compression, (3) reasoning degradation, (4) retrieval fragility, and (5) multimodal misalignment. While existing surveys describe these phenomena empirically, they lack a rigorous theoretical synthesis connecting them to the foundational limits of computation, information, and learning. This work closes that gap by presenting a unified, proof-informed framework that formalizes the innate theoretical ceilings of LLM scaling. First, computability and uncomputability imply an irreducible residue of error: for any computably enumerable model family, diagonalization guarantees inputs on which some model must fail, and undecidable queries (e.g., halting-style tasks) induce infinite failure sets for all computable predictors. Second, information-theoretic and statistical constraints bound attainable accuracy even on decidable tasks, finite description length enforces compression error, and long-tail factual knowledge requires prohibitive sample complexity. Third, geometric and computational effects compress long contexts far below their nominal size due to positional under-training, encoding attenuation, and softmax crowding. We further show how likelihood-based training favors pattern completion over inference, how retrieval under token limits suffers from semantic drift and coupling noise, and how multimodal scaling inherits shallow cross-modal alignment. Across sections, we pair theorems and empirical evidence to outline where scaling helps, where it saturates, and where it cannot progress, providing both theoretical foundations and practical mitigation paths like bounded-oracle retrieval, positional curricula, and sparse or hierarchical attention.</p><br><hr><br><a href="https://arxiv.org/pdf/2511.12868v1" target="_blank"><h2>Video Finetuning Improves Reasoning Between Frames <span style="color: #999; font-size: 0.8em;">(seen before)</span></h2></a><strong><u>Authors:</u></strong> Ruiqi Yang, Tian Yun, Zihan Wang, Ellie Pavlick<br><strong><u>Categories:</u></strong> cs.CV, cs.AI<br><strong><u>Comments:</u></strong> Accepted at CogInterp @ NeurIPS 2025<br><strong><u>Published:</u></strong> 2025-11-17<br><strong><u>Matching Keywords:</u></strong> multimodal (abstract)<br><p><strong><u>Abstract:</u></strong> Multimodal large language models (LLMs) have made rapid progress in visual understanding, yet their extension from images to videos often reduces to a naive concatenation of frame tokens. In this work, we investigate what video finetuning brings to multimodal LLMs. We propose Visual Chain-of-Thought (vCoT), an explicit reasoning process that generates transitional event descriptions between consecutive frames. Using vCoT, we systematically compare image-only LVLMs with their video-finetuned counterparts, both with and without access to these transitional cues. Our experiments show that vCoT significantly improves the performance of image-only models on long-form video question answering, while yielding only marginal gains for video-finetuned models. This suggests that the latter already capture frame-to-frame transitions implicitly. Moreover, we find that video models transfer this temporal reasoning ability to purely static settings, outperforming image models' baselines on relational visual reasoning tasks.</p><br><hr><br><hr><p><em>Summary: Showing 468 papers (258 new, 210 seen before)</em></p></body></html>